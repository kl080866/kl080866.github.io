<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.1">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.1" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.1.1',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Pandas的数据结构Series:Series数据的建立 -&amp;gt;In [1]: import numpy as np In [2]: import pandas as pd In [3]: pddata_1=pd.Series([1,2,3]) In [4]: pddata_1Out[4]:0 11 22 3dtype: int64数据取值和获取搜索索引In [7]: pddata_1.va">
<meta name="keywords" content="数据分析">
<meta property="og:type" content="article">
<meta property="og:title" content="pandas和绘图课件">
<meta property="og:url" content="http://kl66.top/2019/06/11/pandas和绘图课件/index.html">
<meta property="og:site_name" content="Mr kuai">
<meta property="og:description" content="Pandas的数据结构Series:Series数据的建立 -&amp;gt;In [1]: import numpy as np In [2]: import pandas as pd In [3]: pddata_1=pd.Series([1,2,3]) In [4]: pddata_1Out[4]:0 11 22 3dtype: int64数据取值和获取搜索索引In [7]: pddata_1.va">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2019-06-11T07:34:31.906Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="pandas和绘图课件">
<meta name="twitter:description" content="Pandas的数据结构Series:Series数据的建立 -&amp;gt;In [1]: import numpy as np In [2]: import pandas as pd In [3]: pddata_1=pd.Series([1,2,3]) In [4]: pddata_1Out[4]:0 11 22 3dtype: int64数据取值和获取搜索索引In [7]: pddata_1.va">





  
  
  <link rel="canonical" href="http://kl66.top/2019/06/11/pandas和绘图课件/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>pandas和绘图课件 | Mr kuai</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Mr kuai</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">追忆似水流年</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-meh-o"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-legal"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-ravelry"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-snowflake-o"></i> <br>归档</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://kl66.top/2019/06/11/pandas和绘图课件/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="kl">
      <meta itemprop="description" content="66其实不太6">
      <meta itemprop="image" content="/images/head.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mr kuai">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">pandas和绘图课件

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-11 15:31:11 / 修改时间：15:34:31" itemprop="dateCreated datePublished" datetime="2019-06-11T15:31:11+08:00">2019-06-11</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon">
            <i class="fa fa-eye"></i>
             阅读次数： 
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="Pandas的数据结构"><a href="#Pandas的数据结构" class="headerlink" title="Pandas的数据结构"></a>Pandas的数据结构</h2><h4 id="Series"><a href="#Series" class="headerlink" title="Series:"></a>Series:</h4><p>Series数据的建立 -&gt;<br>In [1]: import numpy as np</p>
<p>In [2]: import pandas as pd</p>
<p>In [3]: pddata_1=pd.Series([1,2,3])</p>
<p>In [4]: pddata_1<br>Out[4]:<br>0 1<br>1 2<br>2 3<br>dtype: int64<br>数据取值和获取搜索索引<br>In [7]: pddata_1.values<br>Out[7]: array([1, 2, 3], dtype=int64)<br>In [8]: pddata_1.index<br>Out[8]: RangeIndex(start=0, stop=3, step=1)</p>
<p>例子：</p>
<p>In [9]: pddata_2=pd.Series([‘this’,’is’,’pandas’])</p>
<p>In [10]: pddata_2.values<br>Out[10]: array([‘this’, ‘is’, ‘pandas’], dtype=object)</p>
<p>In [11]: pddata_2.index<br>Out[11]: RangeIndex(start=0, stop=3, step=1)</p>
<p>带有独特索引的Series数据：</p>
<p>In [15]: pddata_3=pd.Series([344,33,-456],index=[‘andereas’,’hachenberger’,’dieter’])<br>In [16]: pddata_3<br>Out[16]:<br>andereas 344<br>hachenberger 33<br>dieter -456<br>dtype: int64<br>In [17]: pddata_3.index,pddata_3.values<br>Out[17]:<br>(Index([‘andereas’, ‘hachenberger’, ‘dieter’], dtype=’object’),<br>array([ 344, 33, -456], dtype=int64))</p>
<p>例子：<br>In [21]: testa=[245,788,90]<br>In [22]: pddata_4=pd.Series(testa,index=[‘wir’,’sind’,’Menschen’])<br>In [23]: pddata_4<br>Out[23]:<br>wir 245<br>sind 788<br>Menschen 90<br>dtype: int64<br>In [25]: pddata_4[‘wir’]<br>Out[25]: 245<br>In [27]: pddata_4[[‘wir’,’sind’,’Menschen’]]<br>Out[27]:<br>wir 245<br>sind 788<br>Menschen 90<br>dtype: int64</p>
<p>例子：<br>In [31]: seriesdata_1=pd.Series([‘Profession’,’student’,’bechelor’,’professor’,’informatics developer’],index=[‘Name’,’Linker’,’Hamacher’,’Schuhmacher’,’Heintze’])</p>
<p>In [32]: seriesdata_1<br>Out[32]:<br>Name Profession<br>Linker student<br>Hamacher bechelor<br>Schuhmacher professor<br>Heintze informatics developer<br>dtype: object</p>
<p>Series数据中的过滤、计算。</p>
<p>In [33]: seriesdata_2=pd.Series([34,-4,-45,-37,32,9,1,3],index=[‘a’,’d’,’e’,’y’,’f’,’t’,’u’,’o’])</p>
<p>In [34]: seriesdata_2<br>Out[34]:<br>a 34<br>d -4<br>e -45<br>y -37<br>f 32<br>t 9<br>u 1<br>o 3<br>dtype: int64</p>
<p>In [35]: seriesdata_2[seriesdata_2&gt;1]<br>Out[35]:<br>a 34<br>f 32<br>t 9<br>o 3<br>dtype: int64</p>
<p>In [36]: seriesdata_2[seriesdata_2&gt;0]<br>Out[36]:<br>a 34<br>f 32<br>t 9<br>u 1<br>o 3<br>dtype: int64</p>
<p>In [37]: seriesdata_2*3<br>Out[37]:<br>a 102<br>d -12<br>e -135<br>y -111<br>f 96<br>t 27<br>u 3<br>o 9<br>dtype: int64</p>
<p>In [38]: np.sin(seriesdata_2)<br>Out[38]:<br>a 0.529083<br>d 0.756802<br>e -0.850904<br>y 0.643538<br>f 0.551427<br>t 0.412118<br>u 0.841471<br>o 0.141120<br>dtype: float64</p>
<p>Serie 和字典十分相似，因此原本字典的函数也可以用：</p>
<p>我们先复习下字典：<br>例子：</p>
<p>In [39]: dic={‘Winne’:178,’Johanis’:189,’Banach’:186}</p>
<p>In [40]: dic[‘Li’]=176</p>
<p>In [41]: dic<br>Out[41]: {‘Banach’: 186, ‘Johanis’: 189, ‘Li’: 176, ‘Winne’: 178}</p>
<p>例子：<br>In [19]: list11=[‘zhang’,’wang’,’li’]</p>
<p>In [20]: list22=range(3)</p>
<p>In [21]: indirect=zip(list22,list11)</p>
<p>In [22]: diction={index:value for index,value in indirect}</p>
<p>In [23]: diction<br>Out[23]: {0: ‘zhang’, 1: ‘wang’, 2: ‘li’}</p>
<p>例3：</p>
<p>In [26]: list11=[‘a’,’b’,’c’]</p>
<p>In [27]: list22=[1,2,3]</p>
<p>In [28]: dfgeg=zip(list11,list22)</p>
<p>In [29]: sss=dict((index,value) for index,value in dfgeg)</p>
<p>In [30]: sss<br>Out[30]: {‘a’: 1, ‘b’: 2, ‘c’: 3}</p>
<p>应用字典函数“in”和“Update”到Series</p>
<p>例子<br>In [34]: datas_pys=pd.Series(range(4),index=[‘i’,’want’,’to’,’do’])</p>
<p>In [35]: datas_pys<br>Out[35]:<br>i 0<br>want 1<br>to 2<br>do 3<br>dtype: int32</p>
<p>In [36]: ‘want’ in datas_pys<br>Out[36]: True<br>例子：<br>In [56]: datas_pys.update(pd.Series([2,3,4],index=[‘want’,’to’,’do’]))</p>
<p>In [57]: datas_pys<br>Out[57]:<br>i 0<br>want 2<br>to 3<br>do 4<br>dtype: int32</p>
<p>In [58]: s1 = pd.Series([1, 2, 3])</p>
<p>In [59]: s2 = pd.Series([4, 5, 6])</p>
<p>In [60]: s3 = pd.Series([4, 5, 6], index=[3,4,5])</p>
<p>In [61]: s1.append(s2)<br>Out[61]:<br>0 1<br>1 2<br>2 3<br>0 4<br>1 5<br>2 6<br>dtype: int64</p>
<p>In [62]: s1.append(s3)<br>Out[62]:<br>0 1<br>1 2<br>2 3<br>3 4<br>4 5<br>5 6<br>dtype: int64</p>
<p>In [63]: s1.append(s2, ignore_index=True)<br>Out[63]:<br>0 1<br>1 2<br>2 3<br>3 4<br>4 5<br>5 6<br>dtype: int64<br>重新编号</p>
<p>字典可以直接转化为Serie</p>
<p>In [68]: dicttna={1:’foo’,3:’drt’,8:’tyue’}</p>
<p>In [69]: serie_12=pd.Series(dicttna)</p>
<p>In [70]: serie_12<br>Out[70]:<br>1 foo<br>3 drt<br>8 tyue<br>dtype: object</p>
<p>isnull和notnull函数可用于检测数据缺失。</p>
<p>In [79]: dit_113={‘lin’:139,’zhang’:134,’wang’:173,’tan’:None}</p>
<p>In [80]: serie_123=pd.Series(dit_113)</p>
<p>In [81]: serie_123<br>Out[81]:<br>lin 139.0<br>tan NaN<br>wang 173.0<br>zhang 134.0<br>dtype: float64</p>
<p>In [82]: pd.isnull(serie_123)<br>Out[82]:<br>lin False<br>tan True<br>wang False<br>zhang False<br>dtype: bool</p>
<p>In [83]: pd.notnull(serie_123)<br>Out[83]:<br>lin True<br>tan False<br>wang True<br>zhang True<br>dtype: bool</p>
<p>serie_123.notnull()<br>Out[85]:<br>lin       True<br>tan      False<br>wang      True<br>zhang     True<br>dtype: bool</p>
<p>Series 的索引可以修改：<br>In [87]: serie_123.index<br>Out[87]: Index([‘lin’, ‘tan’, ‘wang’, ‘zhang’], dtype=’object’)<br>In [88]: serie_123.index=[‘lin’, ‘tan’, ‘shan’, ‘zhang’]<br>In [90]: serie_123<br>Out[90]:<br>lin 139.0<br>tan NaN<br>shan 173.0<br>zhang 134.0<br>dtype: float64</p>
<h4 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h4><p>简单数据框的构成：<br>由字典直接形成</p>
<p>In [93]: sales=[12365,34563,45673,23461,89034]</p>
<p>In [94]: seller=[‘zhanghui’,’dongyibo’,’yangqian’,’liujuntao’,’zhangshanshan’]</p>
<p>In [95]: sales_quantity=[213,305,452,302,190]</p>
<p>In [96]: table_sales={‘Seller’:seller,’Sales’:sales,’SalesQuantity’:sales_quantity}</p>
<p>In [97]: framesample_1=pd.DataFrame(table_sales)</p>
<p>In [98]: framesample_1<br>Out[98]:<br>Sales SalesQuantity Seller<br>0 12365 213 zhanghui<br>1 34563 305 dongyibo<br>2 45673 452 yangqian<br>3 23461 302 liujuntao<br>4 89034 190 zhangshanshan</p>
<p>与Series一样，索引会自动加上。</p>
<p>我们还可以指定列序列的左右顺序。</p>
<p>In [99]: framesample_1=pd.DataFrame(table_sales,columns=[‘SalesQuantity’,’Seller’,’Sales’])</p>
<p>In [100]: framesample_1<br>Out[100]:<br>SalesQuantity Seller Sales<br>0 213 zhanghui 12365<br>1 305 dongyibo 34563<br>2 452 yangqian 45673<br>3 302 liujuntao 23461<br>4 190 zhangshanshan 89034</p>
<p>如果数据框中不包含所要找的值，自动返回NaN</p>
<p>In [101]: framesample_1=pd.DataFrame(table_sales,columns=[‘SalesQuantity’,’Seller’,’Sales’,’Profit’])</p>
<p>In [102]: framesample_1<br>Out[102]:<br>SalesQuantity Seller Sales Profit<br>0 213 zhanghui 12365 NaN<br>1 305 dongyibo 34563 NaN<br>2 452 yangqian 45673 NaN<br>3 302 liujuntao 23461 NaN<br>4 190 zhangshanshan 89034 NaN</p>
<p>索引DataFrame 的数据。类似字典</p>
<p>列数据索引<br>In [104]: framesample_1[‘Seller’]<br>Out[104]:<br>0 zhanghui<br>1 dongyibo<br>2 yangqian<br>3 liujuntao<br>4 zhangshanshan<br>Name: Seller, dtype: object</p>
<p>行数据索引：</p>
<p>In [106]: framesample_1.ix[2]<br>Out[106]:<br>SalesQuantity 452<br>Seller yangqian<br>Sales 45673<br>Prodit NaN<br>Name: 2, dtype: object</p>
<p>Dateframe的索引也可以改变。</p>
<p>In [108]: framesample_1.index=[5,6,7,8,9]</p>
<p>In [109]: framesample_1<br>Out[109]:<br>SalesQuantity Seller Sales Prodit<br>5 213 zhanghui 12365 NaN<br>6 305 dongyibo 34563 NaN<br>7 452 yangqian 45673 NaN<br>8 302 liujuntao 23461 NaN<br>9 190 zhangshanshan 89034 NaN</p>
<p>NAN值可以通过赋值语句替换。<br>In [13]: framesample_1[‘Profit’]=[278,967,654,234,432]<br>In [14]: framesample_1<br>Out[14]:<br>SalesQuantity Seller Sales Profit<br>5 213 zhanghui 12365 278<br>6 305 dongyibo 34563 967<br>7 452 yangqian 45673 654<br>8 302 liujuntao 23461 234<br>9 190 zhangshanshan 89034 432</p>
<p>当用Series赋值时，可以精确地赋值到行列交叉位，没有指定的行列交叉位不会被赋值，将会以NAN的形式显示。<br>In [17]: val_Series=pd.Series([0,1,1],index=[6,7,9])<br>In [19]: framesample_1[‘Profit’]=val_Series<br>In [20]: framesample_1<br>Out[20]:<br>SalesQuantity Seller Sales Profit<br>5 213 zhanghui 12365 NaN<br>6 305 dongyibo 34563 0.0<br>7 452 yangqian 45673 1.0<br>8 302 liujuntao 23461 NaN<br>9 190 zhangshanshan 89034 1.0</p>
<p>为不存在的列赋值会产生新的列。<br>In [21]: framesample_1[‘loss’]=pd.Series([165,0,0,34,0],index=range(5,10,1))<br>In [22]: framesample_1<br>Out[22]:<br>SalesQuantity Seller Sales Profit loss<br>5 213 zhanghui 12365 NaN 165<br>6 305 dongyibo 34563 0.0 0<br>7 452 yangqian 45673 1.0 0<br>8 302 liujuntao 23461 NaN 34<br>9 190 zhangshanshan 89034 1.0 0</p>
<p>可用del删除列</p>
<p>In [25]: del framesample_1[‘Profit’]</p>
<p>In [26]: framesample_1<br>Out[26]:<br>SalesQuantity Seller Sales loss<br>5 213 zhanghui 12365 165<br>6 305 dongyibo 34563 0<br>7 452 yangqian 45673 0<br>8 302 liujuntao 23461 34<br>9 190 zhangshanshan 89034 0</p>
<p>嵌套字典：<br>In [28]: Qtditc={‘TeacherLiu’:{‘height’:172,’weight’:67,’age’:34},’TeacherHuang’:{‘height’:182,’weight’:77,’age’:36},’TeacherTao’:{‘height’:192,’weight’:98,’age’:56} }</p>
<p>In [29]: Qtditc<br>Out[29]:<br>{‘TeacherHuang’: {‘age’: 36, ‘height’: 182, ‘weight’: 77},<br>‘TeacherLiu’: {‘age’: 34, ‘height’: 172, ‘weight’: 67},<br>‘TeacherTao’: {‘age’: 56, ‘height’: 192, ‘weight’: 98}}</p>
<p>如果把嵌套字典传给数据框，构造数据框，外键为列，内键为行索引。<br>In [30]: GetNewFrame=pd.DataFrame(Qtditc)</p>
<p>In [31]: GetNewFrame<br>Out[31]:<br>TeacherHuang TeacherLiu TeacherTao<br>age 36 34 56<br>height 182 172 192<br>weight 77 67 98</p>
<p>数据框的转置 </p>
<p>In [32]: GetNewFrame.T<br>Out[32]:<br>age height weight<br>TeacherHuang 36 182 77<br>TeacherLiu 34 172 67<br>TeacherTao 56 192 98</p>
<p>Series组成的字典也可以直接转化为数据框：</p>
<p>In [33]: Popdic={‘LinFeng’:pd.Series([23,45,165],index=[‘age’,’weight’,’height’]),’Zhangduoli’:pd.Series([43,75,175],index=[‘age’,’weight’,’height’]),’JinChang’:pd.Series([51,46,185],index=[‘age’,’weight’,’height’])}</p>
<p>In [35]: Getnumerpop=pd.DataFrame(Popdic)</p>
<p>In [36]: Getnumerpop<br>Out[36]:<br>JinChang LinFeng Zhangduoli<br>age 51 23 43<br>weight 46 45 75<br>height 185 165 175</p>
<p>数据框名字的添加：</p>
<p>In [45]: Getnumerpop.columns.name=’Name’</p>
<p>In [46]: Getnumerpop.index.name=’personal information’</p>
<p>In [47]: Getnumerpop<br>Out[47]:<br>Name              JinChang LinFeng Zhangduoli<br>personal information<br>age                    51     23       43<br>weight                 46     45       75<br>height                 185    165      175</p>
<p>数据框的值：<br>In [53]: Getnumerpop.values<br>Out[53]:<br>array([[ 51, 23, 43],<br>[ 46, 45, 75],<br>[185, 165, 175]], dtype=int64)</p>
<p>数据框的index：</p>
<p>例1：</p>
<p>Getnumerpop.index=[‘p_age’,’p_weight’,’p_height’]</p>
<p>Getnumerpop.index<br>Out[57]: Index([‘p_age’, ‘p_weight’, ‘p_height’], dtype=’object’)</p>
<p>Getnumerpop<br>Out[58]:<br>Name      JinChang  LinFeng  Zhangduoli<br>p_age           51       23          43<br>p_weight        46       45          75<br>p_height       185      165         175</p>
<p>In [64]:index1=Getnumerpop.index<br>In [65]:index2=GetNewFrame.index</p>
<p>In [66]:index1.append(index2)<br>Out[66]: Index([‘p_age’, ‘p_weight’, ‘p_height’, ‘age’, ‘height’, ‘weight’], dtype=’object’)</p>
<p>例2<br>index2.intersection(index1)<br>Out[82]: Index([], dtype=’object’)</p>
<p>例3：<br>index2.union(index1)<br>Out[86]: Index([‘age’, ‘height’, ‘p_age’, ‘p_height’, ‘p_weight’, ‘weight’], dtype=’object’)</p>
<p>例4<br>index1.delete(1)<br>Out[94]: Index([‘p_age’, ‘p_height’], dtype=’object’)<br>22<br>例5<br>index1.insert(1,’pheight’)<br>Out[97]: Index([‘p_age’, ‘pheight’, ‘p_weight’, ‘p_height’], dtype=’object’)<br>index1.is_monotonic<br>Out[104]: False<br>index1.is_unique<br>Out[105]: True<br>index1.unique()<br>Out[107]: Index([‘p_age’, ‘p_weight’, ‘p_height’], dtype=’object’)<br>Series索引重建<br>In [2]: import numpy as np<br>In [3]: import pandas as pd<br>In [4]: Seriestest_1=pd.Series([10,-34,-89,36,50],index=[‘a’,’b’,’c’,’d’,’e’])</p>
<p>In [5]: Seriestest_1<br>Out[5]:<br>a 10<br>b -34<br>c -89<br>d 36<br>e 50<br>dtype: int64</p>
<p>In [7]: Seriestest_2=Seriestest_1.reindex([‘c’,’b’,’d’,’a’,’e’,’f’])</p>
<p>In [8]: Seriestest_2<br>Out[8]:<br>c -89.0<br>b -34.0<br>d 36.0<br>a 10.0<br>e 50.0<br>f NaN<br>dtype: float64</p>
<p>NAN值可以被替换</p>
<p>In [10]: Seriestest_2=Seriestest_1.reindex([‘c’,’b’,’d’,’a’,’e’,’f’],fill_value=0)</p>
<p>In [11]: Seriestest_2<br>Out[11]:<br>c -89<br>b -34<br>d 36<br>a 10<br>e 50<br>f 0<br>dtype: int64</p>
<p>使用ffill 和 bfill函数可以自动向前和向后补充缺失索引。（缺失值补充）<br>In [6]: testarray1=pd.Series([23,22,34],index=[3,7,9])<br>In [10]: test_12=testarray1.reindex(range(11),method=’ffill’)</p>
<p>In [11]: test_12<br>Out[11]:<br>0 NaN<br>1 NaN<br>2 NaN<br>3 23.0<br>4 23.0<br>5 23.0<br>6 23.0<br>7 22.0<br>8 22.0<br>9 34.0<br>10 34.0<br>dtype: float64</p>
<p>In [12]: test_12=testarray1.reindex(range(11),method=’bfill’)</p>
<p>In [13]: test_12<br>Out[13]:<br>0 23.0<br>1 23.0<br>2 23.0<br>3 23.0<br>4 22.0<br>5 22.0<br>6 22.0<br>7 22.0<br>8 34.0<br>9 34.0<br>10 NaN<br>dtype: float64</p>
<p>数据框的索引修改：（reindex可以任意删除，添加，交换行列）</p>
<p>对于数据框，reindex可以修改行索引和列，或两个都更改。</p>
<p>In [14]: #这里我用一个新的方法构造数据框<br>In [18]: frame_1=pd.DataFrame(np.arange(9).reshape(3,3),index=[‘row1’,’row2’,’row3’],columns=[‘one’,’two’,’three’])</p>
<p>In [19]: frame_1<br>Out[19]:<br>one two three<br>row1 0 1 2<br>row2 3 4 5<br>row3 6 7 8</p>
<p>In [20]: frame2=frame_1.reindex([‘row0’,’row1’,’row2’,’row3’])</p>
<p>In [21]: frame2<br>Out[21]:<br>one two three<br>row0 NaN NaN NaN<br>row1 0.0 1.0 2.0<br>row2 3.0 4.0 5.0<br>row3 6.0 7.0 8.0</p>
<p>In [22]: #reindex 修改列<br>In [28]: frame3=frame2.reindex(columns=[‘four’,’three’])</p>
<p>In [29]: frame3<br>Out[29]:<br>four three<br>row0 NaN NaN<br>row1 NaN 2.0<br>row2 NaN 5.0<br>row3 NaN 8.0</p>
<p>In [30]: #同时修改列和行索引</p>
<p>In [31]: frame4=frame_1.reindex([‘row1’,’row2’,’row3’,’row4’],columns=[‘five’,’three’,’six’])</p>
<p>In [32]: frame_1<br>Out[32]:<br>one two three<br>row1 0 1 2<br>row2 3 4 5<br>row3 6 7 8</p>
<p>In [33]: frame4<br>Out[33]:<br>five three six<br>row1 NaN 2.0 NaN<br>row2 NaN 5.0 NaN<br>row3 NaN 8.0 NaN<br>row4 NaN NaN NaN</p>
<p>利用ix函数修改数据框行索引与列，快捷整洁，节约时间。但是Ix已经几乎被启用，现在用loc函数。<br>In [35]: frame5=frame_1.reindex(columns=[‘one’,’four’,’two’,’three’])</p>
<p>In [36]: frame5<br>Out[36]:<br>one four two three<br>row1 0 NaN 1 2<br>row2 3 NaN 4 5<br>row3 6 NaN 7 8</p>
<p>In [40]: frame5[‘four’]=pd.Series([34,56,78],index=[‘row1’,’row2’,’row3’])</p>
<p>In [41]: frame5<br>Out[41]:<br>one four two three<br>row1 0 34 1 2<br>row2 3 56 4 5<br>row3 6 78 7 8<br>In [42]: frame5.ix[[‘row2’,’row1’,’row3’],[‘one’,’two’,’four’,’three’]]<br>C:\Users\dongfeng\Anaconda3\lib\site-packages\ipykernel_launcher.py:1: DeprecationWarning:<br>.ix is deprecated. Please use<br>.loc for label based indexing or<br>.iloc for positional indexing</p>
<p>See the documentation here:<br><a href="http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated" target="_blank" rel="noopener">http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated</a><br>“””Entry point for launching an IPython kernel.<br>Out[42]:<br>one two four three<br>row2 3 4 56 5<br>row1 0 1 34 2<br>row3 6 7 78 8</p>
<p>#现在用loc的比较多，ix已经不再推荐使用。</p>
<p>In [44]: frame5.loc[[‘row2’,’row1’,’row3’],[‘one’,’two’,’four’,’three’]]<br>Out[44]:<br>one two four three<br>row2 3 4 56 5<br>row1 0 1 34 2<br>row3 6 7 78 8</p>
<p>指定轴上项目的丢弃：</p>
<p>Drop 函数应用到“Series”<br>In [46]: Seriestest_1=pd.Series([233,356,997],index=[‘as’,’a’,’sample’])<br>In [47]: Seriestest_1<br>Out[47]:<br>as 233<br>a 356<br>sample 997<br>dtype: int64</p>
<p>Seriestest_1.drop(‘a’)<br>Out[95]:<br>as        233<br>sample    997<br>dtype: int64</p>
<p>In [48]: #drop函数或者说方法返回的是一个在指定轴上删除指定值的新对象</p>
<p>Drop函数应用到“DataFrame”<br>Drop函数删除指定值时必须指定轴，如果不指定轴，默认零轴。也就是说删除“1”轴上的指定项时必须指定轴号。删除“0”轴上的指定项时不需要指定轴号。<br>In [53]: Dataframe_1_test=pd.DataFrame(np.floor(np.random.randn(4,4)),index=[‘xu’,’liu’,’zhang’,’feng’],columns=[‘stufe’,’klasse’,’degree’,’group’])</p>
<p>In [54]: Dataframe_1_test<br>Out[54]:<br>stufe klasse degree group<br>xu -1.0 0.0 -1.0 1.0<br>liu 0.0 -2.0 -1.0 -1.0<br>zhang 0.0 -3.0 -2.0 1.0<br>feng -1.0 0.0 -1.0 0.0</p>
<p>In [57]: Dataframe_1_test.drop([‘stufe’,’klasse’],axis=1)<br>Out[57]:<br>degree group<br>xu -1.0 1.0<br>liu -1.0 -1.0<br>zhang -2.0 1.0<br>feng -1.0 0.0</p>
<p>In [59]: Dataframe_1_test.drop([‘xu’,’feng’])<br>Out[59]:<br>stufe klasse degree group<br>liu 0.0 -2.0 -1.0 -1.0<br>zhang 0.0 -3.0 -2.0 1.0</p>
<p>In [60]: Dataframe_1_test.drop([‘liu’])<br>Out[60]:<br>stufe klasse degree group<br>xu -1.0 0.0 -1.0 1.0<br>zhang 0.0 -3.0 -2.0 1.0<br>feng -1.0 0.0 -1.0 0.0</p>
<p>索引与选取</p>
<p>Series的索引和字典没有太大不同,唯一的区别是Series不仅可以通过设置的索引进行检索，也可以用默认的索引进行检索。另外Series也可以做切片，下面我们先看几个例子：<br>In [6]: Testser_1=pd.Series([23,43,789,674,90,65],index=[‘apple’,’pear’,’persimmon’,’watermelon’,’strawberry’,’orange’])</p>
<p>In [7]: Testser_1<br>Out[7]:<br>apple 23<br>pear 43<br>persimmon 789<br>watermelon 674<br>strawberry 90<br>orange 65<br>dtype: int64</p>
<p>#按默认索引检索<br>In [9]: Testser_1[2:4]<br>Out[9]:<br>persimmon 789<br>watermelon 674<br>dtype: int64</p>
<p>#按给定索引检索<br>In [25]: Testser_1[‘apple’:’watermelon’]<br>Out[25]:<br>apple 23<br>pear 43<br>persimmon 789<br>watermelon 674<br>dtype: int64</p>
<p>#注意，按给定标签（也叫索引）检索切片末端不会减一末端是被包含的，这就是说末端是封闭区间。（是’]’，不是‘）’）<br>In [26]: Testser_1[[‘apple’,’watermelon’]]<br>Out[26]:<br>apple 23<br>watermelon 674<br>dtype: int64</p>
<p>#注意上面两个例子的区别</p>
<p>#也可单个索引，按默认和按给定索引（也叫标签）均可检索<br>In [27]: Testser_1[‘apple’]<br>Out[27]: 23</p>
<p>In [28]: Testser_1[0]<br>Out[28]: 23</p>
<p>#也可以按照bool值进行索引，此时检索不再按照给定的默认索引，而是与Series中的数据进行比较。</p>
<p>In [30]: Testser_1[Testser_1&gt;100]<br>Out[30]:<br>persimmon 789<br>watermelon 674<br>dtype: int64</p>
<p>给Series赋值：<br>In [31]: Testser_1[‘apple’:’watermelon’]=[112,22,32,42]</p>
<p>In [32]: Testser_1<br>Out[32]:<br>apple 112<br>pear 22<br>persimmon 32<br>watermelon 42<br>strawberry 90<br>orange 65<br>dtype: int64<br>In [34]: Testser_1[‘apple’:’watermelon’]=56</p>
<p>In [35]: Testser_1<br>Out[35]:<br>apple 56<br>pear 56<br>persimmon 56<br>watermelon 56<br>strawberry 90<br>orange 65<br>dtype: int64<br>例子：<br>In [37]: Testser_1[[2,3]]=34</p>
<p>In [38]: Testser_1<br>Out[38]:<br>apple 56<br>pear 56<br>persimmon 34<br>watermelon 34<br>strawberry 90<br>orange 65<br>dtype: int64</p>
<p>In [39]: Testser_1[‘apple’]=34</p>
<p>In [40]: Testser_1<br>Out[40]:<br>apple 34<br>pear 56<br>persimmon 34<br>watermelon 34<br>strawberry 90<br>orange 65<br>dtype: int64</p>
<p>DataFrame的索引</p>
<p>In [5]: Data=pd.DataFrame(np.arange(16).reshape(4,4),index=(‘one’,’two’,’three’,’four’),columns=(‘wir’,’sie’,’ihr’,’ich’))</p>
<p>In [6]: Data<br>Out[6]:<br>wir sie ihr ich<br>one 0 1 2 3<br>two 4 5 6 7<br>three 8 9 10 11<br>four 12 13 14 15</p>
<p>In [7]: Data[‘wir’]<br>Out[7]:<br>one 0<br>two 4<br>three 8<br>four 12<br>Name: wir, dtype: int32</p>
<p>#单个索引时只能索引列，因此用“bool”值来检索时只能先检索出列数据，然后再与设定值比较，得出bool值Series，见下例：<br>In [22]: Data[Data[‘ihr’]&gt;6]<br>Out[22]:<br>wir sie ihr ich<br>three 8 9 10 11<br>four 12 13 14 15</p>
<p>Bool Series是可以当做索引的，见下<br>Data[pd.Series([True,False,True,False],index=[‘one’,’two’,’three’,’four’])]<br>Out[44]:<br>       wir  sie  ihr  ich<br>one      0    1    2    3<br>three    8    9   10   11<br>但这种索引容易出错，往往结果与我们的目的不一致，比如：<br>ata=pd.DataFrame([[1,2,3,4],[0,4,8,2],[0.3,0.6,2.3,4],[3,8,9,0]],index=(‘one’,’two’,’three’,’four’),columns=(‘wir’,’sie’,’ihr’,’ich’))</p>
<p>ata<br>Out[54]:<br>       wir  sie  ihr  ich<br>one    1.0  2.0  3.0    4<br>two    0.0  4.0  8.0    2<br>three  0.3  0.6  2.3    4<br>four   3.0  8.0  9.0    0</p>
<p>ata[ata[‘ihr’]&gt;3]<br>Out[55]:<br>      wir  sie  ihr  ich<br>two   0.0  4.0  8.0    2<br>four  3.0  8.0  9.0    0</p>
<p>我们想选出大于‘3’，而结果并非如此。</p>
<p>In [8]: Data[[‘wir’,’ihr’]]<br>Out[8]:<br>wir ihr<br>one 0 2<br>two 4 6<br>three 8 10<br>four 12 14<br>In [13]: #DataFrame 切片只能依靠行索引</p>
<p>In [16]: Data[‘one’:’three’]<br>Out[16]:<br>wir sie ihr ich<br>one 0 1 2 3<br>two 4 5 6 7<br>three 8 9 10 11</p>
<p>In [17]: Data[1:3]<br>Out[17]:<br>wir sie ihr ich<br>two 4 5 6 7<br>three 8 9 10 11</p>
<p>bool值索引：<br>In [23]: Data&gt;8<br>Out[23]:<br>wir sie ihr ich<br>one False False False False<br>two False False False False<br>three False True True True<br>four True True True True<br>In [25]: Data[Data&gt;8]=0</p>
<p>In [26]: Data<br>Out[26]:<br>wir sie ihr ich<br>one 0 1 2 3<br>two 4 5 6 7<br>three 8 0 0 0<br>four 0 0 0 0<br>上面我们已经讲过单个索引时我们只能索引列，为了解决这个问题numpy创造了ix方法。但是ix函数或许太老了，目前已经有新的函数loc来替代它。通过这个函数你几乎可以为所欲为。loc函数只能用给定行列索引进行检索</p>
<p>In [31]: Data.loc[‘two’]<br>Out[31]:<br>wir 4<br>sie 5<br>ihr 6<br>ich 7<br>Name: two, dtype: int32<br>In [33]: Data.loc[‘four’,’wir’:’ihr’]<br>Out[33]:<br>wir 12<br>sie 13<br>ihr 14<br>Name: four, dtype: int32</p>
<p>In [34]: Data.loc[‘four’,[‘wir’,’ihr’]]<br>Out[34]:<br>wir 12<br>ihr 14<br>Name: four, dtype: int32</p>
<p>Data.loc[[‘one’,’four’],[‘wir’,’ich’]]<br>Out[136]:<br>      wir  ich<br>one     0    3<br>four   12   15</p>
<p>In [35]: Data.loc[‘one’:’three’,’ich’]<br>Out[35]:<br>one 3<br>two 7<br>three 11<br>Name: ich, dtype: int32<br>In [38]: Data.loc[‘one’:,’wir’]<br>Out[38]:<br>one 0<br>two 4<br>three 8<br>four 12<br>Name: wir, dtype: int32</p>
<p>bool值索引<br>In [45]: Data.loc[Data.wir&gt;5,:’ich’]<br>Out[45]:<br>wir sie ihr ich<br>three 8 9 10 11<br>four 12 13 14 15</p>
<p>Reindex 方法：</p>
<p>In [50]: Data<br>Out[50]:<br>wir sie ihr ich<br>one 0 1 2 3<br>two 4 5 6 7<br>three 8 9 10 11<br>four 12 13 14 15</p>
<p>In [51]: Data.reindex(index=[‘one’,’three’,’four’,’two’])<br>Out[51]:<br>wir sie ihr ich<br>one 0 1 2 3<br>three 8 9 10 11<br>four 12 13 14 15<br>two 4 5 6 7</p>
<p>In [52]: Data.reindex(columns=[‘wir’,’ihr’,’sie’,’ich’])<br>Out[52]:<br>wir ihr sie ich<br>one 0 2 1 3<br>two 4 6 5 7<br>three 8 10 9 11<br>four 12 14 13 15</p>
<p>Xs方法：根据标签选取单行和单列，返回‘Series’</p>
<p>In [66]: Data.xs(‘one’)<br>Out[66]:<br>wir 0<br>sie 1<br>ihr 2<br>ich 3<br>Name: one, dtype: int32</p>
<p>In [69]: Data.xs(‘sie’,axis=1)<br>Out[69]:<br>one 1<br>two 5<br>three 9<br>four 13<br>Name: sie, dtype: int32</p>
<p>Get_value 方法得到数据框内单个值：</p>
<p>Data.get_value(‘three’,’ihr’)<br>Out[72]: 10</p>
<p>简单的算数运算和数据对齐</p>
<p>加法<br>Series数据相加必须按照索引一一相加，如果索引不能配对，将返回空值。<br>In [1]: import numpy as np</p>
<p>In [2]: import pandas as pd</p>
<p>In [3]: Se1=pd.Series([2.3,78,2.5,3.8],index=[‘as’,’a’,’pupil’,’and’])</p>
<p>In [4]: Se2=pd.Series([2,7.7,2.1,3.3,9,12],index=[‘as’,’a’,’studen’,’teacher’,’and’,’or’])<br>In [6]: tesseq_1=Se1+Se2</p>
<p>In [7]: tesseq_1<br>Out[7]:<br>a 85.7<br>and 12.8<br>as 4.3<br>or NaN<br>pupil NaN<br>studen NaN<br>teacher NaN<br>dtype: float64<br>与Series相同，DataFrame数据相加必须按照行索引和列一一相加，如果索引和列不能配对（也就是两个数据的行列标签不相等时），将返回空值。（先行索引匹配，然后在进行列匹配）<br>In [22]: add1=pd.DataFrame(np.arange(9).reshape(3,3),index=[‘ein’,’zwei’,’drei’],columns=list(‘bde’))</p>
<p>In [23]: add2=pd.DataFrame(np.arange(12).reshape(4,3),index=[‘zwei’,’sechs’,’drei’,’seven’],columns=list(‘bef’))<br>In [26]: add1<br>Out[26]:<br>b d e<br>ein 0 1 2<br>zwei 3 4 5<br>drei 6 7 8</p>
<p>In [27]: add2<br>Out[27]:<br>b e f<br>zwei 0 1 2<br>sechs 3 4 5<br>drei 6 7 8<br>seven 9 10 11</p>
<p>In [24]: Test_add=add1+add2</p>
<p>In [25]: Test_add<br>Out[25]:<br>b d e f<br>drei 12.0 NaN 15.0 NaN<br>ein NaN NaN NaN NaN<br>sechs NaN NaN NaN NaN<br>seven NaN NaN NaN NaN<br>zwei 3.0 NaN 6.0 NaN<br>这里我们发现如果我们没有把两个数据框的行标签和列完全一一对应或者说一一相等的话，就会出现很多空值。空值的重新赋值比较麻烦，所以尽量不要出现这种情况。<br>接下来我们尝试给这个数据框赋值。也就是说消除空值.</p>
<p>首先我们根据数据缺失的实际状况构造出一个新的数据框：</p>
<p>In [9]: Newframe=pd.DataFrame(np.array([[2,45,11,30],[-34,45,89,63],[44,90,36,27]]),index=[‘ein’,’sechs’,’seven’],columns=[‘b’,’d’,’e’,’f’])<br>In [10]: Test_add.add(Newframe,fill_value=0)<br>Out[10]:<br>b d e f<br>drei 12.0 NaN 15.0 NaN<br>ein 2.0 45.0 11.0 30.0<br>sechs -34.0 45.0 89.0 63.0<br>seven 44.0 90.0 36.0 27.0<br>zwei 3.0 NaN 6.0 NaN</p>
<p>In [12]: Diframe=Test_add.add(Newframe,fill_value=0)</p>
<p>In [13]: Diframe<br>Out[13]:<br>b d e f<br>drei 12.0 NaN 15.0 NaN<br>ein 2.0 45.0 11.0 30.0<br>sechs -34.0 45.0 89.0 63.0<br>seven 44.0 90.0 36.0 27.0<br>zwei 3.0 NaN 6.0 NaN<br>In [26]: Diframe.loc[‘drei’,’d’]=36<br>In [28]: Diframe.loc[‘zwei’,’d’]=0</p>
<p>In [29]: Diframe.loc[‘drei’,’f’]=0</p>
<p>In [30]: Diframe.loc[‘zwei’,’f’]=0</p>
<p>In [31]: Diframe<br>Out[31]:<br>b d e f<br>drei 12.0 36.0 15.0 0.0<br>ein 2.0 45.0 11.0 30.0<br>sechs -34.0 45.0 89.0 63.0<br>seven 44.0 90.0 36.0 27.0<br>zwei 3.0 0.0 6.0 0.0</p>
<p>减法乘法和除法<br>In [32]: S1=pd.Series([3,6,9])<br>In [34]: S2=pd.Series([4,3.2,3])<br>S3=S1-S2</p>
<p>S3<br>Out[38]:<br>0   -1.0<br>1    2.8<br>2    6.0<br>dtype: float64</p>
<p>S4=S1*S2</p>
<p>S4<br>Out[40]:<br>0    12.0<br>1    19.2<br>2    27.0<br>dtype: float64</p>
<p>S5=S1/S2</p>
<p>S5<br>Out[42]:<br>0    0.750<br>1    1.875<br>2    3.000<br>dtype: float64</p>
<p>In [43]: Frame1=pd.DataFrame([[2,2,1],[1,1,3],[0,2,3]],index=[‘Nr1’,’Nr2’,’Nr3’],columns=[‘mon’,’tue’,’wen’])</p>
<p>In [44]: Frame2=pd.DataFrame([[1,3,1],[0,1,2],[2,0,1]],index=[‘Nr1’,’Nr2’,’Nr3’],columns=[‘mon’,’tue’,’wen’])</p>
<p>In [45]: Frame3=Frame1-Frame2</p>
<p>In [46]: Frame3<br>Out[46]:<br>mon tue wen<br>Nr1 1 -1 0<br>Nr2 1 0 1<br>Nr3 -2 2 2</p>
<p>In [47]: Frame4=Frame1*Frame2</p>
<p>In [48]: Frame4<br>Out[48]:<br>mon tue wen<br>Nr1 2 6 1<br>Nr2 0 1 6<br>Nr3 0 0 3</p>
<p>In [49]: Frame5=Frame1/Frame2</p>
<p>In [50]: Frame5<br>Out[50]:<br>mon tue wen<br>Nr1 2.000000 0.666667 1.0<br>Nr2 inf 1.000000 1.5<br>Nr3 0.000000 inf 3.0</p>
<p>DataFrame 和 Series之间运算</p>
<p>数据框与Serie是之间的加减运算分为沿行和沿列运算的。</p>
<p>匹配列索引，沿行运算</p>
<p>In [1]: import numpy as np</p>
<p>In [2]: import pandas as pd</p>
<p>In [3]: Frame1=pd.DataFrame([[2,2,1],[1,1,3],[0,2,3]],index=[‘Nr1’,’Nr2’,’Nr3’],columns=[‘mon’,’tue’,’wen’])</p>
<p>In [4]: Frame1<br>Out[4]:<br>mon tue wen<br>Nr1 2 2 1<br>Nr2 1 1 3<br>Nr3 0 2 3<br>In [7]: S1=pd.Series([2,2,1],index=[‘mon’,’tue’,’wen’])</p>
<p>In [8]: Frame1-S1<br>Out[8]:<br>mon tue wen<br>Nr1 0 0 0<br>Nr2 -1 -1 2<br>Nr3 -2 0 2<br>这里我们要注意到数据框沿着行一行行的被Series减去。这种计算方法在Python里被称作广播。</p>
<p>如果某个索引不出现在在数据框的列索引或Series的索引里就会出现NaN。这对数据分析也是不利的，因此要求我们在设置索引时一定要一一对应。</p>
<p>In [9]: S1=pd.Series([2,2,1],index=[‘mon’,’wen’,’fri’])</p>
<p>In [10]: Frame1-S1<br>Out[10]:<br>fri mon tue wen<br>Nr1 NaN 0.0 NaN -1.0<br>Nr2 NaN -1.0 NaN 1.0<br>Nr3 NaN -2.0 NaN 1.0</p>
<p>匹配行索引，沿列运算(广播）<br>In [11]: S2=pd.Series([2,1,0],index=[‘Nr1’,’Nr2’,’Nr3’])<br>In [15]: Frame1.sub(S2,axis=0)<br>Out[15]:<br>mon tue wen<br>Nr1 0 0 -1<br>Nr2 0 0 2<br>Nr3 0 2 3<br>(乘法和除法可以按照.Multiply 和.devide方法就行求解！！！）</p>
<p>函数如何应用到数据框</p>
<p>适合数组的方法与函数，也可应用到pandas的数据结构上。</p>
<p>In [17]: frame12=pd.DataFrame(np.arange(12).reshape(4,3),index=[‘r1’,’r2’,’r3’,’r4’],columns=[‘c1’,’c2’,’c3’])</p>
<p>In [18]: frame12<br>Out[18]:<br>c1 c2 c3<br>r1 0 1 2<br>r2 3 4 5<br>r3 6 7 8<br>r4 9 10 11<br>In [20]: frame13=frame12*-1</p>
<p>In [21]: frame13<br>Out[21]:<br>c1 c2 c3<br>r1 0 -1 -2<br>r2 -3 -4 -5<br>r3 -6 -7 -8<br>r4 -9 -10 -11</p>
<p>In [22]: np.abs(frame13)<br>Out[22]:<br>c1 c2 c3<br>r1 0 1 2<br>r2 3 4 5<br>r3 6 7 8<br>r4 9 10 11</p>
<p>In [23]: f=lambda x:x.max()-x.min()</p>
<p>In [24]: frame12.apply(f)<br>Out[24]:<br>c1 9<br>c2 9<br>c3 9<br>dtype: int64</p>
<p>In [25]: frame12.apply(f,axis=1)<br>Out[25]:<br>r1 2<br>r2 2<br>r3 2<br>r4 2<br>dtype: int64</p>
<p>.Min（.max）方法和min（max）函数在计算上没有区别，只是方法有更多的选择性。</p>
<p>In [26]: frame12.min()<br>Out[26]:<br>c1 0<br>c2 1<br>c3 2<br>dtype: int32</p>
<p>#不做声明，默认轴为‘0’意思为沿行标签操作</p>
<p>#更多选择性<br>In [37]: frame12.min(0)<br>Out[37]:<br>c1 0<br>c2 1<br>c3 2<br>dtype: int32</p>
<p>In [38]: frame12.min(1)<br>Out[38]:<br>r1 0<br>r2 3<br>r3 6<br>r4 9<br>dtype: int32</p>
<p>In [28]: np.min(frame12)<br>Out[28]:<br>c1 0<br>c2 1<br>c3 2<br>dtype: int32</p>
<p>Lamda 函数应用：</p>
<p>In [23]: f=lambda x:x.max()-x.min()</p>
<p>In [24]: frame12.apply(f)<br>Out[24]:<br>c1 9<br>c2 9<br>c3 9<br>dtype: int64</p>
<p>In [25]: frame12.apply(f,axis=1)<br>Out[25]:<br>r1 2<br>r2 2<br>r3 2<br>r4 2<br>dtype: int64</p>
<p>#应用apply方法可以得到更为整齐的结果，试比较例1和例2<br>In [6]: frame11=pd.DataFrame(np.arange(12).reshape(3,4),index=[‘r1’,’r2’,’r3’],columns=[‘c1’,’c2’,’c3’,’c4’])</p>
<p>In [13]: def table_extrem (x):<br>    …: return pd.Series([x.min(),x.max()],index=[‘min’,’max’])<br>    …: </p>
<p>例子1：<br>In [14]: table_extrem(frame11)<br>Out[14]:<br>min c1 0<br>c2 1<br>c3 2<br>c4 3<br>dtype: int32<br>max c1 8<br>c2 9<br>c3 10<br>c4 11<br>dtype: int32<br>dtype: object<br>例子2.<br>In [15]: frame11.apply(table_extrem)<br>Out[15]:<br>c1 c2 c3 c4<br>min 0 1 2 3<br>max 8 9 10 11</p>
<p>DataFrame 的格式化（十分有用）<br>In [16]: frame11=pd.DataFrame(np.random.randn(3,4),index=[‘r1’,’r2’,’r3’],columns=[‘c1’,’c2’,’c3’,’c4’])</p>
<p>In [17]: frame11<br>Out[17]:<br>c1 c2 c3 c4<br>r1 -2.061714 0.584658 -0.540976 0.090904<br>r2 -0.517271 -0.077818 0.163807 0.418174<br>r3 0.321513 0.769480 2.131075 -0.535560</p>
<p>In [18]: formatierung=lambda x:’%.2f’ % x</p>
<p>In [19]: frame11.applymap(formatierung)<br>Out[19]:<br>c1 c2 c3 c4<br>r1 -2.06 0.58 -0.54 0.09<br>r2 -0.52 -0.08 0.16 0.42<br>r3 0.32 0.77 2.13 -0.54</p>
<p>排序和排名<br>Series 排序：<br>In [118]: import numpy as np</p>
<p>In [119]: import pandas as pd</p>
<p>In [120]: project_1=pd.Series(np.arange(1,5),index=[‘a’,’d’,’e’,’f’])</p>
<p>In [121]: project_1<br>Out[121]:<br>a 1<br>d 2<br>e 3<br>f 4<br>dtype: int32</p>
<p>In [122]: project_1.sort_index()<br>Out[122]:<br>a 1<br>d 2<br>e 3<br>f 4<br>dtype: int32</p>
<p>DataFrame 排序：<br>In [124]: DataFrame_12=pd.DataFrame(np.arange(8).reshape(2,4),index=[‘r1’,’r2’],columns=[‘ted’,’lie’,’qiu’,’send’])</p>
<p>In [125]: DataFrame_12.sort_index()<br>Out[125]:<br>ted lie qiu send<br>r1 0 1 2 3<br>r2 4 5 6 7</p>
<p>#给行标签排序<br>In [126]: DataFrame_12.sort_index(axis=1)<br>Out[126]:<br>lie qiu send ted<br>r1 1 2 3 0<br>r2 5 6 7 4</p>
<p>#给列标签排序</p>
<p>上面的程序行标签和列标签（也叫列索引）都是按升序进行排列。我们也可以按降序排列数据框。</p>
<p>In [7]: DataFrame_12.sort_index(axis=1,ascending=False)<br>Out[7]:<br>ted send qiu lie<br>r1 0 3 2 1<br>r2 4 7 6 5</p>
<p>我们上面都是按标签（索引）进行排序，其实我们还可以用sort_values方法对Series按值进行排序：<br>In [15]: Test_series_1=pd.Series([3,6,2,9,1,0],index=[‘r1’,’r2’,’r3’,’r4’,’r5’,’r6’])</p>
<p>In [16]: Test_series_1<br>Out[16]:<br>r1 3<br>r2 6<br>r3 2<br>r4 9<br>r5 1<br>r6 0<br>dtype: int64</p>
<p>In [17]: Test_series_1.sort_values()<br>Out[17]:<br>r6 0<br>r5 1<br>r3 2<br>r1 3<br>r2 6<br>r4 9<br>dtype: int64</p>
<p>我们也可以对DataFrame进行按值排序 </p>
<p>In [18]: Dataffdic=pd.DataFrame({‘liu’:[-1,0,2,-3],’wang’:[3,2,-5,7],’jiang’:[1,0,1,5]})</p>
<p>In [19]: Dataffdic<br>Out[19]:<br>jiang liu wang<br>0 1 -1 3<br>1 0 0 2<br>2 1 2 -5<br>3 5 -3 7</p>
<p>In [20]: Dataffdic.sort_index(by=’liu’)#对’liu’列进行排序<br>C:\Users\dongfeng\Anaconda3\lib\site-packages\ipykernel_launcher.py:1: FutureWarning: by argument to sort_index is deprecated, pls use .sort_values(by=…)<br>“””Entry point for launching an IPython kernel.<br>Out[20]:<br>jiang liu wang<br>3 5 -3 7<br>0 1 -1 3<br>1 0 0 2<br>2 1 2 -5</p>
<p>In [21]: #目前sort_index方法对数据框某列的值进行排序已经落后，现在用新的方法sort_values 对列值进行排序。</p>
<p>In [22]: Dataffdic.sort_values(by=’liu’)<br>Out[22]:<br>jiang liu wang<br>3 5 -3 7<br>0 1 -1 3<br>1 0 0 2<br>2 1 2 -5</p>
<p>#由于按‘liu’列进行排序，所以我们看到‘liu’列的数值排序是正确的。由于排序是整行移动，所以在对‘liu’列进行排序时，‘jiang’列和‘wang’列数值顺序一定会随其变得杂乱无章。</p>
<p>下面我们讲讲排名，排名和排序从结果上讲是完全不同的，排序是把一个没有顺序的标签或数据按照符号的自然排序规则（比如如果升序排列字符串“as”在顺序排位上应该在“at”之前）或者数值的太小进行排列。排名是通过打破数据的平级关系，从而产生一种有级别差的顺序。通俗的解释。一组没有排名的数据他们在级别上等级的，然后我们通过某种运算规则，使每个数据变成一个名次数值。</p>
<p>另外，排序可以对数据值本身和标签进行操作。而排名通常对数据本身操作。</p>
<p>In [1]: import numpy as np </p>
<p>In [2]: import pandas as pd</p>
<p>In [3]: Series_1=pd.Series(np.array([3,8,34,33]),index=[‘r1’,’r2’,’r3’,’r4’])</p>
<p>In [4]: Frame_1=pd.DataFrame(np.array([[3,2,5,0],[7,4,12,13],[1,0,1,-3],[3,6,2,0]]),index=[‘r1’,’r2’,’r3’,’r4’],columns=[‘a’,’b’,’c’,’d’])</p>
<p>Series排名的四种方法<br>1.‘average’法<br>In [6]: Series_1.rank(method=’average’)<br>Out[6]:<br>r1 1.0<br>r2 2.0<br>r3 4.0<br>r4 3.0<br>dtype: float64</p>
<p>2.‘min’法<br>In [8]: Series_1.rank(method=’min’)<br>Out[8]:<br>r1 1.0<br>r2 2.0<br>r3 4.0<br>r4 3.0<br>dtype: float64</p>
<p>3.‘max’法<br>In [7]: Series_1.rank(method=’max’)<br>Out[7]:<br>r1 1.0<br>r2 2.0<br>r3 4.0<br>r4 3.0<br>dtype: float64</p>
<p>4.‘first’法<br>In [9]: Series_1.rank(method=’first’)<br>Out[9]:<br>r1 1.0<br>r2 2.0<br>r3 4.0<br>r4 3.0<br>dtype: float64<br>上面所有的方法都是产生升序排名的结果，他们也可以产生降序的效果，如果我们填加“ascending”方法。</p>
<p>Series_1.rank(ascending=False, method=’first’)<br>Out[19]:<br>r1    4.0<br>r2    3.0<br>r3    1.0<br>r4    2.0<br>dtype: float64</p>
<p>DataFrame的排名<br>在Series排名上，我们没有发现这四种方法的区别，他们一定有区别，只是我们这个Series的排名结果就是这样，没有显现出区别。我们下面通过DataFrame来区别这四种方法。</p>
<p>In [14]: Frame_1.rank(method=’first’)<br>Out[14]:<br>a b c d<br>r1 2.0 2.0 3.0 2.0<br>r2 4.0 3.0 4.0 4.0<br>r3 1.0 1.0 1.0 1.0<br>r4 3.0 4.0 2.0 3.0</p>
<p>In [15]: Frame_1.rank(method=’average’)<br>Out[15]:<br>a b c d<br>r1 2.5 2.0 3.0 2.5<br>r2 4.0 3.0 4.0 4.0<br>r3 1.0 1.0 1.0 1.0<br>r4 2.5 4.0 2.0 2.5</p>
<p>In [16]: Frame_1.rank(method=’min’)<br>Out[16]:<br>a b c d<br>r1 2.0 2.0 3.0 2.0<br>r2 4.0 3.0 4.0 4.0<br>r3 1.0 1.0 1.0 1.0<br>r4 2.0 4.0 2.0 2.0</p>
<p>In [17]: Frame_1.rank(method=’max’)<br>Out[17]:<br>a b c d<br>r1 3.0 2.0 3.0 3.0<br>r2 4.0 3.0 4.0 4.0<br>r3 1.0 1.0 1.0 1.0<br>r4 3.0 4.0 2.0 3.0</p>
<p>我们发现这四种排名的方式是不一样的，尽管是不一样的，但是排名这个目的都十分正确的实现了。仅仅是穿了不同外壳。</p>
<p>上面都是逐行排名，实际上我们还可以逐列排名。</p>
<p>In [20]: Frame_1.rank(axis=1,method=’max’)<br>Out[20]:<br>a b c d<br>r1 3.0 2.0 4.0 1.0<br>r2 2.0 1.0 3.0 4.0<br>r3 4.0 2.0 4.0 1.0<br>r4 3.0 4.0 2.0 1.0</p>
<p>In [21]: Frame_1.rank(axis=1,method=’first’)<br>Out[21]:<br>a b c d<br>r1 3.0 2.0 4.0 1.0<br>r2 2.0 1.0 3.0 4.0<br>r3 3.0 2.0 4.0 1.0<br>r4 3.0 4.0 2.0 1.0</p>
<p>上面就是逐列排名。个人认为‘first’法排名最为规整，且符合我们的习惯思维。</p>
<p>名词解释：<br>逐列排名（或排序）实际上对一行数据进行排名。<br>逐行排名（或排序）实际上对一列数据进行排名。<br>计算机默认逐行排名。</p>
<p>带有重复值的轴索引</p>
<p>对于数据框来说，轴索引就是指的行标签（或者叫行索引）或列标签（或者叫列索引），也就是我们Excel里的行表头和列表头。如下图</p>
<p>很多时候我们要求轴索引是唯一的，但这个要求并不是强制性的，很多时候它可<br>以重复，重复的轴索引对我们数据分析人员来说，不见得是坏事。</p>
<p>例子:<br>In [1]: import numpy as np</p>
<p>In [2]: import pandas as pd</p>
<p>In [3]: framne_repeat=pd.DataFrame(np.arange(16).reshape(4,4),index=[‘r1’,’r2’,’r3’,’r1’],columns=[‘a’,’b’,’b’,’c’])</p>
<p>In [4]: framne_repeat<br>Out[4]:<br>a b b c<br>r1 0 1 2 3<br>r2 4 5 6 7<br>r3 8 9 10 11<br>r1 12 13 14 15</p>
<p>In [5]: framne_repeat[‘b’]<br>Out[5]:<br>b b<br>r1 1 2<br>r2 5 6<br>r3 9 10<br>r1 13 14<br>In [8]: framne_repeat.loc[‘r1’]<br>Out[8]:<br>a b b c<br>r1 0 1 2 3<br>r1 12 13 14 15</p>
<p>In [12]: framne_repeat.loc[(‘r1’)]<br>Out[12]:<br>a b b c<br>r1 0 1 2 3<br>r1 12 13 14 15<br>上面两个例子可以看出framne_repeat.loc[‘r1’]和framne_repeat.loc[(‘r1’)]运行结果是一样的。也就是说加不加括号都无所谓。</p>
<p>带有重复轴索引的Series<br>In [7]: testserie_1=pd.Series(np.array([1,2,3,9,10,3]),index=[‘a’,’b’,’a’,’d’,’e’,’a’])</p>
<p>In [8]: testserie_1<br>Out[8]:<br>a 1<br>b 2<br>a 3<br>d 9<br>e 10<br>a 3<br>dtype: int32</p>
<p>In [9]: testserie_1[‘a’]<br>Out[9]:<br>a 1<br>a 3<br>a 3<br>dtype: int32</p>
<p>可以通过is_unique 函数来判断是否存在重复轴索引。</p>
<p>In [1]: import pandas as pd</p>
<p>In [2]: import numpy as np</p>
<p>In [3]: testserie_1=pd.Series(np.array([1,2,3,9,10,3]),index=[‘a’,’b’,’a’,’d’,’e’,’a’])</p>
<p>In [4]: testserie_1.is_unique<br>Out[4]: False</p>
<p>In [5]: #False值说明有重复轴索引</p>
<p>In [6]: #如果对没有重复值的索引进行索引时，返回一个标量。</p>
<p>In [7]: testserie_1[‘b’]<br>Out[7]: 2</p>
<p>Serie 和 Dataframe 的计算</p>
<p>dataFrame_1=pd.DataFrame([range(4),[np.nan,2,3.6,0.9],[1.2,4,6,7],[3.4,7.9,0.4,8]],index=[‘r1’,’r2’,’r3’,’r4’],columns=[‘c1’,’c2’,’c3’,’c4’])<br>dataFrame_1<br>Out[9]:<br>     c1   c2   c3   c4<br>r1  0.0  1.0  2.0  3.0<br>r2  NaN  2.0  3.6  0.9<br>r3  1.2  4.0  6.0  7.0<br>r4  3.4  7.9  0.4  8.0<br>dataFrame_1.sum()<br>Out[12]:<br>c1     4.6<br>c2    14.9<br>c3    12.0<br>c4    18.9<br>dtype: float64</p>
<p>dataFrame_1.sum(1)<br>Out[13]:<br>r1     6.0<br>r2     6.5<br>r3    18.2<br>r4    19.7<br>dtype: float64</p>
<p>只要不是整行都是NaN值，python自动排除NAN值然后进行计算。通过skipna可以禁止该功能。</p>
<p>dataFrame_1.sum(axis=1,skipna=False)<br>Out[22]:<br>r1     6.0<br>r2     NaN<br>r3    18.2<br>r4    19.7<br>dtype: float64</p>
<p>dataFrame_1.idxmax()<br>Out[23]:<br>c1    r4<br>c2    r4<br>c3    r3<br>c4    r4<br>dtype: object</p>
<p>#求出每列最大值<br>dataFrame_1.idxmax(1)<br>Out[7]:<br>r1    c4<br>r2    c3<br>r3    c4<br>r4    c4<br>dtype: object</p>
<p>#求出每行最大值<br>dataFrame_1.idxmin(1)<br>Out[9]:<br>r1    c1<br>r2    c4<br>r3    c1<br>r4    c3<br>dtype: object<br>求出每行最小值，空值不参与比较。</p>
<p>dataFrame_1.idxmin(axis=1,skipna=False)<br>Out[10]:<br>r1     c1<br>r2    NaN<br>r3     c1<br>r4     c3<br>dtype: object</p>
<p>#修改skipna的默认值”True”,可以修改NaN值自动忽略功能。</p>
<p>#累计加</p>
<p>按列累计加，遇到NaN值自动忽略，不参与运算。<br>dataFrame_1.cumsum()<br>Out[8]:<br>     c1    c2    c3    c4<br>r1  0.0   1.0   2.0   3.0<br>r2  NaN   3.0   5.6   3.9<br>r3  1.2   7.0  11.6  10.9<br>r4  4.6  14.9  12.0  18.9</p>
<p>dataFrame_1.cumsum(1)<br>Out[10]:<br>     c1    c2    c3    c4<br>r1  0.0   1.0   3.0   6.0<br>r2  NaN   2.0   5.6   6.5<br>r3  1.2   5.2  11.2  18.2<br>r4  3.4  11.3  11.7  19.7</p>
<p>dataFrame_1.describe()<br>Out[12]:<br>             c1     c2        c3       c4<br>count  3.000000  4.000  4.000000  4.00000<br>mean   1.533333  3.725  3.000000  4.72500<br>std    1.724336  3.050  2.388863  3.34203<br>min    0.000000  1.000  0.400000  0.90000<br>25%    0.600000  1.750  1.600000  2.47500<br>50%    1.200000  3.000  2.800000  5.00000<br>75%    2.300000  4.975  4.200000  7.25000<br>max    3.400000  7.900  6.000000  8.00000</p>
<p>#上面的分位数我们后面会有介绍</p>
<p>去重，值计数以及值资格判断</p>
<p>series_3=pd.Series([‘c’,’d’,’we’,’a’,’c’,’d’,’e’,’f’,’we’])<br>series_3.unique()<br>Out[25]: array([‘c’, ‘d’, ‘we’, ‘a’, ‘e’, ‘f’], dtype=object)</p>
<p>#通过uique方法的调用，我们立即可以去除Series中的重复的项目<br>serie_unique=series_3.unique()<br>serie_unique.sort()<br>serie_unique<br>Out[40]: array([‘a’, ‘c’, ‘d’, ‘e’, ‘f’, ‘we’], dtype=object)</p>
<p>#通常去重后要进行排序</p>
<p>通过value_counts()方法对Series中的值进行计数<br>series_3.value_counts()<br>Out[43]:<br>we    2<br>c     2<br>d     2<br>a     1<br>f     1<br>e     1<br>dtype: int64</p>
<p>我们可以通过这种方法求出指定元素出现的次数。</p>
<p>serie_nr=series_3.value_counts()<br>serie_nr[‘we’]<br>Out[46]: 2</p>
<p>上面我们看到都是value_counts()作为方法在应用，其实它也可以作为函数（或者叫Pandas的方法）单独应用。</p>
<p>pd.value_counts(series_3.values)<br>Out[47]:<br>we    2<br>c     2<br>d     2<br>a     1<br>f     1<br>e     1<br>dtype: int64<br>也可以不把计算内容做降序处理：<br>pd.value_counts(series_3.values,sort=False)<br>Out[49]:<br>e     1<br>d     2<br>f     1<br>a     1<br>c     2<br>we    2<br>dtype: int64</p>
<p>value_counts()作为单独函数使用时，是一种超级牛的函数，他可以使用到任何序列和数组。</p>
<p>看下面例子：</p>
<p>list_1=[1,3,4,5,2,6,8,4,3,8,9]</p>
<p>pd.value_counts(list_1)<br>Out[51]:<br>8    2<br>4    2<br>3    2<br>9    1<br>6    1<br>5    1<br>2    1<br>1    1<br>dtype: int64</p>
<p>tuple_1=1,2,4,3,2,5,6,3,9</p>
<p>pd.value_counts(tuple_1)<br>Out[54]:<br>3    2<br>2    2<br>9    1<br>6    1<br>5    1<br>4    1<br>1    1<br>dtype: int64</p>
<p>array_1=[1,2,3,4,52,3,2]</p>
<p>pd.value_counts(array_1)<br>Out[56]:<br>3     2<br>2     2<br>4     1<br>1     1<br>52    1<br>dtype: int64<br>series_3<br>Out[60]:<br>0     c<br>1     d<br>2    we<br>3     a<br>4     c<br>5     d<br>6     e<br>7     f<br>8    we<br>dtype: object</p>
<p>series_3.isin([‘a’,’c’])<br>Out[61]:<br>0     True<br>1    False<br>2    False<br>3     True<br>4     True<br>5    False<br>6    False<br>7    False<br>8    False<br>dtype: bool</p>
<p>series_3.isin([‘a’])<br>Out[63]:<br>0    False<br>1    False<br>2    False<br>3     True<br>4    False<br>5    False<br>6    False<br>7    False<br>8    False<br>dtype: bool</p>
<p>#这是一个布尔Series，我们可以把他作为角码选出我们指定值在Series里的所有信息。<br>boolmatrix_2=series_3.isin([‘a’,’c’])</p>
<p>series_3[boolmatrix_2]<br>Out[68]:<br>0    c<br>3    a<br>4    c<br>dtype: object</p>
<p>我们也可以把value_counts()用到数据框，会得到一个你预料不到的结果。尽管这个结果很奇怪，但他却是巧妙的导出频数分布图。<br>dataFrame_1<br>Out[69]:<br>     c1   c2   c3   c4<br>r1  0.0  1.0  2.0  3.0<br>r2  NaN  2.0  3.6  0.9<br>r3  1.2  4.0  6.0  7.0<br>r4  3.4  7.9  0.4  8.0</p>
<p>dataFrame_1.fillna(0)<br>Out[71]:<br>     c1   c2   c3   c4<br>r1  0.0  1.0  2.0  3.0<br>r2  0.0  2.0  3.6  0.9<br>r3  1.2  4.0  6.0  7.0<br>r4  3.4  7.9  0.4  8.0</p>
<p>dataFrame_2=dataFrame_1.fillna(0)</p>
<p>result=dataFrame_2.apply(pd.value_counts).fillna(0)</p>
<p>result<br>Out[74]:<br>      c1   c2   c3   c4<br>0.0  2.0  0.0  0.0  0.0<br>0.4  0.0  0.0  1.0  0.0<br>0.9  0.0  0.0  0.0  1.0<br>1.0  0.0  1.0  0.0  0.0<br>1.2  1.0  0.0  0.0  0.0<br>2.0  0.0  1.0  1.0  0.0<br>3.0  0.0  0.0  0.0  1.0<br>3.4  1.0  0.0  0.0  0.0<br>3.6  0.0  0.0  1.0  0.0<br>4.0  0.0  1.0  0.0  0.0<br>6.0  0.0  0.0  1.0  0.0<br>7.0  0.0  0.0  0.0  1.0<br>7.9  0.0  1.0  0.0  0.0<br>8.0  0.0  0.0  0.0  1.0</p>
<p>NAN数据处理，</p>
<p>在数据处理工作中，我们经常遇到缺失数据，它们的处理往往很费时间。Pandas设计之初，就已经考虑到这种情况。可以说，快速轻松地处理缺失数据是pandas最大优点之一。</p>
<p>#NaN值和None值都可以被当做空值处理：<br>series_1=pd.Series([1,np.nan,3,None,2.3,6])</p>
<p>series_1<br>Out[5]:<br>0    1.0<br>1    NaN<br>2    3.0<br>3    NaN<br>4    2.3<br>5    6.0<br>dtype: float64</p>
<p>缺失值得快速补充法</p>
<p>frame_123=pd.DataFrame([[1,np.nan,np.nan,2,3],[1,2,3,None,8],[6,2.3,8,None,3],[2,3,2,3,2],[3.2,8.9,3,4.5,3],[2,3,0.45,None,8]])</p>
<p>frame_123<br>Out[8]:<br>     0    1     2    3  4<br>0  1.0  NaN   NaN  2.0  3<br>1  1.0  2.0  3.00  NaN  8<br>2  6.0  2.3  8.00  NaN  3<br>3  2.0  3.0  2.00  3.0  2<br>4  3.2  8.9  3.00  4.5  3<br>5  2.0  3.0  0.45  NaN  8</p>
<p>pd.isnull(frame_123)<br>Out[9]:<br>       0      1      2      3      4<br>0  False   True   True  False  False<br>1  False  False  False   True  False<br>2  False  False  False   True  False<br>3  False  False  False  False  False<br>4  False  False  False  False  False<br>5  False  False  False   True  False</p>
<p>boolmatrix_1=pd.isnull(frame_123)<br>frame_123[boolmatrix_1]=0<br>frame_123<br>Out[13]:<br>     0    1     2    3  4<br>0  1.0  0.0  0.00  2.0  3<br>1  1.0  2.0  3.00  0.0  8<br>2  6.0  2.3  8.00  0.0  3<br>3  2.0  3.0  2.00  3.0  2<br>4  3.2  8.9  3.00  4.5  3<br>5  2.0  3.0  0.45  0.0  8</p>
<p>Dropna过滤NaN数据<br>series_23=pd.Series([1,4,3,np.nan,4.5,None,3.4,4.5,6,np.nan,4,5,3])</p>
<p>series_23<br>Out[20]:<br>0     1.0<br>1     4.0<br>2     3.0<br>3     NaN<br>4     4.5<br>5     NaN<br>6     3.4<br>7     4.5<br>8     6.0<br>9     NaN<br>10    4.0<br>11    5.0<br>12    3.0<br>dtype: float64</p>
<p>series_23.dropna()<br>Out[21]:<br>0     1.0<br>1     4.0<br>2     3.0<br>4     4.5<br>6     3.4<br>7     4.5<br>8     6.0<br>10    4.0<br>11    5.0<br>12    3.0<br>dtype: float64</p>
<p>也可以通过Notnull</p>
<p>series_23[series_23.notnull()]<br>Out[22]:<br>0     1.0<br>1     4.0<br>2     3.0<br>4     4.5<br>6     3.4<br>7     4.5<br>8     6.0<br>10    4.0<br>11    5.0<br>12    3.0<br>dtype: float64</p>
<p>Dropna 用于DataFrame和用于Series稍有不同。用于DataFrame，含有NaN值的行会被删去。</p>
<p>frame_123=pd.DataFrame([[1,np.nan,np.nan,2,3],[1,2,3,None,8],[6,2.3,8,None,3],[2,3,2,3,2],[3.2,8.9,3,4.5,3],[2,3,0.45,None,8]])</p>
<p>frame_123<br>Out[28]:<br>     0    1     2    3  4<br>0  1.0  NaN   NaN  2.0  3<br>1  1.0  2.0  3.00  NaN  8<br>2  6.0  2.3  8.00  NaN  3<br>3  2.0  3.0  2.00  3.0  2<br>4  3.2  8.9  3.00  4.5  3<br>5  2.0  3.0  0.45  NaN  8</p>
<p>frame_123.dropna()<br>Out[27]:<br>     0    1    2    3  4<br>3  2.0  3.0  2.0  3.0  2<br>4  3.2  8.9  3.0  4.5  3</p>
<p>使用how=’all’只会消除全为NaN的行：<br>matrix_2=frame_123.dropna(how=’all’)</p>
<p>matrix_2<br>Out[32]:<br>     0    1     2    3  4<br>0  1.0  NaN   NaN  2.0  3<br>1  1.0  2.0  3.00  NaN  8<br>2  6.0  2.3  8.00  NaN  3<br>3  2.0  3.0  2.00  3.0  2<br>4  3.2  8.9  3.00  4.5  3<br>5  2.0  3.0  0.45  NaN  8</p>
<p>列行的添加和丢弃<br>frame_123<br>Out[47]:<br>     0    1     2    3  4<br>0  1.0  NaN   NaN  2.0  3<br>1  1.0  2.0  3.00  NaN  8<br>2  6.0  2.3  8.00  NaN  3<br>3  2.0  3.0  2.00  3.0  2<br>4  3.2  8.9  3.00  4.5  3<br>5  2.0  3.0  0.45  NaN  8</p>
<p>frame_123[5]=[1,3,56,32,np.nan,0.7]</p>
<p>frame_123<br>Out[49]:<br>     0    1     2    3  4     5<br>0  1.0  NaN   NaN  2.0  3   1.0<br>1  1.0  2.0  3.00  NaN  8   3.0<br>2  6.0  2.3  8.00  NaN  3  56.0<br>3  2.0  3.0  2.00  3.0  2  32.0<br>4  3.2  8.9  3.00  4.5  3   NaN<br>5  2.0  3.0  0.45  NaN  8   0.7</p>
<p>#列的添加<br>frame_123<br>Out[47]:<br>     0    1     2    3  4<br>0  1.0  NaN   NaN  2.0  3<br>1  1.0  2.0  3.00  NaN  8<br>2  6.0  2.3  8.00  NaN  3<br>3  2.0  3.0  2.00  3.0  2<br>4  3.2  8.9  3.00  4.5  3<br>5  2.0  3.0  0.45  NaN  8</p>
<p>frame_123.loc[6,:]=[np.nan,2,2.4,5,6]</p>
<p>frame_123<br>Out[53]:<br>     0    1     2    3    4<br>0  1.0  NaN   NaN  2.0  3.0<br>1  1.0  2.0  3.00  NaN  8.0<br>2  6.0  2.3  8.00  NaN  3.0<br>3  2.0  3.0  2.00  3.0  2.0<br>4  3.2  8.9  3.00  4.5  3.0<br>5  2.0  3.0  0.45  NaN  8.0<br>6  NaN  2.0  2.40  5.0  6.0</p>
<p>#行的添加<br>frame_123<br>Out[55]:<br>     0    1     2    3  4<br>0  1.0  NaN   NaN  2.0  3<br>1  1.0  2.0  3.00  NaN  8<br>2  6.0  2.3  8.00  NaN  3<br>3  2.0  3.0  2.00  3.0  2<br>4  3.2  8.9  3.00  4.5  3<br>5  2.0  3.0  0.45  NaN  8</p>
<p>frame_123[5]=np.nan</p>
<p>frame_123<br>Out[57]:<br>     0    1     2    3  4   5<br>0  1.0  NaN   NaN  2.0  3 NaN<br>1  1.0  2.0  3.00  NaN  8 NaN<br>2  6.0  2.3  8.00  NaN  3 NaN<br>3  2.0  3.0  2.00  3.0  2 NaN<br>4  3.2  8.9  3.00  4.5  3 NaN<br>5  2.0  3.0  0.45  NaN  8 NaN</p>
<p>frame_123[5]=np.nan</p>
<p>frame_123<br>Out[57]:<br>     0    1     2    3  4   5<br>0  1.0  NaN   NaN  2.0  3 NaN<br>1  1.0  2.0  3.00  NaN  8 NaN<br>2  6.0  2.3  8.00  NaN  3 NaN<br>3  2.0  3.0  2.00  3.0  2 NaN<br>4  3.2  8.9  3.00  4.5  3 NaN<br>5  2.0  3.0  0.45  NaN  8 NaN</p>
<p>frame_123.dropna(axis=1,how=’all’)<br>Out[60]:<br>     0    1     2    3  4<br>0  1.0  NaN   NaN  2.0  3<br>1  1.0  2.0  3.00  NaN  8<br>2  6.0  2.3  8.00  NaN  3<br>3  2.0  3.0  2.00  3.0  2<br>4  3.2  8.9  3.00  4.5  3<br>5  2.0  3.0  0.45  NaN  8</p>
<p>frame_1=pd.DataFrame(np.random.randn(7,4))</p>
<p>frame_1<br>Out[65]:<br>          0         1         2         3<br>0 -0.668146  0.034772  0.482339  1.444138<br>1 -1.167959 -0.703595  0.641404 -1.100771<br>2 -1.657068 -2.038607  0.141572  2.525831<br>3 -1.869547 -0.291923  0.275511 -0.459739<br>4 -0.287525 -0.966589  2.145633  0.703735<br>5  0.499207 -0.385792 -1.192131 -1.679805<br>6 -0.529885  2.053872  0.970785 -0.733382</p>
<p>frame_1.loc[:4,1]=np.nan;frame_1.loc[:2,2]=np.nan</p>
<p>frame_1<br>Out[67]:<br>          0         1         2         3<br>0 -0.668146       NaN       NaN  1.444138<br>1 -1.167959       NaN       NaN -1.100771<br>2 -1.657068       NaN       NaN  2.525831<br>3 -1.869547       NaN  0.275511 -0.459739<br>4 -0.287525       NaN  2.145633  0.703735<br>5  0.499207 -0.385792 -1.192131 -1.679805<br>6 -0.529885  2.053872  0.970785 -0.733382</p>
<p>[:4,1]注意这里切片是切到“4”，不是到“3”</p>
<p>frame_1.dropna()<br>Out[72]:<br>          0         1         2         3<br>5  0.499207 -0.385792 -1.192131 -1.679805<br>6 -0.529885  2.053872  0.970785 -0.733382</p>
<p>#把不想看的数据行部分赋值NaN，然后在通过dropna语句把含有NaN的行删除掉。</p>
<p>缺失数据的补充：</p>
<p>通过fillna指令我们可以更快地补充缺失数据。这与通过isnull把数据框转化为bool数据框，然后把bool数据框作为索引来赋值‘0’要快的多。（前面已经讲过）</p>
<p>但是这个指令只能赋一个值。通过字典，我们可以给每一列赋不同的值。</p>
<p>frame_1.fillna({1:5,2:4})<br>Out[7]:<br>          0         1         2         3<br>0  1.651360  5.000000  4.000000 -0.600997<br>1 -0.020649  5.000000  4.000000 -0.529843<br>2 -0.850476  5.000000  4.000000 -0.893813<br>3  0.812226  5.000000  0.614715  0.112509<br>4  0.074320  5.000000  1.145727  0.168718<br>5 -0.516371 -1.022050  0.774645  0.705847<br>6  0.617606  0.881843 -0.619870 -1.527961</p>
<p>_=frame_1.fillna(0,inplace=True)</p>
<p>frame_1<br>Out[13]:<br>          0         1         2         3<br>0  1.651360  0.000000  0.000000 -0.600997<br>1 -0.020649  0.000000  0.000000 -0.529843<br>2 -0.850476  0.000000  0.000000 -0.893813<br>3  0.812226  0.000000  0.614715  0.112509<br>4  0.074320  0.000000  1.145727  0.168718<br>5 -0.516371 -1.022050  0.774645  0.705847<br>6  0.617606  0.881843 -0.619870 -1.527961</p>
<p>#可以直接对数据框修改，也就是说可以直接改变源数据，不产生副本。</p>
<p>frame_1=pd.DataFrame(np.random.randn(7,4));frame_1.loc[:4,1]=np.nan;frame_1.loc[:2,2]=np.nan</p>
<p>frame_1<br>Out[107]:<br>          0         1         2         3<br>0 -0.748195       NaN       NaN -0.539246<br>1 -2.360077       NaN       NaN -0.061078<br>2 -1.743350       NaN       NaN -0.233060<br>3 -2.100371       NaN  0.187509  0.893321<br>4 -0.477253       NaN  1.876733  0.208389<br>5  0.770411 -2.262797  0.321646 -2.490215<br>6  1.265209 -0.453140  0.045817  0.951077</p>
<p>frame_1.fillna({1:pd.Series([2,3,7,9,0],index=[0,1,2,3,4]),2:pd.Series([1,9,8],index=[0,1,2])})<br>Out[108]:<br>          0         1         2         3<br>0 -0.748195  2.000000  1.000000 -0.539246<br>1 -2.360077  3.000000  9.000000 -0.061078<br>2 -1.743350  7.000000  8.000000 -0.233060<br>3 -2.100371  9.000000  0.187509  0.893321<br>4 -0.477253  0.000000  1.876733  0.208389<br>5  0.770411 -2.262797  0.321646 -2.490215<br>6  1.265209 -0.453140  0.045817  0.951077</p>
<p>层次化索引：</p>
<p>series_1=pd.Series(np.random.randn(12),index=[[‘r1’,’r1’,’r1’,’t1’,’t1’,’s1’,’s1’,’s1’,’s1’,’p1’,’q1’,’q1’],[9,8,7,6,5,4,3,2,1,0,12,11]])</p>
<p>series_1<br>Out[8]:<br>r1  9     1.536272<br>    8     0.378965<br>    7    -0.292986<br>t1  6    -0.254990<br>    5     0.765191<br>s1  4     2.204928<br>    3     0.662662<br>    2    -0.595029<br>    1    -1.905753<br>p1  0    -1.029524<br>q1  12   -1.038748<br>    11   -1.192589<br>dtype: float64</p>
<p>series_1.index<br>Out[9]:<br>MultiIndex(levels=[[‘p1’, ‘q1’, ‘r1’, ‘s1’, ‘t1’], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12]],<br>           labels=[[2, 2, 2, 4, 4, 3, 3, 3, 3, 0, 1, 1], [9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 11, 10]])</p>
<p>可以直接索引外层标签，也就是第一层标签。<br>series_1[‘s1’]<br>Out[6]:<br>4   -0.012306<br>3    0.800679<br>2   -0.862986<br>1    0.048458<br>dtype: float64</p>
<p>也可以做切片。但是外层标签必须是<br>1.经过排序的，必须！<br>2.每个外层标签的第一个字母不能不一致，要大写，全大写，要小写全小写。</p>
<p>Series_123=pd.Series(np.random.randint(10),index=[[‘apple’,’apple’,’apple’,’apricot’,’apricot’,’apricot’,’banana’,’banana’,’blackberry’,’cherry’],[3,2,5,6,0,4,9,11,21,17]])</p>
<p>Series_123[‘apple’]<br>Out[4]:<br>3    9<br>2    9<br>5    9<br>dtype: int64</p>
<p>Series_123[‘apple’:’banana’]<br>Out[64]:<br>apple    3     5<br>         2     5<br>         5     5<br>apricot  6     5<br>         0     5<br>         4     5<br>banana   9     5<br>         11    5<br>dtype: int64</p>
<p>Series_123[‘apple’:’apricot’]<br>Out[65]:<br>apple    3    5<br>         2    5<br>         5    5<br>apricot  6    5<br>         0    5<br>         4    5<br>dtype: int64</p>
<p>Series_123[‘apple’:’blackberry’]<br>Out[67]:<br>apple       3     5<br>            2     5<br>            5     5<br>apricot     6     5<br>            0     5<br>            4     5<br>banana      9     5<br>            11    5<br>blackberry  21    5<br>dtype: int64</p>
<p>Series_67=pd.Series(np.random.randint(10),index=[[‘apple’,’apple’,’apple’,’apricot’,’apricot’,’apricot’,’banana’,’banana’,’blackberry’,’cherry’],[3,2,5,6,5,4,9,5,21,17]])</p>
<p>Series_67<br>Out[7]:<br>apple       3     4<br>            2     4<br>            5     4<br>apricot     6     4<br>            5     4<br>            4     4<br>banana      9     4<br>            5     4<br>blackberry  21    4<br>cherry      17    4<br>dtype: int64</p>
<p>Series_67[:,5]<br>Out[8]:<br>apple      4<br>apricot    4<br>banana     4<br>dtype: int64</p>
<p>#对于Series，中括号的第一个位置是代表外层标签，第二位置代表内存标签</p>
<p>例子：</p>
<p>Series_67[‘apple’,5]<br>Out[10]: 4</p>
<p>带有层次化索引的Series可以立马转化为数据框，反之，数据框也可以转化为带有层次化索引的Series<br>Series_67.unstack()<br>Out[20]:<br>             2    3    4    5    6    9    17   21<br>apple       4.0  4.0  NaN  4.0  NaN  NaN  NaN  NaN<br>apricot     NaN  NaN  4.0  4.0  4.0  NaN  NaN  NaN<br>banana      NaN  NaN  NaN  4.0  NaN  4.0  NaN  NaN<br>blackberry  NaN  NaN  NaN  NaN  NaN  NaN  NaN  4.0<br>cherry      NaN  NaN  NaN  NaN  NaN  NaN  4.0  NaN</p>
<p>#Series的内层索引变化为列标签，外层索引转化为行标签,并且重复索引删除，索引保持唯一性。</p>
<p>Series_67.unstack(0)<br>Out[22]:<br>    apple  apricot  banana  blackberry  cherry<br>2     4.0      NaN     NaN         NaN     NaN<br>3     4.0      NaN     NaN         NaN     NaN<br>4     NaN      4.0     NaN         NaN     NaN<br>5     4.0      4.0     4.0         NaN     NaN<br>6     NaN      4.0     NaN         NaN     NaN<br>9     NaN      NaN     4.0         NaN     NaN<br>17    NaN      NaN     NaN         NaN     4.0<br>21    NaN      NaN     NaN         4.0     NaN</p>
<p>#axis=0时，Series的内层索引变化为行标签，外层索引转化为列标签</p>
<p>Series_67.unstack(1)<br>Out[24]:<br>             2    3    4    5    6    9    17   21<br>apple       4.0  4.0  NaN  4.0  NaN  NaN  NaN  NaN<br>apricot     NaN  NaN  4.0  4.0  4.0  NaN  NaN  NaN<br>banana      NaN  NaN  NaN  4.0  NaN  4.0  NaN  NaN<br>blackberry  NaN  NaN  NaN  NaN  NaN  NaN  NaN  4.0<br>cherry      NaN  NaN  NaN  NaN  NaN  NaN  4.0  NaN</p>
<p>Series_67.unstack().stack()<br>Out[29]:<br>apple       2     4.0<br>            3     4.0<br>            5     4.0<br>apricot     4     4.0<br>            5     4.0<br>            6     4.0<br>banana      5     4.0<br>            9     4.0<br>blackberry  21    4.0<br>cherry      17    4.0<br>dtype: float64</p>
<p>Gamedata=pd.DataFrame(np.array([[31,27,24,60],[26,28,13,29],[27,17,12,27],[29,9,5,18],[31,12,4,70],[45,11,12,12]]),index=[[‘Morning’,’Morning’,’Morning’,’Afternoon’,’Afternoon’,’Afternoon’],[1,2,3,1,2,3]],columns=[[‘Junior’,’Junior’,’Youth’,’Youth’],[‘Zhang’,’Wang’,’Li’,’Zhao’]])</p>
<p>Gamedata<br>Out[35]:<br>            Junior      Youth<br>             Zhang Wang    Li Zhao<br>Morning   1     31   27    24   60<br>          2     26   28    13   29<br>          3     27   17    12   27<br>Afternoon 1     29    9     5   18<br>          2     31   12     4   70<br>          3     45   11    12   12</p>
<p>Gamedata=pd.DataFrame(np.array([[31,27,24,60],[26,28,13,29],[27,17,12,27],[29,9,5,18],[31,12,4,70],[45,11,12,12]]),index=[[‘A_Morning’,’A_Morning’,’A_Morning’,’B_Afternoon’,’B_Afternoon’,’B_Afternoon’],[1,2,3,1,2,3]],columns=[[‘Junior’,’Junior’,’Youth’,’Youth’],[‘Zhang’,’Wang’,’Li’,’Zhao’]])</p>
<p>Gamedata.index.names=[‘time’,’sequence’]</p>
<p>#因为行有双层索引，因此需要两个名字</p>
<p>#列也是如此，也是两个名字</p>
<p>Gamedata.columns.names=[‘age’,’name’]</p>
<p>Gamedata<br>Out[50]:<br>age                  Junior      Youth<br>name                  Zhang Wang    Li Zhao<br>time        sequence<br>A_Morning   1            31   27    24   60<br>            2            26   28    13   29<br>            3            27   17    12   27<br>B_Afternoon 1            29    9     5   18<br>            2            31   12     4   70<br>            3            45   11    12   12</p>
<p>Gamedata[‘Junior’]<br>Out[51]:<br>name                  Zhang  Wang<br>time        sequence<br>A_Morning   1            31    27<br>            2            26    28<br>            3            27    17<br>B_Afternoon 1            29     9<br>            2            31    12<br>            3            45    11</p>
<p>Gamedata.loc[‘Morning’]<br>Out[31]:<br>Age       Junior      Youth<br>Name       Zhang Wang    Li Zhao<br>series_nr<br>1             31   27    24   60<br>2             26   28    13   29<br>3             27   17    12   27</p>
<p>Gamedata.loc[‘Morning’,2]<br>Out[32]:<br>Age     Name<br>Junior  Zhang    26<br>        Wang     28<br>Youth   Li       13<br>        Zhao     29<br>Name: (Morning, 2), dtype: int32</p>
<p>多重标签的顺序互换：</p>
<p>Gamedata.swaplevel(‘sequence’,’time’)<br>Out[52]:<br>age                  Junior      Youth<br>name                  Zhang Wang    Li Zhao<br>sequence time<br>1        A_Morning       31   27    24   60<br>2        A_Morning       26   28    13   29<br>3        A_Morning       27   17    12   27<br>1        B_Afternoon     29    9     5   18<br>2        B_Afternoon     31   12     4   70<br>3        B_Afternoon     45   11    12   12</p>
<p>Gamedata.swaplevel(‘age’,’name’,axis=1)<br>Out[60]:<br>name                  Zhang   Wang    Li  Zhao<br>age                  Junior Junior Youth Youth<br>time        sequence<br>A_Morning   1            31     27    24    60<br>            2            26     28    13    29<br>            3            27     17    12    27<br>B_Afternoon 1            29      9     5    18<br>            2            31     12     4    70<br>            3            45     11    12    12</p>
<p>Gamedata.sortlevel(1)<br>C:\Users\dongfeng\Anaconda3\lib\site-packages\ipykernel_launcher.py:1: FutureWarning: sortlevel is deprecated, use sort_index(level= …)<br>  “””Entry point for launching an IPython kernel.<br>Out[55]:<br>age                  Junior      Youth<br>name                  Zhang Wang    Li Zhao<br>time        sequence<br>A_Morning   1            31   27    24   60<br>B_Afternoon 1            29    9     5   18<br>A_Morning   2            26   28    13   29<br>B_Afternoon 2            31   12     4   70<br>A_Morning   3            27   17    12   27<br>B_Afternoon 3            45   11    12   12</p>
<p>#sortlevel则根据单个级别的值进行排序，上例根据sequence列数据。</p>
<p>Gamedata.swaplevel(‘age’,’name’,axis=1).sortlevel(0)<br>C:\Users\dongfeng\Anaconda3\lib\site-packages\ipykernel_launcher.py:1: FutureWarning: sortlevel is deprecated, use sort_index(level= …)<br>  “””Entry point for launching an IPython kernel.<br>Out[61]:<br>name                  Zhang   Wang    Li  Zhao<br>age                  Junior Junior Youth Youth<br>time        sequence<br>A_Morning   1            31     27    24    60<br>            2            26     28    13    29<br>            3            27     17    12    27<br>B_Afternoon 1            29      9     5    18<br>            2            31     12     4    70<br>            3            45     11    12    12</p>
<p>#交换两层列标签，然后对第二行列标签排序<br>Gamedata<br>Out[62]:<br>age                  Junior      Youth<br>name                  Zhang Wang    Li Zhao<br>time        sequence<br>A_Morning   1            31   27    24   60<br>            2            26   28    13   29<br>            3            27   17    12   27<br>B_Afternoon 1            29    9     5   18<br>            2            31   12     4   70<br>            3            45   11    12   12</p>
<p>Gamedata.swaplevel(‘age’,’name’,axis=1).sortlevel(1)<br>C:\Users\dongfeng\Anaconda3\lib\site-packages\ipykernel_launcher.py:1: FutureWarning: sortlevel is deprecated, use sort_index(level= …)<br>  “””Entry point for launching an IPython kernel.<br>Out[63]:<br>name                  Zhang   Wang    Li  Zhao<br>age                  Junior Junior Youth Youth<br>time        sequence<br>A_Morning   1            31     27    24    60<br>B_Afternoon 1            29      9     5    18<br>A_Morning   2            26     28    13    29<br>B_Afternoon 2            31     12     4    70<br>A_Morning   3            27     17    12    27<br>B_Afternoon 3            45     11    12    12</p>
<p>#交换两层行标签，然后对第二列行标签排序</p>
<p>根据级别计算：</p>
<p>Gamedata<br>Out[62]:<br>age                  Junior      Youth<br>name                  Zhang Wang    Li Zhao<br>time        sequence<br>A_Morning   1            31   27    24   60<br>            2            26   28    13   29<br>            3            27   17    12   27<br>B_Afternoon 1            29    9     5   18<br>            2            31   12     4   70<br>            3            45   11    12   12</p>
<p>Gamedata.sum(level=’sequence’)<br>Out[64]:<br>age      Junior      Youth<br>name      Zhang Wang    Li Zhao<br>sequence<br>1            60   36    29   78<br>2            57   40    17   99<br>3            72   28    24   39</p>
<p>#每个级别包含两个元素，（因为此级别标签分别重复两次。）<br>Gamedata.sum(level=’name’,axis=1)<br>Out[68]:<br>name                  Li  Wang  Zhang  Zhao<br>time        sequence<br>A_Morning   1         24    27     31    60<br>            2         13    28     26    29<br>            3         12    17     27    27<br>B_Afternoon 1          5     9     29    18<br>            2          4    12     31    70<br>            3         12    11     45    12</p>
<p>每个级别只包含一个元素，因此无法相加，只能排序。</p>
<p>Gamedata.sum(level=’age’,axis=1)<br>Out[69]:<br>age                   Junior  Youth<br>time        sequence<br>A_Morning   1             58     84<br>            2             54     42<br>            3             44     39<br>B_Afternoon 1             38     23<br>            2             43     74<br>            3             56     24</p>
<p>#每个级别包含两个元素，分别两两相加。</p>
<p>Gamedata.sum(level=’time’)<br>Out[71]:<br>age         Junior      Youth<br>name         Zhang Wang    Li Zhao<br>time<br>A_Morning       84   72    49  116<br>B_Afternoon    105   32    21  100</p>
<p>每个级别包含三个元素，分别相加即可</p>
<p>把数据框的列转化为行索引：</p>
<p>frame_1123=pd.DataFrame({‘a’:range(4),’b’:range(4,0,-1),’c’:[‘one’,’one’,’two’,’two’],’d’:[0,1,2,3]})</p>
<p>frame_1123<br>Out[73]:<br>   a  b    c  d<br>0  0  4  one  0<br>1  1  3  one  1<br>2  2  2  two  2<br>3  3  1  two  3</p>
<p>frame_1224=frame_1123.set_index([‘c’,’d’])</p>
<p>frame_1224<br>Out[76]:<br>       a  b<br>c   d<br>one 0  0  4<br>    1  1  3<br>two 2  2  2<br>    3  3  1</p>
<p>#我们发现列从数据框中消失，变成了双重行索引。</p>
<p>然而而这些列也可以不消失。</p>
<p>frame_1123.set_index([‘c’,’d’],drop=False)<br>Out[79]:<br>       a  b    c  d<br>c   d<br>one 0  0  4  one  0<br>    1  1  3  one  1<br>two 2  2  2  two  2<br>3  3  1  two  3</p>
<p>我们可以把双重索引再转化到数据框的数据区域。</p>
<p>frame_1123.reset_index()<br>Out[82]:<br>   index  a  b    c  d<br>0      0  0  4  one  0<br>1      1  1  3  one  1<br>2      2  2  2  two  2<br>3      3  3  1  two  3</p>
<p>面板数据：</p>
<p>Panel实质上是一个三维的数据框，3维说明其有三个轴，每个轴的含义如下：</p>
<p>items: 0轴， 每个项目对应其中的一个DataFrame<br><strong>major_axis（主轴）: </strong> 1轴，它是每个DataFrame的index<br><strong>minor_axis（副轴）: </strong> 2轴，它是每个DataFrame的column<br>创建panel：<br>A.数组创建,三维数组<br>   Dataframe_3 = pd.Panel(np.random.randn(2,5,4),items=[‘Item1’,’Item2’],major_axis=pd.date_range(‘2000-01-01’,’2000-01-05’),minor_axis=[‘A’,’B’,’C’,’D’])</p>
<p>Dataframe_4=pd.Panel(np.array([[[1,4,67,45],[34,56,2,0],[78,90,3,4],[4,23,67,5],[34,89,67,1]],[[67,90,64,7],[789,345,6,2],[33,89,467,8],[43,93,2,8],[33,74,89,6]]]),items=[‘Item1’,’Item2’],major_axis=pd.date_range(‘2000-01-01’,’2000-01-05’),minor_axis=[‘A’,’B’,’C’,’D’])</p>
<p>Dataframe_3<br>Out[107]: </p>
<p><class 'pandas.core.panel.panel'><br>Dimensions: 2 (items) x 5 (major_axis) x 4 (minor_axis)<br>Items axis: Item1 to Item2<br>Major_axis axis: 2000-01-01 00:00:00 to 2000-01-05 00:00:00<br>Minor_axis axis: A to D</class></p>
<p>Dataframe_4<br>Out[108]: </p>
<p><class 'pandas.core.panel.panel'><br>Dimensions: 2 (items) x 5 (major_axis) x 4 (minor_axis)<br>Items axis: Item1 to Item2<br>Major_axis axis: 2000-01-01 00:00:00 to 2000-01-05 00:00:00<br>Minor_axis axis: A to D</class></p>
<p>B.通过字典创建<br>data = {‘Item1’ : pd.DataFrame(np.random.randn(4, 3),index=pd.date_range(‘2017-09-05’,’2017-09-08’),columns=[‘X’,’D’,’F’]),’Item2’ : pd.DataFrame(np.random.randn(4, 2),index=pd.date_range(‘2017-09-05’,’2017-09-08’),columns=[‘X’,’D’])}</p>
<p>panel_1=pd.Panel(data)</p>
<p>panel_1<br>Out[115]: </p>
<p><class 'pandas.core.panel.panel'><br>Dimensions: 2 (items) x 4 (major_axis) x 3 (minor_axis)<br>Items axis: Item1 to Item2<br>Major_axis axis: 2017-09-05 00:00:00 to 2017-09-08 00:00:00<br>Minor_axis axis: D to X</class></p>
<p>C.通过数据框创建</p>
<p>midx = pd.MultiIndex(levels=[[‘one’, ‘two’], [‘x’,’y’]], labels=[[1,1,0,0],[1,0,1,0]])<br>midx<br>Out[117]:<br>MultiIndex(levels=[[‘one’, ‘two’], [‘x’, ‘y’]],<br>           labels=[[1, 1, 0, 0], [1, 0, 1, 0]])</p>
<p>第一层行标是[‘one’, ‘two’]，labels[1, 1, 0, 0]；第二层行标是 [‘x’,’y’]，labels[1, 0, 1, 0]，注意’one’对应‘0’，’two’对应‘1’</p>
<p>df = pd.DataFrame({‘A’:[1,2,3,4],’B’:[5,6,7,8]},index=midx)</p>
<p>df<br>Out[119]:<br>       A  B<br>two y  1  5<br>    x  2  6<br>one y  3  7<br>    x  4  8<br>df.to_panel()</p>
<p>面板操作<br>A.选取<br>Dataframe_3[‘Item1’]<br>Out[124]: Dataframe_3<br>                   A         B         C         D<br>2000-01-01  0.836830 -1.856472  0.345340  0.576771<br>2000-01-02 -1.529758 -1.646630  0.635996 -0.337408<br>2000-01-03  0.451765  0.156648 -1.225328 -0.177641<br>2000-01-04  1.123645  0.364549  0.684536  1.558884<br>2000-01-05 -0.082263  1.472391 -0.379373  2.410845</p>
<p>Dataframe_3.major_axis<br>Out[128]:<br>DatetimeIndex([‘2000-01-01’, ‘2000-01-02’, ‘2000-01-03’, ‘2000-01-04’,<br>               ‘2000-01-05’],<br>              dtype=’datetime64[ns]’, freq=’D’)</p>
<p>Dataframe_3[‘Item1’]<br>Out[131]:<br>                   A         B         C         D<br>2000-01-01  0.836830 -1.856472  0.345340  0.576771<br>2000-01-02 -1.529758 -1.646630  0.635996 -0.337408<br>2000-01-03  0.451765  0.156648 -1.225328 -0.177641<br>2000-01-04  1.123645  0.364549  0.684536  1.558884<br>2000-01-05 -0.082263  1.472391 -0.379373  2.410845</p>
<p>Dataframe_3.major_xs(Dataframe_3.major_axis[2])</p>
<p>Dataframe_3.minor_axis<br>Out[134]: Index([‘A’, ‘B’, ‘C’, ‘D’], dtype=’object’)</p>
<p>Dataframe_3.minor_xs(Dataframe_3.minor_axis[3])<br>Out[137]:<br>               Item1     Item2<br>2000-01-01  0.576771  1.387176<br>2000-01-02 -0.337408 -0.159394<br>2000-01-03 -0.177641 -1.622084<br>2000-01-04  1.558884  0.393800<br>2000-01-05  2.410845 -0.249931</p>
<p>转置：</p>
<p>Dataframe_3=pd.Panel(np.random.randn(2,5,4),items=[‘Item1’,’Item2’],major_axis=pd.date_range(‘2000-01-01’,’2000-01-05’),minor_axis=[‘A’,’B’,’C’,’D’])</p>
<p>Dataframe_3<br>Out[5]: </p>
<p><class 'pandas.core.panel.panel'><br>Dimensions: 2 (items) x 5 (major_axis) x 4 (minor_axis)<br>Items axis: Item1 to Item2<br>Major_axis axis: 2000-01-01 00:00:00 to 2000-01-05 00:00:00<br>Minor_axis axis: A to D</class></p>
<p>Dataframe_3[‘Item1’,’2000-01-03’,’B’]<br>Out[9]: 2.3125077493899666</p>
<p>#转置前的元素定位查询<br>Dataframe_3.transpose(0,2,1)<br>Out[10]: </p>
<p><class 'pandas.core.panel.panel'><br>Dimensions: 2 (items) x 4 (major_axis) x 5 (minor_axis)<br>Items axis: Item1 to Item2<br>Major_axis axis: A to D<br>Minor_axis axis: 2000-01-01 00:00:00 to 2000-01-05 00:00:00</class></p>
<p>Data_transpose=Dataframe_3.transpose(0,2,1)</p>
<p>Data_transpose[‘Item1’,’B’,’2000-01-03’]<br>Out[12]: 2.3125077493899666</p>
<p>#转置后的同一元素查询，两者查询结果相同，说明转置成功。</p>
<p>面板转化为分层索引数据框<br>Dataframe_3.to_frame()<br>Out[16]:<br>                     Item1     Item2<br>major      minor<br>2000-01-01 A      1.345174 -1.719012<br>           B     -1.075240  1.258681<br>           C      0.366470 -0.004046<br>           D     -0.149155 -0.405295<br>2000-01-02 A      0.005256  0.204166<br>           B     -0.032269 -0.667655<br>           C      1.825649  1.050139<br>           D     -1.505179  0.534874<br>2000-01-03 A      0.876495 -0.414982<br>           B      2.312508 -0.731893<br>           C     -0.129701 -1.470191<br>           D     -0.637856 -0.083188<br>2000-01-04 A     -0.287276  0.633456<br>           B     -0.623472 -0.229308<br>           C      0.530747 -0.896306<br>           D      0.229674  0.288064<br>2000-01-05 A      0.163915 -1.836235<br>           B      0.618230  1.353955<br>           C      0.260995  0.808007<br>           D      1.673127 -0.275785<br>切片：</p>
<p>Dataframe_3[‘Item2’,:,’B’]<br>Out[18]:<br>2000-01-01    1.258681<br>2000-01-02   -0.667655<br>2000-01-03   -0.731893<br>2000-01-04   -0.229308<br>2000-01-05    1.353955<br>Freq: D, Name: B, dtype: float64</p>
<p>Dataframe_3[‘Item2’,’2000-01-02’:’2000-01-04’,’B’]<br>Out[19]:<br>2000-01-02   -0.667655<br>2000-01-03   -0.731893<br>2000-01-04   -0.229308<br>Freq: D, Name: B, dtype: float64</p>
<p>Dataframe_3[‘Item2’,’2000-01-02’]<br>Out[20]:<br>A    0.204166<br>B   -0.667655<br>C    1.050139<br>D    0.534874<br>Name: 2000-01-02 00:00:00, dtype: float64</p>
<p>Dataframe_3[‘Item2’,’2000-01-02’:’2000-01-04’]<br>Out[21]:<br>                   A         B         C         D<br>2000-01-02  0.204166 -0.667655  1.050139  0.534874<br>2000-01-03 -0.414982 -0.731893 -1.470191 -0.083188<br>2000-01-04  0.633456 -0.229308 -0.896306  0.288064</p>
<p>Dataframe_3[‘Item1’:’Item2’,’2000-01-02’:’2000-01-04’,’C’]<br>Out[22]:<br>               Item1     Item2<br>2000-01-02  1.825649  1.050139<br>2000-01-03 -0.129701 -1.470191<br>2000-01-04  0.530747 -0.896306</p>
<p>Dataframe_3[:,’2000-01-02’:’2000-01-04’,’C’:’D’]<br>Out[28]: </p>
<p><class 'pandas.core.panel.panel'><br>Dimensions: 2 (items) x 3 (major_axis) x 2 (minor_axis)<br>Items axis: Item1 to Item2<br>Major_axis axis: 2000-01-02 00:00:00 to 2000-01-04 00:00:00<br>Minor_axis axis: C to D</class></p>
<p>#切出新面板。</p>
<p>数据加载，存储，清理，转换，合并与重塑</p>
<p>数据的加载与存储<br>Python在文本文件的加载与存储方面极其方便，这是它成为深受大家喜爱语言的原因之一。</p>
<p>Pandas 提供了一些直接将表格文件读取为DataFrame对象的函数<br>下面我们一一讲解：<br>A, read_csv<br>import pandas as pd<br>!type example_1.txt<br>系统找不到指定的文件。我们做一下简单修改<br>!type Desktop\example_1.txt<br>Liu,Zhang,Wang,Li,Class<br>23,34,78,32,’primary’<br>77,32,89,66,’intermediate’<br>99,34,78,66,’senior’<br>66,34,6,33,’intermediate’<br>也可以直接读取‘CSV’文件<br>!type Desktop\example_1.csv<br>Liu,Zhang,Wang,Li,Class<br>23,34,78,32,’primary’<br>77,32,89,66,’intermediate’<br>99,34,78,66,’senior’<br>66,34,6,33,’intermediate’<br>直接读取的数据相当于源数据，即没有转化为数据框的数据。</p>
<p>下面我们直接把CSV文件读成数据框：<br>frame_1=pd.read_csv(‘Desktop\example_1.csv’)</p>
<p>frame_1<br>Out[10]:<br>   Liu  Zhang  Wang  Li           Class<br>0   23     34    78  32       ‘primary’<br>1   77     32    89  66  ‘intermediate’<br>2   99     34    78  66        ‘senior’<br>3   66     34     6  33  ‘intermediate’</p>
<p>#我们发现我们得到一个完美的数据框<br>下面我们尝试用‘read_table’<br>frame_1=pd.read_table(‘Desktop\example_1.csv’)</p>
<p>frame_1<br>Out[15]:<br>      Liu,Zhang,Wang,Li,Class<br>0       23,34,78,32,’primary’<br>1  77,32,89,66,’intermediate’<br>2        99,34,78,66,’senior’<br>3   66,34,6,33,’intermediate’</p>
<p>我们发现被读成的DataFrame排列很混乱，下面我们加上分割符号‘，’再运行一次。<br>frame_1=pd.read_table(‘Desktop\example_1.csv’,sep=’,’)</p>
<p>frame_1<br>Out[13]:<br>   Liu  Zhang  Wang  Li           Class<br>0   23     34    78  32       ‘primary’<br>1   77     32    89  66  ‘intermediate’<br>2   99     34    78  66        ‘senior’<br>3   66     34     6  33  ‘intermediate’</p>
<p>总结：用read_table一定要加分割符号。<br>上面的例子中的csv数据我们都加了列名，如果没有列名会怎么样呢?<br>frame_2=pd.read_table(‘Desktop\example_2.csv’,sep=’,’)</p>
<p>frame_2<br>Out[19]:<br>   23  34  78  32       ‘primary’<br>0  77  32  89  66  ‘intermediate’<br>1  99  34  78  66        ‘senior’<br>2  66  34   6  33  ‘intermediate’</p>
<p>我们发现计算机自动把第一行数据当做列名，这样的话我们的源数据就遭到破坏。</p>
<p>为了避免这种错误，我们加入‘header’属性。让计算机自动加列名或者自己显性指定：</p>
<p>frame_3=pd.read_csv(‘Desktop\example_2.csv’,header=None)</p>
<p>frame_3<br>Out[21]:<br>    0   1   2   3               4<br>0  23  34  78  32       ‘primary’<br>1  77  32  89  66  ‘intermediate’<br>2  99  34  78  66        ‘senior’<br>3  66  34   6  33  ‘intermediate’</p>
<p>frame_3=pd.read_csv(‘Desktop\example_2.csv’,names=[‘a’,’b’,’c’,’d’,’class’])</p>
<p>frame_3<br>Out[23]:<br>    a   b   c   d           class<br>0  23  34  78  32       ‘primary’<br>1  77  32  89  66  ‘intermediate’<br>2  99  34  78  66        ‘senior’<br>3  66  34   6  33  ‘intermediate’<br>源文件中去掉双引号，这里双引号也会消失！<br>我们可以把最右边的列直接转化成行索引列。</p>
<p>frame_3=pd.read_csv(‘Desktop\example_2.csv’,names=[‘a’,’b’,’c’,’d’,’class’],index_col=’class’)</p>
<p>frame_3<br>Out[28]:<br>                 a   b   c   d<br>class<br>‘primary’       23  34  78  32<br>‘intermediate’  77  32  89  66<br>‘senior’        99  34  78  66<br>‘intermediate’  66  34   6  33</p>
<p>除了可以读取为普通的数据框，还可以读取为带有重索引的数据框：</p>
<p>pd.read_csv(‘Desktop\example_5.csv’,index_col=[‘group’,’games’])<br>Out[45]:<br>             a  b  c  a.1  b.1  c.1  a.2  b.2  c.2<br>group games<br>one   Att    4  3  4    2    4    3    4    2    1<br>      Cgt    4  3  4    2    4    3    4    2    1<br>      Aer    4  3  4    2    4    3    4    2    1<br>two   Att    4  3  4    2    4    3    4    2    1<br>      Cgt    4  3  4    2    4    3    4    2    1<br>      Aer    4  3  4    2    4    3    4    2    1</p>
<p>对于那些用数量不等空格或者字符串隔开的数据，我们可以通过正则操作符一步处理到位，把数据处理整齐：</p>
<p>!type Desktop\exampel_6.csv<br>         A     B     C’<br>aaa     -3.45  2.36   8.90’<br>bbb   0.334  0.457      -4.5’<br>ccc     0.76  -7.34  -8.99’<br>ddd      0.37  -7.8   -4.45’</p>
<p>pd.read_csv(‘Desktop\exampel_6.csv’,sep=’\s+’)<br>Out[63]:<br>         A      B      C’<br>aaa -3.450  2.360   8.90’<br>bbb  0.334  0.457   -4.5’<br>ccc  0.760 -7.340  -8.99’<br>ddd  0.370 -7.800  -4.45’</p>
<p>如果这里的源数据没有引号的话，这里引号也会自动消失。<br>利用skiprows语句可以跳过任意无意义行信息。<br>!type Desktop\example_9.csv<br>title:Good Boy<br>Little Robert asked his mother for two cents. “What did you do with the money I gave you yesterday?”<br>a,b,c,d,name<br>“I gave it to a poor old woman,” he answered.<br>“You’re a good boy,” said the mother proudly. “Here are two cents more. But why are you so interested in the old woman?”<br>group1,12,2,2,3,robert<br>group2,10,19,18,17,linz<br>“She is the one who sells the candy.”<br>group3,29,28,27,27,hans<br>group4,34,35,35,36,manfried</p>
<p>整理后数据</p>
<p>pd.read_csv(‘Desktop\example_9.csv’,skiprows=[0,1,3,4,7])<br>Out[69]:<br>         a   b   c   d      name<br>group1  12   2   2   3    robert<br>group2  10  19  18  17      linz<br>group3  29  28  27  27      hans<br>group4  34  35  35  36  manfried</p>
<p>缺失数据要么没有，要么用某个标记值表示。请看下面两个例子：</p>
<p>!type Desktop\example_10.csv<br>Liu,Zhang,Wang,Li,Class<br>23,34,78,32,NA<br>77,32,,66,’intermediate’<br>99,34,78,66,’senior’<br>66,34,6,33,’intermediate’</p>
<p>pd.read_csv(‘Desktop\example_10.csv’)<br>Out[83]:<br>   Liu  Zhang  Wang  Li           Class<br>0   23     34  78.0  32             NaN<br>1   77     32   NaN  66  ‘intermediate’<br>2   99     34  78.0  66        ‘senior’<br>3   66     34   6.0  33  ‘intermediate’</p>
<p>#上面例子产生NAN值，（直接在数字处空一位，就会产生NaN，比如数据1,2,4对应列标签，A,B,C,D）</p>
<p>!type Desktop\example_11.csv<br>23,34,78,32,’ ‘<br>77,32,89,66,’intermediate’<br>99,34, ,66,’senior’<br>66,34,6,33,’intermediate’</p>
<p>pd.read_csv(‘Desktop\example_11.csv’)<br>Out[86]:<br>   23  34  78  32             ‘ ‘<br>0  77  32  89  66  ‘intermediate’<br>1  99  34      66        ‘senior’<br>2  66  34   6  33  ‘intermediate’</p>
<p>#上面的例子什么都不显示（只有空字符串和空格）<br>通过na_values python可以用字符串标识NAN值。但要注意源数据中字符串上不能有引号。</p>
<p>!type Desktop\example_12.csv<br>A,B,C,D<br>1,2,3,4<br>5,me,7,8<br>8,10,11,me<br>12,13,14,16</p>
<p>pd.read_table(‘Desktop\example_12.csv’,na_values=[‘me’],sep=’,’)<br>Out[14]:<br>    A     B   C     D<br>0   1   2.0   3   4.0<br>1   5   NaN   7   8.0<br>2   8  10.0  11   NaN<br>3  12  13.0  14  16.0</p>
<p>可以用字符串字典的形式标识多空值数据框<br>!type Desktop\example_13.csv<br>A,B,C,D<br>1,2,3,4<br>too,to,56,33<br>45,dee,78,69<br>12,21,34,43</p>
<p>pd.read_table(‘Desktop\example_13.csv’,na_values={‘A’:’too’,’B’:[‘to’,’dee’]},sep=’,’)<br>Out[18]:<br>      A     B   C   D<br>0   1.0   2.0   3   4<br>1   NaN   NaN  56  33<br>2  45.0   NaN  78  69<br>3  12.0  21.0  34  43</p>
<p>爬虫数据例子：<br>df=pd.read_csv(‘Desktop\pachong.csv’,encoding=’gbk’,sep=’,’,header=None, skiprows=[14,47,49,50,51,59,68,83,104,125,127,128,136,150,173,184])<br>用这个指令可以处理中文数据</p>
<p>Pd.read_csv读取excel数据：</p>
<p>import pandas as pd</p>
<p>pd.read_excel(r’Desktop\1.xls’ ,sheetname=[0,1])<br>Out[2]:<br>OrderedDict([(0,      1   2   3   4<br>              0    2   3   4   5<br>              1    3   4   5   6<br>              2    4   5   6   7<br>              3    5   6   7   8<br>              4    6   7   8   9<br>              5    7   8   9  10<br>              6    8   9  10  11<br>              7    9  10  11  12<br>              8   10  11  12  13<br>              9   11  12  13  14<br>              10  12  13  14  15<br>              11  13  14  15  16<br>              12  14  15  16  17<br>              13  15  16  17  18<br>              14  16  17  18  19<br>              15  17  18  19  20<br>              16  18  19  20  21<br>              17  19  20  21  22<br>              18  20  21  22  23<br>              19  21  22  23  24), (1,     23  22  34  55  67<br>              0   24  23  35  56  68<br>              1   25  24  36  57  69<br>              2   26  25  37  58  70<br>              3   27  26  38  59  71<br>              4   28  27  39  60  72<br>              5   29  28  40  61  73<br>              6   30  29  41  62  74<br>              7   31  30  42  63  75<br>              8   32  31  43  64  76<br>              9   33  32  44  65  77<br>              10  34  33  45  66  78<br>              11  35  34  46  67  79<br>              12  36  35  47  68  80<br>              13  37  36  48  69  81<br>              14  38  37  49  70  82<br>              15  39  38  50  71  83<br>              16  40  39  51  72  84<br>              17  41  40  52  73  85<br>              18  42  41  53  74  86<br>              19  43  42  54  75  87<br>              20  44  43  55  76  88<br>              21  45  44  56  77  89<br>              22  46  45  57  78  90)])</p>
<p>#sheetname=表格时，返回多个指定的在一个workbook里面的表格，sheetname=none，将返回一个workbook里的所有表格。sheetname=int时，int指的是表格索引号，sheetname=’sheet1‘返回指定表格。</p>
<p>pd.read_excel(r’Desktop\1.xls’ ,sheetname=None)<br>Out[4]:<br>OrderedDict([(‘Sheet1’,      1   2   3   4<br>              0    2   3   4   5<br>              1    3   4   5   6<br>              2    4   5   6   7<br>              3    5   6   7   8<br>              4    6   7   8   9<br>              5    7   8   9  10<br>              6    8   9  10  11<br>              7    9  10  11  12<br>              8   10  11  12  13<br>              9   11  12  13  14<br>              10  12  13  14  15<br>              11  13  14  15  16<br>              12  14  15  16  17<br>              13  15  16  17  18<br>              14  16  17  18  19<br>              15  17  18  19  20<br>              16  18  19  20  21<br>              17  19  20  21  22<br>              18  20  21  22  23<br>              19  21  22  23  24), (‘Sheet2’,     23  22  34  55  67<br>              0   24  23  35  56  68<br>              1   25  24  36  57  69<br>              2   26  25  37  58  70<br>              3   27  26  38  59  71<br>              4   28  27  39  60  72<br>              5   29  28  40  61  73<br>              6   30  29  41  62  74<br>              7   31  30  42  63  75<br>              8   32  31  43  64  76<br>              9   33  32  44  65  77<br>              10  34  33  45  66  78<br>              11  35  34  46  67  79<br>              12  36  35  47  68  80<br>              13  37  36  48  69  81<br>              14  38  37  49  70  82<br>              15  39  38  50  71  83<br>              16  40  39  51  72  84<br>              17  41  40  52  73  85<br>              18  42  41  53  74  86<br>              19  43  42  54  75  87<br>              20  44  43  55  76  88<br>              21  45  44  56  77  89<br>              22  46  45  57  78  90), (‘Sheet3’,     234  22   44   78    990<br>              0   235   23   45   79   991<br>              1   236   24   46   80   992<br>              2   237   25   47   81   993<br>              3   238   26   48   82   994<br>              4   239   27   49   83   995<br>              5   240   28   50   84   996<br>              6   241   29   51   85   997<br>              7   242   30   52   86   998<br>              8   243   31   53   87   999<br>              9   244   32   54   88  1000<br>              10  245   33   55   89  1001<br>              11  246   34   56   90  1002<br>              12  247   35   57   91  1003<br>              13  248   36   58   92  1004<br>              14  249   37   59   93  1005<br>              15  250   38   60   94  1006<br>              16  251   39   61   95  1007<br>              17  252   40   62   96  1008<br>              18  253   41   63   97  1009<br>              19  254   42   64   98  1010<br>              20  255   43   65   99  1011)])</p>
<p>pd.read_excel(r’Desktop\1.xls’ ,sheetname=2)<br>Out[5]:<br>    234  22   44   78    990<br>0   235   23   45   79   991<br>1   236   24   46   80   992<br>2   237   25   47   81   993<br>3   238   26   48   82   994<br>4   239   27   49   83   995<br>5   240   28   50   84   996<br>6   241   29   51   85   997<br>7   242   30   52   86   998<br>8   243   31   53   87   999<br>9   244   32   54   88  1000<br>10  245   33   55   89  1001<br>11  246   34   56   90  1002<br>12  247   35   57   91  1003<br>13  248   36   58   92  1004<br>14  249   37   59   93  1005<br>15  250   38   60   94  1006<br>16  251   39   61   95  1007<br>17  252   40   62   96  1008<br>18  253   41   63   97  1009<br>19  254   42   64   98  1010<br>20  255   43   65   99  1011</p>
<p>pd.read_excel(r’Desktop\1.xls’ ,sheetname=0)<br>Out[6]:<br>     1   2   3   4<br>0    2   3   4   5<br>1    3   4   5   6<br>2    4   5   6   7<br>3    5   6   7   8<br>4    6   7   8   9<br>5    7   8   9  10<br>6    8   9  10  11<br>7    9  10  11  12<br>8   10  11  12  13<br>9   11  12  13  14<br>10  12  13  14  15<br>11  13  14  15  16<br>12  14  15  16  17<br>13  15  16  17  18<br>14  16  17  18  19<br>15  17  18  19  20<br>16  18  19  20  21<br>17  19  20  21  22<br>18  20  21  22  23<br>19  21  22  23  24</p>
<p>#sheetname=整数时，整数表示表格编号，编号从0开始。</p>
<p>pd.read_excel(r’Desktop\1.xls’ ,sheetname=’Sheet3’)<br>Out[9]:<br>    234  22   44   78    990<br>0   235   23   45   79   991<br>1   236   24   46   80   992<br>2   237   25   47   81   993<br>3   238   26   48   82   994<br>4   239   27   49   83   995<br>5   240   28   50   84   996<br>6   241   29   51   85   997<br>7   242   30   52   86   998<br>8   243   31   53   87   999<br>9   244   32   54   88  1000<br>10  245   33   55   89  1001<br>11  246   34   56   90  1002<br>12  247   35   57   91  1003<br>13  248   36   58   92  1004<br>14  249   37   59   93  1005<br>15  250   38   60   94  1006<br>16  251   39   61   95  1007<br>17  252   40   62   96  1008<br>18  253   41   63   97  1009<br>19  254   42   64   98  1010<br>20  255   43   65   99  1011</p>
<p>#上面是用字符串‘Sheet3’来调用表格三数据</p>
<p>#header : int, list of ints, default 0 指定列索引行，默认0，即取第一行，即数据第一行被自动读取为列索引，这一点我们并不希望，因为会丢失掉第一行数据，因此通常设定 header = None，这种情况下，计算机自动为整个数据添加新的列索引。</p>
<p>pd.read_excel(r’Desktop\1.xls’ ,sheetname=’Sheet3’,header=None)<br>Out[10]:<br>      0   1   2   3     4<br>0   234  22  44  78   990<br>1   235  23  45  79   991<br>2   236  24  46  80   992<br>3   237  25  47  81   993<br>4   238  26  48  82   994<br>5   239  27  49  83   995<br>6   240  28  50  84   996<br>7   241  29  51  85   997<br>8   242  30  52  86   998<br>9   243  31  53  87   999<br>10  244  32  54  88  1000<br>11  245  33  55  89  1001<br>12  246  34  56  90  1002<br>13  247  35  57  91  1003<br>14  248  36  58  92  1004<br>15  249  37  59  93  1005<br>16  250  38  60  94  1006<br>17  251  39  61  95  1007<br>18  252  40  62  96  1008<br>19  253  41  63  97  1009<br>20  254  42  64  98  1010<br>21  255  43  65  99  1011</p>
<p>#上例中我们发现计算机没有破坏原数据，额外为我们添加了一行列索引。</p>
<p>#skiprows : list-like，跳过指定行数的数据<br>pd.read_excel(r’Desktop\1.xls’ ,sheetname=’Sheet3’,header=None,skiprows=[1,5,19,10])<br>Out[11]:<br>      0   1   2   3     4<br>0   234  22  44  78   990<br>1   236  24  46  80   992<br>2   237  25  47  81   993<br>3   238  26  48  82   994<br>4   240  28  50  84   996<br>5   241  29  51  85   997<br>6   242  30  52  86   998<br>7   243  31  53  87   999<br>8   245  33  55  89  1001<br>9   246  34  56  90  1002<br>10  247  35  57  91  1003<br>11  248  36  58  92  1004<br>12  249  37  59  93  1005<br>13  250  38  60  94  1006<br>14  251  39  61  95  1007<br>15  252  40  62  96  1008<br>16  254  42  64  98  1010<br>17  255  43  65  99  1011</p>
<p>#我们发现数据少了4行，原因是我们分别跳过了4行不同行</p>
<p>#skip_footer : int,default 0, 省略从尾部数的整数行数据 </p>
<p>pd.read_excel(r’Desktop\1.xls’ ,sheetname=’Sheet3’,header=None,skip_footer=8)<br>Out[13]:<br>      0   1   2   3     4<br>0   234  22  44  78   990<br>1   235  23  45  79   991<br>2   236  24  46  80   992<br>3   237  25  47  81   993<br>4   238  26  48  82   994<br>5   239  27  49  83   995<br>6   240  28  50  84   996<br>7   241  29  51  85   997<br>8   242  30  52  86   998<br>9   243  31  53  87   999<br>10  244  32  54  88  1000<br>11  245  33  55  89  1001<br>12  246  34  56  90  1002<br>13  247  35  57  91  1003</p>
<p>#从下向上数8行，然后跳过这8行</p>
<p>#index_col : int, list of ints, default None指定某列为索引列<br>pd.read_excel(r’Desktop\1.xls’ ,sheetname=’Sheet3’,header=None,index_col=4)<br>Out[15]:<br>        0   1   2   3<br>4<br>990   234  22  44  78<br>991   235  23  45  79<br>992   236  24  46  80<br>993   237  25  47  81<br>994   238  26  48  82<br>995   239  27  49  83<br>996   240  28  50  84<br>997   241  29  51  85<br>998   242  30  52  86<br>999   243  31  53  87<br>1000  244  32  54  88<br>1001  245  33  55  89<br>1002  246  34  56  90<br>1003  247  35  57  91<br>1004  248  36  58  92<br>1005  249  37  59  93<br>1006  250  38  60  94<br>1007  251  39  61  95<br>1008  252  40  62  96<br>1009  253  41  63  97<br>1010  254  42  64  98<br>1011  255  43  65  99</p>
<p>#上面指定第四列为索引列</p>
<p>#names : array-like, default None, 给所有列索引重新命名。</p>
<p>pd.read_excel(r’Desktop\1.xls’ ,sheetname=’Sheet3’,names=[‘col_1’,’col_2’,’col_3’,’col_4’,’col_5’])<br>Out[17]:<br>    col_1  col_2  col_3  col_4  col_5<br>0     235     23     45     79    991<br>1     236     24     46     80    992<br>2     237     25     47     81    993<br>3     238     26     48     82    994<br>4     239     27     49     83    995<br>5     240     28     50     84    996<br>6     241     29     51     85    997<br>7     242     30     52     86    998<br>8     243     31     53     87    999<br>9     244     32     54     88   1000<br>10    245     33     55     89   1001<br>11    246     34     56     90   1002<br>12    247     35     57     91   1003<br>13    248     36     58     92   1004<br>14    249     37     59     93   1005<br>15    250     38     60     94   1006<br>16    251     39     61     95   1007<br>17    252     40     62     96   1008<br>18    253     41     63     97   1009<br>19    254     42     64     98   1010<br>20    255     43     65     99   1011</p>
<p>#上面的列索引都被更换。</p>
<p>文本文件的块读取</p>
<p>如果只想读取数据一部分，或者数据过大，我们想逐步读取，我们可以通过附加属性函数来实现。</p>
<p>通过nrows属性函数可以选择所需要的行数<br>!type Desktop\example_14.csv<br>one,two,three,four<br>0,1,2,3<br>4,5,6,7<br>8,9,10,11<br>12,13,14,15<br>16,17,18,19<br>20,21,22,23<br>12,13,14,15<br>16,17,18,19<br>20,21,22,23<br>12,13,14,15<br>16,17,18,19<br>20,21,22,23<br>12,13,14,15<br>16,17,18,19<br>20,21,22,23<br>0,1,2,3<br>4,5,6,7<br>8,9,10,11<br>12,13,14,15<br>16,17,18,19<br>20,21,22,23<br>12,13,14,15<br>16,17,18,19</p>
<p>pd.read_table(‘Desktop\example_14.csv’,sep=’,’,nrows=8)<br>Out[21]:<br>   one  two  three  four<br>0    0    1      2     3<br>1    4    5      6     7<br>2    8    9     10    11<br>3   12   13     14    15<br>4   16   17     18    19<br>5   20   21     22    23<br>6   12   13     14    15<br>7   16   17     18    19</p>
<p>实验项目：数据的分块读取：</p>
<p>原始数据的部分截取：</p>
<p>block_2=pd.read_csv(‘Desktop\lockreading.csv’,chunksize=59)<br>key_quantity=pd.Series([])</p>
<p>#block_2是对整个数据分块后的返回值，可看做一个三维数据框，key_quantity是一个存储行编号数量的series</p>
<p>for block_var in block_2:<br>key_quantity=key_quantity.add(block_var[‘Key’].value_counts(),fill_value=0)</p>
<p>{千万注意：这里fill_value很重要，因为在循环过程中，由于相加的两个Seires的行标签不可能每一次都完全匹配，行标签不完全匹配的两个Series相加如果会产生空值，这种情况下会影响下次循环series的相加，因为空值+实数会继续产生空值，从而计数错误。为了消除这种现象的产生，最好的办法是每一次相加前的空值立马填充为‘0’值，这样杜绝空值产生，不会影响下一步两个series的相加。<br>看下面例子：<br>import pandas<br>import pandas as pd<br>serie_1=pd.Series([1,2,3],index=list(‘abc’))<br>serie_2=pd.Series([4,5,8],index=list(‘abd’))<br>serie_3=serie_1+serie_2<br>serie_3<br>Out[6]:<br>a    5.0<br>b    7.0<br>c    NaN<br>d    NaN<br>dtype: float64</p>
<p>#假如我们添加’fill_value=0‘<br>serie_1.add(serie_2,fill_value=0)<br>Out[8]:<br>a    5.0<br>b    7.0<br>c    3.0<br>d    8.0<br>dtype: float64       }</p>
<p>接下来我们继续我们的项目：<br>key_quantity=key_quantity.sort_values(ascending=False)<br>key_quantity</p>
<p>Out[16]:<br>Nr.36    10.0<br>Nr.30    10.0<br>Nr.32    10.0<br>Nr.33    10.0<br>Nr.34    10.0<br>Nr.35    10.0<br>Nr.37    10.0<br>Nr.38    10.0<br>Nr.39    10.0<br>Nr.29    10.0<br>Nr.40    10.0<br>Nr.41    10.0<br>Nr.28    10.0<br>Nr.27    10.0<br>Nr.26    10.0<br>Nr.25    10.0<br>Nr.24    10.0<br>Nr.23    10.0<br>Nr.31    10.0<br>Nr.42     9.0<br>Nr.21     7.0<br>Nr.20     7.0<br>Nr.2      7.0<br>Nr.19     7.0<br>Nr.18     7.0<br>Nr.17     7.0<br>Nr.22     7.0<br>Nr.15     7.0<br>Nr.14     7.0<br>Nr.3      7.0<br>Nr.13     7.0<br>Nr.12     7.0<br>Nr.11     7.0<br>Nr.10     7.0<br>Nr.16     7.0<br>Nr.9      7.0<br>Nr.8      7.0<br>Nr.52     7.0<br>Nr.7      7.0<br>Nr.6      7.0<br>Nr.59     7.0<br>Nr.58     7.0<br>Nr.57     7.0<br>Nr.56     7.0<br>Nr.55     7.0<br>Nr.54     7.0<br>Nr.53     7.0<br>Nr.51     7.0<br>Nr.4      7.0<br>Nr.50     7.0<br>Nr.5      7.0<br>Nr.49     7.0<br>Nr.48     7.0<br>Nr.47     7.0<br>Nr.46     7.0<br>Nr.45     7.0<br>Nr.44     7.0<br>Nr.43     7.0<br>Nr.1      7.0<br>dtype: float64</p>
<p>#上面这个Series左边是行编号，（注意不是行索引，行编号是key列中的值，它是每一行的编号），右边是每个编号在整个数据里出现的次数。</p>
<p>通过一个小函数我们能够瞬间遍查每个数据块寻找任何一个给定的行编号。</p>
<p>def search_rowindizies(str_test):<br>    list_1=[]<br>    for block_var in block_2:<br>        boll=block_var[‘Key’]==str_test<br>        st=block_var.loc[boll,:’A24’].values<br>        lst=st.astype(np.float32)</p>
<pre><code>    #alst=list(lst)
    list_1.append(lst)
return list_1
</code></pre><p>block_2=pd.read_csv(‘Desktop\lockreading.csv’,chunksize=59)</p>
<p>search_rowindizies(‘Nr.42’)<br>Out[19]:<br>[array([[  4.57160000e+04,   2.33750000e+04,   2.23410000e+04,<br>           1.60100000e+03,   6.03000000e+02,   9.98000000e+02,<br>           2.27580000e+04,   1.01930000e+04,   1.25650000e+04,<br>           1.37070000e+04,   7.65300000e+03,   6.05400000e+03,<br>           4.13700000e+03,   2.58400000e+03,   1.55300000e+03,<br>           2.97900000e+03,   2.01500000e+03,   9.64000000e+02,<br>           5.19000000e+02,   3.16000000e+02,   2.03000000e+02,<br>           1.50000000e+01,   1.10000000e+01,   4.00000000e+00]], dtype=float32),<br> array([[  4.57160000e+04,   2.33750000e+04,   2.23410000e+04,<br>           1.60100000e+03,   6.03000000e+02,   9.98000000e+02,<br>           2.27580000e+04,   1.01930000e+04,   1.25650000e+04,<br>           1.37070000e+04,   7.65300000e+03,   6.05400000e+03,<br>           4.13700000e+03,   2.58400000e+03,   1.55300000e+03,<br>           2.97900000e+03,   2.01500000e+03,   9.64000000e+02,<br>           5.19000000e+02,   3.16000000e+02,   2.03000000e+02,<br>           1.50000000e+01,   1.10000000e+01,   4.00000000e+00]], dtype=float32),<br> array([[  4.57160000e+04,   2.33750000e+04,   2.23410000e+04,<br>           1.60100000e+03,   6.03000000e+02,   9.98000000e+02,<br>           2.27580000e+04,   1.01930000e+04,   1.25650000e+04,<br>           1.37070000e+04,   7.65300000e+03,   6.05400000e+03,<br>           4.13700000e+03,   2.58400000e+03,   1.55300000e+03,<br>           2.97900000e+03,   2.01500000e+03,   9.64000000e+02,<br>           5.19000000e+02,   3.16000000e+02,   2.03000000e+02,<br>           1.50000000e+01,   1.10000000e+01,   4.00000000e+00]], dtype=float32),<br> array([[  4.57160000e+04,   2.33750000e+04,   2.23410000e+04,<br>           1.60100000e+03,   6.03000000e+02,   9.98000000e+02,<br>           2.27580000e+04,   1.01930000e+04,   1.25650000e+04,<br>           1.37070000e+04,   7.65300000e+03,   6.05400000e+03,<br>           4.13700000e+03,   2.58400000e+03,   1.55300000e+03,<br>           2.97900000e+03,   2.01500000e+03,   9.64000000e+02,<br>           5.19000000e+02,   3.16000000e+02,   2.03000000e+02,<br>           1.50000000e+01,   1.10000000e+01,   4.00000000e+00]], dtype=float32),<br> array([[  4.57160000e+04,   2.33750000e+04,   2.23410000e+04,<br>           1.60100000e+03,   6.03000000e+02,   9.98000000e+02,<br>           2.27580000e+04,   1.01930000e+04,   1.25650000e+04,<br>           1.37070000e+04,   7.65300000e+03,   6.05400000e+03,<br>           4.13700000e+03,   2.58400000e+03,   1.55300000e+03,<br>           2.97900000e+03,   2.01500000e+03,   9.64000000e+02,<br>           5.19000000e+02,   3.16000000e+02,   2.03000000e+02,<br>           1.50000000e+01,   1.10000000e+01,   4.00000000e+00]], dtype=float32),<br> array([[  4.57160000e+04,   2.33750000e+04,   2.23410000e+04,<br>           1.60100000e+03,   6.03000000e+02,   9.98000000e+02,<br>           2.27580000e+04,   1.01930000e+04,   1.25650000e+04,<br>           1.37070000e+04,   7.65300000e+03,   6.05400000e+03,<br>           4.13700000e+03,   2.58400000e+03,   1.55300000e+03,<br>           2.97900000e+03,   2.01500000e+03,   9.64000000e+02,<br>           5.19000000e+02,   3.16000000e+02,   2.03000000e+02,<br>           1.50000000e+01,   1.10000000e+01,   4.00000000e+00]], dtype=float32),<br> array([[  4.57160000e+04,   2.33750000e+04,   2.23410000e+04,<br>           1.60100000e+03,   6.03000000e+02,   9.98000000e+02,<br>           2.27580000e+04,   1.01930000e+04,   1.25650000e+04,<br>           1.37070000e+04,   7.65300000e+03,   6.05400000e+03,<br>           4.13700000e+03,   2.58400000e+03,   1.55300000e+03,<br>           2.97900000e+03,   2.01500000e+03,   9.64000000e+02,<br>           5.19000000e+02,   3.16000000e+02,   2.03000000e+02,<br>           1.50000000e+01,   1.10000000e+01,   4.00000000e+00]], dtype=float32),<br> array([[  4.57160000e+04,   2.33750000e+04,   2.23410000e+04,<br>           1.60100000e+03,   6.03000000e+02,   9.98000000e+02,<br>           2.27580000e+04,   1.01930000e+04,   1.25650000e+04,<br>           1.37070000e+04,   7.65300000e+03,   6.05400000e+03,<br>           4.13700000e+03,   2.58400000e+03,   1.55300000e+03,<br>           2.97900000e+03,   2.01500000e+03,   9.64000000e+02,<br>           5.19000000e+02,   3.16000000e+02,   2.03000000e+02,<br>           1.50000000e+01,   1.10000000e+01,   4.00000000e+00],<br>        [  4.57160000e+04,   2.33750000e+04,   2.23410000e+04,<br>           1.60100000e+03,   6.03000000e+02,   9.98000000e+02,<br>           2.27580000e+04,   1.01930000e+04,   1.25650000e+04,<br>           1.37070000e+04,   7.65300000e+03,   6.05400000e+03,<br>           4.13700000e+03,   2.58400000e+03,   1.55300000e+03,<br>           2.97900000e+03,   2.01500000e+03,   9.64000000e+02,<br>           5.19000000e+02,   3.16000000e+02,   2.03000000e+02,<br>           1.50000000e+01,   1.10000000e+01,   4.00000000e+00]], dtype=float32)]</p>
<p>#每次执行函数都要调用一次block_2,因为数据用完一次后自动与源数据断开，Block_2自动清零消失<br>到此为止，我们的项目结束！！</p>
<p>把数据下载到csv 文件<br>DataFrame_1=pd.DataFrame(np.arange(16).reshape(4,4),index=list(‘asdt’),columns=list(‘sgdt’))</p>
<p>DataFrame_1<br>Out[5]:<br>    s   g   d   t<br>a   0   1   2   3<br>s   4   5   6   7<br>d   8   9  10  11<br>t  12  13  14  15</p>
<p>DataFrame_1.to_csv(‘Desktop\DataFrame_1.csv’)</p>
<p>#数据储存到桌面，文件名是DataFrame_1。默认情况下，To_csv方法只会把数据存为用逗号隔开的csv数据。我们可以通过!type函数直接调用这个已存到桌面上的文件，就会发现它的确如此，如下：</p>
<p>!type Desktop\DataFrame_1.csv<br>,s,g,d,t<br>a,0,1,2,3<br>s,4,5,6,7<br>d,8,9,10,11<br>t,12,13,14,15</p>
<p>#上面是没有转化为数据框的源数据格式，也就是csv文件的直接调取。<br>当然，我们也可以直接将数据直接存其它分割方式，例如下面的例子</p>
<p>DataFrame_1.to_csv(‘Desktop\DataFrame_2.csv’,sep=’#’)</p>
<p>!type Desktop\DataFrame_2.csv</p>
<p>#s#g#d#t<br>a#0#1#2#3<br>s#4#5#6#7<br>d#8#9#10#11<br>t#12#13#14#15</p>
<p>DataFrame_2=DataFrame_1.copy()</p>
<p>DataFrame_2.loc[‘a’,’s’]=np.nan</p>
<p>DataFrame_2<br>Out[32]:<br>      s   g   d   t<br>a   NaN   1   2   3<br>s   4.0   5   6   7<br>d   8.0   9  10  11<br>t  12.0  13  14  15</p>
<p>DataFrame_2.to_csv(‘Desktop\DataFrame_3.csv’)</p>
<p>!type Desktop\DataFrame_3.csv<br>,s,g,d,t<br>a,,1,2,3<br>s,4.0,5,6,7<br>d,8.0,9,10,11<br>t,12.0,13,14,15</p>
<p>#再重新通过！type指令调回已存储的文件后，发现数据框原有的NaN值变成空位置，然而，空位置在csv文件中并不好辨认，因此我们需要把含有nan值的数据框存储为nan值被指定的容易辨认的符号替代的csv文件。</p>
<p>DataFrame_2<br>Out[50]:<br>      s   g   d   t<br>a   NaN   1   2   3<br>s   4.0   5   6   7<br>d   8.0   9  10  11<br>t  12.0  13  14  15</p>
<p>DataFrame_2.to_csv(‘Desktop\DataFrame_4.csv’,na_rep=’Cat’)<br>!type Desktop\DataFrame_4.csv<br>,s,g,d,t<br>a,Cat,1,2,3<br>s,4.0,5,6,7<br>d,8.0,9,10,11<br>t,12.0,13,14,15</p>
<p>#用!type调用储存在桌面的文件DataFrame_4.csv，我们会发现原来在数据框的NaN值在存储后被‘Cat’替代</p>
<p>这里我们要注意空位置与空字符串不一样<br>试比较：</p>
<p>pd.read_csv(‘Desktop\example_16.csv’)<br>Out[42]:<br>     a    b    c   d<br>0  ‘ ‘  123  456  89<br>1   22   65  ‘ ‘  12<br>2  345   89    0   6<br>3   23  ‘ ‘   33  66</p>
<p>!type Desktop\example_16.csv<br>a,b,c,d<br>‘ ‘,123,456,89<br>22,65,’ ‘,12<br>345,89,0,6<br>23,’ ‘,33,66</p>
<p>上面是空字符串</p>
<p>pd.read_csv(‘Desktop\example_17.csv’)<br>Out[44]:<br>       a      b      c   d<br>0    NaN  123.0  456.0  89<br>1   22.0   65.0    NaN  12<br>2  345.0   89.0    0.0   6<br>3   23.0    NaN   33.0  66</p>
<p>!type Desktop\example_17.csv<br>a,b,c,d<br>,123,456,89<br>22,65,,12<br>345,89,0,6<br>23,,33,66</p>
<p>上面是空位置，空位置再读取成数据框时会产生NaN值。</p>
<p>如果没有特殊指定，数据框行和列的标签会随数据一起存储为csv文件，当然我们也可以不这样。如下：</p>
<p>DataFrame_1<br>Out[51]:<br>    s   g   d   t<br>a   0   1   2   3<br>s   4   5   6   7<br>d   8   9  10  11<br>t  12  13  14  15</p>
<p>DataFrame_1.to_csv(‘Desktop\Dataframe_12.csv’,index=False,header=False)</p>
<p>!type Desktop\Dataframe_12.csv<br>0,1,2,3<br>4,5,6,7<br>8,9,10,11<br>12,13,14,15</p>
<p>我们发现行标签和列标签都不见了。</p>
<p>我们还可以只存储数据框的部分列，并指定顺序：</p>
<p>DataFrame_1.to_csv(‘Desktop\Dataframe_13.csv’,index=False,columns=[‘s’,’t’,’d’])</p>
<p>调用存储文件，测试！！</p>
<p>!type Desktop\Dataframe_13.csv<br>s,t,d<br>0,3,2<br>4,7,6<br>8,11,10<br>12,15,14</p>
<p>我们发现只有部分列被存储，且列的顺序按照指定。</p>
<p>Series 也可以通过to_csv来存储信息到桌面。<br>dates_1=pd.date_range(‘01/06/2017’,periods=14)</p>
<p>dates_1<br>Out[60]:<br>DatetimeIndex([‘2017-01-06’, ‘2017-01-07’, ‘2017-01-08’, ‘2017-01-09’,<br>               ‘2017-01-10’, ‘2017-01-11’, ‘2017-01-12’, ‘2017-01-13’,<br>               ‘2017-01-14’, ‘2017-01-15’, ‘2017-01-16’, ‘2017-01-17’,<br>               ‘2017-01-18’, ‘2017-01-19’],<br>              dtype=’datetime64[ns]’, freq=’D’)<br>series_12=pd.Series(np.arange(14),index=dates_1)<br>series_12.to_csv(‘Desktop\series_123.csv’)<br>!type Desktop\series_123.csv<br>2017-01-06,0<br>2017-01-07,1<br>2017-01-08,2<br>2017-01-09,3<br>2017-01-10,4<br>2017-01-11,5<br>2017-01-12,6<br>2017-01-13,7<br>2017-01-14,8<br>2017-01-15,9<br>2017-01-16,10<br>2017-01-17,11<br>2017-01-18,12<br>2017-01-19,13</p>
<p>csv文件也可以从桌面直接被读取成Series：<br>pd.Series.from_csv(‘Desktop\series_123.csv’)<br>Out[71]:<br>2017-01-06     0<br>2017-01-07     1<br>2017-01-08     2<br>2017-01-09     3<br>2017-01-10     4<br>2017-01-11     5<br>2017-01-12     6<br>2017-01-13     7<br>2017-01-14     8<br>2017-01-15     9<br>2017-01-16    10<br>2017-01-17    11<br>2017-01-18    12<br>2017-01-19    13<br>dtype: int64</p>
<p>注：<br>1.Read_csv可以读取文件，url，文件型对象，但被加载文件必须有分割符，默认的分割符为逗号。<br>2.Read_table可以读取文件，url，文件型对象，但被加载文件必须有分割符，默认的分割符为制表符‘\t’。这一点与read_csv不同，烦请再加载有逗号的数据时用seq属性注明，也即seq=‘，’！<br>3.Read_fwf读取（或称加载）没有分隔符数据，但是各数据之间间距要恒定。<br>4.Read_clipboard 通常用来读取网络数据,在使用前，必须把网页内容先复制到粘贴板上，例如：</p>
<p>pd.read_clipboard(sep=’\s+’)<br>Out[73]:<br>     北      京  220956  209468   11488  697.02  264.30  253555   44087  701.42<br>0    天      津   93162   90080    3082  330.68  126.98  106063   15983  340.10<br>1    河      北  209740  200012    9728  963.39  327.27  262396   62384  982.31<br>2    山      西  131802  114466   17336  628.55  192.27  214625  100159  669.39<br>3  内蒙古  71196   65627    5569  306.82  110.15  101829   36202  308.10<br>4    　      　       　       　       　       　       　       　       　NaN<br>5    辽      宁  211502  199611   11891  838.09  305.58  268741   69130  846.25</p>
<p>上面函数常用关键字列表</p>
<p>filepath_or_buffer:     文件系统位置，url，文件型对象的字符串<br>delimiter或sep    源文件各数据间分隔符或正则表达式<br>header    上载数据成数据框时，数据框的列名，默认为上载数据的第一行。即‘0’行。如果不需要列标签，那么使header=None<br>index_col    就是给行层次化索引命名，行层次索引一般有两列，内列和外列。<br>skiprows    需要忽略的行数，跳过无用行<br>na_values    实质就是把源数据中的指定字符串转化为空值。<br>converters    由列名或者说列号或者说列标签和函数组成字典，例如{‘A’:f}说明f函数应用到’A’列中的每一个数据。<br>nrows    需要读取的数据行数<br>skip_footer    需要忽略的行数，注意从源数据最后一行向上数<br>encoding    用于指明unicode文本文件的文本编码格式<br>Squeeze    如果数据仅有一列，自动返回series<br>thousands    千分位分隔符，如’.’或’,’</p>
<p>converters的应用：</p>
<p>1.数据框的元素作为字符穿处理<br>In [1]: import pandas as pd</p>
<p>In [2]: import numpy as np</p>
<p>In [3]: Data_1=pd.DataFrame([[1,2,1],[3,4,2],[3,5,4]],columns=[‘A’,’B’,’C’],index=[‘r1’,’r2’,’r3’])</p>
<p>In [4]: Data_1<br>Out[4]:<br>A B C<br>r1 1 2 1<br>r2 3 4 2<br>r3 3 5 4</p>
<p>In [7]: Data_1.to_csv(r’Desktop\dong1234.csv’,sep=’,’)</p>
<p>In [8]: pd.read_csv(‘Desktop\dong1234.csv’,delimiter=’,’,usecols=[‘A’,’B’,’C’])<br>Out[8]:<br>A B C<br>0 1 2 1<br>1 3 4 2<br>2 3 5 4</p>
<p>In [10]: pd.read_csv(‘Desktop\dong123.csv’,delimiter=’,’,usecols=[‘A’,’B’,’C’],converters={‘B’:lambda x:x*2})<br>Out[10]:<br>A B C<br>0 1 22 1<br>1 3 44 2<br>2 3 55 4</p>
<p>In [11]: pd.read_csv(‘Desktop\dong123.csv’,delimiter=’,’,usecols=[‘A’,’B’,’C’],converters={‘B’:lambda x:x*2+’1’})<br>Out[11]:<br>A B C<br>0 1 221 1<br>1 3 441 2<br>2 3 551 4</p>
<p>In [12]: pd.read_csv(‘Desktop\dong123.csv’,delimiter=’,’,usecols=[‘A’,’B’,’C’],converters={‘B’:lambda x:x<em>2</em>3+’1’})<br>    …:<br>Out[12]:<br>A B C<br>0 1 2222221 1<br>1 3 4444441 2<br>2 3 5555551 4<br>2.数据框的元素作为数据处理</p>
<p>In [14]: pd.read_csv(‘Desktop\dong123.csv’,delimiter=’,’,usecols=[‘A’,’B’,’C’],converters={‘B’:lambda x:float(x)*<em>3+2</em>float(x)})<br>Out[14]:<br>A B C<br>0 1 12.0 1<br>1 3 72.0 2<br>2 3 135.0 4</p>
<p>In [15]: import math<br>In [21]: converters_1={i: lambda x:float(x)*<em>3+math.sin((np.pi/6)</em>float(x)) for i in range(3)}</p>
<p>In [22]: converters_1<br>Out[22]:<br>{0: &lt;function <strong>main</strong>.<dictcomp>.<lambda>&gt;,<br>1: &lt;function <strong>main</strong>.<dictcomp>.<lambda>&gt;,<br>2: &lt;function <strong>main</strong>.<dictcomp>.<lambda>&gt;}</lambda></dictcomp></lambda></dictcomp></lambda></dictcomp></p>
<p>In [23]: pd.read_csv(‘Desktop\dong123.csv’,delimiter=’,’,usecols=[‘A’,’B’,’C’],converters=converters_1)<br>Out[23]:<br>A B C<br>0 1.5 8.866025 1<br>1 28.0 64.866025 2<br>2 28.0 125.500000 4</p>
<p>分隔符格式的手工处理<br>通常用read_table或者read_csv直接加载csv文件或者TXT文件都是没有问题的，但有时csv文件十分混乱，以致无法加载，也是十分常见的，这就要求我们在加载前要对原始csv文件进行处理，使其能够被顺利加载。</p>
<p>我们先看一个单字符分隔符文件<br>import pandas as pd</p>
<p>import numpy as np</p>
<p>!type Desktop\example_20.csv<br>“f”,”g”,”h”,”d”<br>“1”,”1”,”4”,”5”<br>“7”,”9”,”0”,”4”<br>“2”,”8”,”11”,0<br>import csv<br>调用内置csv模块，将已打开文件直接传给csv.reader<br>f=open(‘Desktop\example_20.csv’)<br>getting_1=csv.reader(f)<br>getting_1<br>Out[25]: &lt;_csv.reader at 0x3077b74320&gt;</p>
<p>#我们无法看到getting_1的庐山真面目，怎么办？通过for循环，我们看到getting_1中的每个元素。这是一个十分好的方法，请大家务必注意：<br>for subgetting in getting_1:<br>    print(subgetting)</p>
<p>[‘f’, ‘g’, ‘h’, ‘d’]<br>[‘1’, ‘1’, ‘4’, ‘5’]<br>[‘7’, ‘9’, ‘0’, ‘4’]<br>[‘2’, ‘8’, ‘11’, ‘0’]<br>我下面再举一个具体的例子：</p>
<p>range(1,9,1)<br>Out[27]: range(1, 9)<br>for subrange in range(1,9,1):<br>    print(subrange)</p>
<p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>当然这里我们还有一个终极解决方案，也就是通过list。<br>Gate_1=list(csv.reader(open(‘Desktop\example_20.csv’)))<br>Gate_1<br>Out[32]:<br>[[‘f’, ‘g’, ‘h’, ‘d’],<br> [‘1’, ‘1’, ‘4’, ‘5’],<br> [‘7’, ‘9’, ‘0’, ‘4’],<br> [‘2’, ‘8’, ‘11’, ‘0’]]</p>
<p>#直到这里我们终于看到了csv.reader()的庐山真面目。</p>
<p>header,values=Gate_1[0],Gate_1[1:]</p>
<p>dic_new_1={u:v for u,v in zip(header,zip(*values))}</p>
<p>dic_new_1<br>Out[36]:<br>{‘d’: (‘5’, ‘4’, ‘0’),<br> ‘f’: (‘1’, ‘7’, ‘2’),<br> ‘g’: (‘1’, ‘9’, ‘8’),<br> ‘h’: (‘4’, ‘0’, ‘11’)}</p>
<p>最终我们把相对杂乱无章的数据转化为规整的字典。</p>
<p>解释：zip（<em>2dim-Array）<br>这个操作相当于把嵌套列表每一行一一对应先组成元组，然后组成列表：<br>c=[[1,2,3],[4,7,0]]<br>zip(</em>c)<br>Out[40]: <zip at 0x3077b9ce08><br>list(zip(*c))<br>Out[41]: [(1, 4), (2, 7), (3, 0)]</zip></p>
<p>我们可以自定义csv文件的读取或写入格式：<br>Import csv<br>class mydialect(csv.Dialect):<br>    lineterminator = ‘\n’ #用于写操作时的行结束符，读操作时将忽略此项，也就是读操作时无需在文件数据行尾处加‘\n’<br>    delimiter = ‘#’ #可以读取用‘#’分割的字段（或者数据）<br>quotechar=’”‘ #用于带有特殊字符的字段（通常指的是带有单引号的字符串）的引用符号，例如源数据这样的字段’12#3’通过csv.reader函数读取到python程序页面后会变成”’12”,”3’”。<br>quoting=csv.QUOTE_ALL #读取所有字段（和数据）<br>skipinitialspace=True #或略分隔符后面的空格<br>doublequote=True #如果读取的字段含有引用符号，则整个字段加双引号。</p>
<p>下面是一个比较简单的例子：<br>Import csv<br>class mydialect(csv.Dialect):<br>    lineterminator = ‘\n’<br>    delimiter = ‘#’<br>    quotechar=’”‘<br>    quoting=csv.QUOTE_ALL<br>    skipinitialspace=True<br>    doublequote=True</p>
<p>myfile_1=open(‘Desktop\example_223.csv’)</p>
<p>content_11=csv.reader(myfile_1,dialect=mydialect)</p>
<p>for contents in content_11:</p>
<pre><code>print(contents)
</code></pre><p>[‘At’, ‘bee’, ‘cotton’, ‘death’, “common’hero’tree”]<br>[‘1’, ‘2’, ‘3’, ‘4’]<br>[‘5’, ‘6’, ‘7’, ‘8’, ‘9’]<br>[‘9’, ‘10’, ‘11’, ‘12’, “‘12”, “3’”]</p>
<p>!type Desktop\example_223.csv<br>At#bee#cotton#  death#common’hero’tree<br>1#2#  3#4<br>5#6#7#8#9<br>9#10#11#12#’12#3’</p>
<p>我们上面一直在讨论通过定义csv.Dialect的一个子类来读取独有格式的csv，下面我们谈谈写入</p>
<p>class mydialect(csv.Dialect):<br>    lineterminator = ‘\n’<br>    delimiter = ‘#’<br>    quotechar=’”‘<br>    quoting=csv.QUOTE_ALL<br>    skipinitialspace=True<br>doublequote=True</p>
<p>with open(‘Desktop\example_25.csv’,’w’) as newsetting_1: #首先open()函数打开文件’Desktop\example_25.csv’，如果不存在这个文件，则创建它，返回文件对象newsetting_1。<br>     writer_object=csv.writer(newsetting_1,dialect=mydialect) #创建一个写对象’writer_object’，可理解成给打开文件添加独有编码风格“mydialect”。然后返回一个带有独有编码风格的新文件对象<br>     writer_object.writerow([“‘one’”,0,  8]) #可通过列表把内容一步步地写入到打开的文件<br>     writer_object.writerow([‘two’,62,43])<br>     mylist=[[‘three’,46,88],[‘four’,  3,12]]<br>     writer_object.writerows(mylist)</p>
<p>!type Desktop\example_25.csv<br>one’#”0”#”8”<br>two#”62”#”43”<br>three#”46”#”88”<br>four#”3”#”12”</p>
<p>JSON数据<br>JSON是一种数据格式，它是通过HTTP请求在web浏览器与其他应用程序之间数据传送格式之一。</p>
<p>例如：<br>Json_documents=”””<br>{“Name”:”Wes”,”places_lived”:[“United States”,”Spain”,”Germany”,”Japan”],”pet”:null,”siblings”:[{“name”:”Scott”,”age”:25,”pet”:”Zuko”,”weight”:56,”height”:172},{“name”:”Katie”,”age”:33,”pet”:”Cisco”,”weight”:89,”height”:189}]}<br>“””<br>这是一个Json数据格式字符串数据对象，可以通过python标准库中的json.loads函数即可将JSON字符串数据转化为python字符串数据。</p>
<p>import json</p>
<p>getting_1=json.loads(Json_documents)</p>
<p>getting_1<br>Out[36]:<br>{‘Name’: ‘Wes’,<br> ‘pet’: None,<br> ‘places_lived’: [‘United States’, ‘Spain’, ‘Germany’, ‘Japan’],<br> ‘siblings’: [{‘age’: 25,<br>   ‘height’: 172,<br>   ‘name’: ‘Scott’,<br>   ‘pet’: ‘Zuko’,<br>   ‘weight’: 56},<br>  {‘age’: 33, ‘height’: 189, ‘name’: ‘Katie’, ‘pet’: ‘Cisco’, ‘weight’: 89}]}</p>
<p>试比较这两种数据结构，我们会发现：<br>A.JSON格式的字符串数据对象的字符串没有排序，确切的说字典或者说JSON对象的键和其所对应的子字典内容都没有任何排序。而python格式的字典缺有严格工整的排序，无论键还是键所对应的字典都有严格排序。<br>B.所有字符串的双引号都变成单引号<br>C.JSON格式的字典最前面和最后面都有‘”””’（也可用“’”）标识，转化成Python格式后所有标识全部消失。<br>D.JSON的空值用null表达，Python用None表达。</p>
<p>有一点我们必须要注意，JSON格式的对象（JSON有对象（即字典）、数组、字符串、数字，bool值以及null等数据结构）的键必须是字符串。如果不是字符串将会在转换时出现错误提示:<br>Json_documents=”””<br>{23:”Wes”,”places_lived”:[“United States”,”Spain”,”Germany”,”Japan”],”pet”:null,”siblings”:[{“name”:”Scott”,”age”:25,”pet”:”Zuko”,”weight”:56,”height”:172},{“name”:”Katie”,”age”:33,”pet”:”Cisco”,”weight”:89,”height”:189}]}<br>“””</p>
<h2 id="getting-1-json-loads-Json-documents"><a href="#getting-1-json-loads-Json-documents" class="headerlink" title="getting_1=json.loads(Json_documents)"></a>getting_1=json.loads(Json_documents)</h2><p>JSONDecodeError                           Traceback (most recent call last)</p>
<p><ipython-input-38-66d19b29dad0> in <module>()<br>—-&gt; 1 getting_1=json.loads(Json_documents)</module></ipython-input-38-66d19b29dad0></p>
<p>~\Anaconda3\lib\json__init__.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)<br>    352             parse_int is None and parse_float is None and<br>.<br>.<br>.<br>    357             raise JSONDecodeError(“Expecting value”, s, err.value) from None</p>
<p>JSONDecodeError: Expecting property name enclosed in double quotes: line 2 column 2 (char 2)</p>
<p>我们当然也可以把python格式的数据对象转化为JSON格式，使用python的函数json.dumps可以实现这一愿望：<br>例子：</p>
<p>python_format_1={‘Name’: ‘Wes’,<br> ‘pet’: None,<br> ‘places_lived’: [‘United States’, ‘Spain’, ‘Germany’, ‘Japan’],<br> ‘siblings’: [{‘age’: 25,<br>   ‘height’: 172,<br>   ‘name’: ‘Scott’,<br>   ‘pet’: ‘Zuko’,<br>   ‘weight’: 56},<br>  {‘age’: 33, ‘height’: 189, ‘name’: ‘Katie’, ‘pet’: ‘Cisco’, ‘weight’: 89}]}</p>
<p> json_format_1=pd.json.dumps(python_format_1)</p>
<p>json_format_1<br>Out[6]:’{“Name”:”Wes”,”pet”:null,”places_lived”:[“United States”,”Spain”,”Germany”,”Japan”],”siblings”:[{“age”:25,”height”:172,”name”:”Scott”,”pet”:”Zuko”,”weight”:56},{“age”:33,”height”:189,”name”:”Katie”,”pet”:”Cisco”,”weight”:89}]}’</p>
<p>再把json格式数据转化为python数据格式后，我们需要把数据作进一步处理，以便我们更好的处理数据。最常用的做法是把数据继续转化为数据框：</p>
<p>newDataFrame=pd.DataFrame(python_format_1[‘siblings’])</p>
<p>newDataFrame<br>Out[8]:<br>   age  height   name    pet  weight<br>0   25     172  Scott   Zuko      56<br>1   33     189  Katie  Cisco      89<br>python_format_1[‘siblings’]表示的是一个由字典组成的列表，通过它可以直接生成一个数据框，这是一个很好的数据框创建方法。我们下面一起做一些用这种模式创建数据框的练习：</p>
<p>例子1：<br>dict_1={‘c1’:[1,2,3,4,5],’c2’:[21,65,32,90,0],’c3’:[90,67,5,6,3],’c4’:[1,2,1,2,0],’c5’:[9,0,8,7,0]}</p>
<p>dict_2={‘c1’:[1,3,3,4,5],’c2’:[21,5,32,90,3],’c3’:[90,7,5,6,3],’c4’:[1,2,155,2,0],’c5’:[9,450,8,7,0]}<br>newframe_0=pd.DataFrame(dict_1)</p>
<p>newframe_0<br>Out[19]:<br>   c1  c2  c3  c4  c5<br>0   1  21  90   1   9<br>1   2  65  67   2   0<br>2   3  32   5   1   8<br>3   4  90   6   2   7<br>4   5   0   3   0   0</p>
<p>例子2：<br>newframe_0=pd.DataFrame([dict_1])<br>newframe_0<br>Out[17]:<br>    c1            c2              c3             c4      \<br>0  [1, 2, 3, 4, 5]  [21, 65, 32, 90, 0]  [90, 67, 5, 6, 3]  [1, 2, 1, 2, 0]   </p>
<pre><code>c5  
</code></pre><p>0  [9, 0, 8, 7, 0]  </p>
<p>例子3：<br>newframe_1=pd.DataFrame([dict_1,dict_2]):<br>newframe_1<br>Out[15]:<br>     c1             c2            c3                c4  \<br>0  [1, 2, 3, 4, 5]  [21, 65, 32, 90, 0]  [90, 67, 5, 6, 3]    [1, 2, 1, 2, 0]<br>1  [1, 3, 3, 4, 5]   [21, 5, 32, 90, 3]   [90, 7, 5, 6, 3]  [1, 2, 155, 2, 0]   </p>
<pre><code>c5  
</code></pre><p>0    [9, 0, 8, 7, 0]<br>1  [9, 450, 8, 7, 0]<br>例子4<br>dic_3={‘c6’:[1,3,3,4,5],’c7’:[21,5,32,90,3],’c8’:[90,7,5,6,3],’c9’:[1,2,155,2,0],’c10’:[9,450,8,7,0]}<br>newFrame_4=pd.DataFrame([dict_1,dic_3])<br>newFrame_4<br>Out[22]:<br>      c1                    c10                 c2               c3       \<br>0  [1, 2, 3, 4, 5]              NaN               [21, 65, 32, 90, 0]  [90, 67, 5, 6, 3]<br>1      NaN               [9, 450, 8, 7, 0]             NaN            NaN   </p>
<pre><code>c4               c5                  c6                  c7       \
</code></pre><p>0  [1, 2, 1, 2, 0]       [9, 0, 8, 7, 0]              NaN              NaN<br>1    NaN              NaN               [1, 3, 3, 4, 5]      [21, 5, 32, 90, 3]   </p>
<pre><code>c8             c9  
</code></pre><p>0    NaN          NaN<br>1  [90, 7, 5, 6, 3]  [1, 2, 155, 2, 0]  </p>
<p>#数据框的列索引由字典的键确定，字典列表中的有多少字典键就有多少列，譬如字典列表的第一个字典（0位置字典）有5个键，第二个字典（1位置字典）有5个键，那么生成的数据框就有10个列。字典列表的长度代表了行的数量，就如例子中字典列表的长度是2，那么数据框就有两行，0行和1行。</p>
<p>通过columns我们可以指定需要显示的数据列：</p>
<p>newframe_5=pd.DataFrame([dict_1,dict_2],columns=[‘c2’,’c3’])</p>
<p>newframe_5<br>Out[24]:<br>     c2               c3<br>0  [21, 65, 32, 90, 0]  [90, 67, 5, 6, 3]<br>1   [21, 5, 32, 90, 3]   [90, 7, 5, 6, 3]</p>
<p>数据的二进制格式存储</p>
<p>定义：将对象转换为可通过网络传输或可以存储到本地磁盘的数据格式的过程称为序列化；反之，则称为反序列化。Python内置的pickle序列化用于实现Python数据类型与Python特定二进制格式之间的转换。<br>import pandas as pd<br>frame_1=pd.read_csv(‘Desktop\exampel_6.csv’)<br>frame_1=pd.read_csv(‘Desktop\exampel_6.csv’,sep=’\s+’)</p>
<p>frame_1<br>Out[6]:<br>         A      B      C’<br>aaa -3.450  2.360   8.90’<br>bbb  0.334  0.457   -4.5’<br>ccc  0.760 -7.340  -8.99’<br>ddd  0.370 -7.800  -4.45’</p>
<p>frame_1.to_pickle(‘Desktop\frame_pickle’)</p>
<p>#上面我们把frame_1存储成pickle格式<br>我们也可以把数据读回到python。</p>
<p>import pickle<br>load_file=open(‘Desktop\Frame_pickle’,’rb’)</p>
<p>pickle.load(load_file)<br>Out[17]:<br>         A      B      C’<br>aaa -3.450  2.360   8.90’<br>bbb  0.334  0.457   -4.5’<br>ccc  0.760 -7.340  -8.99’<br>ddd  0.370 -7.800  -4.45’</p>
<p>HDF5的存取</p>
<p>Hierarchical Data Format(HDF)是一种针对大量数据进行组织和存储的文件格式。它包含了数据模型，库，和文件格式标准。以其便捷有效，移植性强，灵活可扩展的特点受到了广泛的关注和应用。</p>
<p>对大数据的组织和存储得益于HDF文件的系统式节点结构，这样的结构不仅支持元数据，而且使多数据集存储成为现实。</p>
<p>Python中的HDF5库有两个接口Pytables和h5py。</p>
<p>我们先看一下h5py，h5py是python的一种工具包，它提供了一种直接而高级的HDF5 API访问接口。</p>
<p>import h5py<br>import numpy as np</p>
<p>#HDF5的写入<br>Intentity_12=np.ones((10,12,300,305))<br>data_h5py_1=h5py.File(‘Desktop\example_29-1.h5’,’w’)</p>
<p>#创建一个h5文件<br>data_h5py_1[‘data’]=Intentity_12</p>
<p>#把数据写入到data_h5py_1主键‘data’中<br>data_h5py_1[‘labels’]=range(120)</p>
<p>#把数据写入到data_h5py_1主键‘labels’中<br>data_h5py_1.close()</p>
<p>#HDF5的读取<br>data_h5py_2=h5py.File(‘Desktop\example_29-1.h5’,’r’)</p>
<p>#h5文件的读取<br>key_content=data_h5py_2.keys()</p>
<p>#所有键的查看<br>abc_123=data_h5py_2[‘data’][:]</p>
<p>#查看主键‘data’的内容<br>data_h5py_2[‘labels’][:]</p>
<p>#查看主键‘labels’的内容</p>
<p>我们再看另一种接口Pytables，Pytables也是python的一种工具包，它抽像了许多HDF5的细节以提供多种灵活数据容器，及实现表索引和查询功能。</p>
<p>import pandas as pd<br>store=pd.HDFStore(‘Desktop\example_30.h5’,’w’)</p>
<p>#通过HDFStore创建一个h5文件<br>getframe_1=pd.DataFrame(np.arange(16).reshape(4,4),index=[‘r1’,’r2’,’r3’,’r4’],columns=[‘c1’,’c2’,’c3’,’c4’])<br>store[‘layer_1’]=getframe_1<br>store[‘layer_1_1’]=getframe_1[‘c2’]<br>store<br>Out[50]: </p>
<p><class 'pandas.io.pytables.hdfstore'><br>File path: Desktop\example_30.h5<br>/layer_1              frame        (shape-&gt;[4,4])<br>/layer_1_1            series       (shape-&gt;[4])<br>store[‘layer_1’][‘c4’]<br>Out[51]:<br>r1     3<br>r2     7<br>r3    11<br>r4    15<br>Name: c4, dtype: int32</class></p>
<p>注意：<br>1.HDF5最好一次写入，不要多次写入,以免破坏文件<br>2.创建H5文件时，各种文件类型。<br>r：只能读<br>r+：可读可写，不会创建不存在的文件。如果直接写文件，则从顶部开始写，覆盖之前此位置的内容，如果先读后写，则会在文件最后追加内容。<br>w：只能写，覆盖整个文件，文件不存在则创建<br>a：只能写，从文件底部添加内容，文件不存在则创建</p>
<p>数据的合并<br>常用的数据合并函数有pandas.merge和pandas.contact以及combine_first。它们分别有着各自的应用条件。我们根据实际情况选取合适的函数来进行数据合并。</p>
<p>1.pandas.merge<br>通过merge函数可以实现数据框的合并，但这种合并要经过一个或多个键的衔接，这就要求我们必须在数据框中专门添加一个‘Key’列用于衔接。<br>例1：<br>Frame_1=pd.DataFrame({‘schluessel’:[‘Nr.1’,’Nr.2’,’Nr.3’,’Nr.4’,’Nr.5’],’values_1’:[1,2,2,8,9],’values_2’:[4,7,9,0,1]})</p>
<p>Frame_2=pd.DataFrame({‘schluessel’:[‘Nr.1’,’Nr.1’,’Nr.3’,’Nr.3’,’Nr.5’],’values_1’:[1,2,2,8,9],’values_2’:[4,7,9,0,1]})</p>
<p>Frame_1<br>Out[28]:<br>  schluessel  values_1  values_2<br>0       Nr.1         1         4<br>1       Nr.2         2         7<br>2       Nr.3         2         9<br>3       Nr.4         8         0<br>4       Nr.5         9         1</p>
<p>Frame_2<br>Out[29]:<br>  schluessel  values_1  values_2<br>0       Nr.1         1         4<br>1       Nr.1         2         7<br>2       Nr.3         2         9<br>3       Nr.3         8         0<br>4       Nr.5         9         1</p>
<p>pd.merge(Frame_2,Frame_1,on=’schluessel’)<br>Out[27]:<br>  schluessel  values_1_x  values_2_x  values_1_y  values_2_y<br>0       Nr.1           1           4           1           4<br>1       Nr.1           2           7           1           4<br>2       Nr.3           2           9           2           9<br>3       Nr.3           8           0           2           9<br>4       Nr.5           9           1           9           1</p>
<p>多对一（重复键列对不重复键列）内连接注意事项<br>每次合并前都要指定作为键的列，比如上面我们指定“schluessel”<br>作为键的列如有重复键要保留，不要去重，看另一个待合并数据框有没有这个键，如果有，把相同的键合并到一起。<br>如果某些键在各自数据框中都没有重复现象，但是两个数据框相比这些键相同，那么要把两个键合并到一起。<br>如果某些键在各自数据框中都没有重复现象，且两个数据框相比这些键也不相同，那么这些键将不出现在合并后的键列里</p>
<p>注意：默认是内连接</p>
<p>如果键名不同，我们可以通过关键字“left_on”和“right_on”分别安置：<br>frame_2=pd.DataFrame({‘rkey’:[‘a’,’c’,’d’],’data_2’:[3,2,1]})</p>
<p>frame_1=pd.DataFrame({‘lkey’:[‘a’,’a’,’c’,’a’,’b’,’c’,’d’,’b’],’data_1’:[2,6,9,3,2,1,0,5]})</p>
<p>pd.merge(frame_1,frame_2,left_on=’lkey’,right_on=’rkey’)<br>Out[5]:<br>   data_1 lkey  data_2 rkey<br>0       2    a       3    a<br>1       6    a       3    a<br>2       3    a       3    a<br>3       9    c       2    c<br>4       1    c       2    c<br>5       0    d       1    d</p>
<p>在默认的条件下，合并后的键是两个数据框的键的交集，例如上面的例子均是如此，这种键的合并方式被称作内连接，除了内连接还有左连接、右连接以及外连接。下满我们分别比较这几种方式：</p>
<p>frame_2=pd.DataFrame({‘sl’:[‘a’,’c’,’d’,’f’],’data_2’:[3,2,1,7]})<br>frame_1=pd.DataFrame({‘sl’:[‘a’,’a’,’c’,’a’,’b’,’c’,’d’,’b’],’data_1’:[2,6,9,3,2,1,0,5]})</p>
<p>pd.merge(frame_1,frame_2,on=’sl’,how=’inner’)<br>Out[16]:<br>   data_1 sl  data_2<br>0       2  a       3<br>1       6  a       3<br>2       3  a       3<br>3       9  c       2<br>4       1  c       2<br>5       0  d       1</p>
<p>#内连接是键的交集<br>pd.merge(frame_1,frame_2,on=’sl’,how=’left’)<br>Out[17]:<br>   data_1 sl  data_2<br>0       2  a     3.0<br>1       6  a     3.0<br>2       9  c     2.0<br>3       3  a     3.0<br>4       2  b     NaN<br>5       1  c     2.0<br>6       0  d     1.0<br>7       5  b     NaN</p>
<p>pd.merge(frame_2,frame_1,on=’sl’,how=’left’)<br>Out[6]:<br>   data_2 sl  data_1<br>0       3  a     2.0<br>1       3  a     6.0<br>2       3  a     3.0<br>3       2  c     9.0<br>4       2  c     1.0<br>5       1  d     0.0<br>6       7  f     NaN</p>
<p>#左连接以第一个数据框的键列为合并参考键列，观察参考键列中的键是否在两个待合并数据框中有重复，有则在合并时也要重复。<br>pd.merge(frame_1,frame_2,on=’sl’,how=’right’)<br>Out[18]:<br>   data_1 sl  data_2<br>0     2.0  a       3<br>1     6.0  a       3<br>2     3.0  a       3<br>3     9.0  c       2<br>4     1.0  c       2<br>5     0.0  d       1<br>6     NaN  f       7</p>
<p>#右连接以第二个数据框的键列为合并参考键列，观察参考键列中的键是否在两个待合并数据框中有重复，有则在合并时也要重复。</p>
<p>pd.merge(frame_1,frame_2,on=’sl’)<br>Out[19]:<br>   data_1 sl  data_2<br>0       2  a       3<br>1       6  a       3<br>2       3  a       3<br>3       9  c       2<br>4       1  c       2<br>5       0  d       1</p>
<p>我们可以看到，“how=’inner’”和how在缺失的状况下运行结果是一样的。</p>
<p>下面我们讨论多对多键的操作：</p>
<p>内连接<br>frame_2=pd.DataFrame({‘sl’:[‘a’,’c’,’a’,’d’,’f’],’data_2’:[3,2,1,7,6]})<br>frame_1=pd.DataFrame({‘sl’:[‘a’,’a’,’c’,’a’,’b’,’c’,’d’,’b’],’data_1’:[2,6,9,3,2,1,0,5]})</p>
<p>pd.merge(frame_2,frame_1,on=’sl’)<br>Out[9]:<br>   data_2 sl  data_1<br>0       3  a       2<br>1       3  a       6<br>2       3  a       3<br>3       1  a       2<br>4       1  a       6<br>5       1  a       3<br>6       2  c       9<br>7       2  c       1<br>8       7  d       0</p>
<p>多对多键合并注意事项（内连接）:</p>
<p>#指定合并键列</p>
<p>#作为键的列如有重复键要保留，不要去重，看另一个待合并数据框有没有这个键，如果有，把相同的键合并到一起并重复。如果在一个数据框重复的键在另一个数据框中也重复出现，那么合并后它重复的次数符合笛卡尔积。<br>如果某些键在各自数据框中都没有重复现象，但是两个数据框相比这些键相同，那么要把两个键合并到一起。<br>如果某些键在各自数据框中都没有重复现象，且两个数据框相比这些键也不相同，那么这些键将不出现在合并后的键列里</p>
<p>frame_2<br>Out[17]:<br>   data_2 sl<br>0       3  a<br>1       2  c<br>2       1  a<br>3       7  d<br>4       6  f</p>
<p>frame_1<br>Out[18]:<br>   data_1 sl<br>0       2  a<br>1       6  a<br>2       9  c<br>3       3  a<br>4       2  b<br>5       1  c<br>6       0  d<br>7       5  b</p>
<p>pd.merge(frame_2,frame_1,on=’sl’,how=’left’)<br>Out[16]:<br>   data_2 sl  data_1<br>0       3  a     2.0<br>1       3  a     6.0<br>2       3  a     3.0<br>3       2  c     9.0<br>4       2  c     1.0<br>5       1  a     2.0<br>6       1  a     6.0<br>7       1  a     3.0<br>8       7  d     0.0<br>9       6  f     NaN</p>
<p>#左连接以第一个数据框的键列为合并参考键列，观察参考键列中的键是否在两个待合并数据框中有重复，有则在合并时也要重复。<br>如果在一个数据框重复的键在另一个数据框中也重复出现，那么合并后它重复的次数符合笛卡尔积。<br>pd.merge(frame_2,frame_1,on=’sl’,how=’right’)<br>Out[19]:<br>    data_2 sl  data_1<br>0      3.0  a       2<br>1      1.0  a       2<br>2      3.0  a       6<br>3      1.0  a       6<br>4      3.0  a       3<br>5      1.0  a       3<br>6      2.0  c       9<br>7      2.0  c       1<br>8      7.0  d       0<br>9      NaN  b       2<br>10     NaN  b       5</p>
<p>#右连接以第二个数据框的键列为合并参考键列，观察参考键列中的键是否在两个待合并数据框中有重复，有则在合并时也要重复。<br>如果在一个数据框重复的键在另一个数据框中也重复出现，那么合并后它重复的次数符合笛卡尔积。</p>
<p>外连接组合了左连接和右连接的效果。<br>pd.merge(frame_2,frame_1,on=’sl’,how=’outer’)<br>Out[20]:<br>    data_2 sl  data_1<br>0      3.0  a     2.0<br>1      3.0  a     6.0<br>2      3.0  a     3.0<br>3      1.0  a     2.0<br>4      1.0  a     6.0<br>5      1.0  a     3.0<br>6      2.0  c     9.0<br>7      2.0  c     1.0<br>8      7.0  d     0.0<br>9      6.0  f     NaN<br>10     NaN  b     2.0<br>11     NaN  b     5.0</p>
<p>通过上面的一系列例子，我们发现，两个数据框的键列如果不完全一致，或者键不唯一，将是一件很麻烦的事，因此，我建议同学们除非万不得已还是应该给两个待合并的数据框设置带有唯一键的完全相同的键列。</p>
<p>frame_2=pd.DataFrame({‘sl’:[‘a’,’b’,’c’,’d’,’f’],’data_2’:[3,2,1,7,6]})<br>frame_1=pd.DataFrame({‘sl’:[‘a’,’b’,’c’,’d’,’f’],’data_1’:[2,6,9,3,2]})</p>
<p>pd.merge(frame_2,frame_1,on=’sl’)<br>Out[22]:<br>   data_2 sl  data_1<br>0       3  a       2<br>1       2  b       6<br>2       1  c       9<br>3       7  d       3<br>4       6  f       2</p>
<p>indirect_Data=pd.merge(frame_2,frame_1,on=’sl’)<br>indirect_Data.reindex(columns=[‘data_1’,’data_2’,’sl’])<br>Out[27]:<br>   data_1  data_2 sl<br>0       2       3  a<br>1       6       2  b<br>2       9       1  c<br>3       3       7  d<br>4       2       6  f</p>
<p>带有双关键字列的数据框的合并</p>
<p>frame_doppel_1=pd.DataFrame({‘key1’:list(‘wwerrt’),’key2’:list(‘onoodn’),’data1’:[1,9,76,9,5,6]})</p>
<p>frame_doppel_2=pd.DataFrame({‘key1’:list(‘wwt’),’key2’:list(‘onn’),’data1’:[1,9,76]})</p>
<p>frame_doppel_2<br>Out[31]:<br>   data1 key1 key2<br>0      1    w    o<br>1      9    w    n<br>2     76    t    n</p>
<p>frame_doppel_1<br>Out[32]:<br>   data1 key1 key2<br>0      1    w    o<br>1      9    w    n<br>2     76    e    o<br>3      9    r    o<br>4      5    r    d<br>5      6    t    n</p>
<p>pd.merge(frame_doppel_1,frame_doppel_2,on=[‘key1’,’key2’],how=’outer’)<br>Out[33]:<br>   data1_x key1 key2  data1_y<br>0        1    w    o      1.0<br>1        9    w    n      9.0<br>2       76    e    o      NaN<br>3        9    r    o      NaN<br>4        5    r    d      NaN<br>5        6    t    n     76.0</p>
<p>pd.merge(frame_doppel_1,frame_doppel_2,on=[‘key1’,’key2’],how=’left’)<br>Out[34]:<br>   data1_x key1 key2  data1_y<br>0        1    w    o      1.0<br>1        9    w    n      9.0<br>2       76    e    o      NaN<br>3        9    r    o      NaN<br>4        5    r    d      NaN<br>5        6    t    n     76.0</p>
<p>pd.merge(frame_doppel_1,frame_doppel_2,on=[‘key1’,’key2’],how=’right’)<br>Out[35]:<br>   data1_x key1 key2  data1_y<br>0        1    w    o        1<br>1        9    w    n        9<br>2        6    t    n       76</p>
<p>pd.merge(frame_doppel_1,frame_doppel_2,on=[‘key1’,’key2’])<br>Out[36]:<br>   data1_x key1 key2  data1_y<br>0        1    w    o        1<br>1        9    w    n        9<br>2        6    t    n       76</p>
<p>#对于有双从键列的数据框，我们先在各自的数据框内把两个键列组成一个元组序列键列，然后按单键列处理即可。（当然，真正的实际原理并非如此）。</p>
<p>frame_123=pd.DataFrame({‘data’:[1,2,8,9,0,98,6,56],’key1’:list(‘aacabbed’)})<br>frame_456=pd.DataFrame({‘data_1’:[2,67,1]},index=[‘a’,’b’,’e’])</p>
<p>frame_456<br>Out[39]:<br>   data_1<br>a       2<br>b      67<br>e       1</p>
<p>pd.merge(frame_123,frame_456,left_on=’key1’,right_index=True)<br>Out[40]:<br>   data key1  data_1<br>0     1    a       2<br>1     2    a       2<br>3     9    a       2<br>4     0    b      67<br>5    98    b      67<br>6     6    e       1</p>
<p>pd.merge(frame_123,frame_456,left_on=’key1’,right_index=True,how=’outer’)<br>Out[42]:<br>   data key1  data_1<br>0     1    a     2.0<br>1     2    a     2.0<br>3     9    a     2.0<br>2     8    c     NaN<br>4     0    b    67.0<br>5    98    b    67.0<br>6     6    e     1.0<br>7    56    d     NaN</p>
<p>firstar_1=pd.DataFrame({‘sl_1’:[‘doctor’,’doctor’,’doctor’,’patient’,’patient’],’sl_2’:[4,2,4,20,32],’data_1’:np.arange(5)})<br>secondar_1=pd.DataFrame(np.arange(12).reshape(4,3),index=[[‘doctor’,’doctor’,’patient’,’patient’],[4,4,20,20]],columns=[‘day’,’hour’,’second’])</p>
<p>secondar_1<br>Out[7]:<br>            day  hour  second<br>doctor  4     0     1       2<br>        4     3     4       5<br>patient 20    6     7       8<br>        20    9    10      11</p>
<p>firstar_1<br>Out[8]:<br>   data_1     sl_1  sl_2<br>0       0   doctor     4<br>1       1   doctor     2<br>2       2   doctor     4<br>3       3  patient    20<br>4       4  patient    32</p>
<p>combination_1=pd.merge(firstar_1,secondar_1,left_on=[‘sl_1’,’sl_2’],right_index=True)</p>
<p>combination_1<br>Out[11]:<br>   data_1     sl_1  sl_2  day  hour  second<br>0       0   doctor     4    0     1       2<br>0       0   doctor     4    3     4       5<br>2       2   doctor     4    0     1       2<br>2       2   doctor     4    3     4       5<br>3       3  patient    20    6     7       8<br>3       3  patient    20    9    10      11</p>
<p>对于带有重索引的数据框合并时，要先把重索引组成一个个元组对，然后与另一个数据框两个键列组成的元组队进行比较。上面的例子选出两个数据框元组对的交集。多对多相同时符合笛卡尔积</p>
<p>#下面是左连接效果<br>pd.merge(firstar_1,secondar_1,left_on=[‘sl_1’,’sl_2’],right_index=True,how=’left’)<br>Out[5]:<br>   data_1     sl_1  sl_2  day  hour  second<br>0       0   doctor     4  0.0   1.0     2.0<br>0       0   doctor     4  3.0   4.0     5.0<br>1       1   doctor     2  NaN   NaN     NaN<br>2       2   doctor     4  0.0   1.0     2.0<br>2       2   doctor     4  3.0   4.0     5.0<br>3       3  patient    20  6.0   7.0     8.0<br>3       3  patient    20  9.0  10.0    11.0<br>4       4  patient    32  NaN   NaN     NaN</p>
<p>#下面是左连接效果<br>pd.merge(firstar_1,secondar_1,left_on=[‘sl_1’,’sl_2’],right_index=True,how=’right’)<br>Out[6]:<br>   data_1     sl_1  sl_2  day  hour  second<br>0       0   doctor     4    0     1       2<br>2       2   doctor     4    0     1       2<br>0       0   doctor     4    3     4       5<br>2       2   doctor     4    3     4       5<br>3       3  patient    20    6     7       8<br>3       3  patient    20    9    10      11</p>
<p>#下面是外连接效果<br>pd.merge(firstar_1,secondar_1,left_on=[‘sl_1’,’sl_2’],right_index=True,how=’outer’)<br>Out[7]:<br>   data_1     sl_1  sl_2  day  hour  second<br>0       0   doctor     4  0.0   1.0     2.0<br>0       0   doctor     4  3.0   4.0     5.0<br>2       2   doctor     4  0.0   1.0     2.0<br>2       2   doctor     4  3.0   4.0     5.0<br>1       1   doctor     2  NaN   NaN     NaN<br>3       3  patient    20  6.0   7.0     8.0<br>3       3  patient    20  9.0  10.0    11.0<br>4       4  patient    32  NaN   NaN     NaN</p>
<p>同时使用双方索引直接合并也是没有问题的，我们看下例：</p>
<p>DataFrame_1=pd.DataFrame([[2,89,0],[7,9,34],[3.4,5.6,8.9]],index=[‘a’,’g’,’f’],columns=[‘c1’,’c2’,’c3’])</p>
<p>DataFrame_2=pd.DataFrame([[8,45,6,8],[4,5,3,23],[45,90,6.9,0.7],[4.5,6.8,9.2,7.6]],index=[‘g’,’g’,’a’,’f’],columns=[‘c3’,’c4’,’c5’,’c6’])<br>pd.merge(DataFrame_1,DataFrame_2,left_index=True,right_index=True,how=’outer’)<br>Out[12]:<br>    c1    c2  c3_x  c3_y    c4   c5    c6<br>a  2.0  89.0   0.0  45.0  90.0  6.9   0.7<br>f  3.4   5.6   8.9   4.5   6.8  9.2   7.6<br>g  7.0   9.0  34.0   8.0  45.0  6.0   8.0<br>g  7.0   9.0  34.0   4.0   5.0  3.0  23.0</p>
<p>下面我们用join函数来实现按索引的合并：<br>利用join按行索引合并时，两个待合并数据框的列索引不能有重复。<br>DataFrame_2_cor=pd.DataFrame([[8,45,6,8],[4,5,3,23],[45,90,6.9,0.7],[4.5,6.8,9.2,7.6]],index=[‘g’,’g’,’c’,’b’],columns=[‘c7’,’c4’,’c5’,’c6’])</p>
<p>DataFrame_2_cor<br>Out[22]:<br>     c7    c4   c5    c6<br>g   8.0  45.0  6.0   8.0<br>g   4.0   5.0  3.0  23.0<br>c  45.0  90.0  6.9   0.7<br>b   4.5   6.8  9.2   7.6</p>
<p>DataFrame_1<br>Out[23]:<br>    c1    c2    c3<br>a  2.0  89.0   0.0<br>g  7.0   9.0  34.0<br>f  3.4   5.6   8.9</p>
<p>DataFrame_2_cor.join(DataFrame_1)<br>Out[24]:<br>     c7    c4   c5    c6   c1   c2    c3<br>b   4.5   6.8  9.2   7.6  NaN  NaN   NaN<br>c  45.0  90.0  6.9   0.7  NaN  NaN   NaN<br>g   8.0  45.0  6.0   8.0  7.0  9.0  34.0<br>g   4.0   5.0  3.0  23.0  7.0  9.0  34.0</p>
<p>DataFrame_2_cor.join(DataFrame_1,how=’inner’)<br>Out[25]:<br>    c7    c4   c5    c6   c1   c2    c3<br>g  8.0  45.0  6.0   8.0  7.0  9.0  34.0<br>g  4.0   5.0  3.0  23.0  7.0  9.0  34.0</p>
<p>DataFrame_2_cor.join(DataFrame_1,how=’right’)<br>Out[26]:<br>    c7    c4   c5    c6   c1    c2    c3<br>a  NaN   NaN  NaN   NaN  2.0  89.0   0.0<br>f  NaN   NaN  NaN   NaN  3.4   5.6   8.9<br>g  8.0  45.0  6.0   8.0  7.0   9.0  34.0<br>g  4.0   5.0  3.0  23.0  7.0   9.0  34.0</p>
<p>DataFrame_2_cor.join(DataFrame_1,how=’outer’)<br>Out[28]:<br>     c7    c4   c5    c6   c1    c2    c3<br>a   NaN   NaN  NaN   NaN  2.0  89.0   0.0<br>b   4.5   6.8  9.2   7.6  NaN   NaN   NaN<br>c  45.0  90.0  6.9   0.7  NaN   NaN   NaN<br>f   NaN   NaN  NaN   NaN  3.4   5.6   8.9<br>g   8.0  45.0  6.0   8.0  7.0   9.0  34.0<br>g   4.0   5.0  3.0  23.0  7.0   9.0  34.0</p>
<p>#其运行结果完全按照merge.只不过默认是how=’left’。</p>
<p>轴向连接<br>在Numpy阶段我们讲过数组的合并，比如我们用concatenate连接两个数组：</p>
<p>arr=np.arange(12).reshape(3,4)</p>
<p>arr<br>Out[3]:<br>array([[ 0,  1,  2,  3],<br>       [ 4,  5,  6,  7],<br>       [ 8,  9, 10, 11]])</p>
<p>np.concatenate((arr,arr),axis=0)<br>Out[4]:<br>array([[ 0,  1,  2,  3],<br>       [ 4,  5,  6,  7],<br>       [ 8,  9, 10, 11],<br>       [ 0,  1,  2,  3],<br>       [ 4,  5,  6,  7],<br>       [ 8,  9, 10, 11]])</p>
<p>np.concatenate((arr,arr),axis=1)<br>Out[5]:<br>array([[ 0,  1,  2,  3,  0,  1,  2,  3],<br>       [ 4,  5,  6,  7,  4,  5,  6,  7],<br>       [ 8,  9, 10, 11,  8,  9, 10, 11]])</p>
<p>接下来我们看一下两个或两个以上的Series是如何合并的。<br>ps_1=pd.Series([0,1],index=[‘a’,’b’])</p>
<p>ps_2=pd.Series([6,2],index=[‘c’,’d’])</p>
<p>ps_3=pd.Series([7,0,8,6,0],index=[‘e’,’f’,’g’,’h’,’k’])</p>
<p>pd.concat((ps_1,ps_2,ps_3))<br>Out[10]:<br>a    0<br>b    1<br>c    6<br>d    2<br>e    7<br>f    0<br>g    8<br>h    6<br>k    0<br>dtype: int64<br>接下来我们使 ’axis=1’，看看结果如何（注意：通常情况下，对于Series的轴是不允许其为“1”的）：<br>pd.concat((ps_1,ps_2,ps_3),axis=1)<br>Out[11]:<br>     0    1    2<br>a  0.0  NaN  NaN<br>b  1.0  NaN  NaN<br>c  NaN  6.0  NaN<br>d  NaN  2.0  NaN<br>e  NaN  NaN  7.0<br>f  NaN  NaN  0.0<br>g  NaN  NaN  8.0<br>h  NaN  NaN  6.0<br>k  NaN  NaN  0.0</p>
<p>pd.concat((ps_2,ps_4))<br>Out[16]:<br>c    6<br>d    2<br>a    0<br>b    8<br>c    6<br>d    2<br>dtype: int64</p>
<p>pd.concat((ps_2,ps_4),axis=1)<br>Out[17]:<br>     0  1<br>a  NaN  0<br>b  NaN  8<br>c  6.0  6<br>d  2.0  2</p>
<p>pd.concat((ps_2,ps_4),axis=1,join=’inner’)<br>Out[18]:<br>   0  1<br>c  6  6<br>d  2  2</p>
<p>我们还可以给合并后的Series指定索引名：<br>pd.concat((ps_2,ps_4),axis=1,join_axes=[[‘a’,’b’,’u’,’v’]])<br>Out[19]:<br>    0    1<br>a NaN  0.0<br>b NaN  8.0<br>u NaN  NaN<br>v NaN  NaN</p>
<p>通过key参数可以实现把Series合并成层次化Series：<br>pd.concat((ps_1,ps_2*3,ps_3),keys=[‘ein’,’drei’,’fuenf’])<br>Out[6]:<br>ein    a     0<br>       b     1<br>drei   c    18<br>       d     6<br>fuenf  e     7<br>       f     0<br>       g     8<br>       h     6<br>       k     0<br>dtype: int64<br>这里如果我们添加axis=1，结果会如何呢？<br>  ein  drei  fuenf<br>a  0.0   NaN    NaN<br>b  1.0   NaN    NaN<br>c  NaN  18.0    NaN<br>d  NaN   6.0    NaN<br>e  NaN   NaN    7.0<br>f  NaN   NaN    0.0<br>g  NaN   NaN    8.0<br>h  NaN   NaN    6.0<br>k  NaN   NaN    0.0</p>
<p>我们发现此时原本Series的外层索引变成列索引。</p>
<p>对于有重复行索引的Series合并后结果会如何呢？</p>
<p>很不幸，无法运行！</p>
<p>到此，我们需要做一个用concat合并Series的总结：<br>总结：<br>待合并的每个series中不能出现重复的行标签<br>对于axis=0，Series合并后的效果是行标签的直接组合。<br>对于axis=1，要视合并模式而定，默认合并模式join=“outer”，按照单个的Seires，一列一列地排下去，join=“inner”模式意味着在outer模式的基础上只保留个Series共有部分，其他全删除。</p>
<p>下面我们把contact应用到数据框</p>
<p>import pandas as pd<br>import numpy as np<br>frame_1=pd.DataFrame([[1000,2000],[279,1123],[721,877]],index=[‘salary’,’expenditure’,’surplus’],columns=[‘mattias’,’jennifer’])<br>frame_2=pd.DataFrame(np.array([[1000,2000,3000],[279,1123,2000],[721,877,1000]])*12,index=[‘salary per annum’,’expenditure per annum’,’surplus per annum’],columns=[‘mattias’,’jennifer’,’jocker’])</p>
<p>pd.concat((frame_1,frame_2),axis=1,keys=[‘works’,’boss’])<br>Out[2]:<br>                        works              boss<br>                      mattias jennifer  mattias jennifer   jocker<br>expenditure             279.0   1123.0      NaN      NaN      NaN<br>expenditure per annum     NaN      NaN   3348.0  13476.0  24000.0<br>salary                 1000.0   2000.0      NaN      NaN      NaN<br>salary per annum          NaN      NaN  12000.0  24000.0  36000.0<br>surplus                 721.0    877.0      NaN      NaN      NaN<br>surplus per annum         NaN      NaN   8652.0  10524.0  12000.0</p>
<p>pd.concat((frame_1,frame_2),axis=0,keys=[‘works’,’boss’])<br>Out[3]:<br>                             jennifer   jocker  mattias<br>works salary                     2000      NaN     1000<br>      expenditure                1123      NaN      279<br>      surplus                     877      NaN      721<br>boss  salary per annum          24000  36000.0    12000<br>      expenditure per annum     13476  24000.0     3348<br>      surplus per annum         10524  12000.0     8652<br>视axis=1和 =0 而定，Keys可以给数据框的列或行加上重索引</p>
<p>Keys所实现的功能也可以通过字典来实现：</p>
<p>pd.concat({‘works’:frame_1,’boss’:frame_2},axis=1)<br>Out[6]:<br>                          boss                     works<br>                       mattias jennifer   jocker mattias jennifer<br>expenditure                NaN      NaN      NaN   279.0   1123.0<br>expenditure per annum   3348.0  13476.0  24000.0     NaN      NaN<br>salary                     NaN      NaN      NaN  1000.0   2000.0<br>salary per annum       12000.0  24000.0  36000.0     NaN      NaN<br>surplus                    NaN      NaN      NaN   721.0    877.0<br>surplus per annum       8652.0  10524.0  12000.0     NaN      NaN<br>通过names可以给重索引进行命名：<br>pd.concat({‘works’:frame_1,’boss’:frame_2},axis=1,names=[‘Identity’,’Names’])<br>Out[7]:<br>Identity                  boss                     works<br>Names                  mattias jennifer   jocker mattias jennifer<br>expenditure                NaN      NaN      NaN   279.0   1123.0<br>expenditure per annum   3348.0  13476.0  24000.0     NaN      NaN<br>salary                     NaN      NaN      NaN  1000.0   2000.0<br>salary per annum       12000.0  24000.0  36000.0     NaN      NaN<br>surplus                    NaN      NaN      NaN   721.0    877.0<br>surplus per annum       8652.0  10524.0  12000.0     NaN      NaN</p>
<p>通过Ignore_index=Ture可以把自行设置的行或列索引转变成python自动配置的行或列索引。<br>pd.concat((frame_1,frame_2),axis=0,ignore_index=True)<br>Out[11]:<br>   jennifer   jocker  mattias<br>0      2000      NaN     1000<br>1      1123      NaN      279<br>2       877      NaN      721<br>3     24000  36000.0    12000<br>4     13476  24000.0     3348<br>5     10524  12000.0     8652</p>
<p>pd.concat((frame_1,frame_2),axis=1,ignore_index=True)<br>Out[12]:<br>                            0       1        2        3        4<br>expenditure             279.0  1123.0      NaN      NaN      NaN<br>expenditure per annum     NaN     NaN   3348.0  13476.0  24000.0<br>salary                 1000.0  2000.0      NaN      NaN      NaN<br>salary per annum          NaN     NaN  12000.0  24000.0  36000.0<br>surplus                 721.0   877.0      NaN      NaN      NaN<br>surplus per annum         NaN     NaN   8652.0  10524.0  12000.0</p>
<p>如果axis=1，那么join是按0轴操作；同理，axis=1，那么join是按1轴操作。</p>
<p>pd.concat((frame_1,frame_2),axis=1,join=’outer’)<br>Out[16]:<br>                       mattias  jennifer  mattias  jennifer   jocker<br>expenditure              279.0    1123.0      NaN       NaN      NaN<br>expenditure per annum      NaN       NaN   3348.0   13476.0  24000.0<br>salary                  1000.0    2000.0      NaN       NaN      NaN<br>salary per annum           NaN       NaN  12000.0   24000.0  36000.0<br>surplus                  721.0     877.0      NaN       NaN      NaN<br>surplus per annum          NaN       NaN   8652.0   10524.0  12000.0</p>
<p>pd.concat((frame_1,frame_2),axis=0,join=’inner’)<br>Out[17]:<br>                       mattias  jennifer<br>salary                    1000      2000<br>expenditure                279      1123<br>surplus                    721       877<br>salary per annum         12000     24000<br>expenditure per annum     3348     13476<br>surplus per annum         8652     10524</p>
<p>ag_1=pd.DataFrame(np.floor(np.random.randn(4,4)),index=list(‘gfas’),columns=list(‘xcvb’))</p>
<p>ag_2=pd.DataFrame(np.floor(np.random.randint(2,8,12).reshape(4,3)),index=[‘xu’,’de’,’jiang’,’hu’],columns=list(‘xvb’))</p>
<p>pd.concat((ag_1,ag_2),keys=[‘Gr’,’A’],axis=1,join_axes=[ag_1.index])</p>
<p>Out[64]:<br>    Gr                  A<br>     x    c    v    b   x   v   b<br>g -2.0  0.0  0.0  1.0 NaN NaN NaN<br>f -2.0 -1.0  0.0 -1.0 NaN NaN NaN<br>a -2.0  0.0  0.0  2.0 NaN NaN NaN<br>s  0.0 -2.0 -1.0 -2.0 NaN NaN NaN</p>
<p>ag_1=pd.DataFrame(np.floor(np.random.randn(4,4)),index=list(‘gfas’),columns=list(‘xcvb’))</p>
<p>ag_2=pd.DataFrame(np.floor(np.random.randint(2,8,12).reshape(4,3)),index=[‘xu’,’de’,’jiang’,’hu’],columns=list(‘xvb’))</p>
<p>pd.concat((ag_1,ag_2),keys=[‘Gr’,’A’],axis=1,join_axes=[ag_2.index])</p>
<p>Out[65]:<br>       Gr                A<br>        x   c   v   b    x    v    b<br>xu    NaN NaN NaN NaN  3.0  3.0  4.0<br>de    NaN NaN NaN NaN  5.0  2.0  6.0<br>jiang NaN NaN NaN NaN  7.0  2.0  3.0<br>hu    NaN NaN NaN NaN  7.0  2.0  6.0</p>
<p>ag_1=pd.DataFrame(np.floor(np.random.randn(4,4)),index=list(‘gfas’),columns=list(‘xcvb’))</p>
<p>ag_2=pd.DataFrame(np.floor(np.random.randint(2,8,12).reshape(4,3)),index=[‘xu’,’de’,’jiang’,’hu’],columns=list(‘xvb’))</p>
<p>pd.concat((ag_1,ag_2),keys=[‘Gr’,’A’],axis=0,join_axes=[ag_2.columns])</p>
<p>Out[67]:<br>            x    v    b<br>Gr g     -1.0 -1.0  0.0<br>   f     -1.0 -1.0 -1.0<br>   a      1.0 -1.0  1.0<br>   s      0.0 -1.0 -1.0<br>A  xu     7.0  3.0  3.0<br>   de     6.0  6.0  3.0<br>   jiang  2.0  7.0  5.0<br>   hu     7.0  5.0  7.0</p>
<p>ag_1=pd.DataFrame(np.floor(np.random.randn(4,4)),index=list(‘gfas’),columns=list(‘xcvb’))</p>
<p>ag_2=pd.DataFrame(np.floor(np.random.randint(2,8,12).reshape(4,3)),index=[‘xu’,’de’,’jiang’,’hu’],columns=list(‘xvb’))</p>
<p>pd.concat((ag_1,ag_2),keys=[‘Gr’,’A’],axis=0,join_axes=[ag_1.columns])</p>
<p>Out[68]:<br>            x    c    v    b<br>Gr g      0.0  0.0 -1.0  2.0<br>   f      0.0 -1.0  0.0 -2.0<br>   a      0.0 -1.0 -2.0  0.0<br>   s     -1.0 -2.0  0.0 -1.0<br>A  xu     2.0  NaN  2.0  6.0<br>   de     6.0  NaN  4.0  2.0<br>   jiang  7.0  NaN  6.0  5.0<br>   hu     4.0  NaN  7.0  6.0</p>
<p>Concat的参数详解：<br>objs    参与连接的pandas对象的列表或者元组或者字典。是唯一必需的参数。<br>axis    指明按那个轴进行连接，默认为0<br>join    指明连接方式，“inner”或”outer”，默认“outer”。指明其他轴向上的索引是按交集（“inner”）还是并集(“outer”)进行合并。<br>keys    与连接对象有关的值，用于形成连接轴向上的层次化索引。可以是任意值的列表或数组、元组数组、数组列表等。<br>join_axes    指定根据那个数据框的索引来对齐数据，这个索引不参与并\交运算<br>Names    给层索引每一层命名<br>Ignore_index    不保留连接轴上的索引，产生一组新索引</p>
<p>给数据打补丁：<br>尽管我们学习各种各种的数据合并与连接（merge、join、concatenation等）。然而他们都仅仅是对索引的直接处理。很难对Series或者数据框所包含的数据直接处理。<br>通过np.where,可以实现数值的补空合并。</p>
<p>ser_1=pd.Series([np.nan,45,np.nan,3.9,90,np.nan],index=list(‘abcdef’))<br>ser_2=pd.Series(np.arange(len(ser_1)),index=list(‘abcdef’))</p>
<p>ser_1<br>Out[73]:<br>a     NaN<br>b    45.0<br>c     NaN<br>d     3.9<br>e    90.0<br>f     NaN<br>dtype: float64</p>
<p>ser_2<br>Out[74]:<br>a    0<br>b    1<br>c    2<br>d    3<br>e    4<br>f    5<br>dtype: int32</p>
<p>pd.Series(np.where(pd.isnull(ser_1),ser_2,ser_1),index=list(‘abcdef’))<br>Out[79]:<br>a     0.0<br>b    45.0<br>c     2.0<br>d     3.9<br>e    90.0<br>f     5.0<br>dtype: float64</p>
<p>通过combine_first来实现相同功能：<br>ser_1.combine_first(ser_2)<br>Out[80]:<br>a     0.0<br>b    45.0<br>c     2.0<br>d     3.9<br>e    90.0<br>f     5.0<br>dtype: float64</p>
<p>#拿fra_2值补fra_1</p>
<p>fra_1=pd.DataFrame({‘a’:[1.,np.nan,5.,np.nan],’b’:[np.nan,2.,np.nan,6.],’c’:range(2,9,2)})</p>
<p>fra_2=pd.DataFrame({‘a’:[3,np.nan,6,3,8],’b’:[np.nan,np.nan,2,6,8]})</p>
<p>fra_1<br>Out[84]:<br>     a    b  c<br>0  1.0  NaN  2<br>1  NaN  2.0  4<br>2  5.0  NaN  6<br>3  NaN  6.0  8</p>
<p>fra_2<br>Out[85]:<br>     a    b<br>0  3.0  NaN<br>1  NaN  NaN<br>2  6.0  2.0<br>3  3.0  6.0<br>4  8.0  8.0</p>
<p>fra_1.combine_first(fra_2)<br>Out[86]:<br>     a    b    c<br>0  1.0  NaN  2.0<br>1  NaN  2.0  4.0<br>2  5.0  2.0  6.0<br>3  3.0  6.0  8.0<br>4  8.0  8.0  NaN</p>
<p>#拿fra_2值补fra_1<br>数据的重塑（仔细读注解额）<br>数据的重塑其本质就是对数据表格进行重排。所用到的函数通常被称为重塑函数或轴向旋转函数。</p>
<p>重塑层次化索引：</p>
<p>带有层次化索引数据框的重塑主要是通过以下函数来实现的：<br>Stack：将列索引旋转为行索引<br>Unstack：将行索引旋转为列索引<br>注意：以上只适合重索引数据框<br>我们先从一个简单的数据框开始。<br>data_1=pd.DataFrame(np.arange(6).reshape(2,3),index=pd.Index([‘XDL_1’,’XDL_2’],name=’education_group’),columns=pd.Index([‘one’,’three’,’two’],name=’nr.’))</p>
<p>data_1<br>Out[4]:<br>nr.              one  three  two<br>education_group<br>XDL_1              0      1    2<br>XDL_2              3      4    5</p>
<p>data_1.stack()<br>Out[5]:<br>education_group  nr.<br>XDL_1            one      0<br>                 three    1<br>                 two      2<br>XDL_2            one      3<br>                 three    4<br>                 two      5<br>dtype: int32</p>
<p>#对于没有重索引的数据框，用stack方法会实现列索引成为内层行索引<br>data_1.unstack()<br>Out[14]:<br>nr.    education_group<br>one    XDL_1              0<br>       XDL_2              3<br>three  XDL_1              1<br>       XDL_2              4<br>two    XDL_1              2<br>       XDL_2              5<br>dtype: int32</p>
<p>#对于没有重索引的数据框，用unstack方法会实现列索引成为外层行索引<br>data_double_ser=data_1.stack()</p>
<p>#生成重索引series<br>data_double_ser<br>Out[21]:<br>education_group  nr.<br>XDL_1            one      0<br>                 three    1<br>                 two      2<br>XDL_2            one      3<br>                 three    4<br>                 two      5<br>dtype: int32<br>data_double_ser.unstack()<br>Out[6]:<br>nr.              one  three  two<br>education_group<br>XDL_1              0      1    2<br>XDL_2              3      4    5</p>
<p>#通过unstack把内层行索引旋转到列索引，默认旋转内层级行索引到列索引。<br>data_1.stack().unstack(0)<br>Out[10]:<br>education_group  XDL_1  XDL_2<br>nr.<br>one                  0      3<br>three                1      4<br>two                  2      5</p>
<p>#默认按内层索引操作，我们可以指定操作级别。这里的‘0’代表外层级别。也可以用外层索引名来代替‘0’<br>data_1.stack().unstack(‘education_group’)<br>Out[13]:<br>education_group  XDL_1  XDL_2<br>nr.<br>one                  0      3<br>three                1      4<br>two                  2      5</p>
<p>#指定重索引行外层级，并通过unstack把外层行索引旋转到列索引</p>
<p>s1=pd.Series([0,3,4,-3],index=list(‘abcd’))</p>
<p>s2=pd.Series([6,2,9],index=list(‘bca’))</p>
<p>data_ser_double=pd.concat((s1,s2),keys=[‘one’,’two’])</p>
<p>data_ser_double<br>Out[25]:<br>one  a    0<br>     b    3<br>     c    4<br>     d   -3<br>two  b    6<br>     c    2<br>     a    9<br>dtype: int64</p>
<p>data_ser_double.unstack()<br>Out[26]:<br>       a    b    c    d<br>one  0.0  3.0  4.0 -3.0<br>two  9.0  6.0  2.0  NaN</p>
<p>#如果S1和S2的索引数目不等或者数目相等但索引内容不同都会产生空值，因此我们要求S1和S2的索引必须完全一致，才能在运算中不产生空值！！</p>
<p>#产生空值的情况下，如果我们设置dropna=True（默认）那么上面的运算可逆。如果我们设置dropna=False的情况下，上面的运算时不可逆的。</p>
<p>data_ser_double.unstack().stack()<br>Out[28]:<br>one  a    0.0<br>     b    3.0<br>     c    4.0<br>     d   -3.0<br>two  a    9.0<br>     b    6.0<br>     c    2.0<br>dtype: float64</p>
<p>data_ser_double.unstack().stack(dropna=False)<br>Out[29]:<br>one  a    0.0<br>     b    3.0<br>     c    4.0<br>     d   -3.0<br>two  a    9.0<br>     b    6.0<br>     c    2.0<br>     d    NaN<br>dtype: float64</p>
<p>移除重复数据<br>data=pd.DataFrame({‘k1’:[‘one’]<em>3+[‘two’]</em>4,’k2’:[1,2,3,4,5,6,7]})</p>
<p>data<br>Out[8]:<br>    k1  k2<br>0  one   1<br>1  one   2<br>2  one   3<br>3  two   4<br>4  two   5<br>5  two   6<br>6  two   7</p>
<p>利用duplicated可以判断重复行</p>
<p>#默认k1和k2列必须都重复才算重复行<br>data.duplicated()<br>Out[5]:<br>0    False<br>1    False<br>2    False<br>3    False<br>4    False<br>5    False<br>6    False<br>dtype: bool</p>
<p>data=pd.DataFrame({‘k1’:[‘one’]<em>3+[‘two’]</em>4,’k2’:[2,3,2,3,6,6,6]})</p>
<p>data<br>Out[8]:<br>    k1  k2<br>0  one   2<br>1  one   3<br>2  one   2<br>3  two   3<br>4  two   6<br>5  two   6<br>6  two   6</p>
<p>data.duplicated()<br>Out[9]:<br>0    False<br>1    False<br>2     True<br>3    False<br>4    False<br>5     True<br>6     True<br>dtype: bool</p>
<p>#注意，原项不会判断为True，只有真正的重复项才会，如上例子，红色不会判断为True，蓝色才会判断为True。</p>
<p>#通过drop_duplicates方法，可以移除重复行，返回一个没有重复行的DataFrame。<br>data.drop_duplicates()<br>Out[11]:<br>    k1  k2<br>0  one   2<br>1  one   3<br>3  two   3<br>4  two   6</p>
<p>#可以指定某一列或几列中行重复就算重复行<br>data[‘k3’],data[‘k4’],data[‘k5’]=[0,0,0,0,3,6,7],[4,8,6,7,6,7,7],[2,2,4,4,6,6,8]<br>data<br>Out[20]:<br>    k1  k2  k3  k4  k5<br>0  one   2   0   4   2<br>1  one   3   0   8   2<br>2  one   2   0   6   4<br>3  two   3   0   7   4<br>4  two   6   3   6   6<br>5  two   6   6   7   6<br>6  two   6   7   7   8</p>
<p>data.drop_duplicates([‘k1’])<br>Out[22]:<br>    k1  k2  k3  k4  k5<br>0  one   2   0   4   2<br>3  two   3   0   7   4</p>
<p>data.drop_duplicates([‘k1’,’k2’])<br>Out[24]:<br>    k1  k2  k3  k4  k5<br>0  one   2   0   4   2<br>1  one   3   0   8   2<br>3  two   3   0   7   4<br>4  two   6   3   6   6</p>
<p>data<br>Out[27]:<br>    k1  k2  k3  k4  k5<br>0  one   2   0   4   2<br>1  one   3   0   8   2<br>2  one   2   0   6   4<br>3  two   3   0   7   4<br>4  two   6   3   6   6<br>5  two   6   6   7   6<br>6  two   6   7   7   8</p>
<p>data.drop_duplicates([‘k1’],keep=’last’)<br>Out[29]:<br>    k1  k2  k3  k4  k5<br>2  one   2   0   6   4<br>6  two   6   7   7   8</p>
<p>data.duplicated([‘k1’],keep=’last’)<br>Out[34]:<br>0     True<br>1     True<br>2    False<br>3     True<br>4     True<br>5     True<br>6    False<br>dtype: bool</p>
<p>#默认保留重复行的第一行，也可以设置保留最后一个，keep=’last’,删除其他所有重复行。</p>
<p>利用字典映射对数据进行操作：</p>
<p>data_1=pd.DataFrame({‘food’:[‘bacon火腿’,’sausage红肠’,’pulled pork手撕肉’,’sirloin牛里脊肉’,’beef jerky牛肉干’,’mutton shashlik烤羊肉串’,’Chicken salad鸡肉’,’Minced chicken鸡肉泥’,’dried squids鱿鱼干’,’dried fish鱼干’,’sausage红肠’,’bacon火腿’,’sausage红肠’,’bacon火腿’,’Chicken salad鸡肉’,’dried squids鱿鱼干’,’dried fish鱼干’,’mutton shashlik烤羊肉串’,’beef jerky牛肉干’,’Minced chicken鸡肉泥’,’pulled pork手撕肉’,’sirloin牛里脊肉’,’mutton shashlik烤羊肉串’,’Chicken salad鸡肉’,’Minced chicken鸡肉泥’,’bacon火腿’,’sausage红肠’,’pulled pork手撕肉’,’sirloin牛里脊肉’,’beef jerky牛肉干’,’mutton shashlik烤羊肉串’,’Chicken salad鸡肉’,’Minced chicken鸡肉泥’,’dried squids鱿鱼干’,’dried fish鱼干’,’sausage红肠’,’bacon火腿’,’sausage红肠’,’bacon火腿’,’Chicken salad鸡肉’,’dried squids鱿鱼干’,’dried fish鱼干’,’mutton shashlik烤羊肉串’,’beef jerky牛肉干’,’sirloin牛里脊肉’,’mutton shashlik烤羊肉串’,’Chicken salad鸡肉’,’Minced chicken鸡肉泥’],’weight(g)’:abs((np.random.normal(0,1,size=48)*24+10)).astype(np.int32)})</p>
<p>data_1<br>Out[4]:<br>                   food       weight(g)<br>0               bacon火腿         27<br>1             sausage红肠         41<br>2        pulled pork手撕肉         16<br>3           sirloin牛里脊肉         12<br>4         beef jerky牛肉干          0<br>5   mutton shashlik烤羊肉串          7<br>6       Chicken salad鸡肉         31<br>7     Minced chicken鸡肉泥         14<br>8       dried squids鱿鱼干          3<br>9          dried fish鱼干         38<br>10            sausage红肠         34<br>11              bacon火腿         12<br>12            sausage红肠          9<br>13              bacon火腿         23<br>14      Chicken salad鸡肉         34<br>15      dried squids鱿鱼干          1<br>16         dried fish鱼干         19<br>17  mutton shashlik烤羊肉串          6<br>18        beef jerky牛肉干          0<br>19    Minced chicken鸡肉泥          3<br>20       pulled pork手撕肉         22<br>21          sirloin牛里脊肉          9<br>22  mutton shashlik烤羊肉串         14<br>23      Chicken salad鸡肉         34<br>24    Minced chicken鸡肉泥         37<br>25              bacon火腿         27<br>26            sausage红肠          0<br>27       pulled pork手撕肉         32<br>28          sirloin牛里脊肉          0<br>29        beef jerky牛肉干         38<br>30  mutton shashlik烤羊肉串         16<br>31      Chicken salad鸡肉         35<br>32    Minced chicken鸡肉泥          4<br>33      dried squids鱿鱼干          5<br>34         dried fish鱼干          3<br>35            sausage红肠         26<br>36              bacon火腿         12<br>37            sausage红肠          2<br>38              bacon火腿          5<br>39      Chicken salad鸡肉         11<br>40      dried squids鱿鱼干          6<br>41         dried fish鱼干         24<br>42  mutton shashlik烤羊肉串         26<br>43        beef jerky牛肉干         56<br>44          sirloin牛里脊肉          2<br>45  mutton shashlik烤羊肉串          5<br>46      Chicken salad鸡肉         12<br>47    Minced chicken鸡肉泥         13</p>
<p>我们发现数据框‘food’列中有很多重复。如果我们给数据假如新列，并要求新列与food存在某种逻辑关系。例如给‘food’标记食材来源。<br>通过map函数可以很轻松实现上面的要求。</p>
<p>meat_source={‘bacon火腿’:’猪’,’sausage红肠’:’猪’,’pulled pork手撕肉’:’猪’,’sirloin牛里脊肉’:’牛’,’beef jerky牛肉干’:’牛’,’mutton shashlik烤羊肉串’:’羊’,’chicken salad鸡肉’:’鸡’,’minced chicken鸡肉泥’:’鸡’,’dried squids鱿鱼干’:’鱿鱼’,’dried fish鱼干’:’鱼’}<br>data_1[‘animal’]=data_1[‘food’].map(meat_source)<br>data_1<br>Out[25]:<br>                   food  weight(g)       animal<br>0               bacon火腿         17      猪<br>1             sausage红肠         38      猪<br>2        pulled pork手撕肉          3      猪<br>3           sirloin牛里脊肉         12      牛<br>4         beef jerky牛肉干         31      牛<br>5   mutton shashlik烤羊肉串         16      羊<br>6       Chicken salad鸡肉         17    NaN<br>7     Minced chicken鸡肉泥         14    NaN<br>8       dried squids鱿鱼干         12     鱿鱼<br>9          dried fish鱼干          7      鱼<br>10            sausage红肠         25      猪<br>11              bacon火腿         30      猪<br>12            sausage红肠         12      猪<br>13              bacon火腿         32      猪<br>14      Chicken salad鸡肉         10    NaN<br>15      dried squids鱿鱼干         42     鱿鱼<br>16         dried fish鱼干          8      鱼<br>17  mutton shashlik烤羊肉串         13      羊<br>18        beef jerky牛肉干         34      牛<br>19    Minced chicken鸡肉泥         44    NaN<br>20       pulled pork手撕肉          3      猪<br>21          sirloin牛里脊肉         25      牛<br>22  mutton shashlik烤羊肉串          4      羊<br>23      Chicken salad鸡肉         39    NaN<br>24    Minced chicken鸡肉泥         20    NaN<br>25              bacon火腿          5      猪<br>26            sausage红肠         38      猪<br>27       pulled pork手撕肉         28      猪<br>28          sirloin牛里脊肉         12      牛<br>29        beef jerky牛肉干         21      牛<br>30  mutton shashlik烤羊肉串         13      羊<br>31      Chicken salad鸡肉         13    NaN<br>32    Minced chicken鸡肉泥         26    NaN<br>33      dried squids鱿鱼干         43     鱿鱼<br>34         dried fish鱼干         14      鱼<br>35            sausage红肠         25      猪<br>36              bacon火腿         19      猪<br>37            sausage红肠         24      猪<br>38              bacon火腿         57      猪<br>39      Chicken salad鸡肉         25    NaN<br>40      dried squids鱿鱼干         38     鱿鱼<br>41         dried fish鱼干         18      鱼<br>42  mutton shashlik烤羊肉串         54      羊<br>43        beef jerky牛肉干         24      牛<br>44          sirloin牛里脊肉         15      牛<br>45  mutton shashlik烤羊肉串         17      羊<br>46      Chicken salad鸡肉         61    NaN<br>47    Minced chicken鸡肉泥         29    NaN</p>
<p>#map的实质是把一种映射法则应用到所制定的列上，比如这里的‘food’列。这种映射法则通常通过字典与函数来体现。</p>
<p>#map把与meat_source字典的键相对应food值一个个投影到字典建对应的值然后把投影结果赋值给data_1[‘animal’]</p>
<p>#我们发现‘animal’列有空值。原因是meat_source中的键与‘food’列值不完全一致。</p>
<p>#通过map(str.lower)把food列中的值的第一个字母转变成小写，这样meat_source中的键与‘food’列值完全一致，这样Nan会自动消失。</p>
<p>data_1[‘animal’]=data_1[‘food’].map(lambda x:meat_source[x.lower()])<br>或<br>data_1[‘animal’]=data_1[‘food’].map(str.lower).map(meat_source)</p>
<p>data_1<br>Out[23]:<br>                   food  weight(g)        animal<br>0               bacon火腿         17      猪<br>1             sausage红肠         38      猪<br>2        pulled pork手撕肉          3      猪<br>3           sirloin牛里脊肉         12      牛<br>4         beef jerky牛肉干         31      牛<br>5   mutton shashlik烤羊肉串         16      羊<br>6       Chicken salad鸡肉         17      鸡<br>7     Minced chicken鸡肉泥         14      鸡<br>8       dried squids鱿鱼干         12     鱿鱼<br>9          dried fish鱼干          7      鱼<br>10            sausage红肠         25      猪<br>11              bacon火腿         30      猪<br>12            sausage红肠         12      猪<br>13              bacon火腿         32      猪<br>14      Chicken salad鸡肉         10      鸡<br>15      dried squids鱿鱼干         42     鱿鱼<br>16         dried fish鱼干          8      鱼<br>17  mutton shashlik烤羊肉串         13      羊<br>18        beef jerky牛肉干         34      牛<br>19    Minced chicken鸡肉泥         44      鸡<br>20       pulled pork手撕肉          3      猪<br>21          sirloin牛里脊肉         25      牛<br>22  mutton shashlik烤羊肉串          4      羊<br>23      Chicken salad鸡肉         39      鸡<br>24    Minced chicken鸡肉泥         20      鸡<br>25              bacon火腿          5      猪<br>26            sausage红肠         38      猪<br>27       pulled pork手撕肉         28      猪<br>28          sirloin牛里脊肉         12      牛<br>29        beef jerky牛肉干         21      牛<br>30  mutton shashlik烤羊肉串         13      羊<br>31      Chicken salad鸡肉         13      鸡<br>32    Minced chicken鸡肉泥         26      鸡<br>33      dried squids鱿鱼干         43     鱿鱼<br>34         dried fish鱼干         14      鱼<br>35            sausage红肠         25      猪<br>36              bacon火腿         19      猪<br>37            sausage红肠         24      猪<br>38              bacon火腿         57      猪<br>39      Chicken salad鸡肉         25      鸡<br>40      dried squids鱿鱼干         38     鱿鱼<br>41         dried fish鱼干         18      鱼<br>42  mutton shashlik烤羊肉串         54      羊<br>43        beef jerky牛肉干         24      牛<br>44          sirloin牛里脊肉         15      牛<br>45  mutton shashlik烤羊肉串         17      羊<br>46      Chicken salad鸡肉         61      鸡<br>47    Minced chicken鸡肉泥         29      鸡</p>
<p>添加amount=weight<em>price项<br>product=[‘bacon火腿’,’sausage红肠’,’pulled pork手撕肉’,’sirloin牛里脊肉’,’beef jerky牛肉干’,’mutton shashlik烤羊肉串’,’Chicken salad鸡肉’,’Minced chicken鸡肉泥’,’dried squids鱿鱼干’,’dried fish鱼干’]<br>price=[1,1.5,2,2.3,2.6,3.6,2,2.6,8,9.9]<br>get_Data={k:p</em>i for i,k in zip(list(data_1[‘weight(g)’].values),range(len(data_1[‘weight(g)’]))) for m,p in zip(product,price) if m==data_1[‘food’][k]}<br>data_1[‘key’]=np.arange(48)<br>data_1[‘anount’]=data_1[‘key’].map(get_Data)<br>data_1.reindex(columns=[‘food’,’weight(g)’, ‘animal’,  ‘anount’,’key’  ])<br>Out[108]:<br>                   food        weight(g) animal  anount  key<br>0               bacon火腿         17      猪    17.0    0<br>1             sausage红肠         38      猪    57.0    1<br>2        pulled pork手撕肉          3      猪     6.0    2<br>3           sirloin牛里脊肉         12      牛    27.6    3<br>4         beef jerky牛肉干         31      牛    80.6    4<br>5   mutton shashlik烤羊肉串         16      羊    57.6    5<br>6       Chicken salad鸡肉         17      鸡    34.0    6<br>7     Minced chicken鸡肉泥         14      鸡    36.4    7<br>8       dried squids鱿鱼干         12     鱿鱼    96.0    8<br>9          dried fish鱼干          7      鱼    69.3    9<br>10            sausage红肠         25      猪    37.5   10<br>11              bacon火腿         30      猪    30.0   11<br>12            sausage红肠         12      猪    18.0   12<br>13              bacon火腿         32      猪    32.0   13<br>14      Chicken salad鸡肉         10      鸡    20.0   14<br>15      dried squids鱿鱼干         42     鱿鱼   336.0   15<br>16         dried fish鱼干          8      鱼    79.2   16<br>17  mutton shashlik烤羊肉串         13      羊    46.8   17<br>18        beef jerky牛肉干         34      牛    88.4   18<br>19    Minced chicken鸡肉泥         44      鸡   114.4   19<br>20       pulled pork手撕肉          3      猪     6.0   20<br>21          sirloin牛里脊肉         25      牛    57.5   21<br>22  mutton shashlik烤羊肉串          4      羊    14.4   22<br>23      Chicken salad鸡肉         39      鸡    78.0   23<br>24    Minced chicken鸡肉泥         20      鸡    52.0   24<br>25              bacon火腿          5      猪     5.0   25<br>26            sausage红肠         38      猪    57.0   26<br>27       pulled pork手撕肉         28      猪    56.0   27<br>28          sirloin牛里脊肉         12      牛    27.6   28<br>29        beef jerky牛肉干         21      牛    54.6   29<br>30  mutton shashlik烤羊肉串         13      羊    46.8   30<br>31      Chicken salad鸡肉         13      鸡    26.0   31<br>32    Minced chicken鸡肉泥         26      鸡    67.6   32<br>33      dried squids鱿鱼干         43     鱿鱼   344.0   33<br>34         dried fish鱼干         14      鱼   138.6   34<br>35            sausage红肠         25      猪    37.5   35<br>36              bacon火腿         19      猪    19.0   36<br>37            sausage红肠         24      猪    36.0   37<br>38              bacon火腿         57      猪    57.0   38<br>39      Chicken salad鸡肉         25      鸡    50.0   39<br>40      dried squids鱿鱼干         38     鱿鱼   304.0   40<br>41         dried fish鱼干         18      鱼   178.2   41<br>42  mutton shashlik烤羊肉串         54      羊   194.4   42<br>43        beef jerky牛肉干         24      牛    62.4   43<br>44          sirloin牛里脊肉         15      牛    34.5   44<br>45  mutton shashlik烤羊肉串         17      羊    61.2   45<br>46      Chicken salad鸡肉         61      鸡   122.0   46<br>47    Minced chicken鸡肉泥         29      鸡    75.4   47</p>
<p>灵活的替换值方法：<br>前面我们讲过fillna替换空值，还有notnull也可以实现空值替换。这里我们继续推荐一种另外的替换方法-replace法。这种方法不仅简单而且还十分灵活。</p>
<p>#把那些标记为空值的数据替换成pandas能够理解的Na值</p>
<p>data=pd.Series([1.,-999.,2.,999.,-1000.,3.])</p>
<p>data<br>Out[115]:<br>0       1.0<br>1    -999.0<br>2       2.0<br>3     999.0<br>4   -1000.0<br>5       3.0<br>dtype: float64</p>
<p>#假如-999是空值标记</p>
<p>data.replace(-999,np.nan)<br>Out[117]:<br>0       1.0<br>1       NaN<br>2       2.0<br>3     999.0<br>4   -1000.0<br>5       3.0<br>dtype: float64</p>
<p>#也可以一次替换多值</p>
<p>data.replace([-999,-1000],np.nan)<br>Out[119]:<br>0      1.0<br>1      NaN<br>2      2.0<br>3    999.0<br>4      NaN<br>5      3.0<br>dtype: float64<br>data_1=[23,’invalue’,34]<br>data_1=pd.Series(data_1)<br>data_1<br>Out[124]:<br>0         23<br>1    invalue<br>2         34<br>dtype: object<br>data_1.replace(‘invalue’,2)<br>Out[122]:<br>0    23<br>1     2<br>2    34<br>dtype: int64<br>data_1=pd.Series([1.,-999.,2.,-999.,-1000.,3.])</p>
<p>data_1<br>Out[129]:<br>0       1.0<br>1    -999.0<br>2       2.0<br>3    -999.0<br>4   -1000.0<br>5       3.0<br>dtype: float64<br>data_1.replace([-999,-1000],[np.nan,0])<br>Out[126]:<br>0    1.0<br>1    NaN<br>2    2.0<br>3    NaN<br>4    0.0<br>5    3.0<br>dtype: float64</p>
<p>#也可以表示成字典形式</p>
<p>data_1.replace({-999:np.nan,-1000:0})<br>Out[130]:<br>0    1.0<br>1    NaN<br>2    2.0<br>3    NaN<br>4    0.0<br>5    3.0<br>dtype: float64</p>
<p>重命名轴索引<br>data=pd.DataFrame(np.arange(12).reshape(3,4),index=[‘a’,’b’,’c’],columns=[‘c1’,’c2’,’c3’,’c4’])</p>
<p>#Map可以通过字典把要处理的列（键值）映射到对应的值，也可以通过函数把相应的列映射到对应的值</p>
<p>#通过str.upper函数map把行索引列映射到大写行索引列。</p>
<p>data.index=data.index.map(str.upper)</p>
<p>data<br>Out[137]:<br>   c1  c2  c3  c4<br>A   0   1   2   3<br>B   4   5   6   7<br>C   8   9  10  11</p>
<p>#可以用rename对轴索引进行修改<br>data.rename(index=str.title,columns=str.upper)<br>Out[5]:<br>   C1  C2  C3  C4<br>A   0   1   2   3<br>B   4   5   6   7<br>C   8   9  10  11</p>
<p>#rename通过字典的帮助可实现对部分标签的更新</p>
<p>data.rename(index={‘a’:’Sun’,’b’:’Zhang’,’c’:’Wang’},columns={‘c2’:’CNr.’})</p>
<p>data.rename(index={‘b’:’Zhang’,’c’:’Wang’},inplace=True)</p>
<p>data<br>Out[11]:<br>       c1  c2  c3  c4<br>a       0   1   2   3<br>Zhang   4   5   6   7<br>Wang    8   9  10  11</p>
<p>#通过inplace=True，可以改变原数据</p>
<p>数据的离散和面元划分<br>有些时候我们需要把连续数据离散化或拆分成面元。面元指的是一段按组拆分后的数据。面元化就是把数据进行分组。<br>例： ages=[20,22,25,27,26,31,78,45,20,19,67,34,56,66,59,43]</p>
<p>#面元化的实现</p>
<p>bin_nr=[min(ages),35,46,57,63,max(ages)]<br>bin_nr<br>Out[15]: [19, 35, 46, 57, 63, 78]<br>cut_1=pd.cut(ages,bin_nr)<br>cut_1<br>Out[17]:<br>[(19, 35], (19, 35], (19, 35], (19, 35], (19, 35], …, (19, 35], (46, 57], (63, 78], (57, 63], (35, 46]]<br>Length: 16<br>Categories (5, interval[int64]): [(19, 35] &lt; (35, 46] &lt; (46, 57] &lt; (57, 63] &lt; (63, 78]]</p>
<p>#对数据分组后的编号是：<br>ages<br>Out[28]: [20, 22, 25, 27, 26, 31, 78, 45, 20, 19, 67, 34, 56, 66, 59, 43]<br>cut_1.codes<br>Out[24]: array([ 0,  0,  0,  0,  0,  0,  4,  1,  0, -1,  4,  0,  2,  4,  3,  1], dtype=int8)<br>我们发现分组编号中有-1值，这是‘19’这个值造成的，因为我们的分组组限不包括‘19’这个值。解决办法有两个：<br>改变bin_nr<br>bin_nr=[18,min(ages),35,46,57,63,max(ages)]<br>cut_1=pd.cut(ages,bin_nr)<br>cut_1<br>Out[35]:<br>[(19, 35], (19, 35], (19, 35], (19, 35], (19, 35], …, (19, 35], (46, 57], (63, 78], (57, 63], (35, 46]]<br>Length: 16<br>Categories (6, interval[int64]): [(18, 19] &lt; (19, 35] &lt; (35, 46] &lt; (46, 57] &lt; (57, 63] &lt; (63, 78]]<br>cut_1.codes<br>Out[38]: array([1, 1, 1, 1, 1, 1, 5, 2, 1, 0, 5, 1, 3, 5, 4, 2], dtype=int8)<br>修改参数：<br>bin_nr=[min(ages),35,46,57,63,max(ages)]<br>pd.cut(ages,bin_nr,include_lowest=True)<br>Out[41]:<br>[(18.999, 35.0], (18.999, 35.0], (18.999, 35.0], (18.999, 35.0], (18.999, 35.0], …, (18.999, 35.0], (46.0, 57.0], (63.0, 78.0], (57.0, 63.0], (35.0, 46.0]]<br>Length: 16<br>Categories (5, interval[float64]): [(18.999, 35.0] &lt; (35.0, 46.0] &lt; (46.0, 57.0] &lt; (57.0, 63.0] &lt; (63.0, 78.0]]<br>cut2=pd.cut(ages,bin_nr,include_lowest=True)<br>cut2.codes<br>Out[43]: array([0, 0, 0, 0, 0, 0, 4, 1, 0, 0, 4, 0, 2, 4, 3, 1], dtype=int8)<br>ages<br>Out[45]: [20, 22, 25, 27, 26, 31, 78, 45, 20, 19, 67, 34, 56, 66, 59, 43]</p>
<p>#显示分组组限：<br>cut_1.categories<br>Out[31]:<br>IntervalIndex([(19, 35], (35, 46], (46, 57], (57, 63], (63, 78]]<br>              closed=’right’,<br>              dtype=’interval[int64]’)</p>
<p>#统计各组频数<br>pd.value_counts(cut2)<br>Out[55]:<br>(18.999, 35.0]    9<br>(63.0, 78.0]      3<br>(35.0, 46.0]      2<br>(57.0, 63.0]      1<br>(46.0, 57.0]      1<br>dtype: int64</p>
<p>#right=False可改变组限区间闭区间的位置；<br>bin_nr=[min(ages),35,46,57,63,max(ages),79]<br>pd.cut(ages,bin_nr,right=False)<br>Out[50]:<br>[[19, 35), [19, 35), [19, 35), [19, 35), [19, 35), …, [19, 35), [46, 57), [63, 78), [57, 63), [35, 46)]<br>Length: 16<br>Categories (6, interval[int64]): [[19, 35) &lt; [35, 46) &lt; [46, 57) &lt; [57, 63) &lt; [63, 78) &lt; [78, 79)]<br>cut4=pd.cut(ages,bin_nr,right=False)<br>cut4.codes<br>Out[52]: array([0, 0, 0, 0, 0, 0, 5, 1, 0, 0, 4, 0, 2, 4, 3, 1], dtype=int8)</p>
<p>#通过labels给面元设置名称<br>bin_nr=[min(ages),35,46,57,63,max(ages)]<br>pd.cut(ages,bin_nr,include_lowest=True)<br>cut2=pd.cut(ages,bin_nr,include_lowest=True,labels=Name_group)<br>cut2<br>Out[62]:<br>[青年, 青年, 青年, 青年, 青年, …, 青年, 中年, 少老年, 老中年, 壮年]<br>Length: 16<br>Categories (5, object): [青年 &lt; 壮年 &lt; 中年 &lt; 老中年 &lt; 少老年]</p>
<p>#如果给bins参数赋整数值，意味着传入的数值是面元数量，计算机会根据整个数据最大值和最小值计算等长面元。<br>bin_nr=[min(ages),35,46,57,63,max(ages)]<br>pd.cut(ages,bin_nr,include_lowest=True)<br>cut2=pd.cut(ages,6,include_lowest=True)<br>cut2<br>Out[64]:<br>[(18.94, 28.833], (18.94, 28.833], (18.94, 28.833], (18.94, 28.833], (18.94, 28.833], …, (28.833, 38.667], (48.5, 58.333], (58.333, 68.167], (58.333, 68.167], (38.667, 48.5]]<br>Length: 16<br>Categories (6, interval[float64]): [(18.94, 28.833] &lt; (28.833, 38.667] &lt; (38.667, 48.5] &lt; (48.5, 58.333] &lt; (58.333, 68.167] &lt; (68.167, 78.0]]<br>pd.value_counts(cut2)<br>Out[68]:<br>(18.94, 28.833]     7<br>(58.333, 68.167]    3<br>(38.667, 48.5]      2<br>(28.833, 38.667]    2<br>(68.167, 78.0]      1<br>(48.5, 58.333]      1<br>dtype: int64</p>
<p>#等数据点面元化</p>
<p>data_random=np.random.randn(1000)</p>
<p>cut_12=pd.qcut(data_random,8)</p>
<p>cut_12<br>Out[71]:<br>[(-1.193, -0.735], (-0.043, 0.297], (0.297, 0.628], (-1.193, -0.735], (-0.365, -0.043], …, (-3.453, -1.193], (-0.365, -0.043], (0.628, 1.122], (-0.735, -0.365], (1.122, 2.735]]<br>Length: 1000<br>Categories (8, interval[float64]): [(-3.453, -1.193] &lt; (-1.193, -0.735] &lt; (-0.735, -0.365] &lt; (-0.365, -0.043] &lt; (-0.043, 0.297] &lt; (0.297, 0.628] &lt; (0.628, 1.122] &lt; (1.122, 2.735]]<br>pd.value_counts(cut_12)<br>Out[72]:<br>(1.122, 2.735]      125<br>(0.628, 1.122]      125<br>(0.297, 0.628]      125<br>(-0.043, 0.297]     125<br>(-0.365, -0.043]    125<br>(-0.735, -0.365]    125<br>(-1.193, -0.735]    125<br>(-3.453, -1.193]    125<br>dtype: int64</p>
<p>#自定义分位数（必须是0-1之间的分位数，包括端点，因为这里是按百分比分位）<br>cut_12=pd.qcut(data_random,[0,0.3,0.6,0.85,1])<br>cut_12<br>Out[80]:<br>[(-3.453, -0.573], (-0.573, 0.218], (0.218, 0.987], (-3.453, -0.573], (-0.573, 0.218], …, (-3.453, -0.573], (-0.573, 0.218], (0.218, 0.987], (-3.453, -0.573], (0.987, 2.735]]<br>Length: 1000<br>Categories (4, interval[float64]): [(-3.453, -0.573] &lt; (-0.573, 0.218] &lt; (0.218, 0.987] &lt; (0.987, 2.735]]<br>pd.value_counts(cut_12)<br>Out[81]:<br>(-0.573, 0.218]     300(0-0.3)<br>(-3.453, -0.573]    300(0.3-0.6)<br>(0.218, 0.987]      250(0.6-0.85)<br>(0.987, 2.735]      150(0.85-1)<br>dtype: int64</p>
<p>检测和过滤异常值<br>Outlier（out来了）异常值。如何检测那些值是异常值呢？我们来看一个例子：<br>import pandas as pd</p>
<p>import numpy as np</p>
<p>np.random.seed(12876)</p>
<p>data=pd.DataFrame(np.random.randn(1000,4))</p>
<p>data.describe()<br>Out[5]:<br>                 0            1            2            3<br>count  1000.000000  1000.000000  1000.000000  1000.000000<br>mean     -0.032748     0.017600     0.005716     0.052615<br>std       1.029352     1.013877     0.974347     0.990795<br>min      -3.558590    -3.399723    -3.714028    -3.113867<br>25%      -0.731609    -0.654785    -0.703925    -0.604366<br>50%      -0.079017     0.001919     0.020646     0.041902<br>75%       0.686783     0.688564     0.685262     0.703313<br>max       3.040923     2.809213     2.827195     3.389950</p>
<p>#根据各列的最大值和最小值对各列数值进行过滤<br>c_1=data[2]<br>c_1[np.abs(c_1)&gt;3]<br>Out[8]:<br>382   -3.002094<br>466   -3.714028<br>Name: 2, dtype: float64</p>
<p>#对整个数据框中所有&gt;3的值进行过滤<br>Step1:<br>np.abs(data)&gt;3<br>Out[43]:<br>         0      1      2      3<br>0    False  False  False  False<br>1    False  False  False  False<br>2    False  False  False  False<br>3    False  False  False  False<br>4    False  False  False  False<br>5    False  False  False  False<br>6    False  False  False  False<br>7    False  False  False  False<br>8    False  False  False  False<br>9    False  False  False  False<br>10   False  False  False  False<br>11   False  False  False  False<br>12   False  False  False  False<br>13   False  False  False  False<br>14   False  False  False  False<br>15   False  False  False  False<br>16   False  False  False  False<br>17   False  False  False  False<br>18   False  False  False  False<br>19   False  False  False  False<br>20   False  False  False  False<br>21   False  False  False  False<br>22   False  False  False  False<br>23   False  False  False  False<br>24   False  False  False  False<br>25   False  False  False  False<br>26   False  False  False  False<br>27   False  False  False  False<br>28   False  False  False  False<br>29   False  False  False  False<br>..     …    …    …    …<br>970  False  False  False  False<br>971  False  False  False  False<br>972  False  False  False  False<br>973  False  False  False  False<br>974  False  False  False  False<br>975  False  False  False  False<br>976  False  False  False  False<br>977  False  False  False  False<br>978  False  False  False  False<br>979  False  False  False  False<br>980  False  False  False  False<br>981  False  False  False  False<br>982  False  False  False  False<br>983  False  False  False  False<br>984  False  False  False  False<br>985  False  False  False  False<br>986  False  False  False  False<br>987  False  False  False  False<br>988  False  False  False  False<br>989  False  False  False  False<br>990  False  False  False  False<br>991  False  False  False  False<br>992  False  False  False  False<br>993  False  False  False  False<br>994  False  False  False  False<br>995  False  False  False  False<br>996  False  False  False  False<br>997  False  False  False  False<br>998  False  False  False  False<br>999  False  False  False  False</p>
<p>[1000 rows x 4 columns]<br>Step2:</p>
<p>#逐列按行，把含有true值的每一行转化成true，把不含的行转成false，返回series</p>
<p>(np.abs(data)&gt;3).any(1)<br>Out[46]:<br>0      False<br>1      False<br>2      False<br>3      False<br>4      False<br>5      False<br>6      False<br>7      False<br>8      False<br>9      False<br>10     False<br>11     False<br>12     False<br>13     False<br>14     False<br>15     False<br>16     False<br>17     False<br>18     False<br>19     False<br>20     False<br>21     False<br>22     False<br>23     False<br>24     False<br>25     False<br>26     False<br>27     False<br>28     False<br>29     False</p>
<p>970    False<br>971    False<br>972    False<br>973    False<br>974    False<br>975    False<br>976    False<br>977    False<br>978    False<br>979    False<br>980    False<br>981    False<br>982    False<br>983    False<br>984    False<br>985    False<br>986    False<br>987    False<br>988    False<br>989    False<br>990    False<br>991    False<br>992    False<br>993    False<br>994    False<br>995    False<br>996    False<br>997    False<br>998    False<br>999    False<br>Length: 1000, dtype: bool<br>Step3:</p>
<p>#放到索引位置直接索引行<br>data[(np.abs(data)&gt;3).any(1)]<br>Out[16]:<br>            0         1         2         3<br>82  -1.535879 -1.077352  1.293144  3.058112<br>270 -1.131598 -0.273447 -0.979915 -3.113867<br>281  0.022546 -0.526429  0.361507  3.021792<br>283 -0.469536  0.122044 -1.188901  3.389950<br>327 -0.676235  0.155461 -0.706008  3.026979<br>340  3.040923  1.046905 -0.375365 -0.572458<br>358 -3.558590 -1.343718  1.411491  1.001404<br>382 -1.947519  0.556118 -3.002094  0.701169<br>466 -1.817789 -0.187283 -3.714028 -0.049148<br>859  0.906164 -3.399723 -0.360720  0.245022<br>注意：<br>1.参数bool_only意味着只接受布尔值，如果是空值的话，计算机会检验数据，然后仅仅接收是布尔值的数据。如果是False，非布尔值数据参与比较，且自动被设置为True值参与比较。例如：<br>TM_1=TM.astype(object)</p>
<p>TM_1.loc[0]=3</p>
<p>TM_1<br>Out[60]:<br>         0      1      2      3<br>0        3      3      3      3<br>1    False  False  False  False<br>2    False  False  False  False<br>3    False  False  False  False<br>4    False  False  False  False<br>5    False  False  False  False<br>6    False  False  False  False<br>7    False  False  False  False<br>8    False  False  False  False<br>9    False  False  False  False<br>10   False  False  False  False<br>11   False  False  False  False<br>12   False  False  False  False<br>13   False  False  False  False<br>14   False  False  False  False<br>15   False  False  False  False<br>16   False  False  False  False<br>17   False  False  False  False<br>18   False  False  False  False<br>19   False  False  False  False<br>20   False  False  False  False<br>21   False  False  False  False<br>22   False  False  False  False<br>23   False  False  False  False<br>24   False  False  False  False<br>25   False  False  False  False<br>26   False  False  False  False<br>27   False  False  False  False<br>28   False  False  False  False<br>29   False  False  False  False<br>..     …    …    …    …<br>970  False  False  False  False<br>971  False  False  False  False<br>972  False  False  False  False<br>973  False  False  False  False<br>974  False  False  False  False<br>975  False  False  False  False<br>976  False  False  False  False<br>977  False  False  False  False<br>978  False  False  False  False<br>979  False  False  False  False<br>980  False  False  False  False<br>981  False  False  False  False<br>982  False  False  False  False<br>983  False  False  False  False<br>984  False  False  False  False<br>985  False  False  False  False<br>986  False  False  False  False<br>987  False  False  False  False<br>988  False  False  False  False<br>989  False  False  False  False<br>990  False  False  False  False<br>991  False  False  False  False<br>992  False  False  False  False<br>993  False  False  False  False<br>994  False  False  False  False<br>995  False  False  False  False<br>996  False  False  False  False<br>997  False  False  False  False<br>998  False  False  False  False<br>999  False  False  False  False</p>
<p>[1000 rows x 4 columns]</p>
<p>TM_1.any(1,bool_only=True)<br>Out[61]:<br>0      False<br>1      False<br>2      False<br>3      False<br>4      False<br>5      False<br>6      False<br>7      False<br>8      False<br>9      False<br>10     False<br>11     False<br>12     False<br>13     False<br>14     False<br>15     False<br>16     False<br>17     False<br>18     False<br>19     False<br>20     False<br>21     False<br>22     False<br>23     False<br>24     False<br>25     False<br>26     False<br>27     False<br>28     False<br>29     False</p>
<p>970    False<br>971    False<br>972    False<br>973    False<br>974    False<br>975    False<br>976    False<br>977    False<br>978    False<br>979    False<br>980    False<br>981    False<br>982    False<br>983    False<br>984    False<br>985    False<br>986    False<br>987    False<br>988    False<br>989    False<br>990    False<br>991    False<br>992    False<br>993    False<br>994    False<br>995    False<br>996    False<br>997    False<br>998    False<br>999    False<br>Length: 1000, dtype: bool</p>
<p>TM_1.any(1,bool_only=False)<br>Out[62]:<br>0       True<br>1      False<br>2      False<br>3      False<br>4      False<br>5      False<br>6      False<br>7      False<br>8      False<br>9      False<br>10     False<br>11     False<br>12     False<br>13     False<br>14     False<br>15     False<br>16     False<br>17     False<br>18     False<br>19     False<br>20     False<br>21     False<br>22     False<br>23     False<br>24     False<br>25     False<br>26     False<br>27     False<br>28     False<br>29     False</p>
<p>970    False<br>971    False<br>972    False<br>973    False<br>974    False<br>975    False<br>976    False<br>977    False<br>978    False<br>979    False<br>980    False<br>981    False<br>982    False<br>983    False<br>984    False<br>985    False<br>986    False<br>987    False<br>988    False<br>989    False<br>990    False<br>991    False<br>992    False<br>993    False<br>994    False<br>995    False<br>996    False<br>997    False<br>998    False<br>999    False<br>Length: 1000, dtype: bool</p>
<p>2.如果我们这里使用any（0），将会发生错误，原因是返回值仅仅是一个下面的series，把它放到索引位。肯定无法进行索引</p>
<p>(np.abs(data)&gt;3).any(0)<br>Out[36]:<br>0    True<br>1    True<br>2    True<br>3    True<br>dtype: bool<br>data[(np.abs(data)&gt;3).any(0)]<br>C:\Users\dongfeng\Anaconda3\lib\site-packages\ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.</p>
<h2 id="“””Entry-point-for-launching-an-IPython-kernel"><a href="#“””Entry-point-for-launching-an-IPython-kernel" class="headerlink" title="  “””Entry point for launching an IPython kernel."></a>  “””Entry point for launching an IPython kernel.</h2><p>IndexingError                             Traceback (most recent call last)</p>
<p><ipython-input-37-ebe2c62f7641> in <module>()<br>—-&gt; 1 data[(np.abs(data)&gt;3).any(0)]</module></ipython-input-37-ebe2c62f7641></p>
<p>~\Anaconda3\lib\site-packages\pandas\core\frame.py in <strong>getitem</strong>(self, key)<br>   1956         if isinstance(key, (Series, np.ndarray, Index, list)):<br>   1957             # either boolean or fancy integer index<br>-&gt; 1958             return self._getitem_array(key)<br>   1959         elif isinstance(key, DataFrame):<br>   1960             return self._getitem_frame(key)</p>
<p>~\Anaconda3\lib\site-packages\pandas\core\frame.py in _getitem_array(self, key)<br>   1996             # check_bool_indexer will throw exception if Series key cannot<br>   1997             # be reindexed to match DataFrame rows<br>-&gt; 1998             key = check_bool_indexer(self.index, key)<br>   1999             indexer = key.nonzero()[0]<br>   2000             return self.take(indexer, axis=0, convert=False)</p>
<p>~\Anaconda3\lib\site-packages\pandas\core\indexing.py in check_bool_indexer(ax, key)<br>   1937         mask = isnull(result._values)<br>   1938         if mask.any():<br>-&gt; 1939             raise IndexingError(‘Unalignable boolean Series provided as ‘<br>   1940                                 ‘indexer (index of the boolean Series and of ‘<br>   1941                                 ‘the indexed object do not match’)</p>
<p>IndexingError: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match</p>
<p>当然我们可以修改，把其放在列位，但这样的话等于什么也没有做，仅仅选取所有列<br>data.loc[:,(np.abs(data)&gt;3).any(0)]<br>Out[40]:<br>            0         1         2         3<br>0   -0.567046 -0.491171 -0.827547 -0.680408<br>1    0.206551 -1.456287  0.629825  0.148732<br>2   -1.130370 -0.056857  1.431013  1.475764<br>3   -2.125800  0.108440 -1.350079  0.523728<br>4   -0.736537 -0.846202  0.543960 -1.230080<br>5   -0.152478  0.087115 -0.304004  0.050398<br>6   -0.479466 -1.990843 -0.361344  1.096610<br>7   -0.246875  0.595030  1.469306 -0.438550<br>8    0.989888  0.174695  0.241461 -0.550926<br>9   -0.781855  1.660456  1.041865  0.264480<br>10   1.661492  0.356601 -0.699093  0.869719<br>11   0.163815  0.246831 -0.250914 -0.694415<br>12  -1.584598 -1.279383 -1.274184 -0.008277<br>13  -0.266142  1.401936  0.397230 -0.087856<br>14   0.016147  0.710451  1.548756  0.292436<br>15  -0.885661 -0.126581  0.927745 -0.194748<br>16  -0.018011  0.384556  0.545346  0.447323<br>17  -1.785552  0.372100  0.014757 -2.023199<br>18   0.764790  0.900823 -1.039617  1.111810<br>19  -0.603835  0.951399 -1.166977  2.091614<br>20   0.227416  0.606907 -0.533333  0.524420<br>21   0.728552 -0.138527 -1.922079 -2.683756<br>22  -1.777072 -0.610157 -1.141227 -0.527488<br>23  -0.393001  1.573780 -0.270537 -0.552588<br>24   0.142684 -0.237119 -2.089615  0.982052<br>25   0.048209 -0.858588  1.238078  0.605156<br>26  -0.736681  0.402256 -0.247741 -0.549385<br>27  -0.183541 -0.654161  0.539174 -0.597491<br>28   0.641096 -1.708915  0.660372  0.510122<br>29  -0.174187 -0.809638  0.032858 -1.431953<br>..        …       …       …       …<br>970 -0.868857  2.001313  0.852164  0.589976<br>971 -0.557336  0.318217 -0.546586  1.061976<br>972 -0.884070 -0.641405 -0.210885  0.049715<br>973  0.785917  0.212696  1.193491  1.007289<br>974  0.921965 -0.478275 -1.417855  0.911799<br>975  0.131626  0.050366  0.637130  0.267212<br>976 -1.425079  0.923022 -0.208123 -0.697405<br>977  1.723595 -0.022576 -0.954601 -0.374849<br>978 -2.049331  1.803200 -0.894918  0.955312<br>979  0.082268  2.073896  0.072605  0.912109<br>980 -0.166275 -0.463344  1.510438 -0.478526<br>981  0.438580  0.540105  1.023839  0.441328<br>982 -0.260641  1.145658  0.062698  1.655832<br>983 -0.176418 -0.032755  0.906118  2.012923<br>984 -0.870494  0.038007 -0.371591 -0.262396<br>985 -0.683679 -0.182418  0.683105 -0.852608<br>986 -0.641252  1.387388  1.770705 -1.003002<br>987 -2.111872 -0.438135  0.669196 -0.642335<br>988  1.279690 -0.179545 -1.728947  0.837643<br>989 -0.378165  0.507085 -0.106542  0.511993<br>990  0.741188 -0.681142 -0.192551 -0.712643<br>991  1.045758  0.238560 -0.713991 -0.463895<br>992 -1.052059 -0.942410 -0.062352  0.267082<br>993 -0.184833  1.474266  2.345032  1.189559<br>994 -0.245563 -0.513430 -0.998651 -0.858027<br>995  0.745093  0.478441  1.731140  0.848215<br>996 -0.445829 -0.022585 -0.621001 -1.348158<br>997 -1.479772  0.688092 -1.209812 -1.247304<br>998 -0.260688  0.373783 -0.384794  1.073226<br>999 -0.395423  0.955142  0.525140  1.178641</p>
<p>#把所有数据限制到-3到3的范围内。<br>data[np.abs(data)&gt;3]=np.sign(data)*3#左边实现绝对值大于3的值的选取，即设置了x&gt;3或x&lt;-3的选区。右边是符号与左边选区数值对应，绝对值数值为3的数据。通过=赋值到选区的相应位置。<br>count_frame=pd.DataFrame([pd.value_counts(np.abs(data[0])&gt;3),pd.value_counts(np.abs(data[1])&gt;3),pd.value_counts(np.abs(data[2])&gt;3),pd.value_counts(np.abs(data[3])&gt;3)])</p>
<p>count_frame<br>Out[77]:<br>   False<br>0   1000<br>1   1000<br>2   1000<br>3   1000</p>
<p>排列与随机采样</p>
<p>通过permutation函数可以实现对数据框或者Series的随机排列。<br>asr=np.random.randint(-6,6,15).reshape(5,3)<br>asr_fra=pd.DataFrame(asr)<br>asr_fra<br>Out[17]:<br>   0  1  2<br>0 -2  1  0<br>1 -2  4  3<br>2 -5  1  4<br>3  0  4  1<br>4  4  4 -1<br>np.random.permutation(asr_fra)<br>Out[18]:<br>array([[-5,  1,  4],<br>       [-2,  4,  3],<br>       [ 4,  4, -1],<br>       [ 0,  4,  1],<br>       [-2,  1,  0]])</p>
<p>#其实质仅仅是对第一列数据进行排列，其他列随着第一列移动</p>
<p>#下面对Series进行随机排列<br>ser1=pd.Series([3,2,7,9,0,2,1,9,2,2,3,12,17])<br>np.random.permutation(ser1)<br>Out[22]: array([ 9,  7,  3,  0,  2,  3, 17,  2, 12,  2,  1,  9,  2], dtype=int64)</p>
<p>#我们也可以通过产生一个随机排列数组，对数组进行排列</p>
<p>as_frame=pd.DataFrame(np.random.randint(-12,20,42).reshape(7,6))</p>
<p>as_frame<br>Out[26]:<br>    0   1   2   3   4   5<br>0  -5  13  10  14  -2   5<br>1 -10  13  -9  10  -4  -7<br>2   6  11   0  -2  -8   9<br>3   0 -11  -8  -2 -12  16<br>4   3 -12  -5  18   8   3<br>5   0   3   4   4   5  10<br>6   7   0   6  -7   3 -10</p>
<p>pattern=np.random.permutation(6)<br>pattern<br>Out[29]: array([5, 4, 3, 0, 1, 2])<br>as_frame.take(pattern,0)<br>Out[28]:<br>    0   1   2   3   4   5<br>5   0   3   4   4   5  10<br>4   3 -12  -5  18   8   3<br>3   0 -11  -8  -2 -12  16<br>0  -5  13  10  14  -2   5<br>1 -10  13  -9  10  -4  -7<br>2   6  11   0  -2  -8   9<br>注意这里返回的结果是as_frame的一个子数据框，这是因为pattern的结果是从从0到5的一个排列，数据框的行索引是按照这个pattern的这个结果进行排列。<br>这里也可以按列索引进行排列<br>as_frame.take(pattern,1)<br>Out[31]:<br>    5   4   3   0   1   2<br>0   5  -2  14  -5  13  10<br>1  -7  -4  10 -10  13  -9<br>2   9  -8  -2   6  11   0<br>3  16 -12  -2   0 -11  -8<br>4   3   8  18   3 -12  -5<br>5  10   5   4   0   3   4<br>6 -10   3  -7   7   0   6<br>as_frame.take(np.random.permutation([4,3,0,1]),1)#可以按照索引子集的排列进行列的重排<br>Out[33]:<br>    3   4   0   1<br>0  14  -2  -5  13<br>1  10  -4 -10  13<br>2  -2  -8   6  11<br>3  -2 -12   0 -11<br>4  18   8   3 -12<br>5   4   5   0   3<br>6  -7   3   7   0<br>as_frame.take(np.random.permutation(as_frame.shape[1])[:4],1)<br>Out[37]:<br>    0   2   4   5<br>0  -5  10  -2   5<br>1 -10  -9  -4  -7<br>2   6   0  -8   9<br>3   0  -8 -12  16<br>4   3  -5   8   3<br>5   0   4   5  10<br>6   7   6   3 -10</p>
<p>通过random.randint和take函数就行随机抽样</p>
<p>五，绘图和可视化<br>Matplotlib入门，pandas下的绘图函数，画布，画布分割等等<br>Matplotlib入门<br>调用Matplotlib API 程序包</p>
<p>1，创建画布<br>import matplotlib as plt</p>
<p>import matplotlib.pyplot as plt</p>
<p>figure_1=plt.figure()<br>(或者详细定义画布<br>figure_1=plt.figure(1,(4,6),dpi=400,facecolor=’green’,edgecolor=’yellow’,frameon=True)）<br>figure_1<br>Out[43]: &lt;matplotlib.figure.Figure at 0x8ea7987320&gt;</p>
<p>2，分割画布</p>
<p>aapic_1=figure_1.add_subplot(2,2,1)</p>
<p>aapic_2=figure_1.add_subplot(2,2,2)</p>
<p>aapic_3=figure_1.add_subplot(2,2,3)</p>
<p>aapic_4=figure_1.add_subplot(2,2,4)</p>
<p>3，show 出你的画布</p>
<p>plt.show()</p>
<p>import matplotlib.pyplot as plt</p>
<p>import numpy as np</p>
<p>figure_1=plt.figure()</p>
<p>aapic_1=figure_1.add_subplot(2,2,1)</p>
<p>aapic_2=figure_1.add_subplot(2,2,2)</p>
<p>aapic_3=figure_1.add_subplot(2,2,3)</p>
<p>plt.plot(np.random.randn(50).cumsum(),’k–’)<br>Out[7]: [&lt;matplotlib.lines.Line2D at 0xdd16defd0&gt;]</p>
<p>#直接用“plot”绘图，k–代表黑色虚线图</p>
<p>aapic_2.hist(np.random.randn(100),bins=20,color=’k’,alpha=0.3)<br>Out[9]:<br>(array([  1.,   1.,   1.,   5.,   3.,   7.,   8.,  14.,  11.,   6.,  11.,<br>          6.,  13.,   5.,   2.,   3.,   0.,   1.,   0.,   2.]),<br> array([-2.64526298, -2.37172471, -2.09818644, -1.82464817, -1.5511099 ,<br>        -1.27757163, -1.00403336, -0.73049509, -0.45695681, -0.18341854,<br>         0.09011973,  0.363658  ,  0.63719627,  0.91073454,  1.18427281,<br>         1.45781108,  1.73134935,  2.00488762,  2.27842589,  2.55196416,<br>         2.82550243]),<br> <a 20 list of patch objects>)</a></p>
<p>aapic_1.scatter(np.arange(30),np.arange(30)+3*np.random.randn(30))<br>Out[10]: &lt;matplotlib.collections.PathCollection at 0xdd172c390&gt;</p>
<p>#alpha代表透明度，bins代表柱状体个数</p>
<p>plt.show<br>Out[12]: <function matplotlib.pyplot.show></function></p>
<p>plt.show()</p>
<p>这里还有一个简单且更为方便的画图方法；它可以创建一个新的figure（画布）并返回一个含有以创建的subplot对象的numpy数组。<br>pic,axes=plt.subplots(2,3)<br>plt.show(0)</p>
<p>饼图：<br>import numpy as np<br>import matplotlib.pyplot as plt<br>Asx,sd=plt.subplots(1,1)<br>sd.pie(np.arange(4,9),explode=[0.2,0.1,0.3,0.4,0.3],labels=[‘zhang’,’wang’,’li’,’zhao’,’liu’],colors=[‘m’,’r’,’g’,’c’,’b’],autopct=’%.2f%%’,pctdistance=1,shadow=True,labeldistance=1.6,startangle=30,radius=1,frame=True,rotatelabels=True)<br>Plt.show()</p>
<p> Asx,sd=plt.subplots(1,1)<br>sd.pie(np.arange(4,9),explode=[0,0,0,0,0],labels=[‘zhang’,’wang’,’li’,’zhao’,’liu’],colors=[‘m’,’r’,’g’,’c’,’b’],autopct=’%.2f%%’,pctdistance=1,shadow=True,labeldistance=0.5,startangle=30,radius=1,frame=True,rotatelabels=True)<br>plt.show()</p>
<p>#labeldistance&lt;1,图例将会在饼图内</p>
<p>#通过figsize=（6,6）把饼图设置成圆的，然后通过textprops设置字体，通过labeldistance设置标签离圆心距离。通过autopct设置每个部分总总体的百分数，通过pctdistance设置百分数例圆心距离，通过explode设置一个部分的强调。<br>Asx,sd=plt.subplots(1,1,figsize=(6,6))<br>sd.pie(np.arange(4,9),explode=[0,0.1,0,0,0],labels=[‘zhang’,’wang’,’li’,’zhao’,’liu’],colors=[‘m’,’r’,’g’,’c’,’b’],autopct=’%.2f%%’,pctdistance=0.4,shadow=True,labeldistance=0.5,wedgeprops=None,textprops={‘fontsize’:14},startangle=0,radius=1,frame=True,rotatelabels=None)<br>plt.show()</p>
<p>调整Subplot周围的间距：</p>
<p>默认情况下，子图外围是有一定边距的，并且各子图之间上下左右都有一定间距。</p>
<p>间距与子图大小有关，子图像宽，则横向间距小，图像高，则纵向间距小。</p>
<p>除了默认，我们可以选择自己确定间距：</p>
<p>plt.subplots_adjust(left=None,bottom=None,right=None,top=None,wspace=None,hspace=None)</p>
<p>#wspace和hspace用于控制宽度和高度的百分比，通过调整这两个参数，我们的子图间的上下左右间距会发生变化</p>
<p>例子：</p>
<p>import numpy as np<br>fig, axes=plt.subplots(2, 2, sharex=True, sharey=True)<br>for i in range(2):<br>    for j in range(2):<br>        axes[i,j].hist(np.random.randn(500),color=’k’,alpha=0.5)<br>plt.subplots_adjust(wspace=2,hspace=1)</p>
<p>plt.show(0)</p>
<p>&lt;matplotlib.figure.Figure at 0x17becd9908&gt;</p>
<p>&lt;matplotlib.figure.Figure at 0x17befbfda0&gt;</p>
<p>#shareX 和sharey是共X轴和Y轴。调整wspace 盒hspace可以得到不同的子图间距。<br>for i in range(2):<br>    for j in range(2):<br>        axes[i,j].hist(np.random.randn(500),color=’k’,alpha=0.5)<br>plt.subplots_adjust(wspace=0.2,hspace=0.1)</p>
<p>plt.show(0)</p>
<p>&lt;matplotlib.figure.Figure at 0x17becd9908&gt;</p>
<p>&lt;matplotlib.figure.Figure at 0x17befbfda0&gt;</p>
<p>问：上面图像的X轴和Y轴分别指的是什么？？？？</p>
<p>颜色和线型以及标记<br>指定图形函数线的颜色，我们可以通过一个指令实现，例：<br>Pic_11.plot(X,Y,’g–’) 等价于 ax.plot(x,y,linestyle=’–’,color=’g’)</p>
<p>更多颜色可使用指定其RGB值的形式。对于线性图，我们可以给数据点加上标记(marker)，使人更容易发现那些是数据点。</p>
<p>线型和marker表格：<br>‘-‘ solid line style<br>‘–’ dashed line style<br> ‘-.’ dash-dot line style<br>‘:’ dotted line style<br>‘.’ point marker<br>‘,’ pixel marker<br>‘o’ circle marker<br>‘v’ triangle_down marker<br>‘^’ triangle_up marker<br> ‘&lt;’ triangle_left marker<br>‘&gt;’ triangle_right marker<br>‘1’ tri_down marker<br>‘2’ tri_up marker<br>‘3’ tri_left marker<br>‘4’ tri_right marker<br> ‘s’ square marker<br>‘p’ pentagon marker<br>‘*’ star marker<br>‘h’ hexagon1 marker<br>‘H’ hexagon2 marker<br>‘+’ plus marker<br>‘x’ x marker<br>‘D’ diamond marker<br> ‘d’ thin_diamond marker<br>‘|’ vline marker<br>‘_’ hline marker<br>颜色表格：<br>‘b’ blue<br>‘g’ green<br>‘r’ red<br>‘c’ cyan<br>‘m’ magenta<br>‘y’ yellow<br>‘k’ black<br>‘w’ white</p>
<p>美丽温馨的例子：<br>marker_1=[‘.’, ‘,’, ‘o’, ‘v’, ‘^’, ‘&lt;’, ‘&gt;’, ‘1’, ‘2’, ‘3’, ‘4’,’s’,’p’,’<em>‘,’h’,’H’,’+’,’x’,’D’,’d’,’|’, ‘_’]<br>asd,fid_1=plt.subplots(len(marker_1),dpi=180,figsize=(6,6</em>len(marker_1)))<br>for i in range(len(marker_1)):<br>    N = 50 # 点的个数<br>    x = np.random.rand(N) <em> 2 # 随机产生50个0~2之间的x坐标<br>    y = np.random.rand(N) </em> 2 # 随机产生50个0~2之间的y坐标<br>    colors = np.random.rand(N) # 随机产生50个0~1之间的颜色值<br>    area = np.pi <em> (15 </em> np.random.rand(N))**2  # 点的半径范围:0~15 </p>
<pre><code># 画散点图
fid_1[i].scatter(x, y, s=area, c=colors, alpha=0.5, marker=marker_1[i])
fid_1[i].set_xlabel(marker_1[i])
</code></pre><p>plt.show()</p>
<p>如果不涉及子图的话，无需先设置画布。可直接画出图形：<br>import numpy as np<br>import matplotlib.pyplot as plt<br>plt.plot(np.random.randn(30).cumsum(),’g*–’)<br>Out[3]: [&lt;matplotlib.lines.Line2D at 0xb407f4dd30&gt;]<br>plt.show()</p>
<p>plt.plot(np.random.randn(30).cumsum(),’g<em>–’) 等价于<br>Plt.plot(np.random.randn(30).cumsum(),color=’g’,linestyle=’dashed’,marker=’</em>’)</p>
<p>在线性图中，那些非数据点都是根据两个数据点连线插值的，我们可以修改这种插值方式，这里用到drawstyle选项修改：</p>
<p>data=np.array([1,2,3.2,2.3,4.6,7.5,2,3,6.5,7.8,9],dtype=float)<br>np.unique(data)<br>Out[9]: array([ 1. ,  2. ,  2.3,  3. ,  3.2,  4.6,  6.5,  7.5,  7.8,  9. ])<br>data<br>Out[10]: array([ 1. ,  2. ,  3.2,  2.3,  4.6,  7.5,  2. ,  3. ,  6.5,  7.8,  9. ])<br>data_uique=np.unique(data)<br>plt.plot(data_uique,’o-.’,label=’line_point’)<br>Out[28]: [&lt;matplotlib.lines.Line2D at 0xb40947c908&gt;]<br>plt.plot(data_uique,’r-‘,drawstyle=’steps-post’,label=’line’)<br>Out[29]: [&lt;matplotlib.lines.Line2D at 0xb40947cf98&gt;]<br> plt.legend(loc=’best’)<br>Out[30]: &lt;matplotlib.legend.Legend at 0xb4083ca358&gt;<br>plt.show()</p>
<p>legend函数介绍：<br>在画一些曲线图时，常常会出现多条曲线同时画在一张图上面，这时候就需要对不同的曲线进行不同的标注，以使读者能够清晰地知道每条曲线代表的含义。当你画很少的几条曲线时，这时画图命令中自动产生的legend能够基本满足你的需要，此时，你不需要做什么；但当你将很多个曲线画在一张图上时，自动产生的legend矩形框往往会覆盖住已经画出来的曲线，很不美观，这时你就需要写专门的代码对legend的位置进行精确的控制，而不能再依靠系统帮你自动控制了。</p>
<p>比如：<br>plt.legend(loc=’upper center’, bbox_to_anchor=(0.6,0.95),ncol=3,fancybox=True,shadow=True)<br>Ncol=表示我们的图例（legend）里的线的标识可以排成三列<br>Loc=标识图例的位置<br>bbox_to_anchor=图例的精确位置，上面bbox_to_anchor被赋予的二元组中，第一个数值用于控制legend的左右移动，值越大越向右边移动，第二个数值用于控制legend的上下移动，值越大，越向上移动。</p>
<p>刻度与标签<br>图像的刻度与标签都是通过一些方法来实现的，这里有几个方法大家在绘图中经常用到：</p>
<p>Xlim（X值范围）、xticks（X轴刻度值）和xticklabels（X轴刻度标签）</p>
<p>其使用方式有以下两种:<br>1.调用不带参数值，返回当前参数值，即是现在正用的参数值。<br>2.调用时带参数值，使用该参数值。</p>
<p>例：</p>
<p>table_1=plt.figure()</p>
<p>ax=table_1.add_subplot(1,1,1)</p>
<p>ax.plot(np.random.randn(1000).cumsum())<br>Out[36]: [&lt;matplotlib.lines.Line2D at 0xb409514550&gt;]</p>
<p>ax.plot(np.random.randn(100000).cumsum())<br>Out[37]: [&lt;matplotlib.lines.Line2D at 0xb40948f780&gt;]</p>
<p>ticks_1=ax.set_xticks([0,25000,50000,75000,100000])</p>
<p>scale_name=ax.set_xticklabels([‘step1’,’step2’,’step3’,’step4’,’step5’],rotation=45,fontsize=12)</p>
<p>ax.set_title(‘Python-03 Practice’)<br>Out[40]: Text(0.5,1,’Python-03 Practice’)</p>
<p>ax.set_xlabel(‘Steps’)<br>Out[41]: Text(0.5,0,’Steps’)</p>
<p>#下面我们添加图例<br>ax.plot(np.random.randn(100000).cumsum(),color=’m’,linestyle=’-‘,label=’solid’)<br>Out[44]: [&lt;matplotlib.lines.Line2D at 0xb409852748&gt;]</p>
<p>ax.plot(np.random.randn(100000).cumsum(),color=’c’,linestyle=’:’,label=’dotted’)<br>Out[45]: [&lt;matplotlib.lines.Line2D at 0xb4094f52b0&gt;]</p>
<p>ax.legend(loc=’best’)#plt.legend也是可以的<br>Out[46]: &lt;matplotlib.legend.Legend at 0xb409852ac8&gt;<br>plt.show()</p>
<p>加注解<br>用text、arrow和annotate等函数进行添加注解，text可以将文本加到图标的指定坐标。</p>
<p> fig,subpic=plt.subplots(1,1)</p>
<p>subpic.plot([2,77,90,2.3,4,5,6,45,34,67,35,66,34,23,76],[34,32,35,43,34,23,45,56,44,57,56,33,55,66,54],’b*-‘)<br>Out[7]: [&lt;matplotlib.lines.Line2D at 0x2eae706be0&gt;]<br>ticks_1=subpic.set_xticks([0,25,50,75,100])</p>
<p>scale_name=subpic.set_xticklabels([‘jenuary’,’february’,’march’,’april’,’may’],rotation=40,fontsize=12)</p>
<p>subpic.set_title(‘Python-03 Practice_1’,fontsize=16)<br>Out[11]: Text(0.5,1,’Python-03 Practice_1’)<br>subpic.set_xlabel(‘Weight’)<br>Out[13]: Text(0.5,0,’Weight’)<br>subpic.text(90,35,’key point one’,fontsize=10)<br>Out[15]: Text(90,35,’key point one’)</p>
<p>keymenge=[(67,57,’key point two’),(23,66,’key point two’)]</p>
<p>for x,y,label in keymenge:<br>    subpic.text(x,y,label,fontsize=12)</p>
<p>subpic.set_xlim([0,100])<br>Out[21]: (0, 100)</p>
<p>subpic.set_ylim([0,100])<br>Out[22]: (0, 100)<br>subpic.annotate(‘beautyful point’,xy=(5,23),xytext=(5,23))<br>Out[18]: Text(5,23,’beautyful point’)<br>plt.savefig(‘Desktop\python_01.svg’)<br>plt.savefig(‘Desktop\python_01.png’,dpi=400,bbox_inches=’tight’)</p>
<p>plt.show()</p>
<p>Matplotlib.pyplot的画图方式与R语言十分类似，繁琐是他们共同的特点。与Matplotlib.pyplot不同，pandas在作图上不仅方法简单，而且可以完成各种各样的作图工作。</p>
<p>图像的保存<br>在上一章，我们已经使用了savefig这个函数来存储图像。事实上，Savefig还可以作为对象figure的方法存储画布上的图像，例如：</p>
<p>import matplotlib.pyplot as plt<br>pic=plt.figure()<br>pic_1=pic.add_subplot(1,1,1)<br>import numpy as np<br>pic_1.plot(np.random.randn(50).cumsum(),’k–’)<br>Out[6]: [&lt;matplotlib.lines.Line2D at 0xa22b23f0b8&gt;]<br>pic.savefig(‘Desktop\dong_123.pdf’,dpi=300,bbox_inches=’tight’)</p>
<p>Savefig的主要参数如下：<br>fname: 表示绝对或者相对文件路径的字符串，文件具体格式由后缀来决定，譬如.pdf,.png格式等。<br>dpi: 图像分辨率，默认100，（每英寸点数）<br>Facecolor,edgecolor: 背景色，默认为“w”白色<br>Format: 显示设置文件格式，png，jpeg，pdf等等，但不要与fname里的文件格式发生冲突。<br>Bbox_inches，常用值是tight，可剪除图表周围的空白部分。</p>
<p>Pandas作图</p>
<p>线性图</p>
<p>Series 和 DataFrame 都有一个用于生成各类图表的plot方法，默认状态。他只生成线性图。<br>Series生成线性图，索引（index）直接被绘制成X轴。当然我们也可以关闭使用index绘制X轴。Use_index=False</p>
<p>X轴的刻度和界限可以用xsticks和xlim选项来进行调节，ysticks和ylim可以调节Y轴</p>
<p>例子<br>ser=pd.Series(np.random.randn(10).cumsum(),index=np.arange(0,100,10))</p>
<p>ser.plot()<br>Out[18]: &lt;matplotlib.axes._subplots.AxesSubplot at 0xf3e54e0240&gt;</p>
<p>plt.show()</p>
<p>DataFrame的plot方法会在一个子图中为各列数据绘制一条线。自动创建的图例的标签与列索引相同。</p>
<p>例：<br>import numpy as np</p>
<p>import pandas as pd<br>import matplotlib.pyplot as plt</p>
<p>Frame_01=pd.DataFrame([[2,4,8,16,32,64],[3,6,12,24,48,96],[5,10,15,20,25,30],[1,3,5,7,9,11],[16,8,4,2,1,0.5]],index=range(0,100,20),columns=[‘A’,’B’,’C’,’D’,’E’,’F’])</p>
<p>pic_2,subplot_object=plt.subplots(1,1)</p>
<p>Frame_01.plot(kind=’line’,ax=subplot_object,subplots=False,layout=False,logx=True,xlim=[0,100])<br>C:\Users\dongfeng\Anaconda3\lib\site-packages\matplotlib\axes_base.py:2923: UserWarning: Attempted to set non-positive xlimits for log-scale axis; invalid limits will be ignored.<br>  ‘Attempted to set non-positive xlimits for log-scale axis; ‘<br>Out[3]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x5441e71cf8&gt;<br>plt.show()<br>Frame_01<br>Out[5]:<br>     A   B   C   D   E     F<br>0    2   4   8  16  32  64.0<br>20   3   6  12  24  48  96.0<br>40   5  10  15  20  25  30.0<br>60   1   3   5   7   9  11.0<br>80  16   8   4   2   1   0.5</p>
<p>Series.plot 方法的常用参数汇总：<br>label    用于设置图例的标签<br>ax<br>    确定要被绘制的matplotlib subplot对象。如果没有设置，则使用当前matplotlib subplot<br>style    设置传给matplotlib的风格字符串（’g*–’）<br>alpha    图表的填充不透明度（数值为0到1之间的数）<br>kind    各种图形样式line, bar, barh, kde, density, scatter<br>logy    在Y轴上使用对数标尺<br>use_index    将对象（Series and DataFrame）的索引用作刻度标签<br>rot    旋转度数（0到360）<br>xticks    用作X轴刻度的值<br>yticks    用作y轴刻度的值<br>xlim    x的值域<br>ylim    y的值域<br>grid    设置是否显示轴网格线</p>
<p>专用于DataFrame的plot的参数</p>
<p>Subplots    将依据数据框中的每个列绘制的图分别放置到单个的subplot（子画框）里<br>sharex    Subplots=true时，设定是否共享X的刻度和值域<br>sharey    Subplots=true时，设定是否共享y的刻度和值域<br>figsize    元组，用来表示图像大小（宽，高）<br>title    设置图像标题<br>Legend    设定是否添加一个subplot图例<br>sort_columns    设定是否以字母表中字母先后排列顺序绘制各列。</p>
<p>柱状图<br>柱状图分为水平柱状图和垂直柱状图。当kind=’bar’生成垂直柱状图；kind=’barh’生成水平柱状图。</p>
<p>A.Series生成柱状图<br> import numpy as np<br>import pandas as pd<br>import matplotlib.pyplot as plt<br>figure_1,get_information_of_pict=plt.subplots(2,1)<br>datas=pd.Series(np.random.rand(20),index=list(‘qwertyuiopasdfghjklz’))<br>datas.plot(kind=’barh’,ax=get_information_of_pict[0],figsize=(8,12),color=’g’)<br>Out[4]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x5be13ea6d8&gt;<br>datas.plot(kind=’bar’,ax=get_information_of_pict[1],figsize=(8,12),color=’r’)<br>Out[5]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x5be18c41d0&gt;<br>plt.show()</p>
<p>B.数据框生成柱状图<br>import numpy as np<br>import pandas as pd<br>Framedata_1=pd.DataFrame(np.arange(16).reshape(4,4)<strong>(1/2)+np.arange(16).reshape(4,4)*3+6,index=[‘spring’,’sommer’,’autumn’,’winter’],columns=[‘Benz’,’BMW’,’Porsche’,’VW’])<br>Out[11]:<br>    Benz    BMW    Porsche    VW<br>spring    6.000000    10.000000    13.414214    16.732051<br>sommer    20.000000    23.236068    26.449490    29.645751<br>autumn    32.828427    36.000000    39.162278    42.316625<br>winter    45.464102    48.605551    51.741657    54.872983<br>import numpy as np<br>import pandas as pd<br>plt.rcParams[‘font.sans-serif’]=[‘SimHei’]<br>plt.rcParams[‘axes.unicode_minus’] = False<br>Framedata_1=pd.DataFrame(np.arange(16).reshape(4,4)</strong>(1/2)+np.arange(16).reshape(4,4)*3+6,index=[‘spring’,’sommer’,’autumn’,’winter’],columns=[‘Benz’,’BMW’,’Porsche’,’VW’])<br>import matplotlib.pyplot as plt<br>picrange,pic_inf=plt.subplots(2,1)<br>Framedata_1.plot(kind=’bar’,ax=pic_inf[0],title=’2018年德系车销售额’,rot=50,figsize=(8,12))<br>Framedata_1.plot(kind=’barh’,ax=pic_inf[1],title=’2018年德系车销售额’,rot=130,figsize=(8,12))<br>plt.show()</p>
<p>​import numpy as np<br>import pandas as pd<br>import matplotlib.pyplot as plt<br>plt.rcParams[‘font.sans-serif’]=[‘SimHei’]<br>plt.rcParams[‘axes.unicode_minus’] = False<br>Framedata_1=pd.DataFrame(np.arange(16).reshape(4,4)*<em>(1/2)+np.arange(16).reshape(4,4)</em>3+6,index=[‘spring’,’sommer’,’autumn’,’winter’],columns=[‘Benz’,’BMW’,’Porsche’,’VW’])<br>import matplotlib.pyplot as plt<br>picrange,pic_inf=plt.subplots(2,1)<br>Framedata_1.plot(kind=’barh’,ax=pic_inf[1],stacked=True,title=’2018年德系车销售额’,rot=130,figsize=(8,12))<br>Framedata_1.plot(kind=’bar’,ax=pic_inf[0],stacked=True,title=’2018年德系车销售额’,rot=50,figsize=(8,12))<br>plt.show()</p>
<p>serie_1=pd.Series([3,4.5,5,3,6,9,4.7,4.7,4.7,4.7,4.5,4.5,6,6,3,7,7,7,7])</p>
<p>serie_1.value_counts().plot(kind=’bar’)<br>plt.show()</p>
<p>#应用serie_1.value.counts()在series中寻找数据重复的次数并作为纵坐标，Series中的数据作为横坐标。</p>
<p>直方图与密度图：</p>
<p>首先我们要区分直方图与柱状图。</p>
<p>柱状图：<br>柱状图的某一个轴（X或Y轴）可以没有严格的刻度，并且柱的宽度随图形大小，柱的数量等因素的变化而变化，并没有严格的公式来保证，因此没有实际意义，仅仅用来区分类别。通常柱状图是用条形的长度表示各类别对应的实际数据（譬如频数）的大小。<br>柱状图是分开排列<br>主要用于展示分类数据</p>
<p>直方图：<br>直方图通常是用面积表示各组数据大小（例如频数），矩形的高度表示每一组的频数或频率或其他匹配数据，宽度则表示各组的组距，因此其高度与宽度均有意义。<br>由于分组数据具有连续性，直方图的各矩形通常是连续排列。<br>直方图主要用于展示数据型数据。</p>
<p>plt.hist(np.array([1,2,3,4,5,1.2,1.34,1.78,2.1,2.4,2.8,2.9,3.1,3.5,3.7,4.2,4.9,5.6,5.3,5.8,5.9,5,3,6,7,8.5,6.4,7.3,7.8,7.2,5.6,6.6,6.45,6.99,3.45,2.36,5.67,8.13]),bins=5,normed=False,range=(2,8),color=’yellow’)</p>
<p>plt.hist(np.array([1,2,3,4,5,1.2,1.34,1.78,2.1,2.4,2.8,2.9,3.1,3.5,3.7,4.2,4.9,5.6,5.3,5.8,5.9,5,3,6,7,8.5,6.4,7.3,7.8,7.2,5.6,6.6,6.45,6.99,3.45,2.36,5.67,8.13]),bins=5,normed=True,range=(2,8),color=’yellow’)</p>
<p>plt.hist(np.array([1,2,3,4,5,1.2,1.34,1.78,2.1,2.4,2.8,2.9,3.1,3.5,3.7,4.2,4.9,5.6,5.3,5.8,5.9,5,3,6,7,8.5,6.4,7.3,7.8,7.2,5.6,6.6,6.45,6.99,3.45,2.36,5.67,8.13]),bins=5,normed=True,range=(2,8),color=’yellow’)</p>
<p>关于Normed（数据标准化）算法的解释：</p>
<p>sf,axes=plt.subplots()<br>data= np.array([1,1,2,3,3,3,3,3,4,5.1])<br>counts= axes.hist(data, normed= True)<br>counts</p>
<p>Out[9]:<br>(array([ 0.48780488,  0.        ,  0.24390244,  0.        ,  1.2195122 ,</p>
<pre><code>0.        ,  0.        ,  0.24390244,  0.        ,  0.24390244]),
</code></pre><p> array([ 1.  ,  1.41,  1.82,  2.23,  2.64,  3.05,  3.46,  3.87,  4.28,<br>         4.69,  5.1 ]),<br> <a 10 list of patch objects>)</a></p>
<p>np.diff(counts[1])<br>Out[10]: array([ 0.41,  0.41,  0.41,  0.41,  0.41,  0.41,  0.41,  0.41,  0.41,  0.41])</p>
<p>#组距的计算<br>(counts[0]*np.diff(counts[1])).sum()<br>Out[13]: 1.0</p>
<p>#可以看到标准化后的结果和组距相乘然后求和等于1，这才是标准化的实际意义，而不是看标准化的结果是否都必须小于1。</p>
<p>#下面我们确定Python标准化的算法：<br>sf,axes=plt.subplots()<br>data= np.array([1,1,2,3,3,3,3,3,4,5.1])<br>counts_1= axes.hist(data, normed= False)<br>counts_1</p>
<p>Out[14]:<br>(array([ 2.,  0.,  1.,  0.,  5.,  0.,  0.,  1.,  0.,  1.]),<br> array([ 1.  ,  1.41,  1.82,  2.23,  2.64,  3.05,  3.46,  3.87,  4.28,<br>         4.69,  5.1 ]),<br> <a 10 list of patch objects>)</a></p>
<p>probality_1=counts_1[0]/np.sum(counts_1[0]*np.diff(counts_1[1]))</p>
<p>probality_1<br>Out[16]:<br>array([ 0.48780488,  0.        ,  0.24390244,  0.        ,  1.2195122 ,</p>
<pre><code>0.        ,  0.        ,  0.24390244,  0.        ,  0.24390244])
</code></pre><p>最终我们得出算法公式为：<br>频数矩阵/Sum(频数矩阵*组距矩阵)<br>验证算法：<br>plt.hist(np.array([1,2,3,4,5,1.2,1.34,1.78,2.1,2.4,2.8,2.9,3.1,3.5,3.7,4.2,4.9,5.6,5.3,5.8,5.9,5,3,6,7,8.5,6.4,7.3,7.8,7.2,5.6,6.6,6.45,6.99,3.45,2.36,5.67,8.13]),bins=5,normed=True,range=(2,8),color=’yellow’)<br>Out[17]:<br>(array([ 0.234375  ,  0.13020833,  0.10416667,  0.234375  ,  0.13020833]),<br> array([ 2. ,  3.2,  4.4,  5.6,  6.8,  8. ]),<br> <a 5 list of patch objects>)</a></p>
<p>pro1=plt.hist(np.array([1,2,3,4,5,1.2,1.34,1.78,2.1,2.4,2.8,2.9,3.1,3.5,3.7,4.2,4.9,5.6,5.3,5.8,5.9,5,3,6,7,8.5,6.4,7.3,7.8,7.2,5.6,6.6,6.45,6.99,3.45,2.36,5.67,8.13]),bins=5,range=(2,8),color=’yellow’)</p>
<p>pro1[0]<br>Out[19]: array([ 9.,  5.,  4.,  9.,  5.])</p>
<p>pro_12=pro1[0]/np.sum(pro1[0]*np.diff(pro1[1]))</p>
<p>pro_12<br>Out[21]: array([ 0.234375  ,  0.13020833,  0.10416667,  0.234375  ,  0.13020833])</p>
<p>密度图通过kind的KDE关键字来实现的，这里的密度指的是概率密度。它是通过计算观测数据可能产生的概率密度分布而产生的<br>ser_1=np.random.normal(0,1,200)</p>
<p>ser_2=np.random.normal(16,1,200)<br>pis_1,sdr_1=plt.subplots()<br>A=np.concatenate((ser_1,ser_2))<br>ser_1=np.random.normal(0,1,200)</p>
<p>ser_2=np.random.normal(12,1,200)<br>value_1=pd.Series(A)<br>value_1.hist(bins=75,color=’m’,normed=True)<br>value_1.plot(kind=’kde’,style=’g–’)<br>plt.show()</p>
<p>散布图<br>散布图必须通过两个数据序列才能绘制而成。</p>
<p>也可以通过一个数据框绘制成散布矩阵,我们先绘制一个复杂的散布矩阵<br>import pandas as pd</p>
<p>import numpy as np</p>
<p>import matplotlib.pyplot as plt</p>
<p>X2=np.random.normal(3,1,1001)</p>
<p>X1=np.arange(1,1002,1)</p>
<p>X3=np.random.randn(1001)</p>
<p>X4=np.random.rand(1001)</p>
<p>X4=np.random.rand(1001)</p>
<p>X5=np.random.beta(2,1,1001)</p>
<p>test_data=np.column_stack((np.column_stack((np.column_stack((np.column_stack((X1,X2)),X3)),X4)),X5))</p>
<p>test_frame=pd.DataFrame(test_data)</p>
<p>pd.scatter_matrix(test_frame,diagonal=’kde’,color=’g’,figsize=(10,10))<br>C:\Users\dongfeng\Anaconda3\lib\site-packages\ipykernel_launcher.py:1: FutureWarning: pandas.scatter_matrix is deprecated. Use pandas.plotting.scatter_matrix instead<br>  “””Entry point for launching an IPython kernel.<br>Out[7]:<br>array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B2D89E8&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000003422A85240&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000003422A9E2B0&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342AA85860&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342AAD87F0&gt;],<br>       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342AAD8828&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B33FFD0&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B3860F0&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B39BDA0&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B419518&gt;],<br>       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B452898&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B489828&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B4C3828&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B4D4F98&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B522F98&gt;],<br>       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B5684A8&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B5A23C8&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B53A7F0&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B603A58&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B63AF28&gt;],<br>       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B67D4A8&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B6A32B0&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342C6B2518&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342C6EA908&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342C722828&gt;]], dtype=object)</p>
<p>plt.show()</p>
<p>散布矩阵数据索引图如下：<br>X1X1密度图    X2X1    X3X1    X4X1    X5X1<br>X1X2    X2X2密度图    X3X2    X4X2    X5X2<br>X1X3    X2X3    X3X3密度图    X4X3    X5X3<br>X1X4    X2X4    X3X4    X4X4密度图    X5X4<br>X1X5    X2X5    X3X5    X4X5    X5X5密度图</p>
<p>接下来我们通过简单的散布图来验证我们的散布矩阵图：<br>X1X1密度图：</p>
<p>X3X1 和 X1X3：<br>fig_123,axec=plt.subplots(1,2,dpi=140)</p>
<p>for j in range(1):<br>    axec[j].scatter(test_frame[2],test_frame[0],color=’m’,marker=’.’)<br>    axec[j].set_xlabel(‘X3’)<br>    axec[j+1].scatter(test_frame[0],test_frame[2],color=’m’,marker=’.’)<br>    axec[j+1].set_xlabel(‘X1’)<br>plt.show()</p>
<p>数据地图（basemap）<br>from mpl_toolkits.basemap import Basemap<br>   …: import matplotlib.pyplot as plt<br>   …: import pandas as pd<br>   …: import numpy as np<br>   …: from matplotlib import cm</p>
<h1 id="绘制基础地图，选择绘制的区域，因为是绘制中国地图，故选取如下经纬度，lat-0和lon-0是地图中心的维度和经度。"><a href="#绘制基础地图，选择绘制的区域，因为是绘制中国地图，故选取如下经纬度，lat-0和lon-0是地图中心的维度和经度。" class="headerlink" title="绘制基础地图，选择绘制的区域，因为是绘制中国地图，故选取如下经纬度，lat_0和lon_0是地图中心的维度和经度。"></a>绘制基础地图，选择绘制的区域，因为是绘制中国地图，故选取如下经纬度，lat_0和lon_0是地图中心的维度和经度。</h1><p>china_map=Basemap(projection=’stere’,lat_0=34,lon_0=115,llcrnrlat=28 ,urcrnrlat=42,llcrnrlon=105,urcrnrlon=129,rsphere=(2000,2000),resolution=’l’,area_thresh=350)</p>
<p>#参数解释：</p>
<p>#Projection- 地图投影方式，常用的有’ortho’、’merc’、’stere’和’cyl’,’cass’、’lcc’等。 </p>
<p>#llcrnrlat- 所需地图域左下角的纬度（度）。 </p>
<p>#urcrnrlat- 所需地图域的右上角的纬度（度）。 </p>
<p>#llcrnrlon- 所需地图域左下角的经度（度）。 </p>
<p>#urcrnrlon- 所需地图域的右上角的经度（度）。</p>
<p>china_map.drawmapboundary(color=’g’,zorder=0)   # 绘制边界<br>china_map.fillcontinents(color=’y’,lake_color=’b’,zorder=2)   # 填充大陆，发现填充之后无法显示散点图，应该是被覆盖了,因此取消<br>china_map.drawstates(color=’m’,zorder=3)        # 绘制省<br>china_map.drawcoastlines(color=’r’,zorder=3)    # 绘制海岸线，必须绘制，即使是不靠海也需绘制<br>china_map.drawcountries(color=’r’,zorder=3)</p>
<p>#linewidth 设置线宽</p>
<p>#linestyle 设置线形。默认为 solid，可以是 dash，也可以是 matplotlib 其它选项。</p>
<p>#color 设置颜色。默认为 black(k)。</p>
<p>#antialiased 抗锯齿选项。默认为 True.<br>china_map.drawrivers(linewidth=0.5, linestyle=’solid’, color=’#1E90FF’,zorder=3)</p>
<p>#zorder 设置图层位置。默认情况下由 Basemap 设置.<br>china_map.drawlsmask(land_color=’0 ‘,ocean_color=’#1E90FF’,zorder=1)</p>
<p>#china_map.drawcountries(color=’y’)     # 绘制国家，不太适合此例子，但需保留</p>
<p>#china_map.bluemarble()<br>parallels = np.arange(28.,42.,2.)<br>china_map.drawparallels(parallels,labels=[1,0,0,0],fontsize=10,zorder=4) # 绘制纬线</p>
<p>meridians = np.arange(105.,129.,3.)<br>china_map.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10,zorder=4) # 绘制经线<br>data_1=pd.read_csv(r’Desktop\dizhenshuju.csv’,header=None,delimiter=’,’).values<br>lat=data_1[:,2];lon=data_1[:,1];Seismic_grade=data_1[:,3]<br>Seismic_grade_float=np.array(Seismic_grade,dtype=np.float64)<br>class_1=(Seismic_grade_float/np.max(Seismic_grade_float))*5<br>x,y = china_map(lon,lat)#地图上的精度维度匹配参数x，y<br>china_map.scatter(x,y,s=class_1,cmap=cm.hsv,c=’#8A2BE2’,zorder=5) # 使用matplotlib的散点图绘制函数<br>plt.savefig(‘Desktop\dong_1217.pdf’,dpi=300,bbox_inches=’tight’)<br>plt.show()</p>
<p>画布的复杂分割：<br>Subplot2grid</p>
<p>在网格中创建一个子图。网格是由shape指定的，位于loc指定的位置，横跨各个方向上的rowspan个，colspan个单元格。 loc的索引是基于0的。<br>matplotlib.pyplot.subplot2grid(shape, loc, rowspan=1, colspan=1, **kwargs)</p>
<p>例子：<br>def subpicnr_invisible(fig):<br>    for i, ax in enumerate(fig.axes): #利用list(enumerate(plt.gcf().axes))枚举函数生成元组列表[(画框编号,画框对象)(),…]<br>        ax.text(0.5, 0.5, “ax%d” % (i+1))</p>
<p>plt.figure(0,dpi=150)<br>pic_1=plt.subplot2grid((3,3),(0,0),colspan=3)</p>
<p>#画框pic_1位于（0,0），这里的0意味着0行和0列，colspan=3意味着横跨三列，默认的rowspan=1意味着横跨一行</p>
<p>#ax2 = plt.subplot2grid((3, 3), (1, 0), colspan=2)<br>pic_2 = plt.subplot2grid((3, 3), (1, 0), colspan=2)</p>
<p>#画框pic_2位于（1,0），这里的0意味着1行和0列,（‘1’这一行有0和1两列），colspan=2意味着横跨两列，默认的rowspan=1意味着横跨一行。<br>pic_3 = plt.subplot2grid((3,3), (1, 2), rowspan=2)<br>pic_4 = plt.subplot2grid((3,3), (2, 0))<br>pic_5 = plt.subplot2grid((3,3), (2, 1))<br>plt.subplots_adjust(wspace=0.25,hspace=0.4)<br>plt.suptitle(“python_practice_08”)<br>subpicnr_invisible(plt.gcf())#plt.gcf返回所有画布信息,<br>plt.show()</p>
<p>例子2<br>def subpicnr_invisible(fig):<br>    for i, ax in enumerate(fig.axes): #利用list(enumerate(plt.gcf().axes))枚举函数生成元组列表[(画框编号,画框对象)(),…]<br>        ax.text(0.5, 0.5, “ax%d” % (i+1))</p>
<p>plt.figure(0,dpi=150)<br>pic_1=plt.subplot2grid((3,3),(0,0),colspan=2)</p>
<p>#画框pic_1位于（0,0），这里的0意味着0行和0列，colspan=3意味着横跨三列，默认的rowspan=1意味着横跨一行<br>pic_2 = plt.subplot2grid((3, 3), (0, 2), colspan=1)</p>
<p>#pic_2 = plt.subplot2grid((3, 3), (1, 0), colspan=2)</p>
<p>#画框pic_2位于（1,0），这里的0意味着1行和0列,（‘1’这一行有0和1两列），colspan=2意味着横跨两列，默认的rowspan=1意味着横跨一行。<br>pic_3 = plt.subplot2grid((3,3), (1, 0),rowspan=2,colspan=2)<br>pic_4 = plt.subplot2grid((3,3), (1, 2))<br>pic_5 = plt.subplot2grid((3,3), (2, 2))<br>plt.subplots_adjust(wspace=0.25,hspace=0.4)<br>plt.suptitle(“python_practice_08”)<br>subpicnr_invisible(plt.gcf())#plt.gcf返回所有画布信息,<br>plt.show()</p>
<p>GridSpec：<br>例子1：<br>def subpicnr_invisible(fig):<br>    for i, ax in enumerate(fig.axes): #利用list(enumerate(plt.gcf().axes))枚举函数生成元组列表[(画框编号,画框对象)(),…]<br>        ax.text(0.5, 0.5, “ax%d” % (i+1))</p>
<p>import matplotlib.gridspec as gridspec<br>plt.figure(0,dpi=150)<br>gs = gridspec.GridSpec(3, 3)<br>pic_1=plt.subplot(gs[0,:])#按照数组索引理解即可<br>pic_2 = plt.subplot(gs[1,:2])<br>pic_3 = plt.subplot(gs[1:,-1])<br>pic_4 = plt.subplot(gs[2,0])<br>pic_5 = plt.subplot(gs[2,1])<br>plt.subplots_adjust(wspace=0.25,hspace=0.4)<br>plt.suptitle(“python_practice_08”)<br>subpicnr_invisible(plt.gcf())#plt.gcf返回所有画布信息,<br>plt.show()</p>
<p>例子2：<br>def subpicnr_invisible(fig):<br>    for i, ax in enumerate(fig.axes): #利用list(enumerate(plt.gcf().axes))枚举函数生成元组列表[(画框编号,画框对象)(),…]<br>        ax.text(0.5, 0.5, “ax%d” % (i+1))</p>
<p>import matplotlib.gridspec as gridspec<br>plt.figure(0,dpi=150)<br>gs = gridspec.GridSpec(3, 3)<br>pic_1=plt.subplot(gs[0,:2])#按照数组索引理解即可<br>pic_2 = plt.subplot(gs[0,-1])<br>pic_3 = plt.subplot(gs[1:,:-1])<br>pic_4 = plt.subplot(gs[1,-1])<br>pic_5 = plt.subplot(gs[2,-1])<br>plt.subplots_adjust(wspace=0.25,hspace=0.4)<br>plt.suptitle(“python_practice_08”)<br>subpicnr_invisible(plt.gcf())#plt.gcf返回所有画布信息,<br>plt.show()</p>
<p>例子：</p>
<p>plt.figure(dpi=100,figsize=(10,10))<br>gs = gridspec.GridSpec(3, 3)<br>pic_1=plt.subplot(gs[0,:2])#按照数组索引理解即可<br>pic_2 = plt.subplot(gs[0,-1])<br>pic_3 = plt.subplot(gs[1:,:-1])<br>pic_4 = plt.subplot(gs[1,-1])<br>pic_5 = plt.subplot(gs[2,-1])<br>plt.subplots_adjust(wspace=0.25,hspace=0.4)<br>plt.suptitle(“python_practice_08”,fontsize=’20’)<br>n = 730<br>X = np.linspace(-2<em>np.pi,2</em>np.pi,n)<br>Y = np.sin(2<em>X)</em>X+np.pi<br>Y_1=np.sin(2<em>X)</em>X-np.pi<br>pic_1.plot(X,Y,linestyle=’-‘,color=’#FFB6C1’,alpha=1.00)<br>pic_1.plot(X,Y_1,linestyle=’-.’,color=’#1E90FF’,alpha=1.00)<br>n_1=1000<br>X_2 = np.random.normal(0,1,n_1)<br>Y_2= np.random.normal(0,1,n_1)<br>pic_2.scatter(X_2,Y_2,c=np.linspace(0,1,1000),cmap=’coolwarm’)<br>n_2=10<br>X_3 = np.arange(n_2); Y_3 = (1-X_3/np.float(n_2)) <em> np.random.uniform(0,0.5,n_2); Y_3_1 = -(1-X_3/np.float(n_2)) </em> np.random.uniform(0,0.5,n_2)<br>pic_3.bar(X_3,Y_3,facecolor=’#FFB6C1’,width=0.6,align=’center’)<br>pic_3.bar(X_3,Y_3_1,facecolor=’#87CEFA’,width=0.6,align=’center’)<br>for x,y in zip(X_3,Y_3):<br>    pic_3.text(x, y+0.03,’%.2f’ % y,ha=’center’)<br>for x,y in zip(X_3,Y_3_1):<br>    pic_3.text(x, y-0.03,’%.2f’ % y,ha=’center’)<br>f=lambda x,y: (1-x/2+x<strong>5+y</strong>3)*np.exp(-x<strong>2-y</strong>2)<br>x_4 = np.linspace(-3,3,1000)<br>y_4 = np.linspace(-3,3,1000)<br>X_4,Y_4 = np.meshgrid(x_4,y_4)<br>pic_4.contourf(X_4, Y_4, f(X_4,Y_4), 8, alpha=.75, cmap=’coolwarm’)<br>pic_4.contour(X_4, Y_4, f(X_4,Y_4), 8, colors=’black’)<br>t=np.linspace(-np.pi,np.pi,20)<br>u,v=np.array([np.cos(theta) for theta in t]),np.array([np.sin(theta) for theta in t])<br>X_5,Y_5= np.mgrid[0:10,0:10]#晶格化<br>pic_5.quiver(X_5,Y_5,u,v,np.random.randn(10))<br>plt.show()</p>
<p>绘制立体图<br>曲面图：<br>plot_surface<br>plot_surface(X, Y, Z, <em>args, **kwargs)<br>默认情况下，它将以纯色着色，但它也通过提供</em> cmap *参数来支持颜色映射。</p>
<p>‘rstride<code>和</code>cstride` kwargs设置了用于对输入数据进行采样以生成图形的步幅。如果传入1k个1k数组，则步幅的默认值将导致绘制100x100的网格。 默认为10。 如果同时提供了stride和count kwargs（rcount、ccount），则引发ValueError。</p>
<p><code>rcount</code>和<code>ccount</code> kwargs取代<code>rstride</code>和<code>cstride</code>作为表面绘图的默认采样方法。这些参数将决定从输入数据中最多取多少个均匀间隔的样本来生成图。 这是默认的采样方法，除非使用“经典”风格。 如果同时指定了步幅和数量（stride and count），将会引发ValueError。</p>
<p>参数：</p>
<ul>
<li>X <em>，</em> Y <em>，</em> Z *  数据值为二维数组      </li>
<li>rstride *    数组行步幅（步长）      </li>
<li>cstride *    数组列步幅（步长）      </li>
<li>rcount *    最多使用行，默认为50      </li>
<li>ccount <em>    最多使用列，默认为50      
</em>颜色*       曲面片的颜色      </li>
<li>cmap *     曲面片调色板。      </li>
<li>facecolors * 单个曲面片的表面色      </li>
<li>norm *     一个标准化实例，用于将值映射到颜色      </li>
<li>vmin *     映射的最小值      </li>
<li>vmax <em>    映射的最大值      
</em>shade*     是否遮蔽表面色</li>
</ul>
<p>例子：参数化坐标轴下的三维球<br>from mpl_toolkits.mplot3d import Axes3D<br>import numpy as np<br>import matplotlib.pyplot as plt<br>u = np.linspace(0,2<em>np.pi,1000)<br>v = np.linspace(0,np.pi,1000)<br>x=10</em>np.outer(np.sin(v),np.cos(u))<br>y=10<em>np.outer(np.sin(v),np.sin(u))<br>z=10</em>np.outer(np.cos(v),np.ones(len(np.cos(v))))</p>
<p>#创建二维数据集X,Y和Z，注意他们的值必须在各矩阵相同位置处一一对应。<br>fig=plt.figure(0,figsize=(8,8),dpi=120)<br>ax=fig.add_subplot(1,1,1,projection=’3d’)</p>
<p>#ax.plot_surface(x,y,z,rcount=1000,ccount=1000,cmap=’coolwarm’)#太浪费时间了，用下面替换语句<br>ax.plot_surface(x,y,z,cmap=’hot’)<br>plt.show()</p>
<p>例子：非参数化坐标轴下曲面图<br>import numpy as np<br>from mpl_toolkits.mplot3d import Axes3D<br>import matplotlib.pyplot as plt<br>import random<br>def fun(x, y):<br>  return np.sqrt(x<strong>2 + y</strong>2)<br>fig = plt.figure(0,dpi=120,figsize=(6,6))<br>ax = fig.add_subplot(111, projection=’3d’)<br>x = y = np.linspace(-5.0, 5.0, 100)<br>X, Y = np.meshgrid(x, y)<br>zs = np.array([fun(x,y) for x,y in zip(np.ravel(X), np.ravel(Y))])<br>Z = zs.reshape(Y.shape)<br>ax.plot_surface(X, Y, Z,cmap=’jet’)<br>ax.set_xlabel(‘X Label’)<br>ax.set_ylabel(‘Y Label’)<br>ax.set_zlabel(‘Z Label’)</p>
<p>#(np.ravel(X)).shape<br>plt.show()</p>
<p>import numpy as np<br>from mpl_toolkits.mplot3d import Axes3D<br>import matplotlib.pyplot as plt<br>import random</p>
<p>def fun(x, y):<br>  return np.cos(np.sqrt(x<strong>2 + y</strong>2))</p>
<p>fig = plt.figure(0,dpi=120,figsize=(6,6))<br>ax = fig.add_subplot(111, projection=’3d’)<br>x = y = np.linspace(-5.0, 5.0, 100)<br>X, Y = np.meshgrid(x, y)<br>zs = np.array([fun(x,y) for x,y in zip(np.ravel(X), np.ravel(Y))])<br>Z = zs.reshape(Y.shape)</p>
<p>ax.plot_surface(X, Y, Z,cmap=’jet’)</p>
<p>ax.set_xlabel(‘X Label’)<br>ax.set_ylabel(‘Y Label’)<br>ax.set_zlabel(‘Z Label’)</p>
<p>#(np.ravel(X)).shape<br>plt.show()</p>
<p>例子，带色度条的三维图<br>import numpy as np<br>from mpl_toolkits.mplot3d import Axes3D<br>import matplotlib.pyplot as plt<br>import random</p>
<p>f=lambda x,y: (1-x/2+x<strong>5+y</strong>3)*np.exp(-x<strong>2-y</strong>2)<br>fig = plt.figure(0,dpi=160,figsize=(10,10))<br>ax = fig.add_subplot(111, projection=’3d’)<br>x = y = np.linspace(-5.0, 5.0, 100)<br>X, Y = np.meshgrid(x, y)<br>zs = np.array([f(x,y) for x,y in zip(np.ravel(X), np.ravel(Y))])<br>Z = zs.reshape(Y.shape)</p>
<p>pic_3dim=ax.plot_surface(X, Y, Z,cmap=’Spectral_r’)<br>plt.colorbar(pic_3dim)  #画出色度条<br>ax.set_xlabel(‘X Label’)<br>ax.set_ylabel(‘Y Label’)<br>ax.set_zlabel(‘Z Label’)</p>
<p>#(np.ravel(X)).shape<br>plt.show()</p>
<p>另外的一种表面图绘制方法：<br>例如：<br>import matplotlib.pyplot as plt<br>from mpl_toolkits.mplot3d import Axes3D #绘制3D坐标的函数<br>import numpy as np  </p>
<p>def fun(x,y):<br>    return np.power(x,2)+np.sin(np.power(y,2))*x  </p>
<p>fig1=plt.figure(0,dpi=160,figsize=(8,8))<br>ax=Axes3D(fig1)#三维化画布并产生三维画框<br>X=np.arange(-3,3,0.05)<br>Y=np.arange(-3,3,0.05)<br>X,Y=np.meshgrid(X,Y)#生成坐标点<br>Z=fun(X,Y)<br>plt.title(‘python-08’)<br>ax.plot_surface(X, Y, Z,cmap=’coolwarm’)<br>ax.set_xlabel(‘x label’, color=’r’)<br>ax.set_ylabel(‘y label’, color=’g’)<br>ax.set_zlabel(‘z label’, color=’b’)<br>plt.show()</p>
<p>曲线图<br>例子：我的葫芦<br>from mpl_toolkits.mplot3d import Axes3D</p>
<p>fig = plt.figure(0,dpi=100,figsize=(6,6))<br>ax = fig.add_subplot(1,1,1, projection=’3d’)<br>theta = np.linspace(-20 <em> np.pi, 20 </em> np.pi, 1000)<br>z = np.linspace(0,10,1000);phi=np.linspace(0,2<em>np.pi,1000)<br>r=z</em>np.sin(phi)<br>x = r <em> np.sin(theta)<br>y = r </em> np.cos(theta)<br>ax.plot(x, y, z, label=’curve’)<br>ax.legend()<br>plt.show()</p>
<p>散点图（见LDA）<br>线框图<br>例子：<br>import numpy as np  </p>
<p>def fun(x,y):<br>    return np.power(x,2)+np.sin(np.power(y,2))*x  </p>
<p>fig1=plt.figure(0,dpi=160,figsize=(8,8))<br>ax=Axes3D(fig1)#三维化画布并产生三维画框<br>X=np.arange(-3,3,0.05)<br>Y=np.arange(-3,3,0.05)<br>X,Y=np.meshgrid(X,Y)#生成坐标点<br>Z=fun(X,Y)<br>plt.title(‘python-08’)<br>ax.plot_wireframe(X, Y, Z, rstride=3, cstride=3)#一定要调节成大的扫描步长才有效果<br>ax.set_xlabel(‘x label’, color=’r’)<br>ax.set_ylabel(‘y label’, color=’g’)<br>ax.set_zlabel(‘z label’, color=’b’)<br>plt.show()</p>
<p>等高线图<br>contour和contourf都是画三维等高线图的，不同点在于contourf会对等高线间的区域进行填充。<br>参数：</p>
<p><em>X</em>, <em>Y</em>, <em>Z</em>     数组型数据<br><em>extend3d</em>      是否在3D中扩展等高线图（默认值：False）<br><em>stride</em>         <em>步幅</em>，用于扩展等高线图的步幅（步长）<br><em>zdir</em>           等高线图产生方向: x, y 或 z (default)<br><em>offset</em>         如果赋值，绘制等高线投影到垂直于zdir并且通过偏移量确定位置的平面</p>
<p>例子：<br>import numpy as np  </p>
<p>def fun(x,y):<br>    return (1/(2<em>np.pi))</em>np.exp(-0.5*(x<strong>2+y</strong>2))  </p>
<p>fig1=plt.figure(0,dpi=160,figsize=(8,8))<br>ax=Axes3D(fig1)#三维化画布并产生三维画框<br>X=np.arange(-3,3,0.05)<br>Y=np.arange(-3,3,0.05)<br>X,Y=np.meshgrid(X,Y)#生成坐标点<br>Z=fun(X,Y)<br>plt.title(‘python-08’)<br>ax.plot_surface(X, Y, Z, rstride=3, cstride=3,cmap=’jet’)<br>ax.contour(X, Y, Z, zdir=’z’,offset=0.16,cmap=’coolwarm’)<br>cset = ax.contour(X, Y, Z, zdir=’x’, offset=-3, cmap=’coolwarm’)<br>cset = ax.contour(X, Y, Z, zdir=’y’, offset=3, cmap=’coolwarm’)<br>ax.set_xlabel(‘x label’, color=’r’)<br>ax.set_ylabel(‘y label’, color=’g’)<br>ax.set_zlabel(‘z label’, color=’b’)<br>plt.show()</p>
<p>动态图<br>运动中的布朗运动<br>from matplotlib import pyplot as plt</p>
<p>from matplotlib import animation<br>import numpy as np<br>%matplotlib qt5<br>def randn_point():</p>
<pre><code># 产生随机散点图的x和y数据
x=np.random.randn(100)
y=np.random.randn(100)
return x,y
</code></pre><p>fig,ax1=plt.subplots(1,1,dpi=130,figsize=(8,6))</p>
<h1 id="先绘制初始图形"><a href="#先绘制初始图形" class="headerlink" title="先绘制初始图形"></a>先绘制初始图形</h1><p>x1,y1=randn_point()<br>sca1 = ax1.scatter(x1,y1)   # 散点图<br>def init():</p>
<pre><code># 构造开始帧函数init
# 改变散点图数据
x1, y1 = randn_point()
data1 = [[x,y] for x,y in zip(x1,y1)]
sca1.set_offsets(data1)  # 散点图
label = &apos;timestep {0}&apos;.format(0)
ax1.set_xlabel(label)
return sca1,ax1  # 注意返回值，我们要更新的就是这些数据
</code></pre><p>def animate(i):</p>
<pre><code># 接着，构造自定义动画函数animate，用来更新每一帧上各个x对应的y坐标值，参数表示第i帧

x1, y1 = randn_point()

x2, y2 = randn_point()

data1 = [[x,y] for x,y in zip(x1,y1)]

sca1.set_offsets(data1)  # 散点图

label = &apos;timestep {0}&apos;.format(i)

ax1.set_xlabel(label)

return sca1,ax1
</code></pre><h1 id="接下来，我们调用FuncAnimation函数生成动画。参数说明："><a href="#接下来，我们调用FuncAnimation函数生成动画。参数说明：" class="headerlink" title="接下来，我们调用FuncAnimation函数生成动画。参数说明："></a>接下来，我们调用FuncAnimation函数生成动画。参数说明：</h1><h1 id="fig-进行动画绘制的figure"><a href="#fig-进行动画绘制的figure" class="headerlink" title="fig 进行动画绘制的figure"></a>fig 进行动画绘制的figure</h1><h1 id="func-自定义动画函数，即传入刚定义的函数animate"><a href="#func-自定义动画函数，即传入刚定义的函数animate" class="headerlink" title="func 自定义动画函数，即传入刚定义的函数animate"></a>func 自定义动画函数，即传入刚定义的函数animate</h1><h1 id="frames-动画长度，一次循环包含的帧数"><a href="#frames-动画长度，一次循环包含的帧数" class="headerlink" title="frames 动画长度，一次循环包含的帧数"></a>frames 动画长度，一次循环包含的帧数</h1><h1 id="init-func-自定义开始帧，即传入刚定义的函数init"><a href="#init-func-自定义开始帧，即传入刚定义的函数init" class="headerlink" title="init_func 自定义开始帧，即传入刚定义的函数init"></a>init_func 自定义开始帧，即传入刚定义的函数init</h1><h1 id="interval-更新频率，以ms计"><a href="#interval-更新频率，以ms计" class="headerlink" title="interval 更新频率，以ms计"></a>interval 更新频率，以ms计</h1><h1 id="blit-选择更新所有点，还是仅更新产生变化的点。应选择True，但mac用户请选择False，否则无法显示动画"><a href="#blit-选择更新所有点，还是仅更新产生变化的点。应选择True，但mac用户请选择False，否则无法显示动画" class="headerlink" title="blit 选择更新所有点，还是仅更新产生变化的点。应选择True，但mac用户请选择False，否则无法显示动画"></a>blit 选择更新所有点，还是仅更新产生变化的点。应选择True，但mac用户请选择False，否则无法显示动画</h1><p>ani = animation.FuncAnimation(fig=fig,func=animate,frames=1000,init_func=init,interval=100,blit=False)<br>plt.show()</p>
<p>布朗运动过程模拟：</p>
<p>from matplotlib import pyplot as plt</p>
<p>from matplotlib import animation<br>import numpy as np<br>%matplotlib qt5<br>def randn_point(t):<br>    Data=np.random.randn(100,10001)<br>    brown_moving=Data.T[0]<em>t+sum([Data_x</em>((np.sqrt(2)<em>np.sin(i</em>np.pi<em>t))/(i</em>np.pi)) for Data_x,i in list(zip(Data.T[1:],range(1,10001)))])<br>angel=np.random.randn(100)<em>360<br>return brown_moving</em>np.cos(angel),brown_moving*np.sin(angel)<br>fig,ax1=plt.subplots(1,1,dpi=130,figsize=(8,6))</p>
<h1 id="先绘制初始图形-1"><a href="#先绘制初始图形-1" class="headerlink" title="先绘制初始图形"></a>先绘制初始图形</h1><p>plt.xlim([-100,100])</p>
<p>#plt.ylim([-50,50])</p>
<p>x1,y1=randn_point(np.random.randint(100))<br>sca1 = ax1.scatter(x1,y1)<br>def init():</p>
<pre><code># 构造开始帧函数init
# 改变散点图数据
x1, y1 = randn_point(np.random.randint(100))
data1 = [[x,y] for x,y in zip(x1,y1)]
sca1.set_offsets(data1)  # 散点图
label = &apos;timestep {0}&apos;.format(0)
ax1.set_xlabel(label)
return sca1,ax1  # 注意返回值，我们要更新的就是这些数据
</code></pre><p>def animate(i):</p>
<pre><code># 接着，构造自定义动画函数animate，用来更新每一帧上各个x对应的y坐标值，参数表示第i帧

x1, y1 = randn_point(i)

x2, y2 = randn_point(i)

data1 = [[x,y] for x,y in zip(x1,y1)]

sca1.set_offsets(data1)  # 散点图

label = &apos;timestep {0}&apos;.format(i)

ax1.set_xlabel(label)

return sca1,ax1
</code></pre><p>ani = animation.FuncAnimation(fig=fig,func=animate,frames=100,init_func=init,interval=1,blit=False)<br>plt.show()</p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/数据分析/" rel="tag"># 数据分析</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/06/05/Typora常用语法/" rel="next" title="Typora常用语法">
                <i class="fa fa-chevron-left"></i> Typora常用语法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/06/12/概率论/" rel="prev" title="概率论">
                概率论 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC80NDExNC8yMDY0OQ=="></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/head.jpg" alt="kl">
            
              <p class="site-author-name" itemprop="name">kl</p>
              <div class="site-description motion-element" itemprop="description">66其实不太6</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">118</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">31</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">49</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Pandas的数据结构"><span class="nav-number">1.</span> <span class="nav-text">Pandas的数据结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Series"><span class="nav-number">1.0.1.</span> <span class="nav-text">Series:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DataFrame"><span class="nav-number">1.0.2.</span> <span class="nav-text">DataFrame</span></a></li></ol></li></ol><li class="nav-item nav-level-2"><a class="nav-link" href="#getting-1-json-loads-Json-documents"><span class="nav-number">2.</span> <span class="nav-text">getting_1=json.loads(Json_documents)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#“””Entry-point-for-launching-an-IPython-kernel"><span class="nav-number">3.</span> <span class="nav-text">  “””Entry point for launching an IPython kernel.</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#绘制基础地图，选择绘制的区域，因为是绘制中国地图，故选取如下经纬度，lat-0和lon-0是地图中心的维度和经度。"><span class="nav-number"></span> <span class="nav-text">绘制基础地图，选择绘制的区域，因为是绘制中国地图，故选取如下经纬度，lat_0和lon_0是地图中心的维度和经度。</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#先绘制初始图形"><span class="nav-number"></span> <span class="nav-text">先绘制初始图形</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#接下来，我们调用FuncAnimation函数生成动画。参数说明："><span class="nav-number"></span> <span class="nav-text">接下来，我们调用FuncAnimation函数生成动画。参数说明：</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#fig-进行动画绘制的figure"><span class="nav-number"></span> <span class="nav-text">fig 进行动画绘制的figure</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#func-自定义动画函数，即传入刚定义的函数animate"><span class="nav-number"></span> <span class="nav-text">func 自定义动画函数，即传入刚定义的函数animate</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#frames-动画长度，一次循环包含的帧数"><span class="nav-number"></span> <span class="nav-text">frames 动画长度，一次循环包含的帧数</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#init-func-自定义开始帧，即传入刚定义的函数init"><span class="nav-number"></span> <span class="nav-text">init_func 自定义开始帧，即传入刚定义的函数init</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#interval-更新频率，以ms计"><span class="nav-number"></span> <span class="nav-text">interval 更新频率，以ms计</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#blit-选择更新所有点，还是仅更新产生变化的点。应选择True，但mac用户请选择False，否则无法显示动画"><span class="nav-number"></span> <span class="nav-text">blit 选择更新所有点，还是仅更新产生变化的点。应选择True，但mac用户请选择False，否则无法显示动画</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#先绘制初始图形-1"><span class="nav-number"></span> <span class="nav-text">先绘制初始图形</span></a></li></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>
    

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2023</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">kl</span>

  

  
</div>


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>














  
    
    
      
    
  
  <script color="0,0,0" opacity="0.8" zindex="-1" count="66" src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@1/canvas-nest.min.js"></script>













  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.1.1"></script>

  <script src="/js/motion.js?v=7.1.1"></script>



  
  


  <script src="/js/affix.js?v=7.1.1"></script>

  <script src="/js/schemes/pisces.js?v=7.1.1"></script>




  
  <script src="/js/scrollspy.js?v=7.1.1"></script>
<script src="/js/post-details.js?v=7.1.1"></script>



  


  <script src="/js/next-boot.js?v=7.1.1"></script>


  

  

  

  


  
    <script>
  window.livereOptions = {
    refer: '2019/06/11/pandas和绘图课件/'
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
</script>

  


  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
<script type="text/javascript" src="/js/src/clicklove.js"></script>
<!-- <script type="text/javascript" src="/js/src/fish.js"></script> -->
<!-- <script src='https://blog-static.cnblogs.com/files/elkyo/star.js'></script> -->
<!-- 雪花特效 -->
<!-- 雪花特效 -->
<!-- <script type="text/javascript" src="\js\snow.js"></script> -->
