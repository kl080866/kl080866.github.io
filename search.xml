<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[反向推包算法]]></title>
    <url>%2F2023%2F04%2F26%2F%E5%8F%8D%E5%90%91%E6%8E%A8%E5%8C%85%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[整体流程1234567891011121314151617181920212223242526271. 理解什么是推包和反向推包: 从海量资产中逆向筛选出满足要求的基础资产池,即条件已知反过来推符合条件的2. 模型的实现流程:前期处理-&gt;建立模型-&gt;求解模型 前期处理: 备选池处理 建立模型: 确定决策变量-&gt;决策变量取值限制-&gt;建立目标函数-&gt;设立约束条件 模型求解: 单纯形法,分支切割法,目前代码使用的是CBC和GLOP要求是线性的: 非线性要转换成线性 a/b&gt;2 转换成a-2b&gt;0使用的是混合整数规划的思想,分成线性规划和纯整数规划.约束条件分成几类,其中规模必须有,按照优先级排序. 全部约束条件获取不到,则依次减少代码层面: 一: 对备选池数据进行筛选 ①按照约束条件进行排序 ②排序完,需要针对约束条件获取到最少获取的资产数量,目的是减少计算过程,省得浪费时间 1)按照资产规模上限,使用累计值,获取最低索引位置 2)按照2和3的资产规模限制获取最低的索引位置 3)当没有排序只有枚举值时候,使用枚举条件的第一个获取该字段大于0的最大索引和三倍规模上限获取最小的值. (3倍最大规模所在位置)总次数TOTAL_TIME 的获取逻辑是什么,没看懂glop_cha=0.00的作用是什么跑这个代码对服务器的要求是怎么样的,内存,cpu 笔记121. File_parameter存放各个文件的地址,包含线性规划限制条件Parameter.csv,推包结果集result.csv. 资产池文件Asset_IN_10w.csv核心点就是Parameter文件]]></content>
  </entry>
  <entry>
    <title><![CDATA[Python进行PDF识别]]></title>
    <url>%2F2023%2F04%2F24%2FPython%E8%BF%9B%E8%A1%8CPDF%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[1.1主要是文字和表格识别,图片识别相对较少或者没有,针对图片识别,可以使用paddler-ocr或者pytenseract实现,或者自己训练一个卷积神经网络的模型进行识别(lspn+crnn) 2.业务知识12345678910111213141516171819202122232425NPL: 代表不良情况说明: 目前仅支持pdf的识别,doc文档不支持. 针对PDF版本,支持受托报告和发行说明书. 受托报告分为不良NPL和正常两种情况.两个代码. ABS+ABN合并识别,只要针对发行说明书.用到了ocr提取图片,不涉及文字提取. ABS:资产证券化, 分为信贷ABS,企业ABS(格式不统一,正在做),ABN. ABS受托报告: 分为不良和正常两种,不良的pdf格式相对固定,需要返回的字段也不尽相同匹配逻辑: 先看能不能匹配到对应的数据,能匹配到再查找想要的结果存在一些指标已经写好给定值,只需要判断在文档里能不能找到对应的值即可先匹配表格,没有再匹配文档,部分需要使用ocr去获取图片进行保存存在需要提取交易结构图的情况,分为两种情况,交易结构本来就是图和本来不是图.针对本来就是图的情况,需要使用坐标定位图片所在的位置(四个坐标位置),然后使用裁剪功能裁剪图片开发思路: 改bug或者匹配不准,先看规则的正则,然后看代码的正则新识别的开发思路: 按照某一个版本,重新写猜想:是否可以结合jieba提词难点:1.针对一个pdf文档,我怎么确定哪些是我需要的数据2.不同pdf格式的差别]]></content>
  </entry>
  <entry>
    <title><![CDATA[Clickhouse集群部署]]></title>
    <url>%2F2023%2F04%2F18%2FClickhouse%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[详细部署文档见D:\Blog\source_posts\Clickhouse集群部署\下的word文档 ROOT部署 一: 安装docker和docker-compose 服务器 操作用户 序列 步骤 操作命令 168.130.7.11 (APP1) 168.130.7.12 (APP2) 168.130.7.13 (DB1) 168.130.7.14 (DB2) 168.130.7.15 (CP1) 168.130.7.16 (CP2) root 1 准备工作:hylc.zip 上传 hylc.zip 到 /home/hybrisk 目录 cd /home/hybrisk unzip hylc.zip cd hylc mv docker_deploy ../ cd ../docker_deploy/1.docker-setup 2 安装docker和docker-compose tar -zxf docker-20.10.11.tgz cp docker/* /usr/local/bin/ cp docker-compose-Linux-x86_64 /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose 3 新建docker配置文件 mkdir -p /etc/docker/ 创建配置文件1 vi /etc/docker/daemon.json 填写文件内容（6行） { “registry-mirrors”: [ “https://registry.docker-cn.com&quot; ], “data-root”: “/home/hybrisk/docker” } 创建配置文件2 vi /usr/lib/systemd/system/docker.service 填写文件内容（23行） [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com After=network-online.target firewalld.service Wants=network-online.target [Service] Type=notify ExecStart=/usr/local/bin/dockerd ExecReload=/bin/kill -s HUP $MAINPID LimitNOFILE=infinity LimitNPROC=infinity TimeoutStartSec=0 Delegate=yes KillMode=process Restart=on-failure StartLimitBurst=3 StartLimitInterval=60s [Install] WantedBy=multi-user.target 4 加载并启动docker systemctl daemon-reload systemctl enable docker systemctl start docker 5 把hybrisk加入docker用户组，并对相应的文件赋权 groupadd docker usermod -aG docker hybrisk systemctl daemon-reload systemctl restart docker chown -R hybrisk:docker /home/hybrisk/docker chown -R hybrisk:docker /usr/local/bin/containerd /usr/local/bin/docker /usr/local/bin/ctr /usr/local/bin/runc chown -R hybrisk:docker /etc/docker/daemon.json chown -R hybrisk:docker /usr/lib/systemd/system/docker.service cd /home/hybrisk chown -R hybrisk:hybrisk docker_deploy chown -R hybrisk:hybrisk hylc hylc.zip 二: 安装ClickHouse 服务器 用户 序列 描述 操作步骤 168.130.7.13(DB1) 168.130.7.14(DB2) 168.130.7.16(CP2) root 1 确认clickhouse用户已经创建 确认clickhouse数据目录 如果没有，需要运维团队确认 ls /home/clickhouse ls /home/clickhouse/data 2 准备工作:installs.zip 上传ClickHouse安装文件installs.zip 到 /home/clickhouse/data 目录 cd /home/clickhouse/data unzip installs.zip cd installs 3 解压安装包 tar -zxf clickhouse-client-22.3.6.5-amd64.tgz tar -zxf clickhouse-common-static-22.3.6.5-amd64.tgz tar -zxf clickhouse-server-22.3.6.5-amd64.tgz 4 安装公共包 ./clickhouse-common-static-22.3.6.5/install/doinst.sh 5 安装服务端 ./clickhouse-server-22.3.6.5/install/doinst.sh 输入hybrisk 输入N 6 安装客户端 ./clickhouse-client-22.3.6.5/install/doinst.sh 7 如果防火墙开启，增加端口配置 systemctl status firewalld 确认返回： Active: active(running)。 如返回内容中包含 Active: inactive (dead)，则表示服务器没有开启防火墙，可以直接跳到第9步 firewall-cmd –add-port=8123/tcp –permanent firewall-cmd –add-port=9000/tcp –permanent firewall-cmd –add-port=9004/tcp –permanent firewall-cmd –reload 8 创建数据挂载目录 (根据实际硬盘挂载目录再调整) mkdir -p /home/clickhouse/data/ck1 9 给目录赋权 chown clickhouse:clickhouse -R /home/clickhouse/data/ck1 10 配置目录属主赋予clickhouse用户 chown clickhouse:clickhouse -R /etc/clickhouse-server chown clickhouse:clickhouse -R /etc/clickhouse-client chown clickhouse:clickhouse -R /home/clickhouse/data root 11 备份工具安装： clickhouse-backup cd /home/clickhouse/data/installs tar -xf clickhouse-backup.tar cd clickhouse-backup cp clickhouse-backup /usr/local/bin/ chown clickhouse:clickhouse -R /usr/local/bin/clickhouse-backup chown clickhouse:clickhouse -R /home/clickhouse/data 应用部署 一: 启动ClickHouse 服务器 用户 序列 描述 操作步骤 168.130.7.13(DB1) 168.130.7.14(DB2) 168.130.7.16(CP2) clickhouse 1 创建默认配置文件备份目录 mkdir /etc/clickhouse-server/bk 2 移除默认配置文件 chmod -R 775 /etc/clickhouse-server/ mv /etc/clickhouse-server/* /etc/clickhouse-server/bk 3 上传配置文件/13/* cd /etc/clickhouse-server 中转机下载13.zip unzip 13.zip cd 13 mv * ../ chmod -R 755 /etc/clickhouse-server/ 4 上传配置文件/14/* cd /etc/clickhouse-server 中转机下载14.zip unzip 14.zip cd 14 mv * ../ chmod -R 755 /etc/clickhouse-server/ 5 上传配置文件/16/* cd /etc/clickhouse-server 中转机下载16.zip unzip 16.zip cd 16 mv * ../ chmod -R 755 /etc/clickhouse-server/ 7 手动创建日志目录 mkdir /home/clickhouse/data/log 8 压缩备份脚本 cd /etc/clickhouse-server zip -r bk.zip bk rm -rf bk 9 启动clickhouse clickhouse start 二: ClickHouse 导入数据 服务器 用户 序列 描述 操作步骤 168.130.7.16(CP2) clickhouse 1 创建备份目录(表结构) mkdir /var/lib/clickhouse/backup mkdir /home/clickhouse/data/ck1/backup 2 上传 ch_bk_yyyymmdd.table.zip 上传至 /var/lib/clickhouse/backup 目录 上传 ch_bk_yyyymmdd.data.zip 上传至 /home/clickhouse/data/ck1/backup 目录 3 上传三份配置文件到备份目录 config_13.xml, config_14.xml, config_15,xml 上传至 /var/lib/clickhouse/backup 一台服务器执行 4 登录客户端 clickhouse-client -h 127.0.0.1 -u cebrisk –password 5 创建数据库 create database chdb on cluster jxshares; 6 退出客户端 exit 7 导入表结构及数据 clickhouse-backup restore -c /var/lib/clickhouse/backup/config_13.yml -s ch_bk_yyyymmdd clickhouse-backup restore -c /var/lib/clickhouse/backup/config_14.yml -s ch_bk_yyyymmdd clickhouse-backup restore -c /var/lib/clickhouse/backup/config_16.yml ch_bk_yyyymmdd 三: 导入MySQL数据 服务器 用户 序列 描述 操作步骤 168.130.7.13 (DB1) mysql 1 导入数据 项目组提供数据文件，由行方数据库运维人员导入 完成 mysql -u hybrisk -p use hybrisk source /备份脚本路径/xxx.sql 四: 安装应用 服务器 用户 序列 描述 操作步骤 168.130.7.11(APP1) 168.130.7.12(APP2) hybrisk 1 上传文件 docker_deploy_xx.zip 上传至 /home/hybrisk/ 目录 2 放置安装文件 mkdir /home/hybrisk/tmp mv docker_deploy_xx.zip /home/hybrisk/tmp/ cd /home/hybrisk/tmp/ unzip docker_deploy_xx.zip cd docker_deploy mv * /home/hybrisk/docker_deploy 3 进镜像目录 cd /home/hybrisk/docker_deploy/images 4 加载Nginx镜像 docker load &lt; prmnginx.tar 5 加载金融算法镜像 docker load &lt; fin_server_1.2.0.tar 6 docker load &lt; fin_jupyter_1.0.0.tar 7 docker load &lt; jxhub_1.0.0.tar 8 加载jdk镜像 docker load &lt; openjdk_8u292-jdk_GMT8_1.0.tar docker load &lt; jxhub_1.0.0.tar 五: 安装应用 服务器 用户 序列 描述 操作步骤 168.130.7.15(CP1) 168.130.7.16(CP2) hybrisk 1 上传文件 docker_deploy_xx.zip 上传至 /home/hybrisk/ 目录 2 放置安装文件 mkdir /home/hybrisk/tmp mv docker_deploy_xx.zip /home/hybrisk/tmp/ cd /home/hybrisk/tmp/ unzip docker_deploy_xx.zip cd docker_deploy mv * /home/hybrisk/docker_deploy 3 进镜像目录 cd /home/hybrisk/docker_deploy/images 7 加载jdk镜像 docker load &lt; openjdk_8u292-jdk_GMT8_1.0.tar 六: 启动应用 服务器 用户 序列 描述 操作步骤 168.130.7.11(APP1) 168.130.7.12(APP2) 168.130.7.15(CP1) 168.130.7.16(CP2) hybrisk 1 登录启动目录 cd /home/hybrisk/docker_deploy/dc-start 2 启动所有应用 docker-compose up -d 3 停止所有应用 docker-compose down 4 重启某个应用 docker-compose restart 某应用 七: 验证 服务器 用户 序列 描述 操作步骤 办公终端 1 绩效归因服务 http://168.130.7.17/hxbweb/ 2 绩效归因中台 http://168.130.7.17/dpweb/ 堡垒机/跳板机 3 注册中心 http://168.130.7.11:8761 4 金融算法开发平台 http://168.130.7.11:8001 5 MySQL连接 168.130.7.18:3306/dbrisk 6 ClickHouse连接(dbeaver) 168.130.7.19:8123/chdb 验证]]></content>
      <categories>
        <category>Clickhouse</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[tets]]></title>
    <url>%2F2023%2F04%2F11%2Ftets%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2023%2F03%2F31%2Fclickhouse%E6%9B%B4%E6%96%B0%E6%8F%92%E5%85%A5%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548指定引擎: engine=CollapsingMergeTree逻辑: 1.构建临时表2.向临时表中插入批跑出来的数据, 包含字段: 批返回的字段+ CREATE_TIME, CREATE_USER, UPDATE_TIME, INPUT_TYPE, _sign3.向目标表插入新增的主键数据(eg: 新增的PORT_ID,VALID_DATE)4.向目标表插入批跑出来的数据(关联目标表,按照主键merge,需要更新的字段从临时表获取,不需要更新的从目标表获取)5.整合更新最新数据①无分区键: 直接"optimize table " + table_name + " final"②有分区键: "optimize table dwd_val_detail PARTITION + 分区id+ final"案例:1. CREATE TABLE dwd_val_detail2C3wru86 AS dwd_val_detail engine=CollapsingMergeTree(_sign) PARTITION BY toYYYYMM(VALID_DATE)ORDER BY (SEC_ID, PORT_ID, SUB_PORT_ID, VALID_DATE, ACCOUNT_CLASS_TYPE) SETTINGS index_granularity = 8192 2.INSERT INTO tag_val_detailM7mZbs4d(PORT_ID, SUB_PORT_ID, SEC_ID, ACCOUNT_CLASS_TYPE, VALID_DATE, UPDATE_USER, REAL_SEC_ID, CREATE_TIME, CREATE_USER, UPDATE_TIME, INPUT_TYPE, _sign) VALUES3.insert into dwd_val_detail select * from dwd_val_detailgACo328j where (PORT_ID,SUB_PORT_ID,SEC_ID,VALID_DATE,ACCOUNT_CLASS_TYPE) not in (select DISTINCT PORT_ID,SUB_PORT_ID,SEC_ID,VALID_DATE,ACCOUNT_CLASS_TYPE from dwd_val_detail)4.insert into dwd_val_detail (SEC_ID, VALUATION_METHODS, BACK_AMOUNT, COUNTERPARTY_ID, END_DATE, INTECALC_RULE, UNDERLYING_CODE, UNDERLYING_VOLUME, ORDER_ID, F_JSFXJ, F_HHF, F_RGF, F_SGF, F_SHF, INT_RATE, ASS_CODE, CASH_PS_PAYMENT, BOND_PS_PAYMENT, F_YHS, F_JSF, F_GHF, F_ZGF, F_JSFWF, REGIST_ORG, REMARK, SETTLE_SPEED, PAR_VALUE, CPRICE_AMT, FPRICE_AMT, INV_AIM, TRAN_COMMISION, TRADER_NAME, IMGR_ID_NAME, TRAN_STATUS, CPRICE_DATE, IS_CANCEL, COUNTER_TRADER, ENT_INT_FLAG, LONG_CURRENCY, LONG_QUANTITY, LONG_PRINCIPAL, LONG_INTEREST, LONG_AMOUNT, LONG_TRAN_FEE, TRADE_ID, DEAL_DEP, TRAN_DATE, SETTLE_DATE, TRAN_SIDE, PRICE_YIELD, COUNTER_PARTY, Column39, Column40, Column41, SEC_ABBR, REAL_SEC_ID, NET_VAL_DATE, NET_VAL_PRICE, Column32, Column33, Column34, Column35, Column36, Column37, Column38, Column25, Column26, Column27, Column28, Column29, Column30, Column31, Column18, Column19, Column20, Column21, Column22, Column23, Column24, Column11, Column12, Column13, Column14, Column15, Column16, Column17, Column4, Column5, Column6, Column7, Column8, Column9, Column10, TRADE_MARKET, SEC_CATEGORY, ACC_ID, ACC_NAME, Column1, Column2, Column3, L_MV_LOSS, L_NET_MV_LOSS, NET_MV_LOSS, ASSET_BIG_TYPE, ASSET_CATEGORY, LEVEL_TYPE, PORT_NAME, _sign, SUB_PORT_ID, DEPT_ID, ASSET_LOSS, IS_LT_BOUNDED, L_ASSET_LOSS, FULL_MV_LOSS, UPDATE_USER, UPDATE_TIME, INPUT_TYPE, L_SHADOW_MV, L_SHADOW_DELTA, FULL_COST, L_FULL_COST, L_R_GL, L_GAINLOSS_GL, L_AMOR_GL, L_TOTAL_GL, CREATE_USER, CREATE_DEPT, CREATE_TIME, L_INTEREST, L_DIVIDEND, L_GAINLOSS, L_INCOME_GL, L_INTEREST_GL, L_EXPENSE, L_INVEST_GL, L_CURRENCY, L_ZYJ, L_MV, L_NET_MV, L_NET_COST, L_FULL_PRICE, L_NET_PRICE, EXPENSE, INVEST_GL, R_GL, GAINLOSS_GL, AMOR_GL, EX_GL, TOTAL_GL, NET_COST, FULL_PRICE, NET_PRICE, INTEREST, GAINLOSS, INCOME_GL, INTEREST_GL, SHADOW_MV, SHADOW_DELTA, PRINFXRATE, CURRENCY, ZYJ, FULL_MV, NET_MV, PORT_ID, VALID_DATE, INVALID_DATE, SEC_NAME, ACCOUNT_CLASS_TYPE, HOLDING_SIDE, HOLDING_AMOUNT)SELECT tb2.SEC_ID, tb1.VALUATION_METHODS, tb1.BACK_AMOUNT, tb1.COUNTERPARTY_ID, tb1.END_DATE, tb1.INTECALC_RULE, tb1.UNDERLYING_CODE, tb1.UNDERLYING_VOLUME, tb1.ORDER_ID, tb1.F_JSFXJ, tb1.F_HHF, tb1.F_RGF, tb1.F_SGF, tb1.F_SHF, tb1.INT_RATE, tb1.ASS_CODE, tb1.CASH_PS_PAYMENT, tb1.BOND_PS_PAYMENT, tb1.F_YHS, tb1.F_JSF, tb1.F_GHF, tb1.F_ZGF, tb1.F_JSFWF, tb1.REGIST_ORG, tb1.REMARK, tb1.SETTLE_SPEED, tb1.PAR_VALUE, tb1.CPRICE_AMT, tb1.FPRICE_AMT, tb1.INV_AIM, tb1.TRAN_COMMISION, tb1.TRADER_NAME, tb1.IMGR_ID_NAME, tb1.TRAN_STATUS, tb1.CPRICE_DATE, tb1.IS_CANCEL, tb1.COUNTER_TRADER, tb1.ENT_INT_FLAG, tb1.LONG_CURRENCY, tb1.LONG_QUANTITY, tb1.LONG_PRINCIPAL, tb1.LONG_INTEREST, tb1.LONG_AMOUNT, tb1.LONG_TRAN_FEE, tb1.TRADE_ID, tb1.DEAL_DEP, tb1.TRAN_DATE, tb1.SETTLE_DATE, tb1.TRAN_SIDE, tb1.PRICE_YIELD, tb1.COUNTER_PARTY, tb1.Column39, tb1.Column40, tb1.Column41, tb1.SEC_ABBR, tb2.REAL_SEC_ID, tb1.NET_VAL_DATE, tb1.NET_VAL_PRICE, tb1.Column32, tb1.Column33, tb1.Column34, tb1.Column35, tb1.Column36, tb1.Column37, tb1.Column38, tb1.Column25, tb1.Column26, tb1.Column27, tb1.Column28, tb1.Column29, tb1.Column30, tb1.Column31, tb1.Column18, tb1.Column19, tb1.Column20, tb1.Column21, tb1.Column22, tb1.Column23, tb1.Column24, tb1.Column11, tb1.Column12, tb1.Column13, tb1.Column14, tb1.Column15, tb1.Column16, tb1.Column17, tb1.Column4, tb1.Column5, tb1.Column6, tb1.Column7, tb1.Column8, tb1.Column9, tb1.Column10, tb1.TRADE_MARKET, tb1.SEC_CATEGORY, tb1.ACC_ID, tb1.ACC_NAME, tb1.Column1, tb1.Column2, tb1.Column3, tb1.L_MV_LOSS, tb1.L_NET_MV_LOSS, tb1.NET_MV_LOSS, tb1.ASSET_BIG_TYPE, tb1.ASSET_CATEGORY, tb1.LEVEL_TYPE, tb1.PORT_NAME, tb1._sign, tb2.SUB_PORT_ID, tb1.DEPT_ID, tb1.ASSET_LOSS, tb1.IS_LT_BOUNDED, tb1.L_ASSET_LOSS, tb1.FULL_MV_LOSS, tb2.UPDATE_USER, tb2.UPDATE_TIME, tb2.INPUT_TYPE, tb1.L_SHADOW_MV, tb1.L_SHADOW_DELTA, tb1.FULL_COST, tb1.L_FULL_COST, tb1.L_R_GL, tb1.L_GAINLOSS_GL, tb1.L_AMOR_GL, tb1.L_TOTAL_GL, tb2.CREATE_USER, tb1.CREATE_DEPT, tb2.CREATE_TIME, tb1.L_INTEREST, tb1.L_DIVIDEND, tb1.L_GAINLOSS, tb1.L_INCOME_GL, tb1.L_INTEREST_GL, tb1.L_EXPENSE, tb1.L_INVEST_GL, tb1.L_CURRENCY, tb1.L_ZYJ, tb1.L_MV, tb1.L_NET_MV, tb1.L_NET_COST, tb1.L_FULL_PRICE, tb1.L_NET_PRICE, tb1.EXPENSE, tb1.INVEST_GL, tb1.R_GL, tb1.GAINLOSS_GL, tb1.AMOR_GL, tb1.EX_GL, tb1.TOTAL_GL, tb1.NET_COST, tb1.FULL_PRICE, tb1.NET_PRICE, tb1.INTEREST, tb1.GAINLOSS, tb1.INCOME_GL, tb1.INTEREST_GL, tb1.SHADOW_MV, tb1.SHADOW_DELTA, tb1.PRINFXRATE, tb1.CURRENCY, tb1.ZYJ, tb1.FULL_MV, tb1.NET_MV, tb2.PORT_ID, tb2.VALID_DATE, tb1.INVALID_DATE, tb1.SEC_NAME, tb2.ACCOUNT_CLASS_TYPE, tb1.HOLDING_SIDE, tb1.HOLDING_AMOUNTFROM dwd_val_detail tb1, dwd_val_detail2C3wru86 tb2WHERE tb1.PORT_ID=tb2.PORT_ID AND tb1.SUB_PORT_ID=tb2.SUB_PORT_ID AND tb1.SEC_ID=tb2.SEC_ID AND tb1.VALID_DATE=tb2.VALID_DATE AND tb1.ACCOUNT_CLASS_TYPE=tb2.ACCOUNT_CLASS_TYPE'5."select partition_key from system.tables where database =(select database()) and name = 'dwd_val_detail'"===&gt;找该表分区字段6."select distinct partition_id from system.parts where database =(select database()) and table = 'dwd_val_detail' and active = 1 and `partition` in (select distinct(toYYYYMM(VALID_DATE) ) from dwd_val_detail2C3wru86)"===&gt;找涉及的分区7.optimize table dwd_val_detail PARTITION '202112' final]]></content>
  </entry>
  <entry>
    <title><![CDATA[git入门]]></title>
    <url>%2F2022%2F07%2F02%2Fgit%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[视频b站搜狂神,文档见B站狂神说公众号 注意点: git add . ​ git commit -m”评论” ​ git push ​ 上传完毕 一般直接复制原来git仓库的配置 ideal选择分支时候点几checkout ideal中可以看到log情况]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习]]></title>
    <url>%2F2022%2F04%2F15%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[逻辑回归12345678910常用于二分类算法,分成回归函数和sigmod逻辑函数两部分组成,想要实现多分类可以通过softmax实现,本身为一个判别模型判别模型相比较生成模型一般较好,生成模型是一种先验概率,判别是后验概率,生成模型带有推测扩展的效果,假设样本之间互相独立,一般适用于数据量过少或者样本中含有噪音影响判断的softmax的多分类变成二分类就变成了对应的逻辑回归函数了逻辑回归使用的是cross_entropy函数,不使用线性函数的平方损失是因为,离最优值远的位置的积分较大,可以达到快速实现求解的过程,而平方损失整体较平缓,更新参数较慢(可以联想一下两个图,一个高抖,一个平缓)求解方法是最大似然法和梯度下降法.讲到似然函数和概率函数,已知参数,求是a类别的概率为概率函数,已知类别反推参数的函数为似然函数 时间序列及LSTMcode见: https://github.com/sksujan58/Multivariate-time-series-forecasting-using-LSTM/blob/main/timeseries%20Forecasting.ipynb 聚类1234567891011121314KmeansDbscan 基本介绍:1,2,3,4 1:1个基本的核心思想,基于密度 2:2个重要概念,邻域半径R和最小点数目minpoints 3:3个点,核心点,边界点,噪音点 4:4个点与点之间的关系,密度直达,密度可达,密度相连,非密度相连 算法实现过程: 1). 从数据集中任意选取一个数据对象点 p； 2）如果对于参数 Eps 和 MinPts，所选取的数据对象点 p 为核心点，则找出所有从 p 密度可达的数据对象点，形成一个簇； 3）如果选取的数据对象点 p 是边缘点，选取另一个数据对象点； 4）重复（2）、（3）步，直到所有点被处理。]]></content>
  </entry>
  <entry>
    <title><![CDATA[java基础]]></title>
    <url>%2F2022%2F03%2F15%2Fjava%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[hadoop入门]]></title>
    <url>%2F2022%2F02%2F21%2Fhadoop%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[hadoop基础知识12345678910111213141516171819202122232425262728293031323334353637hadoop:分为1.0、2.0和3.0版本.目前企业最常见的是2.0版本. 1.0版本由hdfs和mapreduce构成,2.0在hdfs的上层添加了yarn,并且计算框架也由单一的mapreduce,添加了其他计算框架如spark,Mr等hadoop 是在nutch distributed file System的基础上开发的.1.hdfs架构的组成:namenode和datanode,然后数据是分块的存储在datanode中(block),namenode相当于一个大管家进行整体的控制.除此之外进行的备份策略,有个secondary namenode,定期和namenode联系进行备份,防止节点挂了影响整体功能.secondnameode 会定期整合fsimage和fsedits发送给namenode) fsimage:元数据的镜像文件 fsedits:元数据的操作日志(并非 NameNode 的热备。当NameNode 挂掉的时候，它并不能马上替换 NameNode 并提供服务。) namenode: 1.管理hdfs的命名空间 2.控制数据块的映射 3.配置备份策略 4.管理读写语句datanode: 1.存储分块数据 2.进行数据的读写操作hadoop本身是具有高吞吐量,高扩展性等优点,但是不具备快速查询功能,可使用hbase实现secondnamenode 是namenode 的冷备份 热备份:一个节点坏了后,宁一个节点可以直接替换这个节点继续工作,称为热节点 冷备份:一个节点坏了后,宁一个节点不能直接替换工作,只是为了减少损失每次重启namenode都会从fsimage读取元文件,然后经过edits中记录的操作,当edits中记录很多时候,namenode启动会非常慢,因而需要定时的合并fsimage文件和edits文件,同时清空edits文件.合并fsimage和edits文件是在secondaryname中实现的,会让namenode暂停写入edits,然后从namenode中获取fsimage和edits文件,合并后得到新的fsimage和edits传输给namenode,视频学习流程: hadoop发展----&gt;hadoop架构解释及面试问题----&gt;hadoop命令----&gt;namenode和datenode详解---&gt;edits文件详解---&gt;fsimage文件详解---&gt;hdfsApi操作-----&gt;hdfs流程图(写入,写出,删除操作流程)----&gt;hadoop源码详解----&gt;租约锁和hadoop特点(见:https://blog.csdn.net/sinat_35667067/article/details/104250251?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-0.pc_relevant_default&amp;spm=1001.2101.3001.4242.1&amp;utm_relevant_index=2) hive是建立在hadoop基础上的数据仓库,会将hdfs中的文件映射到表中,提供sql查询和mapreduce功能.Hive本身不存储和计算数据,依靠于hadoop的hdfs和mapreduce,只是一种映射 hbase是物理表,简单理解为，hive是文件的视图，hbase是建了索引的key-value表。 hadoop API123456789101112131415161718192021222324252627package HDFS.learnself;import java.io.IOException;import java.net.URI;import java.net.URISyntaxException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Hdfs;import org.apache.hadoop.fs.Path;public class Write &#123; public static void main(String[] args) throws IOException, InterruptedException, URISyntaxException &#123; //1.加载hdfs的配置文件 Configuration conf=new Configuration(); //2.获取hdfs的操作对象 FileSystem fs=FileSystem.get(new URI("hdfs://hdp02:9000"), conf, "hdp02"); //3.文件路径 Path File=new Path("hdfs://hdp02:9000/test/cao.txt"); //4.创建FSDataOutputStream对象 FSDataOutputStream out=fs.create(File); //5.写入数据 out.writeUTF("Hello world!"); out.close(); System.out.println("数据写入成功！"); &#125; &#125; hadoop特点12345671.分布式存储架构,支持海量数据(GB,PB,TB级别)2.高容错性,数据块存在多个副本,副本丢失自动恢复3.低成本部署,可在廉价机器上搭建4.能够检测和快速应对硬件故障,通过rpc心跳机制来实现5.hdfs不能实现低延迟数据访问(毫秒级),hadoop的优势是高吞吐量(单位时间内产生的数据流),低延迟可通过hbase实现6.hadoop不支持数据修改,适用于:一次写入,多次读取.但是允许追加数据7.hadoop不适合存储海量小文件,会浪费namenode 的内存空间 map reduce12345678910111213141516171819202122232425262728293031任务分成:map任务和reduce任务。map操作完后会进行shuffle操作,将相同key的分发到一起,同一个reduce任务中mapreduce阶段:泛型对应 String ----&gt;Text,Integer----&gt;Intwritable,Long-----&gt;LongWritable原因: 在map完了之后,需要将数据传递给reduce,这时候涉及到了序列化和反序列化,但是jdk自带的效率较慢,因而hadoop自己实现了序列化接口. * hadoop为jdk中的常用基本类型Long String Integer Float等数据类型封住了自己的实现了hadoop序列化接口的类型：LongWritable,Text,IntWritable,FloatWritable####map阶段1.明确输入的键值是什么类型,输出的键值是什么类型案例: * KEYIN ：是map task读取到的数据的key的类型，是一行的起始偏移量Long * VALUEIN:是map task读取到的数据的value的类型，是一行的内容String * * KEYOUT：是用户的自定义map方法要返回的结果kv数据的key的类型，在wordcount逻辑中，我们需要返回的是单词String * VALUEOUT:是用户的自定义map方法要返回的结果kv数据的value的类型，在wordcount逻辑中，我们需要返回的是整数Integermap_task的数量会根据切片的大小自动分配,reduce_task的数量可以指定,默认1Map是对分块的每一行操作还是需要遍历?是对每一行操作的,不需要遍历map和reduce间的分区,默认的是键的hash值与reducetask的数量取模得到,也可以通过partitioner自定义划分到哪一个分区(分区数等于reduce_task数量),通常一个reduce任务中数据已经经过键排序过了,当需要全局排序,一方面可通过一个reduce task实现,但是会影响整体的效率,宁一方面就是重构parpartitioner来实现每个reduce_task 生成一个文件要点:输出对象,分区指定,序列化指定####################################job任务的执行流程##########################多进程和多线程的区别1.多进程占用内存多,cpu利用率低;多线程占用内存少,cpu利用率高2.多线程进程间不会互相影响,多线程一个断了则全断 Yarn架构 12345678910111213yarn:是在hadoop1.0的基础上衍生出来的,为了缓解jobtracker的压力.主要进行资源调度和任务分配.yarn的三大组件:resourceManager(rm),nodemanager(nm),applicantionmasterrm主要进行资源分配,applicationmaster主要进行任务调度 ResourceManager主要有两个组件：Scheduler和ApplicationManager Scheduler:负责资源的分配,纯调度 ApplicationManager:为应用分配container来运行applicantionmaster,并且负责监控applicantionmaster NodeManager是yarn节点的一个“工作进程”代理，管理hadoop集群中独立的计算节点，主要负责与ResourceManager通信，负责启动和管理应用程序的container的生命周期，监控它们的资源使用情况（cpu和内存），跟踪节点的监控状态，管理日志等。并报告给RM。container:一个抽象资源对象,封装了程序运行需要的资源(如内存,cpu,磁盘等) ApplicationMaster: ApplicationMaster负责与scheduler协商合适的container，跟踪应用程序的状态，以及监控它们的进度，ApplicationMaster是协调集群中应用程序执行的进程。每个应用程序都有自己的ApplicationMaster，负责与ResourceManager协商资源（container）和NodeManager协同工作来执行和监控任务 。 yarn的调度流程 Flume12345678910Flume:海量日志采集、聚合和传输的系统。最主要的作用是实时读取服务器本地磁盘的数据，将数据写入到HDFS中。Flume架构:webserver---&gt;Agent----&gt;HDFS Agent(Source、channel、sink) Agent：Jvm的一个进程，以事务的形式将数据从源头送到目的 Source:责任是将数据传输到Flume Agent的一个组件; channel: 解耦合,具有缓冲的作用.Flume自带两个channel,一个memory channel 和File channel,前者存在内存,存在数据丢失风险,速度快.后者存在磁盘上,安全性高,速度慢 sink:不断轮询channel并删除,然后将这些事件批量写入存储或者索引系统(如hdfs,hbase,avro等) 一个source对应多个channel,一个sink只能对应一个channel,一个channel对应多个sink channel功能:一个是数据传输的可靠性,一个是数据流入流出的异步执行.前面一句是因为只有当数据确认到到目的地或者下一个agent时才会删除channel内对应的event. source可实现扇出,即一个source对应多个channel,然后方式有两种,一个是复制,即每个channel的数据都一样.一个是选择(多路复用),即event有选择的流入哪个channel Kafka12345678910快速了解:https://max.book118.com/html/2021/1027/8005064135004025.shtm定义:一个分布式的流媒体平台(需要具备三个功能) 1.发布和订阅记录流 2.以容错的方式永久订阅记录流 3.记录发生时处理流通用:用在实时数据平台上,由flume到kafka到stormkafka应用场景:kafaka特点: 1. poll模式.消费的速率由下游的消费者(计算框架决定) 2. Kafka提供了消费的持久化机制,无论消费者是否消费都会存储,通过副本冗余机制提供数据的冗余机制 hive12345678910111213141516hive是数据仓库的一种数据仓库的特点: 1.存放的是历史数据 2. 存储大容量数据,gb,tb甚至pb hive本身并不存储数据,而是hadoop中hdfs的映射,进行hql操作时候实际操作的是hdfs的文件夹,且数据源需要配置mysql,hql默认会使用mr,针对数据倾斜问题,一个是group by 一个是join,只需要在当前session改两个配置即可.然后分成外部表和内部表.分区表既可以是外部表也可以是内部表,通常是以时间来作为分区依据,分区的好处是避免全表扫描,(实际上是每个分区键都有对应的文件夹,只到对应文件夹下去读取数据.hive知识点梳理1. 内部表、外部表 和分桶表（用来随机抽样用的）2. metastore： hive元数据，存放在关系型数据库中，如derby和mysql3. hive复杂数据结构：array、map、struct4. hive常用的字符串操作函数5. hive 的udf函数6. hive数据倾斜问题及优化（①group by ②join ③count)7. hive 的优化:大概七八个吧①map参数 ②reduce参数 ③jvm重用 ④严格模式8. sqoop mysql&lt;-&gt;hdfs9. hive api接口:①client ②代码jdbc ③可视化 webui sqoop既可以实现数仓到关系型数据库,也可以实现关系型数据库到数仓 大数据创建总共分成两种一种是离线批处理一种是实时处理 离线批处理流程: 1. 日志通过flume进行采集 2. 将数据存入hdfs中 3. 通过hive进行处理 1. 建立总表 4. 建立清洗表,实现etl 5. 建立业务表,清洗业务字段 6. 建立多维度表 7. sqoop将清洗完的数据导入mysql,进行可视化 ####Scala 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293函数定义:var f1 = (x:Int) =&gt; x*2var f2 : (Int,Int)=&gt;(Int) = (x,y) =&gt; x*ydef fu(name:String):Unit = &#123;println(name)&#125;match case break,continue (导包:import util.control.Breaks._)kl match&#123;case "kl" =&gt; println(2) case "sd" =&gt; println(3)&#125;break写法: Breaks.breakable( | for (i &lt;- 1 to 10)&#123; | if (i==5)&#123; | Breaks.break() | &#125; | println(i) | &#125; | )continue写法: for (i&lt;- 1 to 5) | &#123;Breaks.breakable(if (i==3) Breaks.break() else println(i))&#125;区别就是break是写在循环外的,cntinue是写在循环里的,写法是一样的try catch finally //==============================函数===========================可变参数 def people(name:String*)===&gt;在类型后加一个星号表示可变,实际是个容器,放了同一类型的多个数据,可通过遍历获取出来 函数返回值:Unit表示没有返回值,可以不写返回值的类型,会自己推断(前提是有等号,等号不写,全都默认没有返回值)scala的泛型使用中括号[],不同于java的尖括号&lt;&gt;函数体只有一行的时候可以简写匿名函数 var a = (x:Int,y:Int)=&gt;x*y (x,y)=&gt;x*y x=&gt;x**2 函数支持公开函数中调用匿名函数def f2(a:Int,b:Int,f:(Int,Int)=&gt;&#123;Int&#125;) = &#123;f(a,b)&#125;f2(2,3,(x,y)=&gt;x+y)最简化版本: f2(2,3,_*_) 其中_指代的是参数匿名函数和高阶函数println(s"Lines with a: $numAs, Lines with b: $numBs") scala完全面向对象,因而去掉了java中的非面向对象的元素,如static和voidscala无static关键字,通过object实现类似静态方法的功能void 无返回值scala通过Unit实现类中使用BeanProperty修饰表示该变量会自动生成get和set方法 @BeanProperty var age = 20伴生类和伴生对象,名称相同,伴生对象可以使用伴生类的所有东西(即object和class),又因为不支持静态方法,这样在编译时候就会生成两个同名的文件,加$符号的是伴生对象的生成类,不加$是入口类scala的数据类型:Any:包含Anyval和Anyref(即引用和数值),anyref包含:scala classes 和 other scala classes 和 all java classes.然后all java classes又包含null,null的下一级为Nothinganyval:包含Unit、stringops（java中string的增强）、char、boolean、Double（下属float-&gt;long-&gt;int&gt;short-&gt;byte）下一级为Nothing通常Unit表示函数没有返回值,Null表示的是对象为空(必须是引用类型),Nothing表示没有任何返回值,不同于null和Unit,通常用在函数内抛出异常时使用数据类型转化分为隐式转换和显示转换,隐式转换表名低阶可以自动转换成高阶,高阶转换成低阶需要调用方法如toInt等数值转换成字符串 ①2.toString ②2+""scala循环推荐使用for循环而非while和do while,且scala中没有break和continue关键字,要想实现需要调用方法:scala终止循环实际上是通过抛出异常实现的,不同的是调方法是进行了封装 try&#123; for(i &lt;- 1 to 10; j = 20 - i)&#123; if (i == 9)&#123; throw new RuntimeException &#125; println("i+j="+i+j) &#125;&#125; catch &#123;case exception: Exception =&gt;&#125; println("结束了") Breaks.breakable( for (i &lt;- 1 until 10)&#123; if (i ==6)&#123; Breaks.break() &#125; println(i) &#125;)匿名函数化简: 1.参数类型可以省略 (x:Int)=&gt;&#123;x+1&#125; 变成(x)=&gt;&#123;x+1&#125; 2.参数只有一个时候括号可以省略 x=&gt;&#123;x+1&#125; 3.函数体只有一行,花括号可以省略 x =&gt; x+1 4.如果参数只出现一次可以使用_代替 (x,y) =&gt; x+y 变成 _ + _闭包和柯里化scala导包 com.公司名.项目名.模块名 或者package&#123;&#125;三层嵌套也可以面向对象: scala的构造器分为主构造器和辅助构造器,主构造器是在类名上加参数,辅助构造器是重载this方法构造器参数修饰,var可变参数,val不可变参数,只能访问不能修改集合: Array: 可变的添加:a.+=(2) 或者a.append(2) 插入:a.insert(0,2) 赋值:a(1) = 3 可变与不可变的转换a.toBuffer a.toArray 创建多维矩阵 Array.ofDim[Double](3,4) spark操作12345val a = sc.parallelize(Array((1,"xaioming"),(2,"dsada"),(3,"sda")))val b = a.toDF("ID","name")b.filter($"name" === "sda").show()b.filter($"name".isin("sads","dsda"))b.filter($"name".contains("sads")) 自定义Spark累加器1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.benying.sparkimport org.apache.spark.util.AccumulatorV2import org.apache.spark.&#123;SparkConf, SparkContext&#125;import java.langimport scala.collection.mutable/*accumulator自定义操作*/object Accumulator_learning &#123; def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("spark") val sc = new SparkContext(conf) val rdd1 = sc.makeRDD(List("Hello","Spark","Scala","hello","Spark"),2) //自定义累加器 val myacc = new MyAccumulator_() sc.register(myacc,"spark_ACC") rdd1.foreach(word=&gt;myacc.add(word)) println(myacc.value) //关闭 sc.stop() &#125; class MyAccumulator_ extends AccumulatorV2[String, mutable.Map[String,Long]]&#123; private val word_map = mutable.Map[String,Long]() override def isZero: Boolean = &#123; word_map.isEmpty &#125; override def copy(): AccumulatorV2[String, mutable.Map[String,Long]] = &#123; new MyAccumulator_() &#125; override def reset(): Unit = &#123; word_map.clear() &#125; override def add(word: String): Unit = &#123; val num:Long = word_map.getOrElse(word,0L) + 1 word_map.update(word,num) &#125; override def merge(other: AccumulatorV2[String, mutable.Map[String,Long]]): Unit = &#123; val other_value: mutable.Map[String, Long] = other.value other_value.foreach&#123; case (word,num) =&gt; val new_num:Long = word_map.getOrElse(word,0L) + num word_map.update(word,new_num) &#125; &#125; override def value: mutable.Map[String,Long] = &#123; word_map &#125; &#125;&#125; FLUME–HIVE–KAFKA–HBASE–STORM–SPARK zookeeper:贯穿全部]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java学习code]]></title>
    <url>%2F2022%2F01%2F07%2Fjava%E5%AD%A6%E4%B9%A0code%2F</url>
    <content type="text"><![CDATA[####代码1：大象放冰箱 1234567891011121314151617181920212223242526272829303132333435363738package day1;import java.util.Scanner;class elephone&#123; String name; public String put()&#123; return name; &#125;&#125;class icebox&#123; public void open() &#123; System.out.println("打开了冰箱"); &#125; public void save(String name) &#123; System.out.println("放入"+name); &#125; public void close() &#123; System.out.println("关闭了冰箱"); &#125;&#125;public class day1 &#123; public static void main(String [] args) &#123; icebox ice = new icebox(); //实例化对象 ice.open(); //调用打开方法 elephone ele = new elephone(); Scanner sc = new Scanner(System.in); ele.name = sc.next(); String name = ele.put(); ice.save(name); ice.close();//调用关闭方法 &#125;&#125; 代码2：小汽车1234567891011121314151617181920package day1;class Car&#123; String color; int num; public void run() &#123; System.out.println("汽车运行,颜色为"+color+",车轮数"+num); &#125;&#125;public class day1 &#123; public static void main(String [] args) &#123; /*代码2:小汽车*/ Car car = new Car(); car.color = "绿色"; car.num = 4; car.run(); &#125;&#125; ####代码3：局部变量和成员变量 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class day1 &#123; int day; public static void main(String [] args) &#123; /*局部变量和成员变量*/ int day = 4; day1 day_ = new day1(); day_.get_num(); System.out.println(day); &#125; public void get_num() &#123; day = 5; &#125;&#125;package day2;class Person&#123; private String name = "是打算大"; private int age = 20; public void set_age(int age) &#123; this.age = age; &#125; public int get_age() &#123; return this.age; &#125; public void set_name(String name) &#123; this.name = name; &#125; public String get_name() &#123; return this.name; &#125; public void get_info() &#123; name = this.get_name(); age = this.get_age(); System.out.println(name + age); &#125;&#125;public class get_set &#123; public static void main(String [] args) &#123; Person person = new Person(); person.get_info(); person.set_age(25); person.set_name("小米"); System.out.println(person.get_age()); person.get_info(); &#125;&#125; 代码4:判断是否是同一个人12345678910111213141516171819202122232425262728数值相等判断用 = ,字符串相等用equals A.equals(B)package day2;class Person_ &#123; private String name; private int age; public Person_(String name,int age) &#123; this.name = name; this.age = age; &#125; public String get_name() &#123; return this.name; &#125; public int get_age() &#123; return this.age; &#125;&#125;public class new_Person &#123; public static void main(String [] agrs) &#123; Person_ person1 = new Person_("张三",20); Person_ person2 = new Person_("张三",20); if ((person1.get_name().equals(person2.get_name()))&amp;&amp;((person1.get_age() == (person2.get_age())))) &#123;System.out.println("人一样");&#125;; &#125;&#125; 代码5:随机点名12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package day2;import java.util.ArrayList;import java.util.Random;import java.util.Scanner;//public classclass Student&#123; private String name; //姓名 private int age; //年龄 //设置姓名\年龄,即可以使用构造.也可以set和get public Student(String name,int age) &#123; this.name = name; this.age = age; &#125; //取名字 public String getname() &#123; return this.name; &#125; //取年龄 public int getage() &#123; return this.age; &#125;&#125;public class demo3 &#123; public static void main(String [] agrs)&#123; //随机点名分三步 //第一步,录入学生信息 ArrayList&lt;Student&gt; list = new ArrayList&lt;Student&gt;(); setmessage(list); System.out.println(list.size()); //第二步:打印所有学生信息 Pintlist(list); //第三步:随机取数打印 randomstudent(list); &#125; public static void setmessage(ArrayList&lt;Student&gt; list) &#123; for(int i=0;i&lt;=3;i++) &#123; Scanner sc = new Scanner(System.in); System.out.println("请录入中文名"); String name = sc.next(); System.out.println("请录入年龄"); int age = sc.nextInt(); Student student = new Student(name,age); list.add(student); &#125; &#125; public static void Pintlist(ArrayList&lt;Student&gt; list) &#123; for(Student i:list) &#123; System.out.println("姓名是"+i.getname()+",年龄是"+i.getage()); &#125; &#125; public static void randomstudent(ArrayList&lt;Student&gt; list) &#123; int i = new Random().nextInt(list.size()); System.out.println("中奖劳斯莱斯的姓名是"+list.get(i).getname()+",年龄是"+list.get(i).getage()); &#125;&#125; ####代码6:最基本 的继承方法调用 123456789101112131415161718192021222324252627package day3;public class demo1 &#123; public static void main(String [] args) &#123; algo name = new algo(); name.name = "小明"; name.get_name(); name.work(); &#125;&#125;class Employee&#123; String name; public void work() &#123; System.out.println("在工作"); &#125;&#125;class algo extends Employee&#123; public void get_name() &#123; System.out.println("姓名是:"+name); &#125;&#125; 代码7:子类调用父类同名属性123456789101112131415161718192021222324public class demo2 &#123; public static void main(String [] args) &#123; algo2 al = new algo2(); al.print_(); &#125;&#125;class eploee&#123; String name = "小明";&#125;class algo2 extends eploee&#123; String name = "小志"; public void print_() &#123; System.out.println(this.name); System.out.println(super.name); //同名调用父类的属性 &#125;&#125; #####代码8:方法的重载 12345678910111213141516171819202122232425262728package day3;public class demo2 &#123; public static void main(String [] args) &#123; algo2 al = new algo2(); al.print_(); &#125;&#125;class eploee&#123; String name = "小明"; public void print_()&#123; System.out.println(this.name); &#125;&#125;class algo2 extends eploee&#123; String name = "小志"; public void print_() &#123; System.out.println(this.name); //输出小志 eploee el = new eploee(); el.print_(); &#125;&#125;//通常是为了扩展功能,或者该方法不符合需求了,但是不能在原方法上修改,避免引起一系列反应 ####代码9:方法覆盖手机案例 1234567891011121314151617181920212223242526272829303132333435363738package day3;// a:案例:比如手机，当描述一个手机时，它具有发短信，打电话，显示来电号码功能，// 后期由于手机需要在来电显示功能中增加显示姓名和头像，// 这时可以重新定义一个类描述智能手机，并继承原有描述手机的类。// 并在新定义的类中覆盖来电显示功能，在其中增加显示姓名和头像功能public class demo &#123;public static void main(String [] args)&#123; new_phone phone = new new_phone(); phone.show();&#125;&#125;class Phone&#123; public void send()&#123; System.out.println("发短信"); &#125; public void call() &#123; System.out.println("打电话"); &#125; public void show()&#123; System.out.println("显示号码"); &#125;&#125;class new_phone extends Phone&#123; //扩展 public void show()&#123; System.out.println("显示号码:显示头像:显示姓名"); &#125;&#125;//注意:方法覆盖,子类中的方法权限必须大于父类,如上show方法,new_phone必须是public否则报错权限:public &gt;默认=protected&gt;private b:方法定义:子类方法和要重写的父类的方法:方法的方法名和参数列表都要一样。 关于方法的返回值: 如果是基本数据类型,子类的方法和重写的父类的方法返回值类型必须相同 如果是引用数据类型,子类的方法和重写的父类的方法返回值类型可以相同或者子类方法的返回值类型是父类方法返回值类型的子类 代码10:初识抽象类123456789101112131415161718192021222324252627282930package day3;public class demo3 &#123; public static void main(String [] args) &#123; new android().work(); new algo3().work(); &#125;&#125;abstract class Develop&#123; public abstract void work();//抽象类没法确定主体方法内容&#125;class android extends Develop&#123; //子类必须重载抽象类中的方法 public void work()&#123; System.out.println("你奶奶的"); &#125;&#125;class algo3 extends Develop&#123; public void work()&#123; System.out.println("你妹的"); &#125;&#125;//抽象类和抽象方法都需要被abstract修饰。抽象方法一定要定义在抽象类中。//抽象类不可以直接创建对象，原因：调用抽象方法没有意义。//抽象类的作用:继承的体系抽象类,强制子类重写抽象的方法//抽象类下要也可以创建方法和对应的主体,但没有什么实际意义,可调用 ####代码11:员工类(this) 12345678910111213141516171819202122232425262728293031323334353637383940414243package day3;public class demo4 &#123; public static void main(String [] args) &#123; javaee java = new javaee("小明",66); java.work(); &#125;&#125;abstract class Employee2&#123; private String name; private int id; public Employee2(String name, int id) &#123; this.id = id; this.name = name; &#125; public String get_name() &#123; return this.name; &#125; public int get_id() &#123; return this.id; &#125; public abstract void work();&#125;class javaee extends Employee2&#123; public javaee(String name, int id) &#123; super(name, id); &#125; public void work() &#123; System.out.println("JavaEE的工程师开发淘宝"+ super.get_name()+".."+super.get_id()); &#125;&#125; ####代码12:员工类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package day3;public class demo5 &#123; public static void main (String [] agrs) &#123; javaee2 java = new javaee2(); java.setName("小明"); java.setId(66); java.work(); ceshi ceshi_ = new ceshi(); ceshi_.setId(99); ceshi_.setName("笑话"); ceshi_.work(); &#125;&#125;abstract class employee&#123; private String name; private int id; //存取名字id public void setName(String name) &#123; this.name = name; &#125; public void setId(int id) &#123; this.id = id; &#125; public int getid() &#123; return this.id; &#125; public String getname() &#123; return this.name; &#125; public abstract void work();&#125;abstract class Develop2 extends employee&#123; &#125;abstract class Mainter extends employee&#123; &#125;class javaee2 extends Develop2&#123; public void work()&#123; System.out.println("JAVA员工"+getname()+"id"+getid()); &#125;&#125;class ceshi extends Mainter&#123; public void work()&#123; System.out.println("测试员工"+getname()+"id"+getid()); &#125;&#125; ####代码13:犬和缉毒功能 123456789101112131415161718192021222324252627282930313233343536373839package day4;//接口和抽象类:抽象类是对类的抽象,即单一的类无法描述所有情况的时候,而接口是一种扩展的功能,不是所有都使用//一个类智能继承一个父类,但是可以继承多个接口public class demo1 &#123; public static void main(String [] args) &#123; home_dog dog = new home_dog(); dog.laugh(); dog.eat(); dog.drug_(); &#125;&#125;//犬和缉毒//犬定义为抽象类abstract class dog&#123; public abstract void laugh(); public abstract void eat();&#125;//缉毒interface drug&#123; public void drug_();&#125;class home_dog extends dog implements drug&#123; public void drug_() &#123; System.out.println("狗会缉毒"); &#125; public void laugh() &#123; System.out.println("家园狗大脚"); &#125; public void eat() &#123; System.out.println("吃牛肉"); &#125;&#125; 代码14:多态123456789101112131415161718192021222324252627282930package day4;//调用属性时使用的是父类的,调用方法编译时使用的是父类,执行时使用的是子类,因为被重载了public class demo2 &#123; public static void main(String [] args) &#123; Fu fu = new Zi(); System.out.println(fu.num); fu.get_num(); Zi zi = new Zi(); System.out.println(zi.num); zi.get_num(); &#125;&#125;class Fu&#123; int num = 3; public void get_num() &#123; System.out.println("数字是"+num); &#125;&#125;class Zi extends Fu&#123; int num = 6; public void get_num() &#123; System.out.println("数字是"+num); &#125;&#125; ####代码15:向上转型和向下转型 123456789101112131415161718192021222324252627282930313233343536//向上转型:将子类对象转换成父类对象.可以调用父类的属性和子类的方法(都有的,会重载,子类中独有的无法调用,会报错)//向下转型:父类对象转为子类对象====前提是向上转型,否则会报错////////////////////////////////////向上转型package day4;public class demo3 &#123; public static void main(String [] args) &#123; Animal animal = new dog2(); System.out.println(animal.age); animal.eat(); animal.work();//baocuo &#125;&#125;class Animal&#123; int age = 20; public void eat()&#123; System.out.println("chifan"); &#125; public void work() &#123; System.out.println("kaisgongzuossss"); &#125;&#125;class dog2 extends Animal&#123; int age = 10; public void eat()&#123; System.out.println("chifansssss"); &#125;&#125;////////////////////////////////////向下转型 ####代码16:笔记本电脑案例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package day7;interface USB&#123; void open(); void close();&#125;//鼠标空值class Mouse implements USB&#123; public void open()&#123; System.out.println("鼠标开启"); &#125; public void close()&#123; System.out.println("鼠标关闭"); &#125;&#125;//键盘控住class keyboard implements USB&#123; public void open()&#123; System.out.println("键盘开启"); &#125; public void close()&#123; System.out.println("键盘关闭"); &#125;&#125;//笔记本class Notebook&#123; public void run()&#123; System.out.println("开启笔记本"); &#125; public void useusb(USB usb) &#123; if (usb != null)&#123; usb.open(); usb.close(); &#125; &#125; public void shut()&#123; System.out.println("笔记本关闭"); &#125;&#125;public class demo &#123; public static void main(String [] args) &#123; Notebook note = new Notebook(); note.run(); keyboard ky = new keyboard(); note.useusb(ky); Mouse mu = new Mouse(); note.useusb(mu); note.shut(); &#125;&#125; 代码17:this构造方法的使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778//局部变量和成员变量public class demo3 &#123; private String name; private int age; public static void main(String [] args)&#123; demo3 de = new demo3("小明",26); de.get_name_age(); &#125; //构造方法 public demo3(String na)&#123; name = na; &#125; //构造方法this,this指代当前对象 public demo3(String na,int ag) &#123; this(na); //指代的就是创建的对象 age = ag; &#125; public void get_name_age()&#123; System.out.println("姓名是"+name+"年纪是"+age); &#125;&#125;##进一步,数值比较public class demo4 &#123; private String name; private int age; public static void main(String [] args)&#123; demo4 de = new demo4("小明",26); de.get_name_age(); demo4 de2 = new demo4("小明",26); System.out.println("是否同岁:"+de.judge_age(de2)); &#125; //构造方法 public demo4(String na)&#123; name = na; &#125; //构造方法this,this指代当前对象 public demo4(String na,int ag) &#123; this(na); //指代的就是创建的对象 age = ag; &#125; public void get_name_age()&#123; System.out.println("姓名是"+name+"年纪是"+age); &#125; public boolean judge_age(demo4 d)&#123; return this.age == d.age; &#125;&#125;##构造方法继承时候自动调用父类package day9;//继承的构造方法:会自动调用父类的,有个隐式的super执行public class demo1 &#123; public static void main(String [] args)&#123; new Zi(); &#125;&#125;class Fu&#123; int num; public Fu()&#123; System.out.println("到爸爸这来"); num = 2; &#125;&#125;class Zi extends Fu&#123; public Zi()&#123; //有个隐式super System.out.println(num); &#125;&#125; 代码18:综合案例—完整的员工类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public class demo2 &#123; public static void main(String [] args)&#123; JavaEE ja = new JavaEE("小米",25); ja.work(); JavaEE ja2 = new JavaEE(); ja2.set_age(20); ja2.set_name("笑话"); ja2.work(); &#125;&#125;//工人类abstract class Employee&#123; private String name = "四大神兽多所"; private int age = 26; //无参数构造 public Employee()&#123; super(); &#125; //含参 public Employee(String name,int age)&#123; this.name = name; this.age = age; &#125; //获取姓名 public String get_name()&#123; return name; &#125; //设置姓名 public void set_name(String name)&#123; this.name = name; &#125; //获取年龄 public int get_age()&#123; return age; &#125; //设置年龄 public void set_age(int age)&#123; this.age = age; &#125; //抽象类工作 public abstract void work();&#125;//研发类abstract class Develop extends Employee&#123; public Develop()&#123; super(); &#125; public Develop(String name,int age)&#123; super(name,age); &#125;&#125;//测试类abstract class Test extends Employee&#123; public Test(String name,int age)&#123; super(name,age); &#125;&#125;class JavaEE extends Develop&#123; @Override public void work() &#123; System.out.println("java员工"+get_name()+"年龄"+get_age()); &#125; public JavaEE(String name,int age)&#123; super(name,age); &#125; public JavaEE()&#123; super(); &#125;&#125;//以此类推 code19.内部类的的访问及定义12345678910111213public class demo4 &#123; public static void main(String [] args)&#123; body.heart h = new body().new heart(); System.out.println(h.name); &#125;&#125;class body&#123; class heart&#123; String name = "心脏"; &#125;&#125; code20.打牌1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798import java.util.ArrayList;import java.util.Collections;import java.util.HashMap;/* * 斗地主洗牌发牌排序 */public class Poker &#123; public static void main(String[] args) &#123; //准备花色 ArrayList&lt;String&gt; color = new ArrayList&lt;String&gt;(); color.add("♠"); color.add("♥"); color.add("♦"); color.add("♣"); //准备数字 ArrayList&lt;String&gt; number = new ArrayList&lt;String&gt;();Collections.addAll(number,"3","4","5","6","7","8","9","10","J","Q","K","A","2"); //定义一个map集合：用来将数字与每一张牌进行对应 HashMap&lt;Integer, String&gt; map = new HashMap&lt;Integer, String&gt;(); int index = 0; for (String thisNumber : number) &#123; for (String thisColor : color) &#123; map.put(index++, thisColor+thisNumber); &#125; &#125; //加入大小王 map.put(index++, "小☺"); map.put(index++, "大☻"); //一副54张的牌 ArrayList里边为0-53的数的新牌 ArrayList&lt;Integer&gt; cards = new ArrayList&lt;Integer&gt;(); for (int i = 0; i &lt;= 53; i++) &#123; cards.add(i); &#125; //洗牌 Collections.shuffle(cards); //创建三个玩家和底牌 ArrayList&lt;Integer&gt; iPlayer = new ArrayList&lt;Integer&gt;(); ArrayList&lt;Integer&gt; iPlayer2 = new ArrayList&lt;Integer&gt;(); ArrayList&lt;Integer&gt; iPlayer3 = new ArrayList&lt;Integer&gt;(); ArrayList&lt;Integer&gt; itCards = new ArrayList&lt;Integer&gt;(); //遍历这副洗好的牌，遍历过程中，将牌发到三个玩家和底牌中 for (int i = 0; i &lt; cards.size(); i++) &#123; if(i&gt;=51) &#123; iCards.add(cards.get(i)); &#125; else &#123; if(i%3==0) &#123; iPlayer.add(cards.get(i)); &#125;else if(i%3==1) &#123; iPlayer2.add(cards.get(i)); &#125;else &#123; iPlayer3.add(cards.get(i)); &#125; &#125; &#125; //对每个人手中的牌排序 Collections.sort(iPlayer); Collections.sort(iPlayer2); Collections.sort(iPlayer3); //对应数字形式的每个人手中的牌，定义字符串形式的牌 ArrayList&lt;String&gt; sPlayer = new ArrayList&lt;String&gt;(); ArrayList&lt;String&gt; sPlayer2 = new ArrayList&lt;String&gt;(); ArrayList&lt;String&gt; sPlayer3 = new ArrayList&lt;String&gt;(); ArrayList&lt;String&gt; sCards = new ArrayList&lt;String&gt;(); for (Integer key : iPlayer) &#123; sPlayer.add(map.get(key)); &#125; for (Integer key : iPlayer2) &#123; sPlayer2.add(map.get(key)); &#125; for (Integer key : iPlayer3) &#123; sPlayer3.add(map.get(key)); &#125; for (Integer key : iCards) &#123; sCards.add(map.get(key)); &#125; //看牌 System.out.println(sPlayer); System.out.println(sPlayer2); System.out.println(sPlayer3); System.out.println(sCards); &#125;&#125; code21:异常123456789package day11;public class demo1 &#123; public static void main(String [] args)&#123;int num = 2; try&#123; test(num); &#125;catch (Exception e)&#123;System.out.println(test2(num));&#125;&#125; private static void test(int num) &#123;int a = num/0; &#125; private static int test2(int num) &#123;int b;b = num/1;return b; &#125;&#125;//try catch() code22:加法运算12345678910111213141516171819202122232425262728293031323334353637383940414243public static void main(String[] args)&#123; System.out.println("请输入2个加数"); int result; try &#123; result = add(); System.out.println("结果:"+result); &#125; catch (Exception e)&#123; e.printStackTrace(); &#125;&#125;//获取输入的2个整数返回private static List&lt;Integer&gt; getInputNumbers()&#123; List&lt;Integer&gt; nums = new ArrayList&lt;&gt;(); Scanner scan = new Scanner(System.in); try &#123; int num1 = scan.nextInt(); int num2 = scan.nextInt(); nums.add(new Integer(num1)); nums.add(new Integer(num2)); &#125;catch(InputMismatchException immExp)&#123; throw immExp; &#125;finally &#123; scan.close(); &#125; return nums;&#125;//执行加法计算private static int add() throws Exception&#123; int result; try &#123; List&lt;Integer&gt; nums =getInputNumbers(); result = nums.get(0) + nums.get(1); &#125;catch(InputMismatchException immExp)&#123; throw new Exception("计算失败",immExp); /////////////////////////////链化:以一个异常对象为参数构造新的异常对象。 &#125; return result;&#125; 集合框架下的List123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import java.util.ArrayList;import java.util.Iterator;import java.util.List;import java.util.ListIterator;public class listdemo &#123; public static void main(String [] args)&#123; List&lt;String&gt; name_list = new ArrayList&lt;String&gt;(); System.out.println(name_list.size()); name_list.add("小米");//增 name_list.add("小花");//增 name_list.add(1,"小框"); System.out.println(name_list.size()); get_list(name_list);//获取元素 name_list.remove("小米"); get_list(name_list); name_list.remove(1); get_list(name_list); name_list.set(0,"小狗"); get_list(name_list); System.out.println(name_list.get(0)); &#125; public static void get_list(List&lt;String&gt; list)&#123; ListIterator&lt;String&gt; it = list.listIterator(); System.out.println(list.size()); while (it.hasNext())&#123; System.out.println(it.next()); if(it.next().equals("小花"))&#123; list.add("小猪"); &#125; &#125; &#125;&#125;//vector和arraylist区别(vector加enum别arraylist和itertoor替代)import java.util.*;public class listdemo2 &#123; public static void main(String [] args)&#123; List&lt;String&gt; ls = new ArrayList&lt;&gt;(); ls.add("小明"); ls.add("小李"); ls.add("三大大三"); Iterator&lt;String&gt; it = ls.iterator(); while(it.hasNext())&#123; System.out.println(it.next()); &#125; Vector&lt;String&gt; ls2 = new Vector&lt;&gt;(); ls2.addElement("笑了"); ls2.addElement("是打算"); Enumeration&lt;String&gt; eu = ls2.elements(); while(eu.hasMoreElements())&#123; System.out.println(eu.nextElement()); &#125; &#125;&#125; hashset自定义元素12 多线程1234567891011121314151617181920212223242526272829303132333435363738394041package day12;public class demo &#123; public static void main(String [] args)&#123; ThreadDemo td = new ThreadDemo(); td.start();// td.run(); &#125;&#125;class ThreadDemo extends Thread&#123; public void run()&#123; for(int i = 0;i&lt;=10;i++)&#123; System.out.println(i); &#125; &#125;&#125;//start表示启用多线程,run表示调用方法//***************************方法2*********************************package day12;public class runnerable &#123; public static void main(String [] args)&#123; run_demo run = new run_demo(); Thread th = new Thread(run); th.start(); &#125;&#125;class run_demo implements Runnable&#123; @Override public void run() &#123; for(int i = 0;i&lt;=10;i++)&#123; System.out.println(i); &#125; &#125;&#125; 线程池runnable1234567891011121314151617181920212223package day12;import java.util.concurrent.Executor;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class thread_pool &#123; public static void main(String [] args)&#123; ExecutorService service = Executors.newFixedThreadPool(2); run_thread th = new run_thread(); service.submit(th); service.shutdown(); &#125;&#125;class run_thread implements Runnable&#123; public void run()&#123; for(int i = 0;i&lt;=10;i++)&#123; System.out.println(i); &#125; &#125;&#125; 线程池callable123456789101112131415161718192021222324package day12;import java.util.concurrent.Callable;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class callable_pool &#123; public static void main(String [] args)&#123; ExecutorService service = Executors.newFixedThreadPool(2); call_pool cl = new call_pool(); service.submit(cl); service.shutdown(); &#125;&#125;class call_pool implements Callable&#123; @Override public Object call() throws Exception &#123; for(int i = 0;i&lt;10;i++)&#123; System.out.println(i); &#125; return null; &#125;&#125; 线程池实现两个数相加12345678910111213141516171819202122232425262728293031323334package day12;import java.util.concurrent.*;public class add_by_thread &#123; public static void main(String [] args) throws ExecutionException, InterruptedException &#123; ExecutorService service = Executors.newFixedThreadPool(2); call_func cl = new call_func(2,10); call_func cl2 = new call_func(20,10); Future&lt;Integer&gt; res = service.submit(cl); Future&lt;Integer&gt; res2 = service.submit(cl2); Integer data = res.get(); System.out.println(data); Integer data2 = res2.get(); System.out.println(data2); service.shutdown(); &#125;&#125;class call_func implements Callable&lt;Integer&gt;&#123; int x; int y; public call_func(int x,int y)&#123; this.x = x; this.y = y; &#125; @Override public Integer call() throws Exception &#123; return x+y; &#125;&#125; 线程同步12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667//线程不同步案例,线程不安全package day12;public class buy_ticket &#123; public static void main(String [] args)&#123; ticket t = new ticket(); Thread t1 = new Thread(t,"窗口1"); Thread t2 = new Thread(t,"窗口2"); Thread t3 = new Thread(t,"窗口3"); t1.start(); t2.start(); t3.start(); &#125;&#125;class ticket implements Runnable&#123; int ticket_num = 100; @Override public void run() &#123; while(true)&#123; if (ticket_num&gt;0)&#123; try &#123; Thread.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() +"正在卖票"+ticket_num--); &#125; &#125; &#125; &#125;//解决线程不安全问题package day12;public class buy_ticket &#123; public static void main(String [] args)&#123; ticket t = new ticket(); Thread t1 = new Thread(t,"窗口1"); Thread t2 = new Thread(t,"窗口2"); Thread t3 = new Thread(t,"窗口3"); t1.start(); t2.start(); t3.start(); &#125;&#125;class ticket implements Runnable&#123; int ticket_num = 100; Object lock = new Object(); @Override public void run() &#123; while(true)&#123; synchronized (lock)&#123; if (ticket_num&gt;0)&#123; try &#123; Thread.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() +"正在卖票"+ticket_num--); &#125; &#125; &#125; &#125;&#125; map reduce 案例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;/** * KEYIN ：是map task读取到的数据的key的类型，是一行的起始偏移量Long * VALUEIN:是map task读取到的数据的value的类型，是一行的内容String * * KEYOUT：是用户的自定义map方法要返回的结果kv数据的key的类型，在wordcount逻辑中，我们需要返回的是单词String * VALUEOUT:是用户的自定义map方法要返回的结果kv数据的value的类型，在wordcount逻辑中，我们需要返回的是整数Integer * * * 但是，在mapreduce中，map产生的数据需要传输给reduce，需要进行序列化和反序列化，而jdk中的原生序列化机制产生的数据量比较冗余，就会导致数据在mapreduce运行过程中传输效率低下 * 所以，hadoop专门设计了自己的序列化机制，那么，mapreduce中传输的数据类型就必须实现hadoop自己的序列化接口 * * hadoop为jdk中的常用基本类型Long String Integer Float等数据类型封住了自己的实现了hadoop序列化接口的类型：LongWritable,Text,IntWritable,FloatWritable * * * * * @author ThinkPad * */public class WordcountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 切单词 String line = value.toString(); String[] words = line.split(" "); for(String word:words)&#123; context.write(new Text(word), new IntWritable(1)); &#125; &#125;&#125;import java.io.IOException;import java.util.Iterator;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class WordcountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; int count = 0; Iterator&lt;IntWritable&gt; iterator = values.iterator(); while(iterator.hasNext())&#123; IntWritable value = iterator.next(); count += value.get(); &#125; context.write(key, new IntWritable(count)); &#125; &#125; 代码100:api:object克隆1234567891011package day5;/*object克隆*/public class demo1 implements Cloneable&#123; int i; public static void main(String [] args) throws Exception&#123; demo1 dem = new demo1(); dem.i = 10; demo1 dem2 = (demo1) dem.clone(); System.out.println(dem2.i); &#125;&#125;]]></content>
      <categories>
        <category>基础</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm]]></title>
    <url>%2F2021%2F11%2F02%2Fjvm%2F</url>
    <content type="text"><![CDATA[类加载1234567891011121314151617181920类加载机制: 1.类加载器分类(主要用来加载不同的类) 2.类生命周期(可看成类加载过程) ①加载 -- -- -- ②连接 --验证 --准备 --解析 ③初始化 ④使用 ⑤卸载运行时数据区(java内存区域) 方法区--- 堆区--- 这两个线程共用 程序计数器 栈区(本地方法栈,虚拟机栈)===&gt;线程私有 直接内存 .jpg)]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动化操作鼠标键盘]]></title>
    <url>%2F2021%2F09%2F03%2F%E8%87%AA%E5%8A%A8%E5%8C%96%E6%93%8D%E4%BD%9C%E9%BC%A0%E6%A0%87%E9%94%AE%E7%9B%98%2F</url>
    <content type="text"><![CDATA[12345678910import pyautoguipyautogui.moveTo(36,240,1,pyautogui.easeOutQuad)pyautogui.moveTo(1683,126,1,pyautogui.easeOutQuad)pyautogui.moveTo(1798,735,1,pyautogui.easeOutQuad)pyautogui.moveTo(188,24,1,pyautogui.easeOutQuad)pyautogui.click()pyautogui.dragTo(257,556,duration=1,button='left')pyautogui.typewrite(message='dongzixinshishazi',interval=0.5)pyautogui.press('5')pyautogui.press('1')]]></content>
      <categories>
        <category>自动化</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[链路预测]]></title>
    <url>%2F2021%2F08%2F18%2F%E9%93%BE%E8%B7%AF%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[典型的链路预测算法123456789101112131.CN,共同邻居 2.资源分配(RA)3.Katz指标4.平均通勤时间(ACT)5.有重启的随机游走(RWR)6.simrank指标(simr)]]></content>
      <categories>
        <category>研发</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态网络]]></title>
    <url>%2F2021%2F08%2F03%2F%E5%8A%A8%E6%80%81%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[概念123很多网络是随时间发生变化的,即动态网络从抽象的数学模型来看,动态网络其实是一个有序的图序列,表示复杂系统在不同时刻的快照方向:拓扑特征分析、社团模式挖掘、子图模式挖掘以及模式预测问题 论文论文一:复杂网络动力学机器学习自动建模1234介绍: 1.网络上的动力学:大多数网络上的个体在网络上是相互作用的,他们的相互影响遵循着某种规则 2.效果:对未来任意时刻的网络上的节点状态进行预测 3.难点:①网络非常复杂②非线性动力学的困难]]></content>
  </entry>
  <entry>
    <title><![CDATA[java大数据]]></title>
    <url>%2F2021%2F07%2F13%2Fjava%E5%A4%A7%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[java基本了解1234567jvm:java虚拟机,java跨平台的关键,实现将程序转换成操作系统可识别的指令jre:java runtime environment,java运行环境,包含jvmjdk:java development kit,java扩展包java1.0-java15版本的发展及LTS版本,目前java8和11用的最多java se:java 标准版,最基本的 ----&gt;可认为做电脑上软件的java ee:java 企业版 ------&gt;可认为做网站的java me:java 微型版,嵌入式开发,现在用的少,如安卓是基于me开发的------&gt;可认为做手机软件的 视频学习流程123java基本数据类型---&gt;面向对象,什么继承、封装、多态、构造方法、抽象类、接口、关键字---&gt;常见的包,object,string,date,time,Arrays---&gt;接口List(arraylist、linkedlist、vector）和Set（hashset、linkedhashset）---&gt;collection和map(类似于字典,由键值构成,键不能相同)，常用的工具:datax(拉数据),dolphinscheduler 基本运算1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491.四舍五入,(int)为强制类型转换 float a = 1.5f; int b = (int)(a+0.5) 2.不会自动转换类型 double d = 1.2 + 24 / 5; // 5.2 可强制:double d = 1.2 + (double)24 / 5;3.基本的逻辑判断形如if-else a = 2; System.out.println(a==2?"通过":"失败");4.除法规范 整数相处,除数不能为0 浮点数相除,除数可以为0 NaN表示Not a Number 0/0.0 Infinity表示无穷大 2/0.0 -Infinity表示负无穷大 -2/0.05.三元运算符:b ? x : y6.字符串变量命名,假如先定义了一个a="66",然后在重新赋值a="77",实际上66并没有被删除,只是无法获取到7.空字符串不等于空值null8.数组操作 int[] a new int[5] //创建数组a,长度为5 a.length //数组长度 a[0] //获取指定索引的元素 a[0] = 2 a[1] = 2 或者直接int[] b = new int[] &#123;1,2,3,4,5&#125;; int[] b = &#123;1,2,3,4,5&#125;;9.逻辑控制: public class Hello&#123; public static void main(String[] args) &#123; int a = 20; int b = 10; int c = 30; if (a==20) System.out.println("正确"); if (b==10) &#123; System.out.println("进来了"+b); &#125; if (c&gt;85) &#123; System.out.println("非常棒"); &#125;else if (c&gt;60) &#123; System.out.println("还行"); &#125;else if (c&gt;30) &#123; System.out.println("不太行"); &#125;else &#123; System.out.println("垃圾"); &#125; &#125;&#125;10.switch casepublic class Hello&#123; public static void main(String[] args) &#123; int a = 10; switch(a) &#123; case 10: System.out.println("10"); break; case 2: System.out.println("2"); break; &#125; &#125;&#125;11.while 先判断条件在循环public class Hello&#123; public static void main(String[] args) &#123; int a = 10; int sum = 0; while (a &lt; 20) &#123; sum+=a; a+=1; &#125; System.out.println(sum); &#125;&#125;12.do while 先循环在判断条件 public class Hello&#123; public static void main(String[] args) &#123; int a = 8; do &#123; a+=1; &#125;while(a &gt;15); System.out.println(a); &#125;&#125;13.for循环public class Hello&#123; public static void main(String[] args) &#123; int a = 8; for (int b=1;b&lt;=a;b++) &#123; System.out.println(b); &#125; &#125;&#125;14.遍历列表public class Hello&#123; public static void main(String[] args) &#123; int[] ns = &#123;1,2,3,4,5,6&#125;; for (int a=0;a &lt; ns.length;a++) &#123; int b = ns[a]; System.out.println(b); &#125; &#125;15.排序 冒泡排序 import java.util.Arrays; public class Hello&#123; public static void main(String[] args)&#123; int[] ns = &#123;23,34,12,19,23,25&#125;; System.out.println(Arrays.toString(ns)); for (int i = 0;i&lt;ns.length-1;i++) &#123; for (int j = 0;j &lt; ns.length-i-1;j++) &#123; if (ns[j]&gt;ns[j+1]) &#123; int tmp = ns[j+1]; ns[j+1] = ns[j]; ns[j] = tmp; &#125; &#125; &#125; System.out.println(Arrays.toString(ns)); &#125; &#125;16.双层数组//两层数组import java.util.Arrays;public class Hello&#123; public static void main(String[] args) &#123; int[][] ns = &#123; &#123;1,2,3&#125;, &#123;3,4,3&#125;, &#123;1,2,5&#125;, &#125;; System.out.println(Arrays.toString(ns[0]));&#125;&#125; 面向对象123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class Person&#123; public String name; public int a;&#125;public class Hello&#123; public static void main(String[] args) &#123; Person person = new Person(); person.name = "xiaoliang"; person.a = 10; System.out.println(person.name); &#125; &#125;1.成员变量和局部变量 定理在类中属性位置的称为成员变量,为所有成员共有.定位在方法下的属性称为局部变量 2.三大特性：继承、封装、多态 封装： 1.代码复用性高 2.整合起来，提供输出的接口，别人看不到具体的实现细节 3.安全3.private私有属性private int a;对象内的私有化后,无法提供外部获取途径是不合理的,因为可以通过内部方法调用来实现4. ArrayList&lt;Student&gt; list = new ArrayList&lt;Student&gt;(); //1.1创建一个可以存储多个同学名字的容器抽象类没法直接new，抽象类下的抽象方法没有发括号、抽象类中不一定包含抽象方法、抽象类的子类必须重载所有的抽象方法案例: public abstract class Animal &#123; public abstract void eat();//抽象方法,内容不确定,没有发括号 public void normalMethod()&#123;&#125;&#125;public class Cat extends Animal &#123; @Override public void eat()&#123; System.out.println("Cat eat fish"); &#125;&#125;public class DemoMain &#123; public static void main(String[] args) &#123; Cat cat = new Cat(); cat.eat(); &#125;&#125;接口:定义上和类定义一致,可理解为是一个特殊的抽象类，但它不是类，是一个接口接口最重要的体现：解决多继承的弊端。将多继承这种机制在java中通过多实现完成了。接口:比抽象类更抽象的一个数据类型,接口类里只描述方法,具体实现通过子类.接口定义,类去实现(类似于继承不同的是一个是extends一个是implement)interface Fu1&#123; void show1();&#125;interface Fu2&#123; void show2();&#125;class Zi implements Fu1,Fu2// 多实现。同时实现多个接口。&#123; public void show1()&#123;&#125; public void show2()&#123;&#125;&#125; 接口案例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364interface USB &#123; void open();// 开启功能 void close();// 关闭功能&#125;class Mouse implements USB &#123; public void open() &#123; System.out.println("鼠标开启"); &#125; public void close() &#123; System.out.println("鼠标关闭"); &#125;&#125;class KeyBoard implements USB &#123; public void open() &#123; System.out.println("键盘开启"); &#125; public void close() &#123; System.out.println("键盘关闭"); &#125;&#125;class NoteBook &#123; // 笔记本开启运行功能 public void run() &#123; System.out.println("笔记本运行"); &#125; // 笔记本使用usb设备，这时当笔记本对象调用这个功能时，必须给其传递一个符合USB规则的USB设备 public void useUSB(USB usb) &#123; // 判断是否有USB设备 if (usb != null) &#123; usb.open(); usb.close(); &#125; &#125; public void shutDown() &#123; System.out.println("笔记本关闭"); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; // 创建笔记本实体对象 NoteBook nb = new NoteBook();// 笔记本开启 nb.run(); // 创建鼠标实体对象 Mouse m = new Mouse(); // 笔记本使用鼠标 nb.useUSB(m);// 创建键盘实体对象 KeyBoard kb = new KeyBoard(); // 笔记本使用键盘 nb.useUSB(kb); // 笔记本关闭 nb.shutDown(); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960修饰符:final 固定 static 静态static 静态修饰符,如在类中修饰属性,则实例化多个对象时是共享这个值private、protected、default、public要想仅能在本类中访问使用private修饰；要想本包中的类都可以访问不加修饰符即可；要想本包中的类与其他包中的子类可以访问使用protected修饰要想所有包中的所有类都可以访问使用public修饰。常用来修饰类、方法、变量的修饰符如下：public 权限修饰符，公共访问, 类,方法,成员变量protected 权限修饰符，受保护访问, 方法,成员变量默认什么也不写 也是一种权限修饰符，默认访问, 类,方法,成员变量private 权限修饰符，私有访问, 方法,成员变量static 静态修饰符 方法,成员变量final 最终修饰符 类,方法,成员变量,局部变量abstract 抽象修饰符 类 ,方法我们编写程序时，权限修饰符一般放于所有修饰符之前，不同的权限修饰符不能同时使用；同时，abstract与private不能同时使用；同时，abstract与static不能同时使用；同时，abstract与final不能同时使用。☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆注意：如果类用public修饰，则类名必须与文件名相同。一个文件中只能有一个public修饰的类。☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆继承:一个子类只能有一个父类,但是可以有多层结构c继承b,b继承a.自己有的优先调用自己的,没有才去父类调//接口和抽象类:抽象类是对类的抽象,即单一的类无法描述所有情况的时候,而接口是一种扩展的功能,不是所有都使用//一个类智能继承一个父类,但是可以继承多个接口implement a,b,c extends a多态的体现:父类引用变量可以指向子类对象,相同的事物在不同的类有不同的含义(如既是老师也是学生)====变量调用的 是父类的变量,方法调用的是子类重载的方法成员变量，静态方法看左边；非静态方法：编译看左边，运行看右边。”意思是：当父类变量引用子类对象时,Fu f = new Zi();在这个引用变量f指向的对象中，他的成员变量和静态方法与父类是一致的，他的非静态方法，在编译时是与父类一致的，运行时却与子类一致（发生了复写）继承时候,构造方法父类的会先执行,默认含有一个隐式的super()关键词解释: final:写在类前,表示该类不能被继承.写在方法前,表示该方法不能被重载.写在变量前表示该变量不能被二次赋值,否则会报错. static:静态修饰符,放在成员变量前,表示该变量是静态变量,public static int num = 2,可通过类名+变量获取,同时静态变量是所有实例化的对象共享的,一个改变则都跟着改变. 放在方法前表示该方法是静态方法,静态方法中只能访问静态变量或静态方法,同时不能使用super和this关键字 public class demo3 &#123; public static void main(String [] args) &#123; System.out.println(person.get_name()); //调用静态方法 &#125;&#125;class person&#123; public static String name = "xixom"; public static String get_name()&#123; return name; &#125;&#125;注意:接口中的每个成员变量都默认使用public static final修饰。所有接口中的成员变量已是静态常量，由于接口没有构造方法，所以必须显示赋值。可以直接用接口名访问。Inter.COUNT匿名对象:实例化的对象没有赋值给变量 如new person() 面向对象 123456789101112131415161718scala是完全面向对象的,为了实现static,通过伴生对象实现抽象类:里面既可以包含抽象属性、抽象方法、非抽象属性和非抽象方法,针对私有化的属性,可以通过伴生类去调用,在通过伴生类来进行调用.父类中的var属性在子类中可以直接修改,不需要override,会报错,子类继承的时候针对抽象方法可以不写override,针对非抽象方法必须写override特质trait也是抽象类,类似于java的接口,可以看成是方法的补充,区别是不能初始化属性,mixin混入,即可以wit很多特质类型判断和类型转换isinstanceof和asinstanceof 枚举类对象和应用类object WorkDay extends Enumeration&#123;val Monday = value(1,&quot;MONDAY&quot;)&#125;应用类 object test extends APP&#123;&#125;===&gt;省略main方法自定义类型type Mystring = String 则Mystring可以当做string来使用 集合12345678分为三大类seq、set、和map创建数组new Array[Int](10)==&gt;简化Array(1,2,3,4,5...)==&gt;实际上调用的apply方法,apply可省略不可变数组追加数据 array1.:+(2)==&gt;往后追加array1 +: 2 array1.+:(3)===&gt;往前追加 2 +:数组、可变数组的增删改查互相转换 arrai.toBuffer,arra2.toArray 接口API12345678910111213141516171819202122232425262728293031323334353637383940414243api:applicantion promming interfaces 应用程序接口类说明:##########################object#######################1.clone:复制产生的是一个新对象,属性跟原本一样,只能在子类中用2.getclass3.finalize4.equals:重构问题※※※※※5.tostring:重构问题※※※※ogject是一切类的父类jdk中含有包的说明##########################string####################################################map##################Map&lt;String, String&gt; map = new HashMap&lt;String,String&gt;();public class MapDemo &#123; public static void main(String[] args) &#123; //创建Map对象 Map&lt;String, String&gt; map = new HashMap&lt;String,String&gt;(); //给map中添加元素 map.put("邓超", "孙俪"); map.put("李晨", "范冰冰");map.put("刘德华", "柳岩"); //获取Map中的所有key Set&lt;String&gt; keySet = map.keySet(); //遍历存放所有key的Set集合 Iterator&lt;String&gt; it =keySet.iterator(); while(it.hasNext())&#123; //得到每一个key String key = it.next(); //通过key获取对应的value String value = map.get(key); System.out.println(key+"="+value); &#125; &#125;&#125;######排序和打乱collections.sort() 和collections.shuffle() 集合框架123456789101.集合框架(类似Python的容器)分成collection(集合)===&gt;元素的集合,和map(图)===&gt;键值对的形式.2.集合框架下分成list、set和queue3.list下分成arraylist（数组），linkedlist（链表）、vector(抛弃了,被Arraylist和itertool替代了).(list集合下根据数据结构的不同可分成多部分,如栈、队列、数组、链表，其中数组取元素快，增删麻烦。链表增删数据简单，取数据慢---需要一个一个找，手拉手)4.list下最常用的还是Arraylist5.声明 List&lt;String&gt; = new Arraylist&lt;String&gt;;6.因为List中会允许存在重复数据,这时候需要去重,使用的是Set7.set内分成hashset和linkedhashset8.hashset去重的原则是hashcode和equals,如果输入的是自定义的对象,则需要重写hashcode和equals方法9.哈希表:又名哈希数组,底层使用的是数组存放对象,然后会根据对象的特有数据和特定的算法,计算出对象在数组中未位置,然后把对象存放在数据中.10.linkedhashset是hashset下的一个结构,可实现去重及排序,使用的是哈希表和链表实现的 进程、线程12345678910111213141516171819202122进程是程序执行的最小单位,线程是资源调度的最小单位每个核同时只能执行一个线程,window默认只使用一个核,只是切换的速度超过人眼反应,因而可以同时开很多个程序.且速度和核数并非成线性的,开四个核实际提升可能只有2倍左右.线程实现的两种方式一个是继承thread ,另一个是使用接口runnable接口.更多的是使用第二个,因为第一个存在单继承的局限性. 线程池:减少线程的创建和销毁的资源消耗,有放回的获取线程线程安全:针对全局变量如果多个线程只有读的操作,则认为是线程安全的.如果多某个变量进行写操作,需要多个线程同步,否则就是不安全的. 解决线程不安全问题: 使线程同步Synchronized 实现方式: 1.同步代码块 在可能会出现线程不安全的代码前加上Synchronized(锁对象)&#123;代码内容&#125;即可 2.同步方法 将可能会出现问题的代码写成方法,public Synchronized void method()&#123;内容&#125; 3.同步静态方法: 在方法前加上static 。public static Synchronized void method()&#123;内容&#125;死锁:独木桥案例,抢资源谁也不让谁 通常是一个线程锁定了一个资源A，而又想去锁定资源B；在另一个线程中，锁定了资源B，而又想去锁定资源A以完成自身的操作，两个线程都想得到对方的资源，而不愿释放自己的资源，造成两个线程都在等待 1.有多把锁 2.有多个线程 3.同步代码快嵌套 通俗:两个线程,一个线程拿到了锁A,想拿锁B.但是宁外一个线程拿到了锁B,想要拿锁A.对方不释放则都拿不到,就这样僵持,造成了 异常1234561.几个关键词:throw、throws、finally、try、catch2.通常try&#123;&#125; catch（）&#123;&#125;；类似python try except3.throws的作用通常是修饰方法的，表面了该方法可能会抛出的异常，需要自己解决4.finally不管是try还是catch后,都会执行,除非遇到system.exit(),通常是用来释放资源,关闭连接的5.执行顺序,首先执行try,如果有异常则执行catch,若没有catch,则先执行finally,然后再去找合适的catch6. throw 手动抛出异常 throw new IllegalArgumentException("w"); 其他123构造方法的执行流程: 1. 在栈中执行main方法,在堆中创建对象,并对属性值进行默认值赋予.栈中存放着对象的地址. 2. 使用构造方法,会将传入的值赋给对象的参数]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[知识总结]]></title>
    <url>%2F2021%2F07%2F07%2F%E5%85%86%E5%B0%B9%E7%A7%91%E6%8A%80%E8%AF%95%E7%94%A8%E6%9C%9F%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[核心要点1234567sql的规范非常重要： 1. sql语法使用的是clickhouse（目前使用ch） 2. 需要输入的参数使用&#123;&#125;括起来 3. 参数以sql_开头的会将传入的参数值作为整个sql的一部分,如BENCHMARK_IDS=&quot;333,444,555&quot;会将它转换生成注入参数sql_BENCHMARK_IDS=&quot;(a.BENCHMARK_ID = &apos;333&apos; or a.BENCHMARK_ID = &apos;444&apos; or a.BENCHMARK_ID = &apos;555&apos;)&quot;) ①注意sql_后面的名字需要和字段的名字一样,且会加一个s结尾,这样表示会传入多个参数,python转换的时候自动转换成or的形式去匹配数据 ②在参数设置-其他里面需要选择左连接还是右连接的参数设置 4. 最外层字段名要使用AS,且目前看到的字段名全是大写的 中台系统121. 核心要点是某些无法直接查询或者通过sql写出来得指标2. 计算这些指标,首先需要明确需要用到哪些数据,以及计算的公式,然后通过公式将数据转换成模型,将它生成api即可 金融指标开发:核心要点:按照模板修改metadata和公式输入输出1234567891011121314151. 静态指标开发:定期需要跑的指标===&gt;跑批等---&gt;变化不大的指标===&gt;结果存在数据库里 注意事项: 1. 操作数据库使用dao里面的类,不要直接访问数据源 2. 指标发布需要先写代码,写代码建议在元静态模板的基础上修改 3. 静态指标发布:需要将指标在配置文件中进行添加(config文件夹下) 2. 动态指标开发:需要实时计算的===&gt;由前端发起请求,调用notebook的代码实现 1. 复制模板代码,在模板代码的基础上进行修改 2. 动态指标建议使用olap_开头 3. 函数输出的metadata也需要包含输入的参数 4. 动态指标的发布: 同静态,需要在配置文件里添加(config文件夹下) 5. 转换成py代码,都需要在http://localhost:8000/下进行生成 6.动态指标是否创建好,需要在jobs中的indicators_olpa.py文件下查看是否已经包含需要的函数 7.动态指标发布完后可以通过postman进行测试 疑问的地方: 1. locallhost:8000怎么通过api接口自测 1. postman的使用在哪里 代码整体介绍1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950511. 代码里的api_mode apiext apis对应了三个版本的api接口,api_model对应了roboat_api,apis对应了跑批的api2. 跑批程序需要在config/job_conf.json进行配置,否则不会进行代码转换,olap的配置目前看到是不用了所以不用进行配置3. 跑批的py代码生成后(点击),可以在中台批处理任务/任务管理进行新建任务时候,任务方法里面可以看到跑批的算法列表(由跑批的api列表那个提供)4. 接口里面olap实时生成目前已经不用了,在光大版本时候使用的5. 接口里面tool也是光大版本使用的目前已经不用5. 接口里面ext_tool只要是下载保存使用的6. 接口面的robot_tool是邮储版本使用的,负责生成平铺指标函数类和聚合指标函数类==驼峰命名法,同时会将平铺和聚合类的ipynb代码转换成py代码,平铺转换后的代码在functions/ind下,聚合转换后的代码在functions/ind_agg下7. metadata的作用是告诉中台由哪些参数8. 操作数据模型进行数据读写,在inds_function/agg聚合里面有样例9. agg类的聚合算法模型ipynb会生成对应的驼峰命名法类的算法模型,可在API管理的算法模型通过类名搜索到对应的模型,也可以在指标库管理里面搜到10. 不同的级别:什么组合\组合树级别的,不知道什么意思,在meta_data中有定义agg-layers(里面有port\port-tree等这些划分)11. 服务启动的时候会自动转换所有模型.除非有新增才需要手动去api生成.重启不考虑12. 中台数据模型里面,目前只用到了单源模型和数据源,复合模型目前还没有开始使用.单源模型是md结尾的,复合模型是cmd结尾的13. 在中台创建的单元数据模型,每次启动服务时候都会自动生成对应的类的py代码,使得可以直接在代码里使用了,不用进行数据库的读取操作了,也可以自己生成通过/robot_tool/model_to_ind_funcs接口14. 聚合类本来只有那些自带的方法(光大版)==&gt;在api管理的数据分组那里,后来添加了自定义agg的算法模型15. 数据中台的指标库指标的操作在apiext下的api_data_utils.py代码里,进行实例化操作16. 同一指标的时间维度配置,名称可以相同(在指标库/指标设置/新增指标/时间维度)==&gt;循环跑,代码在api_data_utils部分17. 共通参数名在common下的constants.py代码里,否则组合树那些可能无法解析,无法转换18. 字典代码在commmon下的sys_dicts19. dao数据库处理相关的20. settings里面的sql配置是给dao里面的sql使用的,不是给数据模型使用的,数据模型使用的是数据源的配置的连接.在dao/models/data_source_init.py文件下21. dao/data_model.py下负责解析大json,然后解析成文件22. 层级类型的理解,什么组合树啊,port_ids啊23. 单源模型的参数设置时候默认值填 1=1, 多个数据时候通常选在,左字段里面得选择表名.字段名24. 汇总级别里面,组合树、组合、资产树、期间、资产等区别是什么25. 测试流程: 1. 本地测试 ①启动py服务,写jupyter代码,假数据测试 2. 服务器测试 ①在服务器写完ipynb代码,或者更新最新的代码上去,重启服务(上传,生成也可以) ②在api管理里面新建算法模型的api接口 ③通过api接口返回的json去python的某个接口获取大json ④把大json传给模型接口,完成26. jupyter仅仅作为一个写代码的工具,无法和中台那些连起来27. 组合代码:port_ids = &apos;ZY_HB001,ZY_JZ001,ZYB&apos;28. 配置中台数据源连接在dao/data_source_init.py下29. 写代码流程: 目前是在原有代码的基础上修改,添加字段等操作 批处理代码流程: 1. 本地修改代码,添加字段,新建文件还需要添加到配置文件里 2. 重新生成批处理的文件,execute 3. 使用postman的/job/run_batch/&#123;job_name&#125;跑一下该任务 4. 断点调试,通过debug模式启动任务,然后在py代码里大断点 5. 运行postman跑批任务的接口,跑代码 6. 去数据库通过sql查询插入的数据是否正确,没问题则将修改的代码上传30. 参数sql_port_ids等样式的特殊31. 输出字符的name决定了输出字段的名字,是中文则是中文32. 有后置过滤处理因而返回的参数不要与传入的参数有重叠33. 重启服务: cd /home/tams/docker_deploy/dc-start/ docker ps docker restart 79f4e307d214 问题12341. 组合、资产2. 源数据表里放的是哪些数据 ①Bond\dwd\HK\LC\QT\sst等分别代表的含义3. 指标设置里面的参数设置控件,怎么选择的问题 最早的投资时间小于平台日会报错 什么时候看完,什么时候开发完 本地调试代码修改12345678910111213141516171819202122232425262728293031def get_sql_util_instance(cls, data_source_id: str) -&gt; DbUtils: #####本地重载的,源代码是下面那个,不要动 logger.debug(str(cls)) if data_source_id in db_utils_dict: return db_utils_dict[data_source_id] try: data_source_lock.acquire() # 实时请求中台 if data_source_id: ds_dict = rest_post(s.GET_ALL_DB_CONN_URL, &#123;'dataSourceId': ''&#125;) if ds_dict: source_ids =[i for i in rest_post('/dataModel/getAllDbConnList', &#123;'dataSourceId': ''&#125;)['list'] if i['dataSourceId'] == data_source_id] ds_dict = &#123;'size':len(source_ids),'list':source_ids&#125; if ds_dict and ('size' in ds_dict) and (ds_dict['size'] == 1): data_s = __format_ds(ds_dict['list'][0]) data_source_config[data_s['dataSourceId']] = data_s # 实时创建连接池和db_utils db_utils = create_db_utils(data_s['dbType'], data_s) db_utils_dict[data_source_id] = db_utils return db_utils except ConnectionResetError as e: logger.error(repr(e)) msg = '%s 无法访问，可能是服务未正常启动或网络不通。' % s.EUREKA_DATA_MID_NAME logger.error(msg) raise DataSourceNotFoundError(msg=msg) finally: data_source_lock.release() logger.error('找不到数据源：%s的配置信息。' % data_source_id) raise DataSourceNotFoundError(data_source_id) 取模型数据12345678910111213141516PORT_IDS = &apos;DL1,DL2&apos;START_DATE = &apos;2000-01-01&apos;END_DATE = &apos;2020-01-01&apos;BENCHMARK_TYPE = &apos;1&apos;BENCHMARK_ID = &apos;2&apos;ind_instance = get_ind_instance(&#123; gCp.AGG_IND_CLASS_NAME.value: &apos;PortfolioPeriodReturnAgg&apos;, &apos;PORT_IDS&apos;: PORT_IDS, &apos;START_DATE&apos;: START_DATE, &apos;END_DATE&apos;: END_DATE, &apos;BENCHMARK_TYPE&apos;:BENCHMARK_TYPE, &apos;BENCHMARK_ID&apos;:BENCHMARK_ID&#125;)df_list = ind_instance.get_source_df_list()df = ind_instance.calc_port_return(df_list)df 参数设置1234567891011121314PORT_IDS = &apos;DL71,NYLY-6MDK&apos;END_DATE = &apos;2020-01-01&apos;START_DATE = &apos;2000-01-01&apos; BENCHMARK_TYPE=&apos;CONTRACT&apos;BENCHMARK_ID=&apos;GZ001&apos;CALC_BENCHMARK_ID = &apos;01&apos;NO_RISK_BENCHMARK_ID = &apos;02&apos;is_test=Falsecalc_return2(get_source_data(PORT_IDS, START_DATE, END_DATE, BENCHMARK_TYPE=&apos;CONTRACT&apos;,BENCHMARK_ID = BENCHMARK_ID,BENCHMARK_IDS = [&apos;GZ001&apos;,&apos;GZ002&apos;,&apos;GZ003&apos;], CALC_BENCHMARK_ID = CALC_BENCHMARK_ID, NO_RISK_BENCHMARK_ID = NO_RISK_BENCHMARK_ID))df_list = get_source_data(&apos;NYLY-10MDK&apos;, &apos;2019-01-01&apos;, &apos;2019-12-17&apos;,BENCHMARK_IDS = &apos;GY001&apos;, CALC_BENCHMARK_ID = &quot;000007_CNSESH&quot;, NO_RISK_BENCHMARK_ID = &apos;GZ001&apos;) 代码详解12job_convert.py 代码里主要讲述了jupyter代码 转换成python代码的实现过程apiext/api_data_utils.py 方法生成 这个意思是根据使用组合和组合树的不同可以返回不同的指标,要在这里面标明,下面source是为了字段溯源使用的(血缘关系-光大版本的)目前直接读的数据模型不需要考虑来源于哪个表了,所以不用写 平铺类算法指标想要实现多进程可以在meta_data里面写(这里说的是传入很多个组合的时候的情况),这里的写法意思是多进程跑方法 这里是一个优化的点,通过测试查看多进程是否可以快一点 df1 = pd.DataFrame([[‘2019-01-01’,10],[‘2019-01-02’,10],[‘2019-01-10’,10]],columns = [‘month_date’,’values’])df2 = pd.DataFrame([[‘2019-01-13’,10],[‘2019-01-10’,10],[‘2019-01-15’,10]],columns = [‘month_date’,’values’])start_date = min(df1.month_date.min(),df2.month_date.min())end_date = max(df1.month_date.max(),df2.month_date.max())month_num = len(pd.date_range(start_date,end_date,freq = ‘D’,closed = ‘left’))date_df = pd.DataFrame(pd.date_range(start_date,freq = ‘D’,periods = month_num+1),columns = [‘month_date’])date_df[‘month_date’] = date_df[‘month_date’].apply(lambda x:str(x)[:10])date_df_all = date_df.merge(df1,on = ‘month_date’,how = ‘left’).merge(df2,on = ‘month_date’,how = ‘left’)]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[知识总结]]></title>
    <url>%2F2021%2F07%2F07%2F%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[123Python:基础知识,mysql操作,爬虫,机器学习算法,numpy,pandas,数据分析,cython,numba,pypy,dask,pp了解,xgboost,clickhouse,nlp基础了解,pandarallel,pyspark,linux基础命令,数据结构与算法,复杂网络了解,接口了解,特征工程,Python操作excel指定格式存数据其他:联邦学习java、hadoop相关架构 del1用del可以将对象占用内存空间的引用计数值置零（Deletion of a name removes the binding of that name from the local or global namespace）。它并不能让对象占用的内存被回收，但一段内存的引用计数变为零，就说明它可以再次被重新使用了（所以del后，不必要GC介入）]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文本预处理代码案例]]></title>
    <url>%2F2021%2F05%2F27%2F%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86%E4%BB%A3%E7%A0%81%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[连接服务器的jupyter]]></title>
    <url>%2F2021%2F04%2F15%2F%E8%BF%9E%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84jupyter%2F</url>
    <content type="text"><![CDATA[整体概括123456pip3 install jupyter notebook1.在服务器安装jupyter notebook 并启动服务(jupyter notebook --ip 0.0.0.0)2.在windows浏览器输入地址,具体可看上面第一步的打印信息]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>jupyter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[联邦学习]]></title>
    <url>%2F2021%2F04%2F08%2F%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[联邦学习概念理解 ####PHE加密 1234567from phe import paillierpublic_key, private_key = paillier.generate_paillier_keypair() #创建公钥和私钥secret_number_list = [3.141592653, 300, -4.6e-12]encrypted_number_list = [public_key.encrypt(x) for x in secret_number_list] #加密[private_key.decrypt(x) for x in encrypted_number_list] #使用私钥解密a, b, c = encrypted_number_lista_plus_5 =a + 5 案例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103import mathimport numpy as npfromphe import paillierimport pandas as pdfrom sklearn import datasetsfrom sklearn.datasets import load_diabetesfrom sklearn.preprocessing import StandardScalerfrom sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_splitfrom sklearn.utils import shuffleclass Client: def __init__(self, config): ## 模型参数 self.config = config ## 中间计算结果 self.data = &#123;&#125; ## 与其他节点的连接状况 self.other_client = &#123;&#125; ## 与其他参与方建立连接 def connect(self, client_name, target_client): self.other_client[client_name] = target_client ## 向特定参与方发送数据 def send_data(self, data, target_client): target_client.data.update(data)class ClientA(Client): def __init__(self, X, config): super().__init__(config) self.X = X self.weights = np.zeros(X.shape[1]) def compute_z_a(self): z_a = np.dot(self.X, self.weights) return z_a def compute_encrypted_dJ_a(self, encrypted_u): return encrypted_dJ_a def update_weight(self, dJ_a): self.weights = self.weights - self.config["lr"] *dJ_a / len(self.X) return## A: step2 def task_1(self, client_B_name): dt = self.data public_key = dt['public_key'] z_a = self.compute_z_a() u_a = 0.25 * z_a z_a_square =z_a ** 2 encrypted_u_a = np.asarray([public_key.encrypt(x) forx in u_a]) encrypted_z_a_square = np.asarray([public_key.encrypt(x) forx in z_a_square]) dt.update(&#123;"encrypted_u_a": encrypted_u_a&#125;) data_to_B = &#123;"encrypted_u_a": encrypted_u_a, "encrypted_z_a_square": encrypted_z_a_square&#125; self.send_data(data_to_B, self.other_client[client_B_name]) def task_2(self, client_C_name): dt = self.data encrypted_u_b = dt['encrypted_u_b'] encrypted_u =encrypted_u_b + dt['encrypted_u_a'] encrypted_dJ_a = self.compute_encrypted_dJ_a(encrypted_u) mask = np.random.rand(len(encrypted_dJ_a)) encrypted_masked_dJ_a =encrypted_dJ_a + mask dt.update(&#123;"mask": mask&#125;) data_to_C = &#123;'encrypted_masked_dJ_a': encrypted_masked_dJ_a&#125; self.send_data(data_to_C, self.other_client[client_C_name]) def task_3(self): dt = self.data]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[clickhouse底层知识]]></title>
    <url>%2F2021%2F04%2F01%2Fclickhouse%E5%BA%95%E5%B1%82%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[clickhouse:列式存储,一个用于(OLAP–&gt;联机分析)的列式数据库管理系统(DBMS) clickhouse为啥快: ​ 1.有多少CPU吃多少资源 ​ 2.Clickhouse不支持事务,不存在隔离级别 ​ 3.IO方面,mysql是行存储,Clickhouse是列存储.mysql需要大量的随机io.clickhouse基本是顺序io.(顺序io,磁头顺序访问.随机IO,需要先寻址===通俗就是顺序io,找到第一块数据后,其他数据都在这块数据后就不需要寻址了,依次获取即可,随机IO重复到不同的扇区找不同的数据,所以慢) ​ 4.列式存储,向量化引擎 数据处理: ​ OLAP:联机分析处理,—-&gt;数据的聚合,上钻,下钻等==&gt;上,获取数据上一级数据,下,获取更细节的数据 ​ OLTP:联机事务处理,—&gt;数据的增删改查等 #####按行存储和按列存储 123451.只访问查询的列,大大降低IO2.由于查询是通过列实现的,整个数据库是自动索引化的3.按行存储没有索引会使用大量的IO,而建立索引有需要花费大量时间和资源4.按列存储有效减少了扫描的数据量.,按行扫描会获取所有字段,而按列则只获取指定字段5.列式存储除了降低了IO和存储的压力(数据压缩),还为向量化做好了铺垫 基础数据类型12345Int:Int8,Int16,Int32,Int64 toInt32Float:Float32,Float64 toFloat32Decimal:Decimal32,Decimal64,Decimal128======&gt;保留小数位数 toDecimal32String: String,FixedString 定长,UUID 直接定义为主键 toStringDate:DateTime,DateTime64,Date 时间精度不同,Date到天,DateTime到秒,DateTime64到亚秒 toDate 复合数据类型1234567数组 Array ,数组内的数据类型可以不同,但是不能冲突,如int和float是可以的,但是不能和String组成元组 Tuple ,比array强一点,可以支持不同的数据类型,即元组内可以同时出现整形和字符串===但是定义的时候还是需要指定数据类型,如果插入数据类型不匹配仍会出错枚举 Enum ,键值对的形式,a=2,b=3等嵌套 Nested 可以让多个字段嵌套在一个字段下,如dept.id,dept.name,dept.age这种形成字段,字段内为数组,且长度都得一致,且一一对应 #####特殊数据类型 12345Nullable===&gt;辅助修饰,让某些字段可以写入NULL值如:Nullable(String)-----&gt;慎用,会影响性能.只能和基础数据类型搭配不能搭配符合数据类型 Nullable(Array(String))不可行.Array(Nullable(String)) 可行Domain====&gt;分为IPv4和IPv6两种,直接定义数据类型即可 ip IPv4 用处:自带检查功能,是否符合规范]]></content>
      <categories>
        <category>clickhouse</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[clickhouse安装配置]]></title>
    <url>%2F2021%2F03%2F23%2Fclickhouse%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1.安装123456781.安装curl系统包yum install -y curl2.安装clickhouse repositoriessudo yum install -y clickhouse-server clickhouse-client3.查询clickhouse包yum list &apos;clickhouse*&apos;4.安装clickhouse包yum install -y &apos;clickhouse*&apos; 2.修改配置文件1文件位置 /etc/clickhouse-server 2.1.远程可访问配置123456将config.xml文件下的一行代码注释打开找到&lt;!-- &lt;listen_host&gt;::&lt;/listen_host&gt; --&gt; 打开注释之后变成这样&lt;listen_host&gt;::&lt;/listen_host&gt;重启service clickhouse-server restart #####2.2..创建角色 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758对config.xml修改在最后一行加上下面的代码，用于接收创建的角色信息，加完代码之后记得创建这个文件&lt;access_control_path&gt;/data/work/clickhouse/access/&lt;/access_control_path&gt;对users.xml 的配置修改&lt;profiles&gt; &lt;!-- Default settings. --&gt; &lt;default&gt; &lt;!-- Maximum memory usage for processing single query, in bytes. --&gt; &lt;max_memory_usage&gt;10000000000&lt;/max_memory_usage&gt; &lt;!-- Use cache of uncompressed blocks of data. Meaningful only for processing many of very short queries. --&gt; &lt;use_uncompressed_cache&gt;0&lt;/use_uncompressed_cache&gt; &lt;!-- How to choose between replicas during distributed query processing. random - choose random replica from set of replicas with minimum number of errors nearest_hostname - from set of replicas with minimum number of errors, choose replica with minimum number of different symbols between replica&apos;s hostname and local hostname (Hamming distance). in_order - first live replica is chosen in specified order. first_or_random - if first replica one has higher number of errors, pick a random one from replicas with minimum number of errors. --&gt; &lt;load_balancing&gt;random&lt;/load_balancing&gt; &lt;/default&gt; &lt;!-- Profile that allows only read queries. --&gt; &lt;readonly&gt; &lt;readonly&gt;1&lt;/readonly&gt; &lt;/readonly&gt; &lt;authority_r&gt; &lt;readonly&gt;1&lt;/readonly&gt; &lt;allow_ddl&gt;0&lt;/allow_ddl&gt; &lt;/authority_r&gt; &lt;authority_rw&gt; &lt;readonly&gt;2&lt;/readonly&gt; &lt;allow_ddl&gt;0&lt;/allow_ddl&gt; &lt;/authority_rw&gt; &lt;authority_rwd&gt; &lt;readonly&gt;0&lt;/readonly&gt; &lt;allow_ddl&gt;1&lt;/allow_ddl&gt; &lt;/authority_rwd&gt; &lt;/profiles&gt;&lt;users&gt; &lt;admin&gt; ## clickhouse自带default用户，但是该用户拥有所有权限且没有设置登陆密码和开启RBAC &lt;password&gt;123456&lt;/password&gt; ## 自定义密码 &lt;access_management&gt;1&lt;/access_management&gt; ### 默认为0，修改成1 开启RBAC权限控制 &lt;networks incl=&quot;networks&quot; replace=&quot;replace&quot;&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;default&lt;/profile&gt; ## 使用的用户配置。直接写名称default 认为已经引用了所有default的配置 &lt;quota&gt;default&lt;/quota&gt; ## 资源限额，引用default的全部设置 &lt;/admin&gt;&lt;/users&gt; profiles下的对象是对权限的具体定义，users可继承profile中对象的属性这里创建的admin账号全部引用的default的配置 readonly 有三种状态 0不做限制 1可读（select，exists，show，describe） 2可读可设置（比1多了set） allow_ddl 有两种状态 0不允许ddl 1允许ddl #####删除clickhouse配置和文件 1234567891011yum list installed | grep clickhouse yum remove -y clickhouse-common-static yum remove -y clickhouse-server-common rm -rf /var/lib/clickhouse rm -rf /etc/clickhouse-* rm -rf /var/log/clickhouse-server]]></content>
      <categories>
        <category>clickhouse</category>
      </categories>
      <tags>
        <tag>配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark-sql]]></title>
    <url>%2F2021%2F02%2F24%2Fspark-sql%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718191.rdd转换成dataframe ----&gt;rdd需要时列表或者元组构成的rdd1 = sc.parallelize([('a',1),('b',2)])df = spark.createDataFrame(rdd1)df.show()输出结果:+---+---+| _1| _2|+---+---+| a| 1|| b| 2|+---+---+df.first():Row(_1='a', _2=1)df.printSchema() ===&gt;查看df的列字段类型同pandas的inforoot |-- _1: string (nullable = true) |-- _2: long (nullable = true)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rdd操作]]></title>
    <url>%2F2021%2F02%2F23%2FRdd%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[#####基本的RDD转换操作—————-&gt;将一个Rdd转换成宁一个Rdd 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991.map()---&gt;最基本的转换操作========================&gt;对rdd的每一个元素做处理word = sc.parallelize(['a b','c d'])word.map(lambda x:x.split(' ')).collect()返回结果:[['a', 'b'], ['c', 'd']]2.flatMap()---&gt;先map然后flat平铺========&gt;注意M是大写的word = sc.parallelize(['a b','c d'])word.flatMap(lambda x:x.split(' ')).collect()返回结果:['a', 'b', 'c', 'd']3.filter()---&gt;过滤操作===============&gt;筛掉不符合条件的word = sc.parallelize(['a b','c d'])word.filter(lambda x:len(x) == 1).collect()返回结果:[]4.distinct()----&gt;去重操作============&gt;去重rddword = sc.parallelize(['a b','c d','a b'])word.distinct().collect()5.groupBy()-----&gt;按照指定条件对RDD分组==============&gt;B大写,且一般不使用这种方法(要求键对应的值可以全部放入内存中)==========&gt;推荐更高效的aggregateByKey()和reduceByKey()等word = sc.parallelize([('a',1),('a',2),('b',3)])word.groupBy(lambda x:x[0]).collect()返回结果:[('b', &lt;pyspark.resultiterable.ResultIterable object at 0x7f7064cfa7b8&gt;), ('a', &lt;pyspark.resultiterable.ResultIterable object at 0x7f7064cfa1d0&gt;)]6.sortBy()------&gt;按照指定键进行排序word = sc.parallelize(['a b','c d','a b','a c']) word.sortBy(lambda x:x[1],ascending=True)7. cartesian------&gt;求两个rdd的笛卡尔乘积a.cartesian(a).collect()8. glom -----&gt;以列表的形式查看各个分区的数据a = sc.parallelize([1,2,3,4],2)a.glom().collect()9.coalesce----&gt;降低rdd 的分区数(小于等于之前分区数)a.coalesce(1).glom().collect()10. cogroup ----将两个pairrdd 中相同键的值放在一起a = sc.parallelize([("a",2)])b = sc.parallelize([("a",3),("b",2)])[(x,tuple([list(i) for i in y])) for x,y in a.cogroup(b).collect()]输出:[('b', ([], [2])), ('a', ([2], [3]))]10.1 groupWith ------将多个pair rdd中相同的键的值放一起,效果同cogroupa = sc.parallelize([("a",2)])b = sc.parallelize([("a",3),("b",2)])c = sc.parallelize([("a",3),("b",2)])[(x,tuple([list(i) for i in y])) for x,y in a.groupWith(b,c).collect()]输出: [('a', ([2], [3], [3])), ('b', ([], [2], [2]))] 11. union ----&gt;合并两个rdda = sc.parallelize([1,2,3])b = sc.parallelize([3,4,5])a.union(b)12. countByValue() ----&gt;返回RDD值出现的次数,而非pairrdd的键值对应的值x = [('a', 2), ('a', 2), ('b', 1)]x.countByValue() 输出:&#123;('a', 2): 2, ('b', 1): 1&#125; 13. countByKey() -----&gt;针对pairrdd 按照key聚合===&gt;这个得是pairrddx.countByKey()14. getNumPartitions -----&gt;获取rdd的分区个数,查看具体的分区可以使用glom15. groupby ------&gt;按照传入参数的[返回值]进行聚合a = sc.parallelize([1,2,3,4,5])a.groupBy(lambda x:x%2 == 0).map(lambda x:&#123;x[0]:list(x[1])&#125;).collect()[&#123;False: [1, 3, 5]&#125;, &#123;True: [2, 4]&#125;]16. groupByKey -----&gt;pair rdd 按照键进行聚合,值聚合成迭代器a = sc.parallelize([(1,2),(2,3),(1,4)])a.groupByKey().mapValues(list).collect()输出:[(1, [2, 4]), (2, [3])] 17. mapValues ------&gt; 对pair rdd 的值操作a = sc.parallelize([(1,2),(2,3),(1,4)])a.mapValues(lambda x:x+2).collect()输出：[(1, 4), (2, 5), (1, 6)]18. intersection -----&gt;求两个rdd的交集a = sc.parallelize([(1,2),(2,3),(1,4)])a.intersection(a).collect()19. isEmpty -----&gt;判断rdd 是否为空sc.parallelize([]).isEmpty() 20. mapPartitions -------&gt;对rdd 的每个分区做处理,返回各个分区的结果a = sc.parallelize([1,2,3,4,5],2)a.mapPartitions(lambda x:[len(list(x))]).collect()返回结果:[3,2] 21. zip -----&gt; 生成pair rdd,x和y的长度需要一致x.zip(y).collect() #####基本RDD的行动操作———&gt;进行求值或计算或输出 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364651.count() ------&gt;计数=======&gt;rdd中元素的个数word = sc.parallelize([('a',1),('a',2),('b',3)])word.count()返回结果:32.collect() ------&gt;输出======&gt;返回rdd元素构成的列表word = sc.parallelize([('a',1),('a',2),('b',3)])word.collect()输出结果:[('a',1),('a',2),('b',3)] 3.take() -------&gt;输出=====&gt;返回RDD指定个数的元素,返回的元素不固定.选元素没特定的顺序word = sc.parallelize([('a',1),('a',2),('b',3)])word.take(1)输出结果:[('a',1)] 4.top() ------&gt;输出======&gt;返回降序排序好的前N个元素word = sc.parallelize([('a',1),('a',2),('b',3),('b',1)])word.top(1)输出结果:('b',3) 5.first() -----&gt;输出====&gt;同take一样返回结果不固定,不同的是take返回的是列表#无参数word = sc.parallelize([('a',1),('a',2),('b',3),('b',1)])word.first() #无参数输出结果:('a',1) ===================用以聚合的行动操作reduce和fold====================6.reduce()------&gt;聚合========&gt;数据需要同一个类型,空的rdd会报错word = sc.parallelize([('a',1),('a',2),('b',3),('b',1)])word.reduce(lambda x,y:x+y)输出结果('a', 1, 'a', 2, 'b', 3, 'b', 1)7.fold()-------&gt;聚合======&gt;需要给定初始值,且空的rdd不会报错word = sc.parallelize([('a',1),('a',2),('b',3),('b',1)])word.fold((),lambda x,y:x+y) #加法0,乘法1等输出结果:('a', 1, 'a', 2, 'b', 3, 'b', 1) 8.foreach()-----&gt;把函数应用到所有元素上======&gt;不允许对rdd进行操作word = sc.parallelize([('a',1),('a',2),('b',3),('b',1)])word.foreach(lambda x:print(x)) #加法0,乘法1等输出结果:('a', 1)('b', 1)('b', 3)('a', 2)8.1 foreachPartition() 按照分区执行foreacha = sc.parallelize([1,2,3,4,5,6],2)9.aggregate() ------&gt;二次聚合,分成两步执行,第一次是分区执行,第二次是对上一步结果再聚合求均值(函数解释详见https://blog.csdn.net/qingyang0320/article/details/51603243)nums = sc.parallelize([1,2,3,4,5,21],2)nums.aggregate((0,0),lambda x,y:(x[0]+y,x[1]+1),lambda x,y:(x[0]+y[0],x[1]+y[1])) 得到(36,6)10. aggregateByKey()------&gt;也是进行二次聚合,第一次是分区计算(相同key的),第二次是对不同分区处理后的结果进行相同key聚合处理b = sc.parallelize([(1,2),(1,3),(2,3),(3,4),(1,4),(2,7),(3,6)],2) 分成了两个区b.aggregateByKey(0,lambda x,y:max(x,y),lambda x,y:x+y).collect()解释:各个分区相同key的先取出各个分区值的最大值,然后不同分区再把结果值相加得到结果:[(2, 10), (1, 7), (3, 6)] 键值对Rdd的行动操作1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768691.keys()-----&gt;获取键per_info = sc.parallelize([('city','hefei'),('age',25),('name','kuailiang')])per_info.keys().collect()输出结果:['city', 'age', 'name'] 2.values()-----&gt;获取值per_info = sc.parallelize([('city','hefei'),('age',25),('name','kuailiang')])per_info.values().collect()输出结果:['hefei', 25, 'kuailiang']3.keyBy() -----&gt;构建pairRDD类型的数据=======&gt;指定元组中的指定索引数据作为键,原来的援助作为值构成新的pairRDDper_info = sc.parallelize([('city','hefei'),('age',25),('name','kuailiang')])per_info.keyBy(lambda x:x[0]).collect()输出结果:[('city', ('city', 'hefei')), ('age', ('age', 25)), ('name', ('name', 'kuailiang'))]4.mapValues()----&gt;对pairRdd的值统一处理==========&gt;V大写,相当于普通rdd的mapper_info = sc.parallelize([('city','hefei'),('age',25),('name','kuailiang')])per_info.mapValues(lambda x:str(x)+'1').collect()输出结果:[('city', 'hefei1'), ('age', '251'), ('name', 'kuailiang1')]5.flatMapValues()----&gt;对pairRdd的值统一处理,并拍平======&gt;先把键对应的值拍开然后mapvalues==========&gt;就是先对k,v中的value使用函数处理,然后相同的键按照value拆开,value通常为列表y = sc.parallelize([('a', [1, 2, 3])])y.flatMapValues(lambda x:[i**2 for i in x]).collect()输出结果:[('a', 1), ('a', 4), ('a', 9)]6.groupByKey()-----&gt;按照键进行聚合=========&gt;聚合后返回的是一个迭代器===&gt;推荐使用reduceByKey和foldByKeyper_info = sc.parallelize([('city','hefei'),('age',25),('name','kuailiang'),('city','beijing'),('age',28)])per_info.groupByKey().mapValues(lambda x:list(x)).collect()输出结果:[('name', ['kuailiang']), ('age', [25, 28]), ('city', ['hefei', 'beijing'])]7.reduceByKey()------&gt;按照键聚合===========&gt;和reduce一样是一个一个处理的per_info = sc.parallelize([('city','hefei'),('age',25),('name','kuailiang'),('city','beijing'),('age',28)])per_info.reduceByKey(lambda x,y:x + [y] if type(x) == list else [x,y]).collect()输出结果:[('name', 'kuailiang'), ('age', [25, 28, 30]), ('city', ['hefei', 'beijing'])]8.foldByKey()--------&gt;按照键聚合======&gt;需要给定初始值per_info = sc.parallelize([('age',25),('name',11),('age',28),('name',22)])per_info.foldByKey(0,lambda x,y:x+y).collect()输出结果:[('name', 33), ('age', 53)] 9.sortByKey()------&gt;按照键排序per_info = sc.parallelize([('age',25),('name',11),('age',28),('name',22)])per_info.sortByKey(0,lambda x,y:x+y).collect()输出结果:[('age', 25), ('age', 28), ('name', 11), ('name', 22)]10.collectAsMap()------&gt;将pairrdd输出为字典m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap() ,相同键value为最后一次出现的值11.reduceByKey()------&gt;按照key聚合处理,总共三个参数,第一参数表示当遍历到的键从未出现过的时候执行第一个函数,当出现过的时候执行第二个参数,第三个参数表示对相同键的不同分区的结果进行聚合x = sc.parallelize([("a", 1), ("b", 1), ("a", 2)],2)x.combineByKey(lambda x:[x],lambda x,y:x.append(y),lambda x,y:x+y).collect()12. 对pair rdd的各种join操作①join:内连接,共有的键才会连接②leftOuterJoin③rightOuterJoin④fullOuterJoin⑤union: 合并两个rdd 数值运算1234567891011121314151617181920212223242526272829303132333435363738394041421.min()------&gt;返回符合要求的最小值test1 = sc.parallelize([1,2,4,56,123,123,23,5])test1.min(lambda x:1/x)输出结果:1232.max() ------&gt;返回最大值test1 = sc.parallelize([1,2,4,56,123,123,23,5])test1.max(lambda x:x//2)输出结果:1233.mean()------&gt;返回均值test1 = sc.parallelize([1,2,4,56,123,123,23,5])test1.mean()输出结果:42.1254.sum()---------&gt;求和test1 = sc.parallelize([1,2,4,56,123,123,23,5])test1.sum()输出结果:3375.stdev()-------&gt;求标准差test1 = sc.parallelize([1,2,4,56,123,123,23,5])test1.stdev()输出结果:49.715283112942246.variance()----&gt;方差test1 = sc.parallelize([1,2,4,56,123,123,23,5])test1.variance()输出结果:2471.6093757.stats()------&gt;所有的结构,如min,max,sum等等test1 = sc.parallelize([1,2,4,56,123,123,23,5])test1.stats()输出结果:(count: 8, mean: 42.125, stdev: 49.71528311294224, max: 123.0, min: 1.0)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyspark遇到的问题及解决]]></title>
    <url>%2F2021%2F02%2F01%2Fpyspark%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%2F</url>
    <content type="text"><![CDATA[1.当写代码的时候会出现: java.lang.OutOfMemoryError: Java heap space 12解决方法: 配置文件里的spark.driver.memory改大一点 https://blog.csdn.net/Alien_lily/article/details/82018231 2.Caused by: java.sql.SQLException: GC overhead limit exceeded 123需要调大参数:export SPARK_EXECUTOR_MEMORY=6000Mexport SPARK_DRIVER_MEMORY=7000M 3.apply无法使用 1降低java的jdk版本,测试是jdk15和11都不行,只有8可以 4.bigger than spark.driver.maxResultSize (1024.0 MiB) 12 5.驱动不起作用 1pyspark --jars /usr/hdp/3.0.1.0-187/spark2/jars/postgresql-42.2.5.jar 6.java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.(long, int) not avail 123pyspark --conf spark.driver.extraJavaOptions=&apos;-Dio.netty.tryReflectionSetAccessible=true&apos;次要:spark.executor.extraJavaOptions=&apos;-Dio.netty.tryReflectionSetAccessible=true&apos;]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyspark_dataframe]]></title>
    <url>%2F2021%2F01%2F27%2Fpyspark-dataframe%2F</url>
    <content type="text"><![CDATA[生成dataframe12345678910111213141516171819Pandas和Spark的DataFrame两者互相转换： pandas_df = spark_df.toPandas() ---&gt;也会将所有数据收集到驱动器,容易造成memory error spark_df = sqlContext.createDataFrame(pandas_df)与RDD之间的相互转换 rdd_df = df.rdd rdd_df.toDF()1.rdd.toDF()####空DFschema = StructType([ StructField("列名1", StringType(), True), StructField("列名2", StringType(), True), StructField("列名3", StringType(), True), StructField("列名4", StringType(), True) ])df_new = spark.createDataFrame(spark.sparkContext.emptyRDD(),schema)df = spark.createDataFrame([(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],("id", "v")) 1df.collect(),会将所有程序的数据收集到驱动上,如果数据集过大会出现memory error --&gt;所以尽量少用,推荐take,tail 打印数据12df.show() #默认20行df.show(30) 查看前几行12df.head(3)df.take(3) 查看dataframe的总行数1df.count() 查看列名1df.columns 重新设置列名12df.select(df.age.alias('age_value'),'name')df.withColumnRenamed(”原列名“,"新列名") 去重12df.drop_duplicates(['app_text'])df.select('app_texts').distinct() 随机抽样1df.sample(False,0.5,0) 选取列12345df.agedf['age']df.select('age')df.select(df['age'])df.select(df.age,df.name) #选取多列 排序12df.orderBy('group_name',ascending=False) 按指定字段升序df.sort('age',ascending=True) 按条件筛选1234from pyspark.sql import functionsdf.select(df.name,functions.when(df.age &gt; 4,1).when(df.age&lt;3,-1).otherwise(0))df.select(df.name, df.age.between(2, 4)) 过滤数据=两者等价12df.filter(df.age&gt;21) df.where(df.age&gt;21) 数据分割123456789101112https://blog.csdn.net/intersting/article/details/84500978df = spark.createDataFrame([ (1, 144.5,'5.9 2032', 33, 'M'), (2, 167.2, '5.4 2012', 45, 'M'), (3, 124.1, '5.2 2013', 23, 'F'), (4, 144.5, '5.9 2014', 33, 'M'), (5, 133.2, '5.7 2015', 54, 'F'), (3, 124.1, '5.2 2011', 23, 'F'), (5, 129.2, '5.3 2010', 42, 'M'), ], ['id', 'weight', 'height', 'age', 'gender'])df = df.withColumn("s", split(df['height'], " ")).show()===&gt;如果列已存在新数据会替换原来的列==========df.withColumn类似于形成一个新的列,但是参数必须是column...==== 正则表达式匹配列名colRegex12df = spark.createDataFrame([("a", 1), ("b", 2), ("c", 3)], ["Col1", "Col2"])df.select(df.colRegex("`(Col)?.+`")).show() collect12df.collect()#返回列表形式的一个个Row对象 相关性1df_as2.corr('v1','v2',method='pearson') 协方差1df.cov('a','b') 列表中取出123https://blog.csdn.net/intersting/article/details/84500978df.s.getItem(0)====&gt;已知df某一列每个数据都是列表,每个列表取出第0个元素df.withColumn("ss",df.s.getItem(0)) 一行分成多行===&gt;类似于df的.split().stack()123https://blog.csdn.net/intersting/article/details/84500978from pyspark.sql.functions import explode,splitdf_new.withColumn("res",explode(split(df_new.height,' '))).show() 保留位数12from pyspark.sql.functions import broundbround(df_join.count1/df_join[&apos;count&apos;],4) 列数据合并12341.不添加分隔符df_new.withColumn('concat_res',cancat(df_new.gender,df_new.age))2.添加分隔符df_new.withColumn("concat_res",concat_ws(' ',df_new.gender,df_new.age)) 把一列的所有行合并123456from pyspark.sql.functions import collect_listdf = spark.createDataFrame([(&apos;abcd&apos;,&apos;123&apos;),(&apos;xyz&apos;,&apos;123&apos;)], [&apos;s&apos;, &apos;d&apos;])df.show()df.groupBy(&quot;d&quot;).agg(collect_list(&apos;s&apos;).alias(&apos;newcol&apos;)).show() 多行转多列123456789df=spark.sparkContext.parallelize([[15,399,2], \ [15,1401,5], \ [15,1608,4], \ [15,20,4], \ [18,100,3], \ [18,1401,3], \ [18,399,1]])\ .toDF(["userID","movieID","rating"])df.groupby('userID').pivot('movieID').sum('rating').na.fill(-1) 删除列123df.drop(&apos;age&apos;)df.drop(df.age)df = df.na.drop() # 扔掉任何列包含na的行 列截取字符串1df.name.substr(1,2) #####创建column 12lit(2)df.withColumn(&apos;xx&apos;,lit(0)) ====&gt;创造一列全是0 列操作withColumn1df.withColumn('xx',df.xx.cast("Int")) #修改列的类型 合并两个df==join12df_left.join(df_right,df_left.key = df_right.key,&quot;inner&quot;) df.join(df4, [&apos;name&apos;, &apos;age&apos;]).select(df.name, df.age).collect() DF上下拼接12df1.unionALL(df2) #不删除重复数据df1.union(df2) #会删除重复数据 查看数据类型1df.dtypes groupBy===groupby123456df.groupBy(&quot;userID&quot;).avg(&quot;movieID&quot;).show()#应用多个函数df.groupBy(&quot;userID&quot;).agg(functions.avg(&quot;movieID&quot;), functions.min(&quot;rating&quot;),).show()###apply函数apply和applyInPandas名字不同意义相同....用的是pyspark.sql.functions.pandas_udf() 查询空值1234from pyspark.sql.functions import isnullfrom pyspark.sql.functions import isnandf.filter(isnull(&quot;userID&quot;)) #筛选空值的行df.where(isnan(&quot;userID&quot;)) #筛选空值的行 转json内容12sql_context = SQLContext(sc)sql_context.read.json(df.rdd.map(lambda r: r.json)) col1df_as1.join(df_as2, col(&quot;df_as1.name&quot;) == col(&quot;df_as2.name&quot;), &apos;inner&apos;) 包含1df.json.contains('a') 模糊匹配1df.filter(df.gender.like(&apos;%M&apos;)).show()----&gt;sql的模糊匹配 以什么..开始,以什么..结束12df.filter(df.gender.startswith('M'))df.filter(df.gender.endswith('M')) 条件筛选12from pyspark.sql import functions as Fdf.select(df.id, F.when(df.age &gt; 34, 1).when(df.age &lt; 34, -1).otherwise(0)).show() 替换12df.replace(1,2).show()df.na.replace(22).show() Row12====使用方法====Row(name=&quot;Alice&quot;, age=11) ====&gt;理解生成一行数据,字段分别为name和age,数据为.. pandas_udf12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970摘自:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.pandas_udf.html#pyspark.sql.functions.pandas_udf#######################################案例1###################################################from pyspark.sql.functions import pandas_udf@pandas_udf('long') #返回结果的数据类型,多参数也可("col1 string, col2 long")def get_add(a:pd.Series)-&gt;pd.Series: return a+1df.withColumn("one_processed", get_add(df["a"])).show()#######################################案例2####################################################import pandas as pdfrom pyspark.sql.functions import pandas_udf@pandas_udf("col1 string, col2 long") #返回的列类型def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -&gt; pd.DataFrame: #传参类型和返回结果类型 s3['col2'] = s1 + s2.str.len() #针对结果是结构体,使用pd.DataFrame return s3df = spark.createDataFrame( [[1, "a string", ("a nested string",)]], "long_col long, string_col string, struct_col struct&lt;col1:string&gt;")df.printSchema()df.select(func("long_col", "string_col", "struct_col")).printSchema()#######################################案例3####################################################@pandas_udf("string")def to_upper(s: pd.Series) -&gt; pd.Series: return s.str.upper()df = spark.createDataFrame([("John Doe",)], ("name",))df.select(to_upper("name")).show()#######################################案例4####################################################@pandas_udf("first string, last string")def split_expand(s: pd.Series) -&gt; pd.DataFrame: return s.str.split(expand=True)df = spark.createDataFrame([("John Doe",)], ("name",))df.select(split_expand("name")).show()#######################################案例5####################################################@pandas_udf("long")def calculate(iterator: Iterator[pd.Series]) -&gt; Iterator[pd.Series]: # Do some expensive initialization with a state state = very_expensive_initialization() for x in iterator: yield calculate_with_state(x, state)df.select(calculate("value")).show()#######################################案例6####################################################from typing import Iterator@pandas_udf("long")def plus_one(iterator: Iterator[pd.Series]) -&gt; Iterator[pd.Series]: for s in iterator: yield s + 1df = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=["v"]))df.select(plus_one(df.v)).show()#######################################案例7####################################################from typing import Iterator, Tuplefrom pyspark.sql.functions import struct, col@pandas_udf("long")def multiply(iterator: Iterator[Tuple[pd.Series, pd.DataFrame]]) -&gt; Iterator[pd.Series]: for s1, df in iterator: yield s1 * df.vdf = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=["v"]))df.withColumn('output', multiply(col("v"), struct(col("v")))).show()#######################################案例8####################################################@pandas_udf("double")def mean_udf(v: pd.Series) -&gt; float: return v.mean()df = spark.createDataFrame( [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], ("id", "v"))df.groupby("id").agg(mean_udf(df['v'])).show() mapInPandas1234567891011121314151617181920212223242526272829303132############################################案例1#######################################def pandas_filter_func(iterator): for pandas_df in iterator: yield pandas_df[pandas_df.a == 1]df.mapInPandas(pandas_filter_func, schema=df.schema).show()############################################案例2#######################################df = spark.createDataFrame([ ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30], ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60], ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])def plus_mean(pandas_df): return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())df.groupby('color').applyInPandas(plus_mean, schema=df.schema).show()############################################案例3#######################################df1 = spark.createDataFrame( [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)], ('time', 'id', 'v1'))df2 = spark.createDataFrame( [(20000101, 1, 'x'), (20000101, 2, 'y')], ('time', 'id', 'v2'))def asof_join(l, r): return pd.merge_asof(l, r, on='time', by='id')df1.groupby('id').cogroup(df2.groupby('id')).applyInPandas( asof_join, schema='time int, id int, v1 double, v2 string').show() UDF123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687from pyspark.sql.types import DoubleTypefrom pyspark.sql.functions import udfdf = spark.createDataFrame([(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],("id", "v"))def normalize(v): return v+1plus_one_udf = udf(normalize, returnType=DoubleType())df.withColumn("one_processed", plus_one_udf(df["v"]))#######################from pyspark.sql import functions as Ffrom pyspark.sql import types as Ta = sc.parallelize([[1, 'a'], [1, 'b'], [1, 'b'], [2, 'c']]).toDF(['id', 'value'])a.groupBy('id').agg(F.collect_list('value').alias('value_list')).show()def find_a(x): """Count 'a's in list.""" output_count = 0 for i in x: if i == 'a': output_count += 1 return output_countfind_a_udf = F.udf(find_a, T.IntegerType())a.groupBy('id').agg(find_a_udf(F.collect_list('value')).alias('a_count')).show()##################from pyspark.sql import functions as Ffrom pyspark.sql import types as Ta = sc.parallelize([[1, 1, 'a'], [1, 2, 'a'], [1, 1, 'b'], [1, 2, 'b'], [2, 1, 'c']]).toDF(['id', 'value1', 'value2'])a.groupBy('id').agg(find_a_udf( F.collect_list(F.when(F.col('value1') == 1, F.col('value2')))).alias('a_count')).show()#################################def sum_func(key, pdf): # key is a tuple of two numpy.int64s, which is the values # of 'id' and 'ceil(df.v / 2)' for the current group return pd.DataFrame([key + (pdf.v.sum(),)])df.groupby(df.id, F.ceil(df.v / 2)).applyInPandas( sum_func, schema="id long, `ceil(v / 2)` long, v double").show()################################from pyspark.sql.functions import pandas_udf, PandasUDFTypedf = spark.createDataFrame( [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], ("id", "v"))@pandas_udf("id long, v double", PandasUDFType.GROUPED_MAP) def normalize(pdf): v = pdf.v return pdf.assign(v=(v - v.mean()) / v.std())df.groupby("id").apply(normalize).show() ################@pandas_udf('group string,year int,app_texts string , app_num int ,high_apps string ,high_num int', PandasUDFType.GROUPED_MAP)def deal_func(df): ls = [] year_early = min(df['app_date']) gp = df['group'].values[0] for year in range(int(year_early), 2022): res_year = df[(df.app_date &lt;= year) &amp; (df.ceased_date &gt;= year)] # 该年有效核心专利 high_value_year = res_year[res_year.xingji &gt;= 4]['app_text'].unique() ls.append([gp, year,str(list(res_year['app_text'].unique())), len(res_year['app_text'].unique()), str(list(high_value_year)), len(high_value_year)]) return pd.DataFrame(ls)df_sql_res2.groupby('group').apply(deal_func).show() #####spark与pandas的dataframe对比,列举 Pandas Spark 行结构 Series结构 Row结构，属于Spark DataFrame结构 列结构 Series结构 Column结构 列名称 不允许重名 允许重名修改列名,采用alias方法 列添加 df[“xx”] = 0 df.withColumn(“xx”, functions.lit(0)).show() 排序 df.sort() df.sort() df.head(2) df.head(2)或者df.take(2) df.tail(2) 过滤 df[df[‘age’]&gt;21] df.filter(df[‘age’]&gt;21) 或者 df.where(df[‘age’]&gt;21) df.groupby(“age”) df.groupby(“A”).avg(“B”) df.groupBy(“age”) df.groupBy(“A”).avg(“B”).show() df.groupBy(“A”).agg(functions.avg(“B”), functions.min(“B”), functions.max(“B”)).show() df.count() 输出每一列的非空行数 df.count() 输出总行数 df.describe() 描述某些列的count, mean, std, min, 25%, 50%, 75%, max df.describe() 描述某些列的count, mean, stddev, min, max 合并 concat 合并 merge 合并 join df.join() 合并 append fillna df.fillna() df.na.fill() dropna df.dropna() df.na.drop() 两者互相转换 pandas_df = spark_df.toPandas() spark_df = sqlContext.createDataFrame(pandas_df) 函数应用 df.apply(f） df.foreach(f) 或者 df.rdd.foreach(f) 将df的每一列应用函数fdf.foreachPartition(f) 或者 df.rdd.foreachPartition(f) 将df的每一块应用函数f #####某一列去重返回列表 12from pyspark.sql import Functions as Fdf.select(F.collect_set(&apos;applicant_name&apos;).alias(&apos;applicant&apos;))).first()[&apos;applicant&apos;] 按行转成字典1data_select4.rdd.map(lambda row: row.asDict(True)====&gt;跟pandas的to_dict(orient=&apos;records&apos;)一致 列总结123456789101112131415161718df.name.startswith(&apos;Al&apos;)df.name.substr(1, 3)df.name.rlike(&apos;ice$&apos;)df.name.like(&apos;Al%&apos;)df.age.isin([1, 2, 3])df.height.isNull()df.height.isNotNull()df.d.getItem(&quot;key&quot;)df.r.getField(&quot;b&quot;)df.name.endswith(&apos;ice$&apos;)df.name.desc()df.name.contains(&apos;o&apos;)df.age.cast(&quot;string&quot;).alias(&apos;ages&apos;)df.age.between(2, 4)df.name.asc()=====&gt;df.select(df.name).orderBy(df.name.asc()).collect()df4.na.fill(50)df4.na.fill(&#123;&apos;age&apos;: 50, &apos;name&apos;: &apos;unknown&apos;&#125;).show()df4.na.replace(10, 20) 二次学习1234567891011121314151617181920212223242526272829303132agg -----&gt;聚合操作df.groupby('a').agg(&#123;"b":"min"&#125;)alias -------&gt;表别名,实际指向的同一块数据df2 = df.alias('df2') 效果等价于 df2 = dfcache------&gt;df放到内存里,效果等价于persistdf.cache()coalesce ------&gt;降低分区,效果等价于rdd 的 coalescedf.coalesce(1).rdd.getNumPartitions()colRegex --------&gt;写正则匹配符合条件的列数据df.select(df.colRegex("`(Col1)?+.+`")).show()collect--------&gt;以列表形式返回所有的数据df.collect()corr() -----&gt;查看两列的相关性,只支持皮尔逊相关性,新版本才有该函数df.corr()##创建视图表createGlobalTempView() //创建全局视图,重名会报错createOrReplaceGlobalTempView() //创建全局视图,重名会替换createReplaceTempView() //创建本地视图表,重名会替换createTempView() //创建本地视图表,重名会报错distinct -----&gt;dataframe整体去重drop_duplicates(等价于dropduplicates,别名)------&gt;按照指定的列去重mapInPandas]]></content>
      <categories>
        <category>pyspark</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2021%2F01%2F25%2Fclickhouse%2F</url>
    <content type="text"><![CDATA[1234567from clickhouse_driver import Clientimport pandas as pdimport reclient = Client(host='192.168.0.246',port='9000',user='user_r', password='1q2w3e4r', database='default')sql = "select * from test02"df = client.execute(sql,columnar=True, with_column_types=True) #####账号和密码12345678910111213141516171819202122192.168.0.246root密码abc.com123popo密码popo.com123clickhouseip:192.168.0.246tcp端口8123http端口9000超管账号：default密码：123456读：user_r密码：1q2w3e4r读写：user_rw密码：1a2s3d4f读写删：user_rwd密码：1z2x3c4v 方法123456789101112def save_clickhouse(data,tb_name): client = Client(host='192.168.0.246',port='9000',user='user_rwd', password='1z2x3c4v', database='default') cols = ','.join(data.columns) data_tup = tuple(map(tuple, data.values)) client.execute(f"DROP TABLE IF EXISTS %s;"%(tb_name)) if data.shape[1]==2: client.execute(f"CREATE TABLE %s ( %s String, num UInt16, create_date Date DEFAULT toDate(now()) ) ENGINE = MergeTree(create_date,(%s), 8192);"%(tb_name,data.columns[0], data.columns[0]) ) else: client.execute(f"CREATE TABLE %s ( %s String, num UInt16, label UInt16, create_date Date DEFAULT toDate(now()) ) ENGINE = MergeTree(create_date,(%s), 8192);"%(tb_name,data.columns[0], data.columns[0]) ) client.execute(f"INSERT INTO &#123;tb_name&#125; (&#123;cols&#125;) VALUES", data_tup, types_check=True) client.disconnect() return 12345678t1 = datetime.now()sql = 'select applicant_name from company_20210208'client = Client(host='192.168.0.246', port='9000', user='user_rwd', password='1z2x3c4v', database='pre_formal_1')df = client.execute(sql)print(datetime.now()-t1)t1 = datetime.now()df = pd.DataFrame(df)print(datetime.now()-t1) SQL语法增12345678tuple_data = [tuple(i) for i in df_indus.values]client = Client(host=&apos;192.168.0.246&apos;, port=&apos;9000&apos;, user=&apos;user_rwd&apos;, password=&apos;1z2x3c4v&apos;, database=&apos;pre_formal_1&apos;)sql_save = &apos;INSERT INTO industry_company_wilson_20210208 (`industryId`,`companys`,`companysee`,`applicants`,`companyee_num`,`applicant_num`,`company_num`,`industryname`) VALUES&apos;client.execute(sql_save,tuple_data,types_check=True)集群方式创建表create table kl on cluster ch_cluster(id Int32,name String)ENGINE = MergeTree() 改123456789101112131415161718192021221.添加新字段alter table test add column name String afer zonealter table test add column name String Default &apos;xiaoming&apos;#实现拼接字符串和表字段SELECT &apos;SELECT * FROM dwd_ptl_info where PORT_ID = \&apos;&apos;||PORT_ID||&apos;\&apos;;&apos; from dwd_ptl_info 2.修改数据类型alter table test modify column name name2 String 注意:修改类型实际调用的是类似toString的方法,如果将字符串转换为浮点数(不兼容),会报错3.添加字段备注alter table test comment column name &apos;姓名&apos;4.删除字段altet table test drop column if exists name5.清空表truncate table test6.重命名rename table algorithm.zl_zu_L_new_20210608 to algorithm.zl_zu_L_20210608 删12 查123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687读全部字段数据 select * from company_20210208读指定字段数据 select applicant_name from company_20210208筛选指定字段的指定数据has:元素/hasAny:交集/hasAll:子集 select app_text,app_date from zl_zu_L_20210208 where has(applicants,&apos;华为技术有限公司&apos;)筛选为空/不为空的数据(empty/notEmpty) select app_text,inventors,ceased_date from zl_zu_L_20210208 where empty(ceased_date)=1 返回数组长度 select app_text,applicants,length(applicants) from zl_zu_L_20210208 limit 10数组拆分一行变多行 select arrayJoin(applicants) as app1 from zl_zu_L_20210208 limit 10对数组内部去重(每一行数据) select applicants,arrayDistinct(applicants) from zl_zu_L_20210208 limit 10将数据转换为数组,按行 select array(applicant_name) from company_20210208 limit 10 select array(1,2,3,4)合并两列数据 select arrayConcat(applicants,patentee_others) from zl_zu_L_20210208 limit 20 SELECT arrayConcat([1, 2], [3, 4], [5, 6]) AS res 结果:[1,2,3,4,5,6]数组中的元素计数 select arrayCount([1,2,1])====&gt;3,同length ,但是arrayCount统计的是非0元素个数 select arrayCount(x-&gt;x=1,[1,2,1])====&gt;2,数组中指定元素的个数 等价于 select countEqual([1,2,1],1)=====&gt;2获取数组中指定索引的元素 select arrayElement(applicants,1) from zl_zu_L_20210208数组内部排序 select arraySort(applicants) from zl_zu_L_20210208展平数组,合并列表成一个(列表嵌套) select arrayFlatten([[1,3,4],[2,3,4,5]])数组中连续出现的数据去重,数组内部数据类型须一致 select arrayCompact([1,1,2,3,2])Python中的zip操作 select arrayZip([&apos;a&apos;,&apos;b&apos;,&apos;c&apos;],[1,2,3])对数据内部数据过滤arrayFill,下面是筛掉空值null select arrayFill(x -&gt; not isNull(x),applicants) from zl_zu_L_20210208 limit 10对数组切片从索引1开始切0个 arraySlice(groupArray(app_text),1,0)截取字符串(从第一个位置开始往后截取2个) substring(x,1,2)数组内元素拼接 arrayStringConcat(array1,&apos;,&apos;)聚合后某个字段转为列表 select patentee,groupArray(app_text) from zl_zu group by patentee ================================高阶函数================== x -&gt; 2 * x, str -&gt; str != Referer对数组每一个元素处理 arrayMap(x -&gt; substring(x,1,2), groupArray(app_text)) 对数组过滤 SELECT arrayFilter(x -&gt; x LIKE &apos;%World%&apos;, [&apos;Hello&apos;, &apos;abc World&apos;]) AS res 拆分数组为多个数组 SELECT arraySplit((x, y) -&gt; y, [1, 2, 3, 4, 5], [1, 0, 1, 1, 0]) AS res返回数组中最小数 select arrayMin([1,2,3])返回数组中最大数 select arrayMax([1,2,3])返回数组的求和 select arraySum([1,2,3])返回数组均值 select arrayAvg([1,2,3])对数组聚合 select arrayReduce(&apos;max&apos;,[1,2,3])公募基金的底层持仓是不是在持仓表字符串条件判断处理multiIf(grant_date=&apos;&apos;,&apos;999999&apos;,grant_date)合并字符串concatconcat(a,b,c,&apos;,&apos;,toString(d))将字符串转换成数组select applicant_name,splitByString(&apos;,&apos;,`groups`) from algorithm_dis.patent_num_L_20210608按照两列explode SELECT s, `nest.x`, `nest.y` FROM nested_test ARRAY JOIN nest;SELECT s, `nest.x`, `nest.y` FROM nested_test ARRAY JOIN `nest.x`, `nest.y`;其他方法:https://blog.csdn.net/u012111465/article/details/85250030①DECIMAL做除法运算的时候,分子的精度不能小于分母的精度创建分布式表:sql = &quot;CREATE TABLE IF NOT EXISTS algorithm_local.`&#123;&#125;` on cluster cluster_3shards_0replicas (`app_text` String, `pub_text` String,`app_date` Nullable(Date), `pub_date` Nullable(Date), `title` String, `inventors` Array(String), `applicants` Array(String),`patentee_others` Array(String), `ipc` Array(String),`group_name` Array(String),`grant_date` String, `ceased_date` String,`patent_type` String,`current_status` String,`main_family` Array(String),`zl_num_zong` Int64,`app_num_zong` Int64, `count_num` Int64,`paiming` String,`average` Float64,`sm` Float64,`xingji` Int8,`fwdcits` Array(String), `create_date` Date DEFAULT toDate(now())) ENGINE = ReplicatedReplacingMergeTree(&apos;/clickhouse/tables/ch170/&#123;&#123;shard&#125;&#125;/&#123;&#125;&apos;,&apos;&#123;&#123;replica&#125;&#125;&apos;) PRIMARY KEY app_text ORDER BY app_text SETTINGS index_granularity = 8192&quot;.format(zl_zu_L,zl_zu_L)client.execute(sql)sql = &quot;CREATE TABLE IF NOT EXISTS algorithm_dis.`&#123;&#125;` (`app_text` String, `pub_text` String,`app_date` Nullable(Date), `pub_date` Nullable(Date), `title` String, `inventors` Array(String), `applicants` Array(String),`patentee_others` Array(String), `ipc` Array(String),`group_name` Array(String),`grant_date` String, `ceased_date` String,`patent_type` String,`current_status` String,`main_family` Array(String),`zl_num_zong` Int64,`app_num_zong` Int64, `count_num` Int64,`paiming` String,`average` Float64,`sm` Float64,`xingji` Int8,`fwdcits` Array(String), `create_date` Date DEFAULT toDate(now())) ENGINE = Distributed(&apos;cluster_3shards_0replicas&apos;,&apos;algorithm_local&apos;,&apos;&#123;&#125;&apos;,halfMD5(app_text))&quot;.format(zl_zu_L,zl_zu_L)client.execute(sql) #####dataframe写入clickhouse123456789101112131415161718192021222324252627282930313233343536========================================1===================================from clickhouse_driver import Clientclient = Client('localhost')df = pandas.DataFrame.from_records([ &#123;'year': 1994, 'first_name': 'Vova'&#125;, &#123;'year': 1995, 'first_name': 'Anja'&#125;, &#123;'year': 1996, 'first_name': 'Vasja'&#125;, &#123;'year': 1997, 'first_name': 'Petja'&#125;,])# df processing blablabla...client.execute("INSERT INTO your_table VALUES", df.to_dict('records'))========================================2===================================import pandahouse as phpdf = pd.DataFrame.from_records([ &#123;'year': '1994', 'first_name': 'Vova'&#125;, &#123;'year': '1995', 'first_name': 'Anja'&#125;, &#123;'year': '1996', 'first_name': 'Vasja'&#125;, &#123;'year': '1997', 'first_name': 'Petja'&#125;,])connection = dict(database='pre_formal_1', host='http://192.168.0.246:8123', user='user_rwd', password='1z2x3c4v', )ph.to_clickhouse(pdf, 'test_humans2', index=False, chunksize=100000, connection=connection)======================================3======================================client = Client(host='192.168.0.170', port='9000', user='algorithm', password='1a2s3d4f', database='algorithm_dis',settings=&#123;'use_numpy': True&#125;)sql_insert = 'insert into industryid_name_20211208 values'client.insert_dataframe(sql_insert,df) 12分区键:通常按照日期,存在不同的文件.我们这用不到,当版本数据存在一个表的时候可以用到分片:sharding,数据存储在不同的节点,那个distributed那种 ##client.query_dataframe(sql) 读成dataframe ####写dataframe到ch 1234client = Client(host=&apos;192.168.0.170&apos;, port=&apos;9000&apos;, user=&apos;algorithm&apos;, password=&apos;1a2s3d4f&apos;, database=&apos;algorithm_dis&apos;,settings=&#123;&apos;use_numpy&apos;: True&#125;)df[&apos;create_date&apos;] = &apos;20220111&apos;sql_insert = &apos;insert into industryid_name_20211208 values&apos;client.insert_dataframe(sql_insert,df) 123systemctl start clickhouse-serversystemctl stop clickhouse-serversystemctl status clickhouse-server https://github.com/mymarilyn/clickhouse-driver]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark视频笔记]]></title>
    <url>%2F2021%2F01%2F12%2Fspark%E8%A7%86%E9%A2%91%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233宽依赖:一对多,必定会发生shuffle操作窄依赖:一对一一个DAG就是一个job,一个job分层很多个stage,stage划分的依据是宽依赖,task是由分区数决定的一个rdd分层三个区则对应三个task每个阶段的task的数量是最后一个rdd的分区数rdd转换操作和行动操作(也称为算子)转换操作: 分为单value、双value和key-value的形式 单value： map、mapPartitions、mapPartitionsWithIndex、flatMap、glom、groupBy、filter、sample、distinct、coalesce、repartition、sortBy、mapvalues 双value： intersection、union、substract、zip key-value： partitionBy、reduceByKey、groupByKey、aggregateByKey、foldByKey、combineByKey、sortByKey、join、leftOuterJoin、cogroup、groupbyKey性能差于reducebyKey,因为reduce先分区操作了,然后再shuffle操作后,将分区间的数据合并再操作,这个两个算子shuffle过程都是写入文件的,防止数据量导致内存溢出,然后reducebykey的io数据量远远小于groupbykey,但是根据场景的不同可能存在并不需要聚合的情况,这会儿groupbykey就可以使用到了针对reducebykey区内和区间的操作都一样,当区内区外不一样的时候可以使用aggregrative,实现自定义持久化的三种方法cache、persist和checkpoint区别，chechpoint存在磁盘重写分区器累加器和broadcast[只写和只读]: 累加器的原理:分executor执行,然后driver在执行.executor之间互不影响,Accoumolotor支持自定义,累加器本身不执行,当遇到行动算子的时候才会执行,因而推荐放到行动算子里面,否则可能会多次执行: brocast用于闭包变量共享,否则多少线程数就需要复制多少份数据,现在是通过executor划分的,每个executor一份数据:广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个 Spark 操作使用framework框架:工程化代码,整体分为applciantion,controller,service,Dao四部分,再详细分为application: 控制层: 主要用于调度。service: 服务层: 主要用于执行逻辑。dao: 持久层: 主要用于数据交互。application: 应用层: 所有的应用程序从 application 开始启动。common: 用于存放共通类。util: 用于存放工具类。bean: 用于存放实体类。SCALA类里面main方法可以手写也可以继承自Appscala的case用法不会:map&#123;&#125;写法 windows spark 配置1https://blog.csdn.net/l_dsj/article/details/109468288]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2021%2F01%2F11%2Fspark%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[直接用pyspark 的配置1234567891.vim ~/.bashrc2.spark_home补充上 SPARK_HOME=/home/kuailiang/spark/spark-3.0.1-bin-hadoop3.23.export PATH=$PATH:$SPARK_HOME/bin4.export PYSPARK_PYTHON=python3source ~/.bashrc3.spark-env.sh-----&gt;conf下4.export PYSPARK_PYTHON=/usr/bin/python3 运行代码1234spark-submit new.pyspark-submit --master spark://192.168.0.217:7077 pi.py 2000spark-submit wordcount.py file:///home/tst #运行本地文件./bin/spark-submit examples/src/main/python/pi.py 基本须知1234561.spark程序必须做的第一件事就是创建一个sparkcontext对象(Spark如何访问集群)2.数据还要使用则lineLengths.persist()export PATHexport JAVA_HOME=/home/kuailiang/2020/java/jdk-15.0.1export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar RDD操作123456789101112131415161718192021转换： map filter flatmap sample groupByKey reduceByKey union join cogroup crossProduct mapValues sort partitionBy操作： count collect reduce lookup save在转换方法中的函数执行完后生成的还是一个RDD结构 自带案例11234textFile = spark.read.text("README.md")textFile.count()textFile.first()linesWithSpark = textFile.filter(textFile.value.contains("Spark")) 查找包含最多单词的行123456from pyspark.sql.functions import *textFile.select(size(split(textFile.value,"\s+")).name("numWords")).agg(max(col("numWords"))).collect()wordCounts = textFile.select(explode(split(textFile.value,"\s+")).alias("word")).groupBy("word").count()wordCounts.collect() spark操作mysql12345678910111213141516171819202122####注意对应版本的jar包要放在jars文件夹下https://mvnrepository.com/artifact/mysql/mysql-connector-javaclickhouse包 https://mvnrepository.com/artifact/ru.yandex.clickhouse/clickhouse-jdbc/0.2.4============================读数据==================jdbcDF = spark.read.format("jdbc").\ option("url", "jdbc:mysql://192.168.0.251:3306/pre_formal_2").\ option("driver","com.mysql.jdbc.Driver").\ option("dbtable", "ipc_split_10000_c_list_v3").\ option("user", "user_rw").\ option("password", "1a2s3d4f").load()####读指定字段 industry_coms = spark.read.format("jdbc").\ option("url", "jdbc:mysql://192.168.0.251:3306/pre_formal_1").\ option("driver","com.mysql.jdbc.Driver").\ option("dbtable", "(select industryId,companyee_num from industry_company_wilson_20210112) t").\ option("user", "user_rw").\ option("password", "1a2s3d4f").load()#返回的是dataframejdbcDF.show()============================存数据=================== mysql_url = "jdbc:mysql://192.168.0.251:3306/pre_formal_2?user=user_rw&amp;password=1a2s3d4f" mysql_table = "people" jdbcDF.write.mode("append").jdbc(mysql_url, mysql_table) spark操作clickhouse123456companys = spark.read.format("jdbc"). \ option("url", "jdbc:clickhouse://192.168.0.246:8123/pre_formal_1"). \ option("driver", "ru.yandex.clickhouse.ClickHouseDriver"). \ option("dbtable", f"(select applicant_name as applicant_other from company_20210208) t"). \ option("user", "default"). \ option("password", "123456").load().cache() spark存成不同格式(csv,json,text,parquet)1234567jdbcDF.select(&quot;names&quot;).write.text(&quot;/root/mimo/people_text&quot;)jdbcDF.write.csv(&quot;/root/mimo/people_text/people_csv&quot;, sep=&apos;:&apos;)jdbcDF.write.json(&quot;/root/mimo/people_text/people_json&quot;, mode=&apos;overwrite&apos;)peopledf.write.parquet(&quot;/root/mimo/people_text/people_parquet&quot;, mode=&apos;append&apos;) Spark包12pyspark.SparkContext:SparkContext表示与Spark集群的连接，可用于RDD在该集群上创建和广播变量 pyspark.sql统计成绩案例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#studentExample 例子 练习def map_func(x): s = x.split() return (s[0], [int(s[1]),int(s[2]),int(s[3])]) #返回为（key,vaklue）格式，其中key:x[0],value:x[1]且为有三个元素的列表#return (s[0],[int(s[1],s[2],s[3])]) #注意此用法不合法def has100(x): for y in x: if(y == 100): #把x、y理解为 x轴、y轴 return True return Falsedef allis0(x): if(type(x)==list and sum(x) == 0): #类型为list且总分为0 者为true；其中type(x)==list :判断类型是否相同 return True return Falsedef subMax(x,y): m = [x[1][i] if(x[1][i] &gt; y[1][i]) else y[1][i] for i in range(3)] return('Maximum subject score', m)def sumSub(x,y): n = [x[1][i]+y[1][i] for i in range(3)] #或者 n = ([x[1][0]+y[1][0],x[1][1]+y[1][0],x[1][2]+y[1][2]]) return('Total subject score', n)def sumPer(x): return (x[0],sum(x[1]))#停止之前的SparkContext，不然重新运行或者创建工作会失败；另外，只有 sc.stop()也可以，但是首次运行会有误try: sc.stop()except: passfrom pyspark import SparkContext #导入模块sc=SparkContext(appName='Student') #命名lines=sc.textFile("/home/kuailiang/spark/code/dtudent.txt").map(lambda x:map_func(x)).cache() #导入数据且保持在内存中，其中cache()：数据保持在内存中count=lines.count() #对RDD中的数据个数进行计数；其中，RDD一行为一个数据集#RDD'转换'运算 （筛选 关键字filter）whohas100 = lines.filter(lambda x: has100(x[1])).collect() #注意：处理的是value列表，也就是x[1]whois0 = lines.filter(lambda x: allis0(x[1])).collect()sumScore = lines.map(lambda x: (x[0],sum(x[1]))).collect()#‘动作’运算maxScore = max(sumScore,key=lambda x: x[1]) #总分最高者minScore = min(sumScore,key=lambda x: x[1]) #总分最低者sumSubScore = lines.reduce(lambda x,y: sumSub(x,y))avgScore = [x/count for x in sumSubScore[1]]#单科成绩平均值#RDD key-value‘转换’运算subM = lines.reduce(lambda x,y: subMax(x,y))redByK = lines.reduceByKey(lambda x,y: [x[i]+y[i] for i in range(3)]).collect() #合并key相同的value值x[0]+y[0],x[1]+y[1],x[2]+y[2]#RDD'转换'运算sumPerSore = lines.map(lambda x: sumPer(x)).collect() #每个人的总分 #sumSore = lines.map(lambda x: (x[0],sum(x[1]))).collect()sorted = lines.sortBy(lambda x: sum(x[1])) #总成绩低到高的学生成绩排序sortedWithRank = sorted.zipWithIndex().collect()#按总分排序first3 = sorted.takeOrdered(3,key=lambda x:-sum(x[1])) #总分前三者#限定以空格的形式输出到文件中first3RDD = sc.parallelize(first3)\.map(lambda x:str(x[0])+' '+str(x[1][0])+' '+str(x[1][1])+' '+str(x[1][2])).saveAsTextFile("result")#print(lines.collect())print("数据集个数（行）:",count)print("单科满分者：",whohas100)print("单科零分者:",whois0)print("单科最高分者：",subM)print("单科总分：",sumSubScore)print("合并名字相同的分数：",redByK)print("总分/（人）",sumPerSore)print("最高总分者：",maxScore)print("最低总分者：",minScore)print("每科平均成绩：",avgScore)# print("总分倒序：",sortedWithRank)print("总分前三者：",first3)print(first3RDD)sc.stop() saprk sql和dataframe123456789101112131415161718192021222324252627282930313233df.show() #展示数据df.select("name").show() #挑选指定的列df.select(df['name'], df['age'] + 1).show() #展示name和age字段并将age字段+1df.filter(df['age'] &gt; 21).show() #过滤,展示age字段大于21的数据df.groupBy("age").count().show()import pyspark.sql.functions as funcdf.groupBy("department").agg(df["department"], func.max("age"), func.sum("expense"))df.groupBy("department").agg(func.max("age"), func.sum("expense"))sqlContext.setConf("spark.sql.retainGroupColumns", "false")from pyspark.sql import Rowsc = spark.sparkContext# Load a text file and convert each line to a Row.lines = sc.textFile("examples/src/main/resources/people.txt")parts = lines.map(lambda l: l.split(","))people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))# Infer the schema, and register the DataFrame as a table.schemaPeople = spark.createDataFrame(people) #创建dataframeschemaPeople.createOrReplaceTempView("people")# SQL can be run over DataFrames that have been registered as a table.teenagers = spark.sql("SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19")# The results of SQL queries are Dataframe objects.# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.teenNames = teenagers.rdd.map(lambda p: "Name: " + p.name).collect()for name in teenNames: print(name)Spark SQL可以将Row对象的RDD转换为DataFrame，从而推断数据类型。通过将键/值对列表作为kwargs传递给Row类来构造行。该列表的键定义表的列名，并且通过对整个数据集进行采样来推断类型，类似于对JSON文件执行的推断。 createDataFrame123456789101112131415161718192021222324252627282930313233343536373839404142DataFrame从RDD，列表或创建一个pandas.DataFrame。from pyspark.sql.types import Rowdef f(x): rel = &#123;&#125; rel['name'] = x[0] rel['age'] = x[1] return relpeopleDF = sc.textFile("examples/src/main/resources/people.txt").map(lambda line : line.split(',')).map(lambda x: Row(**f(x))).toDF()peopleDF.createOrReplaceTempView("people") #必须注册为临时表才能供下面的查询使用personsDF = spark.sql("select * from people") #########创建DataFrame########=====1l = [('Alice', 1)]spark.createDataFrame(l, ['name', 'age']).collect()=====2d = [&#123;'name': 'Alice', 'age': 1&#125;]spark.createDataFrame(d).collect()=====3l = [('Alice', 1)]rdd = sc.parallelize(l)spark.createDataFrame(rdd).collect()spark.createDataFrame(rdd, ['name', 'age']).collect()=====4from pyspark.sql import Rowl = [('Alice', 1)]rdd = sc.parallelize(l)Person = Row('name', 'age')person = rdd.map(lambda r: Person(*r))spark.createDataFrame(person).collect()=====5from pyspark.sql.types import *schema = StructType([ StructField("name", StringType(), True), StructField("age", IntegerType(), True)])spark.createDataFrame(rdd, schema).collect()=====6spark.createDataFrame(df.toPandas()).collect() #将 pandas的dataframe转换成spark的spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()=====7spark.createDataFrame(rdd, "a: string, b: int").collect() #####DataFrame操作 123456#按照一列进行排序df.sort(df.age.desc()).show()#多列排序df.sort(df.age.desc(), df.name.asc()).show()#对列进行重命名df.select(df.name.alias("username"),df.age).show() SparkConf====&gt;资源控制,配置spark12345678910111213141516171819202122232425262728293031sc_conf = SparkConf()sc_conf.setMaster('spark://192.168.0.217:7077')sc_conf.setAppName('my-app')sc_conf.set('spark.executor.memory', '60g') #executor memory是每个节点上占用的内存。每一个节点可使用内存sc_conf.set("spark.executor.cores", '4') #spark.executor.cores：顾名思义这个参数是用来指定executor的cpu内核个数，分配更多的内核意味着executor并发能力越强，能够同时执行更多的tasksc_conf.set('spark.cores.max', 40) #spark.cores.max：为一个application分配的最大cpu核心数，如果没有设置这个值默认为spark.deploy.defaultCoressc_conf.set('spark.logConf', True) #当SparkContext启动时，将有效的SparkConf记录为INFO。print(sc_conf.getAll())=============================大类下的方法======sc_conf.contains('spark.executor.memory') ====&gt;返回True和Falsesc_conf.getAll() ====&gt;获取配置信息sc_conf.set('spark.executor.memory', '2g') ======&gt;设置spark的配置sc_conf.setAll([('spark.executor.memory', '2g'),('spark.cores.max', 40)]) ===&gt;一次设置多个配置sc_conf.setMaster() 设置连接的主URL动态资源分配:============还不懂==================&gt;反正很重要def conf(self): conf = super(TbtestStatisBase, self).conf conf.update(&#123; 'spark.shuffle.service.enabled': 'true', 'spark.dynamicAllocation.enabled': 'false', 'spark.dynamicAllocation.initialExecutors': 50, 'spark.dynamicAllocation.minExecutors': 1, 'spark.dynamicAllocation.maxExecutors': 125, 'spark.sql.parquet.compression.codec': 'snappy', 'spark.yarn.executor.memoryOverhead': 4096, "spark.speculation": 'true', 'spark.kryoserializer.buffer.max': '512m', &#125;) SparkContext123SparkContext每个JVM仅应激活一个。在创建新 的活动目录之前，必须先停止活动目录getOrCreate:获取或实例化SparkContext并将其注册为单例对象glom()返回通过将每个分区内的所有元素合并到列表中而创建的RDD #####其他方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374parallelize([1,2,3],3) 分发本地Python集合以形成RDDpickleFile() 使用saveAsPickleFile()保存的RDDsc.runJob(sc.parallelize(range(6), 3), lambda part: [x * x for x in part], [0, 2], True) 对指定的分区进行指定的操作,未指定分区则在全部分区执行sc.sparkUser() 获取正在使用spark_context的用户sc.stop() 关闭sparkcontextsc.TextFile() 从对于路径读取文件sc.union([rdd1,rdd2]) 建立rdd列表的并集rdd1.intersection(rdd2) 建立rdd的交集rdd1.subtract(rdd2) 建立rdd的差集rdd1.distinct() RDD去重rdd1.takeOrdered(3,key=lambda x:-x) 从大到小排序,从小到大不用lambdardd1.randomsplit([0.4,0.6]) rdd等比例分割sc.parallelize([100, 200, 300, 400, 500], 5).aggregate((1, 1), seqOp1, combOp1)rdd1.cache() rdd放到内存中rdd1.cartesian(rdd2) 计算两个rdd的笛卡尔乘积rdd.getNumPartitions() 获取分区数量sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect() 减少rdd的分区rdd1.collect() 返回rdd的所有元素列表sorted(sc.parallelize([("a", 1), ("b", 4)]).cogroup(sc.parallelize([("a", 2)])).collect()) 返回一个元组,其中包含key的所有值的列表(类似按照键聚合)sc.parallelize([(1, 2), (3, 4)]).collectAsMap() 构建字典,数据较小时(放到内存中)sc.parallelize([2, 3, 4]).count()sc.parallelize([("a", 1), ("b", 1), ("a", 1)]).countByKey().items() #计算每个键的元素数sc.parallelize([1, 2, 1, 2, 2]).countByValue().items() #返回值的计数sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect()) #返回去重后的RDDsc.parallelize([1, 2, 3, 4, 5]).filter(lambda x: x % 2 == 0).collect() #筛选符合条件的构建新的rddsc.parallelize([2, 3, 4]).first() #返回rdd的第一条数据,空的rdd会报错sorted(sc.parallelize([2, 3, 4]).flatMap(lambda x: range(1, x)).collect()) #对rdd的每一个元素处理,然后将结果展平sc.parallelize([]).isEmpty() #判断RDD是否为空sc.parallelize(range(0,3)).keyBy(lambda x: x*x).collect() 创建元组的键[(0, 0), (1, 1), (4, 2)]sc.parallelize([(1, 2), (3, 4)]).keys() 返回rdd的键rdd1.max() 返回最大值,存在参数keyrdd1.is_cached 判断是否是缓存sc.broadcast(array([1,2,3,4])) 广播变量sc.parallelize([2, 3, 4, 5, 6]).take(3) 获取前三个元素sc.parallelize([2, 3, 4, 5, 6]).takeOrdered(3) 排序后取前三个sc.parallelize([10, 4, 2, 12, 3]).top(3) 取前3个数据,适用于较小的数据sc.parallelize([1, 2, 3]).variance() 计算rdd的方差sc.parallelize([1.0, 2.0, 3.0]).sum() 求和================foreach==============def f(x): print(x)sc.parallelize([1, 2, 3, 4, 5]).foreach(f)============flatMapValues================x = sc.parallelize([("a", ["x", "y", "z"]), ("b", ["p", "r"])])def f(x): return xx.flatMapValues(f).collect()sc.parallelize([1,2,3,4]).fold(0,lambda x,y:x+y) 类似于reduce的聚合,0 表示初始聚合值和聚合类型。==============join,leftOuterjoin,rightOuterjoin========kvRDD1 = sc.parallelize([(3,4),(3,6),(5,6),(1,2)])kvRDD2 = sc.parallelize([(3,8)]) kvRDD1.join(kvRDD2) #按照相同的key值拼接kvRDD1.leftOuterJoin(kvRDD2) #左侧认rdd为准,没有的为None==&gt;[(1,(2,None)),(3,(4,8)),(3,(6,8)),(5,(6,None))]kvRDD1.rightOuterJoin(kvRDD2) 右连接kvRDD1.fullOuterJoin(kvRDD2) 外连接===========subtractByKey===== 删除相同key值的数据kvRDD1.subtractByKey(kvRDD2)===============groupBy==============将RDD中每个键的值分组为单个序列rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])sorted(rdd.groupByKey().mapValues(len).collect())sorted(rdd.groupByKey().mapValues(list).collect())=================mapPartitions==============RDD的每个分区应用函数rdd = sc.parallelize([1, 2, 3, 4], 2)def f(iterator): yield sum(iterator)rdd.mapPartitions(f).collect()=================mapValues============不更改键,对值操作x = sc.parallelize([("a", ["apple", "banana", "lemon"]), ("b", ["grapes"])])def f(x): return len(x)x.mapValues(f).collect()===================zip==================x = sc.parallelize(range(0,5))y = sc.parallelize(range(1000, 1005))x.zip(y).collect() aggregate1234567891011121314151617181920212223242526272829303132333435363738394041421、&gt;&gt;&gt; seqOp = ( lambda x, y: (x[ 0 ] + y, x[ 1 ] + 1 ))2、&gt;&gt;&gt; combOp = ( lambda x, y: (x[ 0 ] + y[ 0 ], x[ 1 ] + y[ 1 ]))3、&gt;&gt;&gt; sc . parallelize([ 1 , 2 , 3 , 4 ]，4) . aggregate(( 0 , 0 ), seqOp, combOp)(10, 4)4、&gt;&gt;&gt; sc.parallelize([ 1, 2, 3, 4,5 ],3).aggregate((1,1),seqOp,combOp)(19,9）依次解释上述函数1、建立各分区内的聚集函数，又初始值依次与分区内的函数做操作2、建立各分区间的组合函数3、使用aggregate 样例14、使用aggregate 样例2样例1 解释：分区数 ： 4 0 ： 11 ： 22 ： 33 ： 4利用zerovalue （0,0） 和 seqOp 对各分区进行聚集 ： ----&gt;看成x为(0,0) y为分区的0 ： （1，1）1 ： （2 , 1）2 ： （3，1）3： （4 , 1）利用 zerovalue和combOp 进行各分区间的聚合 ：（0,0） + （1,1）+ （2,1）+ （3,1）+ （4,1） = （10,4）样例2 解释：分区数 ： 30 ： 11 ： 2,32 ： 4,5利用zerovalue （0,0） 和 seqOp 对各分区进行聚集 ： 0 ： (2,2)1 ： (6,3)===&gt;来源(1,1)+(2,1)=(3,2)===&gt;(3,2)+(3,1)====&gt;(6,3)2 ： (10,3) 利用 zerovalue和combOp 进行各分区间的聚合 ：(1,1) + (2,2) + (6,3) + (10,3) = (19,9) 1234567Application来说，资源是Executor。对于Executor来说资源是内存、corestandalone 集群模式当前只支持一个简单的跨应用程序的 FIFO 调度。然而，为了允许多个并发的用户，您可以控制每个应用程序能用的最大资源数。默认情况下，它将获取集群中的 all cores（核），这只有在某一时刻只允许一个应用程序运行时才有意义, 因为如果此时其他的核被占用, 自然无法获取资源, 运行程序, 此时是有多少核用多少核.Spark中的调度模式主要有两种：FIFO和FAIR。默认情况下Spark的调度模式是FIFO（先进先出），谁先提交谁先执行，后面的任务需要等待前面的任务执行。而FAIR（公平调度）模式支持在调度池中为任务进行分组，不同的调度池权重不同，任务可以按照权重来决定执行顺序。 提交代码优化12345678910111213141516https://www.cnblogs.com/hd-zg/p/6089207.html 原文资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。启动原理:我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。 Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。--total-executor-cores参数指定用的总core数量。若不指定则会用光所有剩下的cores。--executor-memory每个executor分配内存，若超过worker可用剩余内存则不会提交给此worker，若不可提交给任意worker则报错--driver-memory--driver-cores--total-executor-cores 资源参数调优123456789101112131415https://www.cnblogs.com/hd-zg/p/6089207.html☆☆☆☆num-executors:设置spark任务总共需要多少个Executors 建议:一般设置50-100个,设置太少,集群资源得不到利用.设置太大,大部分无法给与足够的资源☆☆☆☆☆executor-memory:每个Executor内存大小 建议:直接决定了spark作业的性能,num-executors*executor-memory不能超过总内存,同时因为要是共享资源,所以通常是总量1/3-1/2☆☆☆☆☆executor-cores:每个executor进程的CPU核心数 建议:num-executors * executor-cores不要超过队列总CPU core的1/3~1/2driver-memory:设置Driver进程的内存 建议:通常不设置或者为1G,当报OOM内存溢出错误的时候,可能跟这个参数有关☆☆☆☆☆spark.default.parallelism:设置每个stage的task数量,直接影响性能 建议:Spark作业的默认task数量为500~1000个较为合适,设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。☆☆☆spark.storage.memoryFraction:RDD持久化数据在Executor内存中可占的比例,不够时候,会写入磁盘 建议:通常默认为0.6,尽量大点,因为spark作业中会有很多的持久化操作,尽量避免写入磁盘☆☆☆spark.shuffle.memoryFraction:shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2 建议:RDD持久化操作较少，shuffle操作较多时,可提高占比.避免溢出写磁盘 开发调优12345678910111213141516171819202122232425262728https://blog.csdn.net/u012102306/article/details/513222091.避免创建重复的RDD2.尽可能复用同一个RDD3.对多次使用的RDD进行持久化4.尽量避免使用shuffle类算子===&gt;shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作.===&gt;很耗时间5.使用map-side预聚合的shuffle操作6.使用高性能的算子7.广播大变量8.使用Kryo优化序列化性能9.优化数据结构*****************************高性能的算子*******************************reduceByKey/aggregateByKey替代groupByKey===&gt;这样实现了现在分区按照key聚合,在整体使用mapPartitions替代普通map使用foreachPartitions替代foreach使用filter之后进行coalesce操作使用repartitionAndSortWithinPartitions替代repartition与sort类操作***********************序列化********************Spark默认使用的是Java的序列化机制Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多*****************************优化数据结构******************************尽量避免使用耗内存的数据结构如以下三种1.对象2.集合,如hashmap和链表等.3.字符串，每个字符串内部都有一个字符数组以及长度等额外信息 数据倾斜调优123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960整个Spark作业的运行进度是由运行时间最长的那个task决定的。数据倾斜只会发生在shuffle过程中 现象:1.绝大多数task执行得都非常快，但个别task执行极慢。 2.某个正常运行的spark作业,突然出现oom.(内存溢出) 原理：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等解决方案: 解决方案1:过滤少数导致倾斜的key 将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。 前提:发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案 优点：实现简单，而且效果也很好，可以完全规避掉数据倾斜。 缺点：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。 解决方案2:提高shuffle操作的并行度 建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案 在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。 原理:更多的task分配更多的key,避免集中. 方案优点：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。 方案缺点：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。 解决方案3:两阶段聚合=====&gt;适用于聚合类shuffle （局部聚合+全局聚合） 原理:分次聚合,将原本相同的key通过附加随机前缀的方式，变成多个不同的key 方案优点：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。 方案缺点：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案 案例:比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。 解决方案4:将reduce join转为map join======&gt;join join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G） 原理:普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join. 方案优点：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。 方案缺点：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。 解决方案5:采样倾斜key并分拆join操作======&gt;join 一个表的key较均匀,而另外一个表的少数几个key数据量较大 实现思路:对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下 每个key的数量，计算出来数据量最大的是哪几个key。 然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n 以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。 接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每 条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也 形成另外一个RDD。 再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的 key打散成n份，分散到多个task中去进行join了。 而另外两个普通的RDD就照常join即可。 最后将两次join的结果使用union算子合并起来即可，就是最终的join结果 方案优点：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。 方案缺点：如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。 解决方案6:使用随机前缀和扩容RDD进行join ======&gt;join 如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义 实现思路:首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key 都对应了超过1万条数据。 然后将该RDD的每条数据都打上一个n以内的随机前缀。 同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打 上一个0~n的前缀。 最后将两个处理后的RDD进行join即可。 原理:将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到 多个task中去处理，而不是让一个task处理大量的相同key。 方案优点：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。 方案缺点：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。 解决方案7:多种方案组合使用 如果只是处理较为简单的数据倾斜场景，那么使用上述方案中的某一种基本就可以解决。但是如果要处理一个较为复杂的数据倾斜场景，那么可能需要将多种方案组合起来使用 shuffle调优====&gt;相比其他三个较为次要123456789101112131415161718192021222324252627282930313233343536 大多数Spark作业的性能主要就是消耗在了shuffle环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。spark.shuffle.file.buffer 默认值：32k 参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。 spark.reducer.maxSizeInFlight 默认值：48m 参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。 spark.shuffle.io.retryWait 默认值：5s 参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。 调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。 spark.shuffle.memoryFraction 默认值：0.2 参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。 调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。 spark.shuffle.manager 默认值：sort 参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。 调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。 spark.shuffle.sort.bypassMergeThreshold 默认值：200 参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。 调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。 spark.shuffle.consolidateFiles 默认值：false 参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。 调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。 123456789&gt;&gt;&gt; from pyspark.sql.functions import pandas_udf, PandasUDFType&gt;&gt;&gt; df = spark.createDataFrame(... [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],... (&quot;id&quot;, &quot;v&quot;))&gt;&gt;&gt; :pandas_udf(&quot;id long, v double&quot;, PandasUDFType.GROUPED_MAP) # doctest: +SKIP... def normalize(pdf):... v = pdf.v... return pdf.assign(v=(v - v.mean()) / v.std())&gt;&gt;&gt; df.groupby(&quot;id&quot;).apply(normalize).show() # doctest: +SKIP 下面是使用RDD的场景和常见案例： 你希望可以对你的数据集进行最基本的转换、处理和控制； 你的数据是非结构化的，比如流媒体或者字符流； 你不希望像进行列式处理一样定义一个模式，通过名字或字段来处理或访问数据属性； 你并不在意通过DataFrame和Dataset进行结构化和半结构化数据处理所能获得的一些优化和性能上的好处； 该什么时候使用DataFrame或Dataset呢？ 如果你需要丰富的语义、高级抽象和特定领域专用的API，那就使用DataFrame或Dataset； 如果你的处理需要对半结构化数据进行高级处理，如filter、map、aggregation、average、sum、SQL查询、列式访问或使用lambda函数，那就使用DataFrame或Dataset； 如果你想在编译时就有高度的类型安全，想要有类型的JVM对象，用上Catalyst优化，并得益于Tungsten生成的高效代码，那就使用Dataset； 如果你想在不同的Spark库之间使用一致和简化的API，那就使用DataFrame或Dataset； 如果你是R语言使用者，就用DataFrame； 如果你是Python语言使用者，就用DataFrame，在需要更细致的控制时就退回去使用RDD； ​ DataFrame与RDD相同之处，都是不可变分布式弹性数据集。不同之处在于，DataFrame的数据集都是按指定列存储，即结构化数据。相似于传统数据库中的表。DataFrame的设计是为了让大数据解决起来更容易。 RDD适合需要low-level函数式编程和操作数据集的情况；DataFrame和Dataset适合结构化数据集，用high-level和特定领域语言(DSL)编程，空间效率高和速度快。 在正常情况下都不推荐使用 RDD 算子 在某种抽象层面来说，使用 RDD 算子编程相当于直接使用最底层的 Java API 进行编程 RDD 算子与 SQL、DataFrame API 和 DataSet API 相比，更偏向于如何做，而非做什么，这样优化的空间很少 RDD 语言不如 SQL 语言友好 仅在一些特殊情况下可以使用 RDD 你希望可以对你的数据集进行最基本的转换、处理和控制； 你的数据是非结构化的，比如流媒体或者字符流； 你想通过函数式编程而不是特定领域内的表达来处理你的数据； 你不希望像进行列式处理一样定义一个模式，通过名字或字段来处理或访问数据属性（更高层次抽象）； 你并不在意通过 DataFrame 和 Dataset 进行结构化和半结构化数据处理所能获得的一些优化和性能上的好处； DataFrame与 RDD 相似，DataFrame 也是数据的一个不可变分布式集合。但与 RDD 不同的是，数据都被组织到有名字的列中，就像关系型数据库中的表一样。设计 DataFrame 的目的就是要让对大型数据集的处理变得更简单，它让开发者可以为分布式的数据集指定一个模式，进行更高层次的抽象。它提供了特定领域内专用的 API 来处理你的分布式数据，并让更多的人可以更方便地使用 Spark，而不仅限于专业的数据工程师。 Spark 2.0 中，DataFrame 和 Dataset 的 API 融合到一起，完成跨函数库的数据处理能力的整合。在整合完成之后，开发者们就不必再去学习或者记忆那么多的概念了，可以通过一套名为 Dataset 的高级并且类型安全的 API 完成工作。 补充12345driver主要负责向excetuor分发task和代码,负责计算的调度,cluster manager负责资源的调度(可以是standalone,yarn,mesos),driver会向cluster申请资源executor负责代码的执行,内部包含很多个executor进程,每个stage计算完成后会将结果写入磁盘进行存储,输入下一个stage,stage的划分是按照shuffle算子driver:主要功能是创建sparkcontext,是一切程序的入口,同时负责和clustermanager进行通信,进行资源的申请和任务的分配,当所有任务完成后会将sparkcontext关闭]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Xgboost算法]]></title>
    <url>%2F2020%2F12%2F30%2FXgboost%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1.介绍123456781.Xgboost本质上还是GBDT(梯度提升树),Xgboost算法是对GBDT算法的改进2.Xgboost是Boosting算法的其中一种，Boosting算法的思想是将许多弱分类器集成在一起，形成一个强分类器。因为Xgboost是一种提升树模型，所以它是将许多树模型集成在一起，形成一个很强的分类器。而所用到的树模型则是CART回归树模型3.xgboost中的基学习器除了可以是CART也可以是线性分类器4.做二分类的时候输出的是模型预测的所有概率中最大的那个值(连续值中取概率最大的)xgboost即可以做回归也可以做分类,但是通常都是做回归5.用到了加法模型和前项分布算法(每次只学习一个基函数及系数，逐步逼近最优解),--&gt;将同时求所有树的参数问题转换成求每个树6.用二阶导数会收敛的更快,从一维向量到二维矩阵,再往后不成熟(张量),算起来慢点单一的回归树不够--&gt;boosting框架--&gt;回归树改良,GBDT--&gt;进一步提升,Xgboost 机器学习常用名词解释正则化: 向你的模型加入某些规则，加入先验，缩小解空间，减小求出错误解的可能性,有效防止过拟合 超参数: 是人为设置值的参数，而不是通过训练得到的参数数据，如学习率 稀疏数据: 数据框中绝大多数数值缺失或者为零的数据,类推稀疏矩阵 离散数据(变量):指其数值只能用自然数或整数单位计算.例如,企业个数,职工人数,设备台数等,只能按计量单位数计数 连续数据(变量):指在一定区间内可以任意取值，相邻的两个数值可作无限分割(即可取无限个值)。比如身高，身高可以是183，也可以是183.1，也可以是183.111……1 离散特征：其数值只能用自然数来表示，只能用计量单位统计，如个数，人数等 连续特征：是按测量或者计量方法得到。连续特征是指在一段长度内可以任意获得的特征，其数值是不间断。比如[0,1]之间的数，可以取n个数。 总之，记住，离散只能用自然数表示，是统计得到的。连续是按测量或者计量到得到数，比如各种传感器采集得到的数。 核外计算: (内存外计算),大数据的数据规模下，一次性把训练数据导到内存里面计算是不实际的 123三个计算级别：（速度越来越慢，数据量越来越大）内存里（核内）内存外（核外） 分布式 spark，Hadoop方面其他:待补充 Cart分类树和回归树实现原理123456789101112131415161718192021222324252627282930313233决策树三种方式:ID3(信息增益) C4.5(信息增益率) Cart(基尼指数)二叉Cart分类树离散特征:(CART的处理思想与C4.5是相同的，即将连续特征值离散化）1.训练数据集为D,计算现有特征对训练数据集的基尼指数，此时对于每一个特征A,对其可能取得每一个值a，根据此值将训练样本切分为D1和D2两部分，然后根据上式计算A=a基尼指数2.在所有可能的特征以及所有可能的值里面选择基尼指数最小的特征及其切分点作为最优的特征及切分点，从结点生成两个子结点，将训练数据集分配到子结点中去3.递归的调用1 2 直到满足停止的条件4.生成分类决策树Cart回归树连续特征:1.训练数据集为D,依次遍历每个特征，以及该特征的每个取值，计算每个切分点的损失函数(平方损失)，选择损失函数最小的切分点,y为每个数值,y^为划分的左右两边各自的均值2.递归的调用1直到满足停止的条件树停止条件:1.只有一个元素,或者元素全部相同时2.达到设置的最大深度3.特征已经划分完4.样本个数小于预定阀值二者区别:1.分类树与回归树的区别在样本的输出，如果样本输出是离散值，这是分类树；样本输出是连续值，这是回归树。分类树的输出是样本的类别，回归树的输出是一个实数。2.连续值的处理方法不同。3.决策树建立后做预测的方式不同。分类模型：采用基尼系数的大小度量特征各个划分点的优劣。回归模型：平方误差最小准则对于决策树建立后做预测的方式，CART分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。回归树输出不是类别，采用叶子节点的均值或者中位数来预测输出结果。剪枝策略:(解决过拟合)预剪枝:在节点划分前来确定是否继续增长,是否继续划分----&gt;降低过拟合,减少训练时间,存在欠拟风险方法:1.节点内数据样本低于某一阈值2.所有节点特征都已分裂3.节点划分前准确率比节点划分后准确率高后剪枝:创建完整的决策树，然后再尝试消除多余的节点，也就是采用减枝的方法区别:预剪枝:运算量小,但是不一定精准,欠拟合风险大后剪枝:运算量大,比较精准,欠拟合风险小 2.Xgboost的优点12345671.正则化项防止过拟合2.xgboost不仅使用到了一阶导数，还使用二阶导数，损失更精确3.XGBoost的并行优化，XGBoost的并行是在特征粒度上的4.考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率5.列抽样。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。6.一种新颖的用于处理稀疏数据的树学习算法7.XGBoost利用了核外计算 正则化内容:包含L1正则化、L2正则化 以线性回归为例 L1正则化 L2正则化 L1正则化最大的特点是能生成稀疏矩阵(主要用于特征选择,0和非0) L2正则能够有效的防止模型过拟合 作用:解决过拟合,降低模型复杂度,防止参数过大(限制模型的参数) 正则化能实现过拟合原因: 1.作为惩罚项,惩罚特征权重w,当λ越大w越小,权重影响越小 2.直观的理解，如果我们的正则化系数λ无穷大，则权重w就会趋近于0。权重变小，非线性程度自然就降低了 ## 3.Xgboost与GBDT的区别123456GBDT:梯度提升算法利用损失函数的负梯度作为残差拟合的方式，如果其中的基函数采用决策树的话，就得到了梯度提升决策树1.XGBoost对损失函数做了二阶泰勒展开,GBDT只用了一阶导数信息.并且在损失函数一阶,二阶可导的条件下XGBoost可以自定义损失函数。2.XGBoost的目标函数加了正则项，相当于预剪枝，使得学习出来的模型更加不容易过拟合。3.XGBoost支持列抽样，与随机森林类似，用于防止过拟合对树中的每个非叶子节点，XGBoost可以自动学习出它的默认分裂方向。如果某个样本该特征值确缺失，会将其划入默认分支。4.XGBoost支持特征维度的并行 拟合残差:最后结果相加就是预测的 4.算法的实现过程 分别代表训练损失和树的复杂度 前t-1颗树的结构已经确定,认定为常数 表示损失函数,如平方损失 表示正则化项,考虑树的复杂度,防止过拟合 表示常数项 损失函数使用平方损失 平方损失函数有许多友好的地方，它具有一阶项（通常称为残差）和二次项。对于其他形式的损失函数，并不容易获得这么好的形式 损失函数不是平方损失 泰勒二阶展开 代入原公式: 化简树结构 定义一棵树q(x)为输入x然后输出叶子结点索引,w为叶子结点向量 树的复杂度 这样就将样本累加操作转换为叶子节点的操作 Gj 表示映射为叶子节点 j 的所有输入样本的一阶导之和 G和H都是常数，那么这个问题就变成了一个二次问题了，求解最小值 #### 5.Xgboost的实现(案例:回归预测房价)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293train=pd.read_csv(r'C:\Users\Administrator\Desktop\train.csv', header=0)test=pd.read_csv(r'C:\Users\Administrator\Desktop/test.csv', header=0)c_test = test.copy()c_train = train.copy()c_train['train'] = 1c_test['train'] = 0df = pd.concat([c_train, c_test], axis=0,sort=False)NAN = [(c, df[c].isna().m ean()*100) for c in df]NAN = pd.DataFrame(NAN, columns=["column_name", "percentage"])NAN = NAN[NAN.percentage &gt; 50]NAN.sort_values("percentage", ascending=False)#得到缺失率超过50%的字段df = df.drop(['Alley','PoolQC','Fence','MiscFeature'],axis=1)object_columns_df = df.select_dtypes(include=['object'])numerical_columns_df =df.select_dtypes(exclude=['object'])null_counts = object_columns_df.isnull().sum() #object数据中每个特征空值的数量columns_None = ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','GarageType','GarageFinish','GarageQual','FireplaceQu','GarageCond']object_columns_df[columns_None]= object_columns_df[columns_None].fillna('None')columns_with_lowNA = ['MSZoning','Utilities','Exterior1st','Exterior2nd','MasVnrType','Electrical','KitchenQual','Functional','SaleType']object_columns_df[columns_with_lowNA] = object_columns_df[columns_with_lowNA].fillna(object_columns_df.mode().iloc[0]) #object_columns_df.mode()返回频数最高的null_counts = numerical_columns_df.isnull().sum()#YearBuilt无空值print((numerical_columns_df['YrSold']-numerical_columns_df['YearBuilt']).median()) #35print(numerical_columns_df["LotFrontage"].median()) #68numerical_columns_df['GarageYrBlt'] = numerical_columns_df['GarageYrBlt'].fillna(numerical_columns_df['YrSold']-35)numerical_columns_df['LotFrontage'] = numerical_columns_df['LotFrontage'].fillna(68)numerical_columns_df= numerical_columns_df.fillna(0)#作图找出方差小的特征,将他们删除(方差小,离散程度低,特征无明显区分左右,故删除)object_columns_df['Utilities'].value_counts().plot(kind='bar',figsize=[10,3])object_columns_df['Utilities'].value_counts() object_columns_df['Street'].value_counts().plot(kind='bar',figsize=[10,3])object_columns_df['Street'].value_counts() object_columns_df['Condition2'].value_counts().plot(kind='bar',figsize=[10,3])object_columns_df['Condition2'].value_counts() object_columns_df['RoofMatl'].value_counts().plot(kind='bar',figsize=[10,3])object_columns_df['RoofMatl'].value_counts()object_columns_df['Heating'].value_counts().plot(kind='bar',figsize=[10,3])object_columns_df['Heating'].value_counts() #======&gt; Drop feature one Typeobject_columns_df = object_columns_df.drop(['Heating','RoofMatl','Condition2','Street','Utilities'],axis=1)#创造特征numerical_columns_df['Age_House']= (numerical_columns_df['YrSold']-numerical_columns_df['YearBuilt'])numerical_columns_df['Age_House'].describe() #对特征分析Negatif = numerical_columns_df[numerical_columns_df['Age_House'] &lt; 0]numerical_columns_df.loc[numerical_columns_df['YrSold'] &lt; numerical_columns_df['YearBuilt'],'YrSold' ] = 2009numerical_columns_df['Age_House']= (numerical_columns_df['YrSold']-numerical_columns_df['YearBuilt'])numerical_columns_df['Age_House'].describe()numerical_columns_df['TotalBsmtBath'] = numerical_columns_df['BsmtFullBath'] + numerical_columns_df['BsmtFullBath']*0.5numerical_columns_df['TotalBath'] = numerical_columns_df['FullBath'] + numerical_columns_df['HalfBath']*0.5 numerical_columns_df['TotalSA']=numerical_columns_df['TotalBsmtSF'] + numerical_columns_df['1stFlrSF'] + numerical_columns_df['2ndFlrSF']bin_map = &#123;'TA':2,'Gd':3, 'Fa':1,'Ex':4,'Po':1,'None':0,'Y':1,'N':0,'Reg':3,'IR1':2,'IR2':1,'IR3':0,"None" : 0, "No" : 2, "Mn" : 2, "Av": 3,"Gd" : 4,"Unf" : 1, "LwQ": 2, "Rec" : 3,"BLQ" : 4, "ALQ" : 5, "GLQ" : 6 &#125;object_columns_df['ExterQual'] = object_columns_df['ExterQual'].map(bin_map)object_columns_df['ExterCond'] = object_columns_df['ExterCond'].map(bin_map)object_columns_df['BsmtCond'] = object_columns_df['BsmtCond'].map(bin_map)object_columns_df['BsmtQual'] = object_columns_df['BsmtQual'].map(bin_map)object_columns_df['HeatingQC'] = object_columns_df['HeatingQC'].map(bin_map)object_columns_df['KitchenQual'] = object_columns_df['KitchenQual'].map(bin_map)object_columns_df['FireplaceQu'] = object_columns_df['FireplaceQu'].map(bin_map)object_columns_df['GarageQual'] = object_columns_df['GarageQual'].map(bin_map)object_columns_df['GarageCond'] = object_columns_df['GarageCond'].map(bin_map)object_columns_df['CentralAir'] = object_columns_df['CentralAir'].map(bin_map)object_columns_df['LotShape'] = object_columns_df['LotShape'].map(bin_map)object_columns_df['BsmtExposure'] = object_columns_df['BsmtExposure'].map(bin_map)object_columns_df['BsmtFinType1'] = object_columns_df['BsmtFinType1'].map(bin_map)object_columns_df['BsmtFinType2'] = object_columns_df['BsmtFinType2'].map(bin_map)PavedDrive = &#123;"N" : 0, "P" : 1, "Y" : 2&#125;object_columns_df['PavedDrive'] = object_columns_df['PavedDrive'].map(PavedDrive)rest_object_columns = object_columns_df.select_dtypes(include=['object'])#独热编码object_columns_df = pd.get_dummies(object_columns_df, columns=rest_object_columns.columns) df_final = pd.concat([object_columns_df, numerical_columns_df], axis=1,sort=False)df_final = df_final.drop(['Id',],axis=1)df_train = df_final[df_final['train'] == 1]df_train = df_train.drop(['train',],axis=1)df_test = df_final[df_final['train'] == 0]df_test = df_test.drop(['SalePrice'],axis=1)df_test = df_test.drop(['train',],axis=1)target= df_train['SalePrice']df_train = df_train.drop(['SalePrice'],axis=1)x_train,x_test,y_train,y_test = train_test_split(df_train,target,test_size=0.33,random_state=0)#前面全是创建特征工程,数据预处理xgb =XGBRegressor( booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.6, gamma=0, importance_type='gain', learning_rate=0.01, max_delta_step=0, max_depth=4, min_child_weight=1.5, n_estimators=2400, n_jobs=1, nthread=None, objective='reg:linear', reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, silent=None, subsample=0.8, verbosity=1)xgb.fit(x_train, y_train)predict1 = xgb.predict(x_test)xgb.fit(df_train, target)predicty = xgb.predict(df_test)predicty 6.Xgboost的参数详解以及使用一般参数:控制总体的功能121.booster[default=gbtree]选择基分类器 gbtree、gblinear 树或线性分类器2.nthread [default to maximum number of threads available if not set]线程数默认最大 Tree Booster参数：控制单个学习器的属性1234567891. learning_rate[default=0.3]:学习率learning rate,步长.一般常用的数值: 0.01-0.22. min_child_weight [default=1]:这个参数用来控制过拟合,如果数值太大可能会导致欠拟合3. max_depth [default=6]: 每颗树的最大深度，树高越深，越容易过拟合。4. gamma [default=0]：如果分裂能够使loss函数减小的值大于gamma，则这个节点才分裂。gamma设置了这个减小的最低阈值。如果gamma设置为0，表示只要使得loss函数减少，就分裂,通常设置为06. subsample [default=1]：对原数据集进行随机采样来构建单个树。这个参数代表了在构建树时候 对原数据集采样的百分比。eg：如果设为0.8表示随机抽取样本中80%的个体来构建树。7. colsample_bytree [default=1]：创建树的时候，从所有的列中选取的比例。e.g：如果设为0.8表示随机抽取80%的列 用来创建树.一般设置为： 0.5-18. lambda [default=1]：控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。9. alpha [default=0]:控制模型复杂程度的权重值的 L1 正则项参数，参数值越大，模型越不容易过拟合。10. scale_pos_weight [default=1]在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。通常可以将其设置为负样本的数目与正样本数目的比值。 学习任务参数：控制调优的步骤12345678910111213141516171819201.objective [缺省值=reg:linear] reg:linear– 线性回归 reg:logistic – 逻辑回归 binary:logistic – 二分类逻辑回归，输出为概率 binary:logitraw – 二分类逻辑回归，输出的结果为wTx count:poisson – 计数问题的poisson回归，输出结果为poisson分布。在poisson回归中，max_delta_step的缺省值为0.7 (used to safeguard optimization) multi:softmax – 设置 XGBoost 使用softmax目标函数做多分类，需要设置参数num_class（类别个数） multi:softprob – 如同softmax，但是输出结果为ndata*nclass的向量，其中的值是每个数据分为每个类的概率 2.seed [ default=0 ]随机种子,为了产生能够重现的结果。因为如果不设置这个种子，每次产生的结果都会不同。3.eval_metric [缺省值=通过目标函数选择] rmse: 均方根误差 mae: 平均绝对值误差 logloss: negative log-likelihood error: 二分类错误率。 merror: 多分类错误率，计算公式为(wrong cases)/(all cases) mlogloss: 多分类log损失 auc: 曲线下的面积 ndcg: Normalized Discounted Cumulative Gain map: 平均正确率 调参方法12345678910111213141516171819202122232425需要借助一个模块sklearn.model_selection.GridSearchCV(模型调参利器,网格搜索) 参数: 1.estimator,优化器,即你建立的模型,这里选xgboost 2.param_grid 一般用字典(通常,也可列表),输入优化的参数 3.scoring 模型评价标准,如roc,auc等调参顺序: 1.n_estimators 迭代次数,就是生成树的个数 如,cv_params = &#123;'n_estimators': [550, 575, 600, 650, 675]&#125; 2.min_child_weight和max_depth 这两个参数是控制树生成的，树的结构对于最终的结果影响还是很大 3.gamma控制节点分裂标准 cv_params = &#123;'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]&#125; 4.subsample和colsample_bytree 采样比例,cv_params = &#123;'subsample': [0.6, 0.7, 0.8, 0.9], 'colsample_bytree': [0.6, 0.7, 0.8, 0.9]&#125; 5. alpha和lambda, 正则化的参数 cv_params = &#123;'alpha': [0.05, 0.1, 1, 2, 3], 'lambda': [0.05, 0.1, 1, 2, 3]&#125; 6.learning_rate 学习率,从较小的开始 cv_params = &#123;'learning_rate': [0.01, 0.05, 0.07, 0.1, 0.2]&#125; 7.调参可以提高模型性能，但是更重要的还是特征选择，数据清洗，特征融合 控制过拟合 当您观察到较高的训练准确率，但测试精度较低时，很可能遇到了过拟合问题。 通常可以通过两种方式来控制xgboost中的过拟合 第一种方式是直接控制模型的复杂性 这包括max_depth，min_child_weight 和 gamma 第二种方法是增加随机性，使训练对噪声更加鲁棒 这包括subsample，colsample_bytree,eta 样本不均衡问题 1) 设置scale_pos_weight,有时会遇到样本不均衡的问题，比如正例占99%，反例占1%，那么如果预测为全正例或者随机抽机，正确率也占99%。此时可使用scale_pos_weight提高反例权重，默认为1，不增加权重。 2)使用xgb自带的调用接口 (非sklearn接口)，需要把数据转成DMatrix格式，如果想给不同实例分配不同权重，可以转换时使用weight参数，它传入与实例个数等长的数组，数组中每个数对应一个实例的权重，在xgb每次迭代后调整权重时也会将它计算在内。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import xgboost as xgbfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import GridSearchCV​cancer = load_breast_cancer()x = cancer.data[:50]y = cancer.target[:50]train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.333, random_state=0)parameters = &#123; 'max_depth': [5, 10, 15, 20, 25], 'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.15], 'n_estimators': [50, 100, 200, 300, 500], 'min_child_weight': [0, 2, 5, 10, 20], 'max_delta_step': [0, 0.2, 0.6, 1, 2], 'subsample': [0.6, 0.7, 0.8, 0.85, 0.95], 'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9], 'reg_alpha': [0, 0.25, 0.5, 0.75, 1], 'reg_lambda': [0.2, 0.4, 0.6, 0.8, 1], 'scale_pos_weight': [0.2, 0.4, 0.6, 0.8, 1]&#125;​xlf = xgb.XGBClassifier(max_depth=10, learning_rate=0.01, n_estimators=2000, silent=True, objective='binary:logistic', nthread=-1, gamma=0, min_child_weight=1, max_delta_step=0, subsample=0.85, colsample_bytree=0.7, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=1440, missing=None)​# 有了gridsearch我们便不需要fit函数gsearch = GridSearchCV(xlf, param_grid=parameters, scoring='accuracy', cv=3)gsearch.fit(train_x, train_y)print("Best score: %0.3f" % gsearch.best_score_)print("Best parameters set:")best_parameters = gsearch.best_estimator_.get_params()for param_name in sorted(parameters.keys()): print("\t%s: %r" % (param_name, best_parameters[param_name]))#比较耗费时间,穷举法 案例2:https://www.jianshu.com/p/7aab084b7f47https://blog.csdn.net/weixin_30838921/article/details/94989933?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.channel_param 7.Xgboost的注意事项123456789101.windows安装推荐方式，直接pip会各种报错https://blog.csdn.net/qq_20412595/article/details/79771490?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param2.在Xgb中需要将离散特征one-hot编码，和连续特征一起输入训练3.Xgb中支持稀疏矩阵训练模型，导致数据稀疏的原因如下 缺省值 稠密数据中零元素 离散特征one-hot编码后产生的0值4.win和mac平台运行相同的xgboost代码，效果如果可能不同,将参数colsample_bytree设置为15.xgboost提供两种调用方式，一种是自身接口，一种是类似sklearn的接口，建议使用自身接口，因为很多重要功能，如存取模型，评估功能都无法通过sklearn接口调用。6.Xgboost可以处理二分类，多分类，回归问题。处理不同问题，主要的区别在于指定不同的误差函数，xgboost会根据不同误差函数计算的结果调整权重进行下一次迭代。通过参数objective可设置xgb自带的误差函数：回归一般用reg:xxx（如reg:linear），二分类用binary:xxx（如binary:logistic），多分类用multi:xxx（如multi:softmax） Xgboost原生接口和Sklearn接口的区别121.xgboost原生接口，数据需要经过标签标准化(LabelEncoder().fit_transform)、输入数据标准化(xgboost.DMatrix)和输出结果反标签标准化(LabelEncoder().inverse_transform)，训练调用train预测调用predict.2.xgboost的sklearn接口，可以不经过标签标准化(即将标签编码为0~n_class-1)，直接喂给分类器特征向量和标签向量，使用fit训练后调用predict就能得到预测向量的预测标签，它会在内部调用sklearn.preprocessing.LabelEncoder()将标签在分类器使用时transform，在输出结果时inverse_transform。 xgb在选择最佳分裂点，进行枚举的时候并行！ xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行 拟合残差只是考虑到损失函数为平方损失的特殊情况，负梯度是更加广义上的拟合项，更具普适性。 无论损失函数是什么形式，每个决策树拟合的都是负梯度。准确的说，不是用负梯度代替残差，而是当损失函数是均方损失时，负梯度刚好是残差，残差只是特例。 为啥要去用梯度拟合不用残差？代价函数除了loss还有正则项，正则中有参数和变量，很多情况下只拟合残差loss变小但是正则变大，代价函数不一定就小，这时候就要用梯度啦，梯度的本质也是一种方向导数，综合了各个方向（参数）的变化，选择了一个总是最优（下降最快）的方向；]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark入门]]></title>
    <url>%2F2020%2F12%2F24%2Fspark%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Apache重要的三个基金会项目(hadoop,spark,storm)Spark提供了内存计算，减少了迭代计算时的IO开销；虽然，Hadoop已成为大数据的事实标准，但其MapReduce分布式计算模型仍存在诸多缺陷，而Spark不仅具备Hadoop MapReduce所具有的优点，且解决了Hadoop MapReduce的缺陷。 Spark:可以作为一个更加快速、高效的大数据计算平台。基于内存的大数据并行计算框架.将计算分解成多个任务在不同的机器上运行. 别人的学习总结:https://blog.csdn.net/qq_33247435/article/details/83653584#8Spark_71 spark的概念​ 1.$RDD$(resilient distribute dataset 弹性分布式训练集).是分布式内存里的一个抽象概念,表示的是高度受限的共享内存模型 ​ 2.$DAG$(directed acyclic gragh),有向无环图,表明了RDD之间的依赖关系 ​ 3.$EXECUTOR:$运行在工作节点上的一个进程,负责运行任务,以及应用程序存储数据 ​ 4.$程序:$编写的spark程序 ​ 5.$任务:$运行在executor上的工作单元 ​ 6.$作业:$包含多个RDD及对应RDD上的操作 ​ 7.$阶段:$作业的基本调度单位,一个作业会分成多组任务,每组任务称为阶段 ​ 8.$shuffle过程:$简单认为就是将不同节点上的相同key拉到同一个节点上计算 ​ 9.$SparkSession:$代表了spark集群中的一个连接,在应用程序实例化的时候启动 ====&gt;spark的入口,2.0之前spark core是sparkcontext,spark sql是sqlcontext,sparkstreaming应用使用streamingContext.2.0之后,sparksession对象把所有的对象组合到一起.称为所有程序统一的入口 RDD详解(待补充)passspark运行架构 ​ $driver:$每个应用的任务控制节点 ​ $cluster manager:$集群资源管理器 ​ $node:$运行作业任务的工作节点 ​ $Executor$:每个工作节点上负责具体任务的执行进程 ​ $\textcolor{red}{关系:}$ ​ 一个应用由一个一个控制节点(driver)和若干个作业构成,一个作业由若干个阶段构成,一个阶段由多个任务构成.当执行一个应用时,任务控制节点会向集群管理器申请资源,启动executor,并向executor发送应用程序和代码和文件.然后在executor上执行任务,运行结束后,执行结果会返回给任务控制节点,或者写到数据库中. ​ $\textcolor{red}{Executor优点:}$ ​ 1.采用的是多线程(map reduce 使用的是进程模型),减少了开销 ​ 2.executor中有一个blockmanager存储模块,会将内存和磁盘作为存储模块,当需要多轮迭代计算的时候,可以将数据存储到这个模块.有效减少了IO开销；或者在交互式查询场景下，预先将表缓存到该存储系统上，从而可以提高读写IO性能。 spark运行基本流程1.当一个spark应用被提交时,需要为这个应用提供基本的运行环境,即有任务控制节点(driver)创建一个sparkcontext,负责和资源管理器的通信以及资源的申请和任务的分配和监控等.sparkcontext会向资源管理器注册并申请运行Executor的资源. ​ 2.资源管理器为Executor分配资源,并启动Executor进程,Executor运行情况将随着心跳发送到资源管理器上 ​ 3.任务在Executor上运行,并将结果返回给任务调度器,然后反馈给DAG调度器,运行完毕,写入资源并释放所有资源. ​ $\textcolor{orange}{详解}$ ​ 1.构建spark applicantion的运行环境,启动SparkContext ​ 2.sparkcontext向资源管理器申请运行Executor ​ 3.Executor向Sparkcontext申请Task ​ 4.SparkContext将应用程序分发给Executor ​ 5.sparkcontext构建DAG图,将DAG图分解成Stage,将tasket发送给Task Scheduler,最后由Task Scheduler将Task发送给Executor运行 ​ 6.Task在Executor上运行,运行完释放所有资源 ​ $\textcolor{red}{SparkContext原理:}$ ​ 依据RDD的依赖关系构建DAG图,然后将DAG图提交给DAG调度器进行解析,将DAG图分解成多个阶段,并计算出各个阶段的依存关系,然后把一个个任务集提交给底层的调度器进行处理.Executor向sparkcontext申请人无,任务调度器将任务发送给Executor并将应用程序代码发送给Executor spark on standalone流程123456789101112131415161718191、我们提交一个任务，任务就叫Application2、初始化程序的入口SparkContext， 2.1 初始化DAG Scheduler 2.2 初始化Task Scheduler3、Task Scheduler向master去进行注册并申请资源（CPU Core和Memory）4、Master根据SparkContext的资源申请要求和Worker心跳周期内报告的信息决定在哪个Worker上分配资源，然后在该Worker上获取资源，然后启动StandaloneExecutorBackend；顺便初始化好了一个线程池5、StandaloneExecutorBackend向Driver(SparkContext)注册,这样Driver就知道哪些Executor为他进行服务了。 到这个时候其实我们的初始化过程基本完成了，我们开始执行transformation的代码，但是代码并不会真正的运行，直到我们遇到一个action操作。生产一个job任务，进行stage的划分6、SparkContext将Applicaiton代码发送给StandaloneExecutorBackend；并且SparkContext解析Applicaiton代码，构建DAG图，并提交给DAG Scheduler分解成Stage（当碰到Action操作 时，就会催生Job；每个Job中含有1个或多个Stage，Stage一般在获取外部数据和shuffle之前产生）。7、将Stage（或者称为TaskSet）提交给Task Scheduler。Task Scheduler负责将Task分配到相应的Worker，最后提交给StandaloneExecutorBackend执行；8、对task进行序列化，并根据task的分配算法，分配task9、对接收过来的task进行反序列化，把task封装成一个线程10、开始执行Task，并向SparkContext报告，直至Task完成。11、资源注销心跳是分布式技术的基础，我们知道在Spark中，是有一个Master和众多的Worker，那么Master怎么知道每个Worker的情况呢，这就需要借助心跳机制了。心跳除了传输信息，另一个主要的作用就是Worker告诉Master它还活着，当心跳停止时，方便Master进行一些容错操作，比如数据转移备份等等。 spark部署​ $\textcolor{red}{三种部署方式:}$1.standalone 2.spark on Mesos 3.spark on YARN ​ $\textcolor{green}{standalone:}$ ​ 分布式集群服务，自带的完整服务，Spark自己进行资源管理和任务监控,一定程度上来说,此模式是其他两个模式的基础 ​ $\textcolor{green}{Sapark on Mesos:}$ ​ 官方推荐(都是apache的),spark设计之初就考虑支持mesos,spark在mesos上比在YARN上更灵活 ​ $Mesos$是一种资源调度管理框架 ​ $两种调度模式:$ ​ 1.粗粒度模式 ​ 每个应用程序的运行环境由一个driver和若干个executor组成.其中,每个executor占用若干资源,内部可运行多个Task,应用程序在开始之前需要将运行环境的资源全部申请好,且运行过程中要一直占用这些资源,即使不用.当程序结束时,会进行回收. ​ 2.细粒度模式 ​ 粗粒度会造成很大的资源浪费,动态分配 ​ $\textcolor{red}{Spark on Yarn}$ ​ 是一种最有前景的部署模式。但限于YARN自身的发展，目前仅支持粗粒度模式 ​ Spark可运行于YARN之上，与Hadoop进行统一部署 ​ 分布式部署集群，这是由于YARN(资源管理器)上的Container资源是不可以动态伸缩的，一旦Container启动之后，可使用的资源不能再发生变化. spark相对hadoop的优势​ hadoop的mapreduce计算模型延迟过高,无法胜任$\textcolor{red}{实时}$和$\textcolor{red}{快速}$计算,只适用离线批处理 ​ $hadoop的缺点$: ​ 1.表达能力有限.分为map阶段和reduce阶段.难以描述复杂数据处理过程. ​ 2.磁盘io开销大.每次执行都需要从磁盘读取数据,且存的时候需要将中间数据存到磁盘中. ​ 3.延迟高,一次计算可能需要分解成一系列按顺序执行的MapReduce任务,任务之间的衔接涉及到IO开销,会产生较高的延迟. ​ $Spark的优点$: ​ 1.计算模式也属于MapReduce,但不局限于map和reduce操作,提供多种数据类型操作,比MapReduce更灵活 ​ 2.spark提供了内存计算,中间结果直接放到内存中 ​ 3.spark使用的是DAG进行任务调度,比MapReduce的迭代执行机制强 ​ $整体:$ ​ spark最大的优点就是将计算结果,中间数据存储到内存中,大大减少了IO开销.因为spark更适合迭代运算多的数据挖掘和机器学习运算 ​ $总:$尽管整体上spark比hadoop要好,但是无法完全替代hadoop,通常是用来替代hadoop的MapReduce部分 spark生态 ​ spark生态主要包含$\textcolor{red}{Spark Core}$ ,$\textcolor{red}{Spark Sql}$,$\textcolor{red}{Spark Screaming}$,$\textcolor{red}{MLlib}$,$\textcolor{red}{Graphx}$ ​ $\textcolor{red}{Spark Core:}$Spark Core包含Spark的基本功能，如内存计算、任务调度、部署模式、故障恢复、存储管理等。通常所说的Apache Spark，就是指Spark Core ​ $\textcolor{red}{Spark Sql:}$Spark SQL允许直接处理RDD，同时也可查询Hive、HBase等外部数据源。Spark SQL的一个重要特点是其能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行查询，并进行更复杂的数据分析； ​ $\textcolor{red}{Spark Screaming:}$ Spark Streaming支持高吞吐量、可容错处理的实时流数据处理，其核心思路是将流式计算分解成一系列短小的批处理作业。 ​ $\textcolor{red}{Graphx:}$GraphX是Spark中用于图计算的API ​ $\textcolor{red}{MLlib:}$MLlib提供了常用机器学习算法的实现，包括聚类、分类、回归、协同过滤等 在pyspark中执行词频统计​ $案例1.词频统计$ ​ ①首先创建一个worldcount目录(shell 命令下) ​ ②然后创建一个txt文件,里面随便写一些文字.作为统计的原材料 ​ ③词频统计需要启动pyspark.cd /usr/local/spark 然后 ./bin/pyspark ​ ④加载文件—&gt;确定是在本地还是在分布式的hdfs上 ​ $本地$: ​ textFile = sc.textFile(‘file:///usr/local/spark/mycode/wordcount/word.txt’)(要加载本地文件，必须采用“file:///”开头的这种格式)–&gt;惰性的,需要first()这种才能打印出数据 ​ textFile.first()(打印第一行数据,文件不存在会显示拒绝连接) ​ $HDFS$: ​ 需要首先启动Hadoop中的HDFS组件 ​ 1. cd /usr/local/hadoop ​ 2. ./sbin/start-dfs.sh ​ $\textcolor{red}{上传文件到hdfs上}$./bin/hdfs dfs -put /usr/local/spark/mycode/wordcount/word.txt . ​ 3.加载文件textFile = sc.textFile(“hdfs://localhost:9000/user/hadoop/word.txt”)–&gt;惰性的 ​ ⑤写代码,在pyspark窗口(类似于ipython那种&gt;&gt;&gt;),代码如下 ​ textFile = sc.textFile(“file:///usr/local/spark/mycode/wordcount/word.txt”) ​ wordCount = textFile.flatMap(lambda line: line.split(“ “)).map(lambda word: (word,1)).reduceByKey(lambda a, b : a + b) ​ wordCount.collect() ​ $\textcolor{red}{代码解释:}$ ​ 1.第一行即从本地加载文件数据 ​ 2.第二行textFile.flatMap(lambda line: line.split(“ “))表示按行处理,每行按照空白符分割.这样每行得到一个单词集合.textFile.flatMap()操作就把这多个单词集合“拍扁”得到一个大的单词集合.map(lambda word: (word,1))会遍历单词集合中的每一个单词,并执行Lamda表达式word : (word, 1). ​ 程序执行到这里，已经得到一个RDD，这个RDD的每个元素是(key,value)形式的tuple。最后，针对这个RDD，执行reduceByKey(labmda a, b : a + b)操作，这个操作会把所有RDD元素按照key进行分组，然后使用给定的函数（这里就是Lamda表达式：a, b : a + b） ​ 如:(“hadoop”,1)和(“hadoop”,1)—–&gt;(“hadoop”,2) 编写独立应用程序执行词频统计​ 1.创建test.py文件内容如下 ​ from pyspark import SparkContext ​ sc = SparkContext( ‘local’, ‘test’) ​ textFile = sc.textFile(“file:///usr/local/spark/mycode/wordcount/word.txt”) ​ wordCount = textFile.flatMap(lambda line: line.split(“ “)).map(lambda word: (word,1)).reduceByKey(lambda a, b : a + b) ​ wordCount.foreach(print) 在集群上运行spark1234567891011121314151617181920211.启动hadoop集群 ①cd /usr/local/hadoop/ ②sbin/start-all.sh2.启动spark的master节点和索引slaves节点 ①cd /usr/local/spark/ ②sbin/start-master.sh ③sbin/start-slaves.sh3.介绍两种资源管理方式standalone 和 spark on yarn独立资源管理器: 1&gt;安装jar包 向独立集群管理器提交应用,需要把spark://master:7077作为主节点参数传递给spark-submit. eg:bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://master:7077 examples/jars/spark-examples_2.11-2.0.2.jar 100 2&gt;&amp;1 | grep &quot;Pi is roughly&quot; 2&gt;在集群中运行pyspark ①在shell中输入命令进入pyspark中 cd /usr/local/spark/ bin/pyspark --master spark://master:7077spark on yarn: 1&gt;安装应用程序jar包 需要把yarn-cluster作为主节点参数传递给spark-submit 2&gt;在集群中运行pyspark ①bin/pyspark --master yarn $\textcolor{red}{spark-submit}$ 可以提交任务到 spark 集群执行，也可以提交到 hadoop 的 yarn 集群执行。 bin/spark-submit –class org.apache.spark.examples.SparkPi –master表示以集群模式启动spark Jupyter Notebook调试PySparkRDD的弹性1231.自动的进行内存和磁盘的存储切换2.Task如果失败，会自动进行特定次数的重试3.数据分片的高度弹性（coalesce）,优先内存,内存不够才放磁盘 主从架构 和 P2P架构 宽依赖 债依赖shuffle操作 fork和join RDD运行原理12345671.RDD无法直接更改数据,每次操作都会生成一个新的RDD2.每个RDD都会分成很多个分区,每个分区都是部分数据集片段,一个RDD的不同分区可以保存到不同集群的不同节点上,从而可以实现在不同节点的并行计算3.执行过程:读入外部的数据源（或者内存中的集合）进行 RDD 创建； RDD 经过一系列的 “转换” 操作，每一次都会产生不同的 RDD，供给下一个转换使用； 最后一个 RDD 经过 “行动” 操作进行处理，并输出指定的数据类型和值。 RDD 采用了惰性调用，即在 RDD 的执行过程中，所有的转换操作都不会执行真正的操作，只会记录依赖关 系，而只有遇到了行动操作，才会触发真正的计算，并根据之前的依赖关系得到最终的结果。4.RDD发生行为操作并生成输出数据时，Spark 才会根据 RDD 的依赖关系生成有向无环图（DAG），并从起点开始执行真正的计算。正是 RDD 的这种惰性调用机制，使得转换操作得到的中间结果不需要保存，而是直接管道式的流入到下一个操作进行处理 RDD可以实现高效计算的原因123451.高效的容错性。可以直接利用 RDD 之间的依赖关系来重新计算得到丢失的分区。2.中间结果持久化到内存。不需要存储到磁盘,降低了IO.3.存放的数据可以是 Java 对象，避免了不必要的对象序列化和反序列化开销。 序列化:将数据转换为字节存储的过程(存储数据到磁盘) 反序列化:将二进制字节码转换成java对象(从磁盘读数据) RDD之间的依赖关系123456宽依赖:父 RDD 与子 RDD 之间的一对多关系，即一个父 RDD 转换成多个子 RDD窄依赖:父 RDD 和子 RDD 之间的一对一关系或者多对一关系,主要包括的操作有 map、filter、union 等宽依赖的RDD通常伴随着Shuffle操作(非常复杂且昂贵的操作,包含在executors和machines上的数据复制)首先需要计算好所有父分区数据，然后在节点之间进行 Shuffle.在进行数据恢复时，窄依赖只需要根据父 RDD 分区重新计算丢失的分区即可，而且可以并行地在不同节点进行重新计算。而对于宽依赖而言，单个节点失效通常意味着重新计算过程会涉及多个父 RDD 分区，开销较大。 RDD编程######1.通过加载数据创建RDD 123456789from pyspark import SparkContextsc = SparkContext( &apos;local&apos;, &apos;test&apos;)lines = sc.textFile(&quot;hdfs://localhost:9000/user/hadoop/word.txt&quot;) #hdfs,HBase、Cassandra、Amazon S3等外部数据源中加载数据集等文件系统加载textFile = sc.textFile(&apos;file:///usr/local/spark/mycode/wordcount/word.txt&apos;) #本地节点加载注意: 1.如果使用了本地文件系统的路径,必须保证在所有的worker节点上,也可以采用相同的路径访问到改文件 (既可以复制到每个worker节点上,也可以使用网络挂载共享文件系统) 2.textFile输入的参数可以是文件名,可以说目录,也可以说压缩文件等. 2.通过并行集合(数组)创建RDD123调用sparkcontext的parallelize方法,在Driver中一个已存在的集合(数组)上创建nums = [1,2,3,4]rdd = sc.parallelize(nums) RDD操作121.转换:基于现有的数据集创建一个新的数据集2.行动:在数据集上进行运算,返回计算值 1.转换操作12345678对于RDD而言，每一次转换操作都会产生不同的RDD，供给下一个“转换”使用。转换得到的RDD是惰性求值的，也就是说，整个转换过程只是记录了转换的轨迹，并不会发生真正的计算，只有遇到行动操作时，才会发生真正的计算，开始从血缘关系源头开始，进行物理的转换操作。下面列出一些常见的转换操作（Transformation API）：* filter(func)：筛选出满足函数func的元素，并返回一个新的数据集* map(func)：将每个元素传递到函数func中，并将结果返回为一个新的数据集* flatMap(func)：与map()相似，但每个输入元素都可以映射到0或多个输出结果* groupByKey()：应用于(K,V)键值对的数据集时，返回一个新的(K, Iterable)形式的数据集* reduceByKey(func)：应用于(K,V)键值对的数据集时，返回一个新的(K, V)形式的数据集，其中的每个值是将每个key传递到函数func中进行聚合 2.行动操作12345678行动操作是真正触发计算的地方。Spark程序执行到行动操作时，才会执行真正的计算，从文件中加载数据，完成一次又一次转换操作，最终，完成行动操作得到结果。下面列出一些常见的行动操作（Action API）：* count() 返回数据集中的元素个数* collect() 以数组的形式返回数据集中的所有元素* first() 返回数据集中的第一个元素* take(n) 以数组的形式返回数据集中的前n个元素* reduce(func) 通过函数func（输入两个参数并返回一个值）聚合数据集中的元素* foreach(func) 将数据集中的每个元素传递到函数func中运行* ######3.惰性机制解释 1234567lines = sc.textFile(&quot;data.txt&quot;)lineLengths = lines.map(lambda s : len(s))totalLength = lineLengths.reduce( lambda a, b : a + b)1.第一行textFile读取文件构建一个RDD,textFile()只是一个转换操作,并不会直接将数据读到内存中,这时的lines只是一个指向这个文件的指针.2.map是一个转换操作,并不会立即计算每行的长度3.reduce是一个动作,这时就会触发真正的计算.spark会把计算分解和产能很多个小任务在不同的机器上运行,每台机器运行位于属于它的map和reduce.最后把结果返回给Driver. 4.持久化1234567891011由于RDD采用的是惰性求值的方法,每次遇到行动操作都会从头开始计算,当程序有多个行动时,这样的代价就会很大.为了解决这个问题,通过持久化(缓存)机制避免重复计算的开销.通过persisit()方法对RDD标记为持久化(当触发第一个行动操作后,会将计算结果持久化,持久化的后的RDD将会保留在计算节点的内存中被后面的行动操作重复使用unpersist()方法手动地把持久化的RDD从缓存中移除。list = ["Hadoop","Spark","Hive"]rdd = sc.parallelize(list)rdd.cache() //会调用persist(MEMORY_ONLY)，但是，语句执行到这里，并不会缓存rdd，这是rdd还没有被计算生成print(rdd.count()) //第一次行动操作，触发一次真正从头到尾的计算，这时才会执行上面的rdd.cache()，把这个rdd放到缓存中print(','.join(rdd.collect())) //第二次行动操作，不需要触发从头到尾的计算，只需要重复使用上面缓存中的rdd $rdd.foreach(print)或者rdd.map(print)打印输出$ $rdd.collect().foreach(print):将所有节点的数据打印,容易爆内存$ $rdd.take(100).foreach(print):打印RDD部分数据$ 5,键值对RDD123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102#首先创建键值对RDD---两种方法##方法1.读文件lines = sc.textFile("file:///usr/local/spark/mycode/pairrdd/word.txt")pairRDD = lines.flatMap(lambda line : line.split(" ")).map(lambda word : (word,1))pairRDD.foreach(print)##方法2.通过列表list = ["Hadoop","Spark","Hive","Spark"]rdd = sc.parallelize(list)pairRDD = rdd.map(lambda word : (word,1))pairRDD.foreach(print)#输出'''(Hadoop,1)(Spark,1)(Hive,1)(Spark,1)'''#-----------------------------常用的键值对转换操作------------------------------#reduceByKey()、groupByKey()、sortByKey()、join()、cogroup()等#==============================reduceByKey()==================================#用func函数合并具有相同键的值pairRDD.reduceByKey(lambda a,b : a+b).foreach(print) #a,b都表示键对应的value,表示按照键合并,将值相加'''输出(Spark,2)(Hive,1)(Hadoop,1)'''#==============================groupByKey()==================================#按照键进行分组pairRDD.groupByKey().foreach(print)'''输出(Spark,(1,1))(Hive,(1,))(Hadoop,(1,))'''#==============================keys()==================================pairRDD.keys().foreach(print) #&#123;“spark”,”spark”,”hadoop”,”hadoop”&#125;'''输出hadoopsparkhivespark'''#==============================values()==================================pairRDD.values().foreach(print)#&#123;1,2,3,5&#125;'''输出1235'''#==============================sortByKey()==================================pairRDD.sortByKey() #返回根据键排序的RDD'''输出(Hadoop,1)(Hive,1)(Spark,1)(Spark,1)'''#==============================mapValues()==================================pairRDD.mapValues(lambda x : x+1) #对RDD键值对的所有value做相同的处理'''输出(Hadoop,2)(Spark,2)(Hive,2)(Spark,2)'''#==============================join()==================================pairRDD1 = sc.parallelize([('spark',1),('spark',2),('hadoop',3),('hadoop',5)])pairRDD2 = sc.parallelize([('spark','fast')])pairRDD1.join(pairRDD2).foreach(print) #join默认内连接,相同的键才会返回'''输出('spark',1,'fast')('spark',2,'fast')'''#===================================实例===================================#计算每个键对应的平均值rdd = sc.parallelize([("spark",2),("hadoop",6),("hadoop",4),("spark",6)])rdd.mapValues(lambda x : (x,1)).reduceByKey(lambda x,y : (x[0]+y[0],x[1] + y[1])).mapValues(lambda x : (x[0] / x[1])).collect()'''输出:[('hadoop', 5.0), ('spark', 4.0)]'''collect()是一个行动操作，功能是以数组的形式返回数据集中的所有元素，当我们要实时查看一个RDD中的元素内容时，就可以调用collect()函数。 #####共享变量 1234目的:要在多个任务之间共享变量，或者在任务（Task）和任务控制节点（Driver Program）之间共享变量Spark提供了两种类型的变量：广播变量（broadcast variables）和累加器（accumulators）广播变量:用来把变量在所有节点的内存之间进行共享。累加器:则支持在所有不同节点之间进行累加计算（比如计数或者求和）。 1.广播变量12345678910111213141516171819通过广播方式进行传播的变量，会经过序列化，然后在被任务使用时再进行反序列化。broadcastVar = sc.broadcast([1, 2, 3])broadcastVar.value一旦广播变量创建后，普通变量v的值就不能再发生修改，从而确保所有节点都获得这个广播变量的相同的值。用途:经常需要把两个数据集组合起来获取结果数据集 方法1:可以直接以rdd形式连接两个数据集====&gt;但是可能会导致数据混洗(shuffle),代价很大 方法2:将小的数据集初始化为广播变量,原理是将将变量复制到所有节点上目的:进程间共享数据 要点: 1.使用广播变量避免了数据混洗 2.每个节点复制一份数据而非每个任务复制一次 3.广播变量可以被多个任务多次使用(广播变量的好处，不需要每个task带上一份变量副本，而是变成每个节点的executor才一份副本) 2.累加器1234通常可以被用来实现计数器（counter）和求和（sum）accum = sc.accumulator(0)sc.parallelize([1, 2, 3, 4]).foreach(lambda x : accum.add(x))accum.value RDD打印数据1231.rdd.foreach(print)2.rdd.collect() ==可能导致内存溢出3.rdd.take(100) ##### RDD运行原理—-阶段的划分和RDD的运行过程 1234宽依赖窄依赖 宽依赖：父RDD的分区被子RDD的多个分区使用(一对一,多对一) 窄依赖：父RDD的每个分区都只被子RDD的一个分区使用 (一对多)shuffle操作:洗牌 fork and join机制 ####standalone环境配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657Spark standalone部署配置1.在home路径下创建文件夹spark,然后cd spark-----&gt;这个文件夹需要所有用户都可以使用1.配置JAVA环境(1)在spark文件夹下,创建java文件夹,然后cd java(2)将压缩包jdk-15.0.1_linux-x64_bin.tar.gz 上传到java目录下(3)解压缩jdk包.使用命令tar -xvf jdk-15.0.1_linux-x64_bin.tar.gz 会出现一个文件夹jdk-15.0.1(4)cd jdk-15.0.1 进入文件夹下,使用pwd命令查看当前路径,并记住路径,下面要用(5)打开配置文件 vim /etc/profile(非管理 ~/.bashrc),加入下面配置,JAVA_HOME为(4)输出的路径JAVA_HOME=/home/spark/java/jdk-15.0.1CLASSPATH=$JAVA_HOME/lib:$JAVA_HOME/jre/libPATH=$PATH:$JAVA_HOME/bin:$JAVA_HOME/jre/bin(export JAVA_HOME=/home/spark/java/jdk-15.0.1export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 上面那个如果没成功试试这个)(6)source /etc/profile 保存修改(7)输入javac,未报错即成功配置java环境(8)执行两次cd .. 进入spark目录下2.配置spark环境(1)在spark目录下创建pyspark文件夹,mkdir pyspark(2)cd pyspark,然后将压缩包spark-3.0.1-bin-hadoop3.2.tgz 上传到pyspark目录下(3)解压缩tar -xvf spark-3.0.1-bin-hadoop3.2.tgz(4)cd spark-3.0.1-bin-hadoop3.2/conf ,进入文件夹,可看到以下文件(5)cp spark-env.sh.template spark-env.sh ,复制一个spark-env.sh文件(6)然后vim spark-env.sh,在末尾加入配置export PYSPARK_PYTHON=/usr/bin/python3(7)source spark-env.sh(8)打开配置文件 vim /etc/profile(非管理vim ~/.bashrc vi ~/.bash_profile),加入下面配置,SPARK_HOME为spark的安装路径SPARK_HOME=/home/spark/pyspark/spark-3.0.1-bin-hadoop3.2export PATH=$PATH:$SPARK_HOME/binexport PYSPARK_PYTHON=python3(9)source /etc/profile(10)pip3 install pyspark -i https://pypi.tuna.tsinghua.edu.cn/simple(11)输入pyspark,进入下面的页面即成功注意:1.需要在227,231和217分别进行相同的配置2.创建的文件夹spark,需要所有用户均可操作(用以启动服务,跑代码)当三台全部配置好的时候: 进入217:1.cd /home/spark/pyspark/spark-3.0.1-bin-hadoop3.2/sbin2.执行 ./start-master.sh 进入231:1. cd /home/spark/pyspark/spark-3.0.1-bin-hadoop3.2/sbin2. 执行 ./start-slave.sh spark://192.168.0.217:7077 进入227:1. cd /home/spark/pyspark/spark-3.0.1-bin-hadoop3.2/sbin2. 执行 ./start-slave.sh spark://192.168.0.217:7077export JAVA_HOME=/home/kuailiang/2020/java/jdk1.8.0_281export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 问题1234567891011121.Spark在计算的过程中，是不是特别消耗内存？ 不是。Spark是在管道中计算的，而管道中不是特别耗内存。即使有很多管道同时进行，也不是特别耗内存。2.什么样的场景最耗内存？ 使用控制类算子的时候耗内存，特别是使用cache时最耗内存。3.如果管道中有cache逻辑，他是如何缓存数据的？ 有cache时，会在一个task运行成功时（遇到action类算子时），将这个task的运行结果缓存到内存中4.RDD（弹性分布式数据集），为什么他不存储数据还叫数据集？ 虽然RDD不具备存储数据的能力，但是他具备操作数据的能力。5.如果有1T数据，单机运行需要30分钟，但是使用Saprk计算需要两个小时（4node），为什么？ 1）、发生了计算倾斜。大量数据给少量的task计算。少量数据却分配了大量的task。 2）、开启了推测执行机制6. 运行spark程序12345sc.master可以查看当前的运行模式1.本地运行pyspark程序pyspark --master local[4] ====&gt;local[4]表示在本地运行,使用四个线程,local[*]表示尽可能多的使用核心 pyspark --master spark://192.168.0.217:7077 暂时有问题不知道是不是231ip问题,后面重启后在尝试一下 pandas的DF和spark的DF对比123456两者的异同：Pyspark DataFrame是在分布式节点上运行一些数据操作，而pandas是不可能的；Pyspark DataFrame的数据反映比较缓慢，没有Pandas那么及时反映；Pyspark DataFrame的数据框是不可变的，不能任意添加列，只能通过合并进行；pandas比Pyspark DataFrame有更多方便的操作以及很强大 1234有效专利:针对一件专利,从申请日期开始,一直到失效日期都是有效的,若专利没有失效日期,需要结合当前状态去判断,如果当前状态为授权状态/再审状态,按照最大年限去认定失效日期,发明为20年,新型为10年.如果当前状态为失效,则判断不了具体失效时间,不做统计(总共只有一条).,没有失效日期的按照最大年限去考虑,如发明专利,从申请日期开始往后20年都是有效的 spark sql架构123451.列式存储2.dataframe api3.DAG部分执行(pde),让我们在执行时根据处理过程中发现的一些数据动态修改和优化DAG. jupyter notebook连接spark123456import osimport sysspark_name = '/home/spark/pyspark/spark-3.0.1-bin-hadoop3.2'sys.path.insert(0,os.path.join(spark_name,'python'))sys.path.insert(0,os.path.join(spark_name,'python/lib/py4j-0.10.9-src.zip'))exec(open(os.path.join(spark_name,'python/pyspark/shell.py')).read())]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[距离和相似度]]></title>
    <url>%2F2020%2F12%2F04%2F%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[综合:​ 距离和相似度是用来判断相似性两种不同的方法 距离(物体在空间中的距离,距离越远越不相似)123456789101112131415161718191.欧式距离:计算的是空间位置的绝对路径,要求是指标维度的度量要一致,如kg和cm会出问题 1.1欧氏距离标准化 -- &gt;对各个维度数据进行标准化,使之在同一度量上 2.曼哈顿距离:2.明式距离 p = 1 曼哈顿 p = 2 欧式 p -&gt; 无穷 契比雪夫 3.切比雪夫距离 国际象棋中国王的走路,每次只能在八个格变化 明式距离P趋近无穷时 4.马氏距离 欧式距离,指标维度不同是,需要进行归一化,归一化后即是马氏距离 5.海明距离 两个等长字符串,相同位置字符不同的个数----&gt;引入,物体压缩成字符串,进行比较 相似度(相似程度,越大越好)1234567891011121314151.余弦相似度: 注重两个向量在方向上的差异，而非距离或长度上,如,在指定夹角里,两点距离可无限远,忽略大距离的影响 2.调整余弦相似度: 3.皮尔逊系数: 自身向量标准化后计算空间向量的余弦夹角 4.jaccard 系数: 5.广义Jaccard系数:6.互信息/信息增益，相对熵/KL散度6.TF-IDF等: 距离和相似度的区别123距离衡量的是空间中的距离,与坐标位置直接相关,而相似度体现的是体现方向上的差异距离:维度的数值大小中体现差异的分析相似度:对绝对的数值不敏感 欧式和余弦的区别:1234567891011121314一致性:衡量个体间差异的大小定义: 欧氏距离:向量在空间中的距离 余弦相似:相似度夹角 差别: 欧氏距离:对数值敏感 余弦相似:对偏向敏感 余弦夹角可以有效规避个体相同认知中不同程度的差异表现，更注重维度之间的差异，而不注重数值上的差异 总结:一个更侧重于偏向,如x轴,y轴.(余弦) 宁一个更侧重数值差异性,值的相似程度 核心节点专利图:1234567#涉及到距离的部分总共涉及两个1.计算专利和专利的相似度 1)上期计算是通过h5文件直接读取专利与专利之间的相似度 2)本期改版,通过专利的词向量来计算,使用余弦相似度 2.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql实战45讲]]></title>
    <url>%2F2020%2F11%2F18%2Fmysql%E5%AE%9E%E6%88%9845%E8%AE%B2%2F</url>
    <content type="text"><![CDATA[第一讲 Mysql查询执行流程(笔记来自极客时间)1.mysql架构 2.架构解释​ ①mysql架构由两部分组成,server端和存储引擎 ​ ② 连接器:建立连接,验证权限,维持和管理连接 ​ 分析器:首先分析每个词代表的含义,然后分析整个sql语句的语法 ​ 查询缓存:键值对形式,sql语句对应数据缓存,命中sql,则直接返回数据。 ​ 优化器:join时优先表的选择,及多索引优先索引选择 ​ 执行器:表权限验证,操作引擎获取数据 ​ ③ 其他补充: ​ 1)查询缓存在8.0版本取消,查询缓存并没有想象中那么好用,命中率很低,而且表变化后缓存就会清空,适合静态表,很久才更新的那种 ​ 2)如果查询缓存命中的时候,会在返回数据的时候做权限验证,验证是否有这张表的权限 ​ 3)执行器,在执行sql前先判断是否对表有执行查询的权限,有则从引擎取数据 第二讲 Mysql更新记录操作流程总体:1.查询的执行过程同查询过程一致2.一个表的数据更新的时候,跟这个表相关的所有缓存都会消失3.不同于查询,更新涉及到两个日志模块。redo log和binlogredo log:(保证数据不会丢失)—-&gt;innodb特有​ 1.目的:每次更新都需要写到磁盘,然后磁盘也要找到那条记录然后更新数据,这样导致整个IO成本和查找成本都很高.同时redo log ​ 2.日志和磁盘和结合,即所说的WAL技术(write ahead logging),即先写日志,再写磁盘(不忙的时候),日志写满则开始写入磁盘 ​ 补充:日志存储在磁盘里,写日志用的是顺序IO,更新操作用的是随机IO,而顺序IO比随机IO 快很多(随机IO要寻址) ​ 3.crash safe能力:数据库异常重启时,不会丢失数据 binlog:(server层特有)—-&gt;所有引擎共用​ 1.没有crash_safe能力 ​ 2.为什么会有两个日志?因为本来mysql 并没有innodb引擎,所以只有binlog日志,后来innodb作为插件引入到mysql,由于binlog没有crash-safe能力,所以引入了新的日志系统redo-log.binlog用于归档 redolog和binlog的区别​ 1.redo是innodb引擎特有的,而binlog是所有引擎共有的 ​ 2.redo是物理日志,记录在某个&lt;数据页&gt;做了什么修改,binlog是逻辑日志.记录了sql ​ Binlog有两种模式，statement 格式的话是记sql语句， row格式(常用)会记录行的内容，记两条，更新前和更新后都有。 ​ 3.redo log是循环写,空间有限制,写完了就写入磁盘然后重新写.而binlog是追加写,一个文件达到一定大小后,开始下一个 update流程 1.获取id=2这行判断是否在缓存里.若果是则直接获取,不是则从磁盘读取. 2.将这行数据的c+1 3.写入新行 4.引擎将这行数据更新到内存中,同时记录在redo log中,次数redo log处于prepare阶段,随时可提交事务 5.执行器生成这个操作的binlog,并将binlog写入磁盘 6.执行器调用引擎执行事务的接口,redo log处于commit阶段—&gt;两阶段提交为了让binlog和redo log的逻辑一致 binlog数据恢复过程1.假如无删除操作,恢复数据需要最近一次的全量备份数据和现在的binlog,但是需要将binlog里无删除操作去掉,然后将binlog放到旧数据的指定位置.这样就恢复了 为什么要分两段提交以及提交的顺序(这里不使用分段提交)​ 1.先写redo log 后写bin log ​ 假设redo log 写完后,系统重启,这时候binlog还没写,redo log 写过,这样当使用binlog恢复数据时,就会出现数据库少了一次更新 ​ 2.先写bin log 后写 redo log ​ 假设在写完binlog时候crash,由于redo log没有提交,则重启后事务无效,但是binlog已经更改,这样,当恢复数据的时候就会出现多了一次事务 第三讲 事务隔离事务:保证对数据库操作的一致性,一组操作全部成功或者全部失败1.Innodb支持事务而Mysiam不支持2.事务的隔离性和隔离级别​ ①事务的特性:ACID,原子性,一致性,隔离性,持久性 ​ ②多事务可能出现的问题:脏读,幻读,不可重复读 ​ 脏读:读到其他事务未提交的数据 ​ 幻读:前后读取的记录数量不一致 ​ 不可重复读:读取前后的数据不一致 ​ ③事务的隔离级别(为了解决上面的多事务问题):读未提交,读已提交,可重复读,序列化(串行化) ​ 读未提交(RU):一个事务没提交,但是变更可被其他事务看到 ​ 读已提交(RC):一个事务提交后,变更才能被其他事务看到 ​ 可重复读(RR):事务在执行过程中读到的数据和事务启动时的数据一致 ​ 串行化(Serial):指的是会给同一行加锁.读锁和写锁.当读写冲突时,后面的事务需要等前一个事务结束后才行 ​ $\textcolor{red}{总结:}$ ​ 1.在实现上,主要是通过MVCC视图(数据库会创建),但是针对RU是没有是没有视图概念的,直接返回记录上的最新值 ​ 2.针对RC,是在每一个SQL语句开始执行的时候创建的视图 ​ 3.RR是在事务启动时候创建的视图 ​ 4.Serial直接用加锁的方式避免并行,无视图概念 ​ $视图的概念:$视图是一种虚拟存在的表，是一个逻辑表，本身并不包含数据。是基于 SQL 语句的结果集的可视化的表，可以包含表的全部或者部分记录，也可以由一个表或者多个表来创建.实际上是在数据库里执行了SELECT语句，SELECT语句包含了字段名称、函数、运算符，来给用户$\textcolor{red}{显示数据}$。 2.事务隔离的实现​ 在数据更新的时候mysql的redo log会做一个变更记录，同时与变更相反的回滚操作记录会记录在undo log上 ​ 例:回滚日志记录 ​ 原操作是从1到2再到3再到4.当前值为4,但是不同时刻启动的事务会有不同的视图.同一条记录在系统中可以存在多个版本,这就是数据库的多版本并发控制(mvcc). ​ 回滚日志并非一直并非一直存在,当系统中没有比日志更早的视图的时候,回滚日志就会被删除 ​ 所以尽量避免长事务.长事务导致undo log一直存在不会被删除.且会占用大量的空间 第四讲深入浅出索引(一种数据结构)1.常见的索引模型12345678910111.哈希表:一种以键值存储数据的结构,原理是当传入一个键的时候,会通过哈希函数返回一个位置,直接将位置的value取出即可.但是存在不同的键经过哈希函数以后返回相同的value.处理这种情况是拉开一个链表。Mysql的hash索引是把索引字段经过hash函数计算得出hash码存放到key里面，value里面存放的是对应记录的位置，遇到相同的值或者计算出相同的hash码，就会在那个hash码上使用链表方式解决相同hash码的问题。----------------适合做等值查询，范围查询效率低-------------------2.有序数组:用二分法操作的，对于等值查询和范围查询性能都很优秀，但是更新数据非常麻烦，中间插入数据，后面所有的记录都得往后挪动，成本高3.搜索树: 二叉搜索树：父节点左子树所有节点的值小于右节点，但是大多数的数据库很少用二叉树，因为索引不仅存在内存中还要写到磁盘上。（高度不可控，IO特别大） 多叉搜索树(B+树)：相对于二叉是为了减少IO。由于在读写上的性能优点，以及适配磁盘的访问模式，被广泛应用在 $\textcolor{red}{总结:}$数据库的底层核心就是这些数据模型,碰到一个新的数据库需要关心它的数据类型,这样才能在理论上分析数据库的适用场景 2.Innodb的索引模型在 InnoDB 中，每一张表其实就是多个 B+ 树，即一个主键索引树和多个非主键索引树 $\textcolor{red}{使用B+树的原因}$B+ 树能够很好地配合磁盘的读写特性，减少单次查询的磁盘访问次数。 主键索引： key:主键的值，value:整行数据。 普通列索引： key：索引列的值， value:主键的值。 $\textcolor{red}{回表}$ 1234ID为主键索引,k为普通索引select * from T where ID=500，即主键查询方式,则只需要搜索 ID 这棵 B+ 树select * from T where k=5，即普通索引查询方式,则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。 3.索引维护1B+Tree为了维护索引的有序性,在插入新值的时候需要做必要的维护.(上图,比如数据现id为700,添加新的行的id为700,则只需要在r5后面记录插入的新纪录,如果插入的值为400,则需要挪动后面的数据,空出位置.假如R5的数据页已经满了,则需要在申请一个新的数据页,然后挪动数据过去---&gt;页分裂.---&gt;影响数据页的利用率,原本一个页的数据放到了) 第六讲:全局锁和表锁123锁涉及的初衷:处理并发问题分类:根据加锁范围分为全局锁,表级锁,行锁 全局锁123456说明:给整个数据库实例加锁状态:如全局读锁,整个数据库只允许读,不允许改和删等使用场景:全库逻辑备份风险：1.如果在主库备份，在备份期间不能更新，业务停摆2.如果在从库备份，备份期间不能执行主库同步的binlog，导致主从延迟 表级锁1234分类:表锁,元数据锁(MDL)对于InnoDB这种支持行锁的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面还是太大在对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。读锁之间不互斥。读写锁之间，写锁之间是互斥的，用来保证变更表结构操作的安全性。 行级锁12行锁是各个引擎自己实现的,并不是所有的引擎都支持行锁,不支持的只能使用表锁Innodb支持行锁,Mysiam不支持行锁.使用的是表锁,任何时候只能有一个更新在执行,影响了并发度.适合读多写少的操作 linux 装mysql5.7 12https://www.suibibk.com/topic/721421244804628480 12 ​]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pypy]]></title>
    <url>%2F2020%2F09%2F24%2Fpypy%2F</url>
    <content type="text"><![CDATA[pypy install12345678vim ~/.bashrcexport PATH=/home/kuailiang/pypy3.6-v7.3.1-linux64/bin:$PATHsource ~/.bashrcpypy3 -m ensurepip curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py安装模块: pypy3 -m pip install requests pip3出现问题时候 12hash -r清除缓存后即可。 查看代码耗时123%timeit results_data[&apos;five_part_divide&apos;] = results_data.sm.apply(lambda x:five_part_divide(x))%prun -l 4 results_data[&apos;five_part_divide&apos;] = results_data.sm.apply(lambda x:five_part_divide(x)) #展示最耗时的四个操作,内置魔法方法]]></content>
      <categories>
        <category>优化</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C语言学习]]></title>
    <url>%2F2020%2F09%2F08%2FC%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[1.helloworld代码1234567#include &lt;stdio.h&gt; /*头文件,C语言的一个文件,主要用于关键词输入和显示输出的功能,这里的作用是键入stdio的所有内容*/int main(void) /*int 表示返回的类型为整形,main为函数名,void表示不传任何参数*/&#123; int num; num = 1; printf("helloworld%d次",num);&#125; ######helloworld注解 123451.注释用法/**/ 或者//2.main函数是c程序的基础,所有c程序都是从main函数开始的3.花括号&#123;&#125;表示函数的主体4.void可写在括号里,也可以卸载main前面,推荐使用前者,后者有些编译器支持,有些不支持5.所有变量使用前必须先定义 C程序的构成12程序是由一个或者多个函数组成,其中必须有一个main函数,函数由函数头和函数体构成,函数头包括预处理(指的是导入的文件)和函数名,函数体位于花括号中并由一系列语句组成,每个语句由分号结束,最后main函数由return结束.2.算子:指参与运算的值 如 2+3,2和3就是算子 2.多个函数12345678910111213#include &lt;stdio.h&gt;void fun(void); /*通知编译器要用到该函数,第一个void表函数没有返回值,第二个void表示函数没有参数*/int main(void)&#123; printf("我进来了"); fun(); /*调用该函数*/ printf("结束了"); return 0; &#125;void fun(void) /*声明函数*/&#123; printf("我是66啊");&#125; 3.c语言关键字12345678910111213141516171819202122232425262728293031323334353637auto casebreakcharconstcontinuedefaultdodoublewhileelseenumextermfloatforgotoifinlineintlongregisterrestrictreturnshortsignedsizeofstaticstructswitchtypedefunionunsignedvoidvolatile_bool_complex_imaginary #####3.体重 12345678910111213#include &lt;stdio.h&gt;int main(void)&#123; float weight; float value; printf("请输入你的体重"); scanf("%f",&amp;weight); value = 770 * weight * 14.5833; return 0;&#125;/*如果scanf报错需要将属性中的常规里的c++/c中的sdl检查改成否*/scanf()函数提供键盘输入功能,%f表示输入的是浮点数,&amp;weight表示将输入的值赋值给weight 基本数据类型1234567891011121314151617整数:整数以二进制存储浮点数:存储方式是将整数部分和小数部分分开存储int有三个修饰符,long short unsigned short int :占用可能比int更少的空间,适用于小数值存储(不同机器,short可能和int占用一样,如16位) long int : 可能占用比int更多的内存,适用于大数值的场合 long long int:比long int更大的的内存和存储空间 unsigned int :无符号整形,正数,范围0-65535,int 为有符号,范围-32768到32767 现在一般情况: longlong 64位,long 32位,short 16位,int 16或者32位当数值超过类型的表示范围时,即溢出,会从头开始有符号从负的开始,无符号从0开始char:(%c) char grad = 'a'; 将a转换成相应的编码值 char grad = "a"; 将a当成字符串float/double: 精度不同float至少6位,double至少10位 sizeof123456789c内置运算符,以字节为单位打印类型的大小在c中char就占一个字节,因为c把char类型的长度定义为一个字节#include &lt;stdio.h&gt;int main(void)&#123; char num = 2; printf("num占用%u",sizeof(num)); return 0;&#125; #####4.年纪 123456789101112//根据年纪算出过了多少秒#include &lt;stdio.h&gt;int main(void)&#123; int age; double total; printf("请输入你的年龄\n"); scanf("%d", &amp;age); total = age * 3.156e7; printf("共经过%f秒", total); return 0&#125; 5.字符串 交流12345678910111213141516171819202122#include &lt;stdio.h&gt;#include &lt;string.h&gt; //提供strlen的原型#define DENSITY 62.4 //定义符号常量DENSITYint main(void)&#123; float weight,value; int size,letters; char name[40]; //数组存放40个字节,每个字节一个字符串 printf("请输入你的姓名\n",&amp;name); scanf("%s", name); printf("请输入你的体重\n",weight); scanf("%f", &amp;weight); size = sizeof(name); letters = strlen(name); //获取字符串的长度 value = weight / DENSITY; printf("名字占%u字节", size); printf("名字长度为%d", letters); printf("人的质量%f", value); return 0;&#125;//sizeof给出数据的大小,strlen给出字符串的长度,因为一个字符占一个字节可以看成相同的结果 6.字符串简介1234567891.双引号不是字符串的一部分,而是通知编译器其中包含一个字符串,单引号代表的是一个字符2.c没有字符串定义专门的变量类型,而是把它存在char数组中3.%s告诉printf()要打印一个字符串4.scanf会在遇到的第一个空白字符串处停止5.字符串常量x和字符常量x不同.字符串常量x属于基本类型char,而字符串常量x属于派生类(char数组)6.char字符,因为char字符对应ascii码,而八位最够对应所有的ascii码,所以一位足够7.string.h文件里包含许多与字符串相关的函数的原型8.c中的char表示一个字符,要表示一个字符序列,c使用字符串,字符串的一种形式是字符常量,用双引号括起来,也可以在字符数组中存储一个个字符,相邻的字符构成字符串.9.程序中最好使用字符常量使用#define或者const表示,使得程序可读性更强,更利于修改维护 7.常量123456789101112131415161718192021222324252627282930313233343536371.第一种定义常量,#define pi 3.1415 ,当程序编译时,会将所有出现pi的地方替换成pi的值3.1415 (注意:这里没有等号,且通常使用大写)2.#define也可以支持定于字符和字符串变量,前者单引号,后者双引号 #define TEE 'T' #define OPS "now you have done it"3.第二种定义常量,const(比define灵活) const int MONTHS = 12;4.第三种,枚举(enum)第一种代码:#include &lt;stdio.h&gt;#include &lt;string.h&gt;#define PI 3.14int main(void)&#123; int r; float size; printf("请输入圆的半径\n"); scanf("%d", &amp;r); size = PI * r * r; printf("圆的面积为%.2f", size); //保留两位小数 return 0;&#125;第二种代码:#include &lt;stdio.h&gt;const float PI = 3.14;int main(void)&#123; int r; float size; printf("请输入圆的半径\n"); scanf("%d", &amp;r); size = PI * r * r; printf("圆的面积为%.2f", size); return 0;&#125; 8.查看整形和浮点型的大小限制123456789//用到了两个头文件,limits.h---&gt;查看整形和float.h---&gt;查看浮点型#include &lt;stdio.h&gt;#include &lt;limits.h&gt;#include &lt;float.h&gt;void main(void)&#123; printf("整形最大值%d和最小值%d",INT_MAX,INT_MIN); printf("浮点型正数最大值%f和最小值%f",FLT_MAX,FLT_MIN);&#125; printf(),scanf()转换说明符123456789101112131415161718192021222324252627%d:有符号十进制整数%s:字符串%u:无符号十进制整数%c:字符%f:有符号十进制浮点数%%:一个%%p:指针其他...#include &lt;stdio.h&gt;const int value = 120;void main(void)&#123; printf("*%d*\n",value); printf("*%2d*\n",value); printf("*%10d*\n",value); //不够10位,左补到10位,空格 printf("*%-10d*",value); //不够10位,右补到10位,空格 printf("*%010d*\n",value); //不够10位左边不够的补0&#125;//printf参数传递解释/*printf("%1d %1d %1d %1d\n",n1,n2,n3,n4);原理:该调用告诉计算机.n1,n2,n3,n4被调用,然后计算机将他们放入到堆栈中,计算机根据变量的类型而非转换说明符将这些值放到堆栈中,每个数占据一定的字节,然后printf函数将数据一个一个读出来prinft作为函数也有返回值,很少被用到,作为打印输出的附带功能前面的转换说明和后面的变量类型要一致,否则会产生奇怪的结果*/ 循环12345678910111213141516/*计算多个鞋对应的英寸尺度 */#include &lt;stdio.h&gt;int main()&#123; const double SCALE = 0.325; const double ADJUST = 7.64; double shoe, foot; shoe = 3.0; while(shoe &lt; 19.5) &#123; foot = SCALE* shoe + ADJUST; printf("鞋的尺码为%.2f,英寸为%.2f\n", shoe, foot); shoe += 1.0; &#125; return 0;&#125; 运算符1234561.+-*/ 四则运算符2.= 赋值运算符3.sizeof 所占字节大小4.++ 两种形式a++ 和 ++a5.--6.%取余 ++和–1234567891011121314151617181920212223/*a++和++a的区别,a++是用过a的值之后加1,而++a是用a前先给他加一,区别见第二个案例*/#include &lt;stdio.h&gt;int main()&#123; int x=0, y=0; while (x &lt; 10) &#123; x++; ++y; printf("x = %d,y = %d\n", x, y); &#125; &#125;#include &lt;stdio.h&gt;int main()&#123; int a = 2; while (a++ &lt; 19) //当这里使用a++时,是先与19比较后才加1,结果就是a++会多打印一个19 //而++a是在与19去比较前先优化 &#123; printf("a的值为%d\n", a); &#125;&#125; 类型转换12 函数传参1234567891011121314151617#include &lt;stdio.h&gt;void fun(int n);int main()&#123; int num; while (num &lt; 10) &#123; printf("请输入一个num\n"); scanf("%d",&amp;num); fun(num); &#125; return 1;&#125;void fun(int n)&#123; printf("num是%d\n", n);&#125; if条件1234567891011121314151617#include &lt;stdio.h&gt;int main()&#123; int num = 0; while (++num &lt; 10) &#123; printf("当前num值为%d\n", num); if (num == 6) &#123; printf("66大顺啊!\n"); &#125; else &#123; printf("%d不怎么样,你觉得呢?\n",num); &#125; &#125;&#125; for循环12345678#include &lt;stdio.h&gt;int main(void)&#123; int num; for (num = 10;num &lt;= 20;num++) printf("num是%d\n", num); &#125; do while循环12]]></content>
      <categories>
        <category>C语言</category>
      </categories>
      <tags>
        <tag>C语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dask]]></title>
    <url>%2F2020%2F08%2F10%2Fdask%2F</url>
    <content type="text"><![CDATA[DaskDask是一款用于分析计算的灵活并行计算库。 Dask是一个并行计算库，能在集群中进行分布式计算，能以一种更方便简洁的方式处理大数据量，与Spark这些大数据处理框架相比较，Dask更轻。Dask更侧重与其他框架，如：Numpy，Pandas，Scikit-learning相结合，从而使其能更加方便进行分布式并行计算 入门Dask DataFrame很容易，但是要很好的使用它需要一些经验。 Dask DataFrame是由许多较小的Pandas DataFrame组成的大型并行DataFrame 当内存无法容纳数据时，可以使用数据分块的方法：以数据块的形式分批加载到内存进行处理。这样就可以通过加载数据集的子集，来逐步处理整个数据集。 Dask存在三种最基本的数据结构，分别是：Arrays、Dataframes以及Bags 安装:pip install dask 1234import pandas as pdimport dask.dataframe as ddtmp = pd.DataFrame(&#123;'name': range(10), 'content': [range(i) for i in range(10)]&#125;)ddf = dd.from_pandas(tmp, npartitions=1) dask.array 1Dask中的Arrays（位于包dask.arrays下），其实就是对Numpy中的ndarray的部分接口进行了改进，从而方便处理大数据量。对于大数据集，特别是其大小大于内存时，如果我们要对其计算，按照传统的方式，，我们会将其全部塞进内存里，那么这就会报Out-Of-Memory错误，当然，我们也可以一次读取一部分数据，那么我们是否可以提前将大数据集进行分块处理了，我们只需要控制每块数据集不超过内存，从而满足In-Memory计算了？Dask就是这样做的 dask.dataframe(调用Pandas API) 12345Dask Dataframe对象则 在处理远大于当前主机内存的表格数据有用。与传统pandas Dataframe在加载完成所有数据后继续数据类型推断不同Dask Datadrame支持部分加载数据时，对表格数据类型进行推断。Dask Dataframe实现了分块并行Dataframe, 对Dask Dataframe的操作将被映射到按索引列划分的子Dataframe上在Pandas上运行缓慢的操作（例如逐行迭代）在Dask DataFrame上仍然运行缓慢处理大型数据集，即使这些数据集不适合存储在内存中通过使用多个内核来加速长计算使用标准的Pandas操作（例如groupby，join和时间序列计算）在大型数据集上进行分布式计算 dask.bag 12对于Bags，其最主要的是用于半结构化的大数据集，比如日志或者博客等等，我们从其read_text（dask.bag.text.py）中来解析如何创建一个Dask Bag对象Dataframe是基于Pandas Dataframe改进的一个可以并行处理大数据量的数据结构，即使对大于内存的数据也是能够处理的 四、Dask分布式 dask.delayed 和 dask.bag1234delayed :延迟计算,并不是立即计算,而是将关系绘制成一个图,可以使用visualize查看并行的可能性,然后使用compute立刻开始计算bag:相当于spark中的rdd操作,bg.from_sequence([1,2,3,4,5], npartitions=5).map(lambda x:x**2) Dask创建和存储数据框1234567891011读: read_csv 将CSV文件读取到Dask.DataFrame中 read_json 从一组JSON文件创建数据框 read_sql_table 从SQL表创建数据框。 from_pandas 从Pandas DataFrame构造Dask DataFrame dask不能读excel存: to_csv 将Dask DataFrame存储到CSV文件 to_sql 将Dask数据框存储到SQL表 to_json 将数据框写入JSON文本文件其他见:https://docs.dask.org/en/latest/dataframe-api.html #####Dask的Sql 12341.Dask不支持任意文本查询,仅支持整个表和sqlalchemyimport dask.dataframe as dden = 'mysql+pymysql://user_rw:1a2s3d4f@192.168.0.251:3306/pre_formal_2?charset=UTF8mb4'dd.read_sql_table("A61_single_us", en, index_col='id',npartitions=2) 默认情况下，Dask DataFrame使用多线程调度程序。 Dask是为处理大于内存的数据集而设计的 2021dask分布式12345671.python -m pip install dask distributed --upgrade2.在某个节点启用调度节点,dask-scheduler3.dask-worker 192.168.0.227:8786 --nprocs 20 --nthreads 2 创建工人,并指向调度节点--memory-limit 2e10 设置内存若启动失败:pip3 install click==7.1.2 使用#####同时使用某函数——&gt;等价分布式 123456789101112131415161718192021222324252627282930313233343536373839404142#==================================1=================================from dask.distributed import Clientclient = Client('192.168.0.227:8786') #使用分布式def test(x): ls = [] for i in range(x): for j in range(x): ls.append(i+j) print(sum(ls)) return sum(ls)a = client.map(test, [10000,10,20,100,50])client.gather(a)client.restart() #重启client.close() #关闭#==================================2=================================from dask.distributed import Clientclient = Client() #使用当前节点多进程from datetime import datetimeimport osimport pandas as pddef func(num): t1 = datetime.now() ls = [] for i in range(num): for j in range(num): ls.append(i+j) print(num,datetime.now()-t1,os.getpid()) return pd.DataFrame(range(num)).sum().values[0]a = client.map(func,range(2000,3000,10))client.gather(a)#====================================3================================from dask.distributed import Client, progresscl(n_workers=20,threads_per_worker=3) #单机设置 #传参[dask_cl.submit(func_patent_expect_life,dic,i) for i in group_name_list][future] = dask_cl.scatter([df_data.values],broadcast=True) #广播变量,将数据放到调度器中 #replicate复制数据到节点client_dask.replicate([group_name_num]) 分布式传参案例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899# -*- coding: utf-8 -*-# author = 'kl'# create_date: 2021/10/14import pandas as pdfrom itertools import chainfrom clickhouse_driver import Clientimport warningswarnings.filterwarnings("ignore")from dask.distributed import Client as clfrom datetime import datetimedef func_bfpm_history(company_list_short, group_name_num, jiqun_company, year,dic_com): try: group_name_num = pd.DataFrame(group_name_num,columns=['applicant_name','group_names','group_num']) jiqun_company = pd.DataFrame(jiqun_company,columns=['group_name','applicants']) storage_table = 'history_jsbj' + '_20211008' data_all = [] company_list_short_ = [company_list_short[i:i+50] for i in range(0,len(company_list_short),50)] for companys in company_list_short_: df_100 = group_name_num[group_name_num.applicant_name.isin(companys)] df_100['applicants'] = df_100.group_names.apply(lambda x: list(set(chain(*jiqun_company.loc[jiqun_company['group_name'].isin(x)].applicants.tolist())))) df_100_split = df_100.explode('applicants') df_100_split['num_com'] = df_100_split['applicants'].apply(lambda x: dic_com[x] if x in dic_com else 0) df_res = df_100_split.groupby('applicant_name').apply(lambda x: round(len(x[x.num_com &gt;= x.group_num.values[0]]) / x.shape[0],4)).reset_index() df_res.columns = ['applicant_name','bf_pm'] df_res['year'] = year df_final = df_res.merge(df_100[['applicant_name','group_names','group_num']],on=['applicant_name'],how='left') df_final.loc[df_final.bf_pm == 1,'bf_pm'] = 0.9999 df_final.loc[df_final.bf_pm == 0, 'bf_pm'] = 0.0001 data_all.append(df_final) data_final = pd.concat(data_all) data_final.rename(columns = &#123;'group_names':'groups'&#125;,inplace=True) data_final_tuple = [tuple(i) for i in data_final.values] client = Client(host='192.168.0.170', port='9000', user='algorithm', password='1a2s3d4f', database='algorithm_dis') sql = "insert into algorithm_dis.&#123;&#125; (`applicant_name`,`bf_pm`,`year`,`groups`,`group_num`) VALUES".format(storage_table) client.execute(sql, data_final_tuple) client.disconnect() print('存完l') return 1 except Exception as e: print(e)if __name__ == '__main__': n = 10000 # 10000 开一个进程 table_tail = '_20210908' # 版本表后缀(开始计算的时间或者) company_formername = 'company_formername_20210908' # 企业最新名跟曾用名的唯一对应 这个后期会有变化，每次都记得调整 similer_company = 'similer_company' # 企业名称别名 es_time = '20210901' # es更新截至的时间 用于计算多个时间相关的数据 country_code = 'country_code' # 国家缩写表 ipc_split_10000_c_list_v = 'ipc_split_10000_c_list_v3' # 集群内ipc表 history_group_patent_num = 'history_group_patent_num' + table_tail zl_zu_L = 'zl_zu_L' + table_tail t1 = datetime.now() for year in range(1985, int(es_time[:4]) + 1): t0 = datetime.now() try: ## 获取每个年份内企业的集群数 client = Client(host='192.168.0.170', port='9000', user='algorithm', password='1a2s3d4f', database='algorithm_dis') sql = ''' SELECT applicant_name, arrayDistinct(arrayFlatten(groupArray(group_name))) AS `group_names`, length(group_names) AS group_num FROM ( SELECT arrayJoin(applicants) AS applicant_name, group_name FROM &#123;&#125; WHERE (applicant_name GLOBAL IN ( SELECT arrayJoin(arrayDistinct(arrayFlatten(groupArray(applicants)))) FROM &#123;&#125; WHERE year = &#123;&#125; GROUP BY year )) AND (toYear(app_date) &lt;= &#123;&#125;) ) GROUP BY applicant_name '''.format(zl_zu_L, history_group_patent_num, year, year) group_name_num = client.execute(sql, columnar=False, with_column_types=True) group_name_num = pd.DataFrame(group_name_num[0], columns=[i[0] for i in group_name_num[1]]) dic_com = dict(zip(group_name_num.applicant_name, group_name_num.group_num)) sql = "select group_name,applicants from &#123;&#125; where `year` = &#123;&#125;".format(history_group_patent_num, year) jiqun_company = pd.DataFrame(client.execute(sql, columnar=False), columns=['group_name', 'applicants']) client.disconnect() company_list = group_name_num['applicant_name'].unique().tolist() company_list_short = [company_list[i:i+1000] for i in range(0,len(company_list),1000)] client_dask = cl('192.168.0.227:8786') [future] = client_dask.scatter([group_name_num.values], broadcast=True) [future2] = client_dask.scatter([jiqun_company.values], broadcast=True) [future3] = client_dask.scatter([dic_com], broadcast=True) a = client_dask.map(func_bfpm_history,company_list_short, [future]*len(company_list_short), [future2]*len(company_list_short), [year]*len(company_list_short), [future3]*len(company_list_short)) client_dask.gather(a) client_dask.close() except Exception as e: print(e) print(year,'完成,耗时:',datetime.now() - t0) print('总耗时:',datetime.now() - t1) ` 1234client.persist(df) #持久化dataframe,主要针对大的数据集client.compute() #处理较小的结果集要点:pandas基于内存的计算和分布式dask混合使用 #####pandas groupby 优化 123456789101112131415161718192021222324252627281.multiprocessing2.joblib3.parallel_apply=======================delayed===================================from joblib import Parallel, delayedimport multiprocessingt1 = datetime.now()def beta_cal_mult(one_fund_df): return pd.DataFrame([[one_fund_df.app_text.values[0],one_fund_df['dict'].tolist()]],columns=['app_text','dict_new'])def applyParallel(dfGrouped, func): retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped) return pd.concat(retLst)app_text_list_json = applyParallel(app_text_list.groupby('app_text'), beta_cal_mult)print(datetime.now()-t1)==========================parallel_apply=============================app_text_list.groupby(by='app_text').parallel_apply(lambda x: list(x.dict)).rename('dict_new').reset_index()不能写成app_text_list.groupby(by='app_text')['dict'].parallel_apply(lambda x: list(x)).rename('dict_new').reset_index()===================================swifter===============cythonnumbaparallel_apply的速度有时候并不一定比单个apply快,注意使用时机,针对多进程能够明显提升, 注意:parallel_apply应用的函数不应该是lambda函数===&gt;尽量np.vectorize,矢量化函数,矢量化操作,可用来优化apply12345678def func(a,b): if a== b: return 1 else: return 0fun = np.vectorize(func)fun([1,2,3,4],[4,5,3,4])array([0, 0, 1, 1]) #####可视化失败时候]]></content>
      <categories>
        <category>优化方法</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[skin]]></title>
    <url>%2F2020%2F08%2F10%2Fskin%2F</url>
    <content type="text"><![CDATA[shed skin1原理:将代码转换成c++,旨在加快计算密集型Python程序的执行速度 不支持的功能 1231.eval,getattr,hasattr,isinstance2.魔术方法__iter__,__call__,__del__3.函数参数 *args,**args]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[modin]]></title>
    <url>%2F2020%2F08%2F07%2Fmodin%2F</url>
    <content type="text"></content>
      <categories>
        <category>优化方法</category>
      </categories>
      <tags>
        <tag>modin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numba]]></title>
    <url>%2F2020%2F07%2F21%2FNumba%2F</url>
    <content type="text"><![CDATA[Numba介绍1234567 Cython毕竟不是原生的 Python 代码，使用起来还是有诸多不便的。为此，numba 就成了一个功能强大又容易上手的替代选择,是一个用于编译python数组和数值计算函数的编译器,在使用NumPy数组和循环的代码上效果最佳使用方法:添加装饰器 Jit(just-in-time compiler)即时编译器，在运行时将某些函数编译成二进制代码☆☆☆☆☆:numba内不能有第三方包 适用情景12使用numpy数组做大量科学计算时使用for循环时 Numba的装饰器12345678910111213@jit 对于函数中能够编译的部分转换成机器码,剩余的代码使用Python解释器 Numba 提供的最灵活的装饰器@njit 与 @jit(nopython=True) 二者等价,将全部代码转换成机器码,无法实现时会报错@generated_jit 有时需要根据输入变量的类型来决定函数的实现功能@vectorize 矢量化@guvectorize 矢量化装饰器可选参数parallel = True @jit的自动并行化仅在64位平台上可用。注意需要和nopython=true一起使用,numba的多线程的数量通过全局变量来设置 import numba numba.config.NUMBA_NUM_THREADS=8 全局变量设置多线程数目cache=True 为了避免每次调用 Python 程序时的编译时间，可以指示 Numba 将函数编译的结果写入基于文件的缓存中,将函数编译完成的结果保存在一个file文件中。nogil = True 一旦编译完成，就会释放GIL,这样的情况下就可以充分利用多核系统，但是需要注意多线程编程中需要注意的同步、一致性、竞争等情况 #####jit12345678910111213#斐波那契数列import timefrom numba import jit@jitdef fib(n): if n&lt;=2 : return 1; else: return fib(n-1)+fib(n-2);start = time.time()fib(40)end = time.time()print("python3+numba cost_seconds:", end-start) #####generated_jit 1234567891011121314151617181920212223import numpy as npfrom numba import generated_jit,types@generated_jit(nopython=True)def is_missing(x): print(type(x)) print(numba.typeof(x)) if isinstance(x, types.Float): return lambda x:x+2 if isinstance(x, types.Integer): return lambda x:x+2 if isinstance(x, types.List): return lambda x:x if isinstance(x, types.Array): return lambda x:list(x) else: return lambda x:xis_missing(np.array([1,2,3]))numba.typeof(x) types,只能在generated_jit中用,generated_jit会将数据转换成Numba中的数据类型上面的代码完成的是根据输入去判断缺省值的事情。注意下面问题：1、在调用的时候，传入的参数，使用变量的numba类型，而不是值；2、这个修饰函数返回的结果不是一个计算结果； #####vectorize和guvectorize(矢量化计算) 12345678910111213vectorize和guvectorize，Numba可以将纯Python函数编译为一个ufunc (ufunc,通用函数是numpy的特点之一,会将函数作用于array对象的每一个元素上,常见的如add,subtract,multiply,divide...)vectorize 是一个一个元素处理的,而guvectorize是一组一组数据处理的eg:[[1,2,3] [4,5,6]]vectorize每次传入一个数据,去计算.而guvectorize是将[1,2,3]这样一组(行)传进去guvectorize() 函数不返回其结果值：它们将其结果作为数组参数传进去。guvectorize格式: @guvectorize([(int64[:], int64, int64[:])], '(n),()-&gt;(n)')'(n),()-&gt;(n)'是输入和输出布局的声明,告诉 NumPy 该函数采用 n 元素的一维数组，一个标量（用符号表示为空元组()）并返回 n 元素的一维数组;[:] 表示一维数组,[::]表示二维数组.int64表示数组内的数据类型vectorize() 和 guvectorize() 都支持传递nopython=True ，如同@jit 装饰器。使用它来确保生成的代码不会回退到对象模式。 ######直接传矩阵会报错 1234567import numpyimport mathdef trig(a, b): return math.sin(a**2) * math.exp(b)a = numpy.ones((5,5))b = numpy.ones((5,5))trig(a, b) jit也无法使用12345678import numpyimport math@jit(nopython=True)def trig(a, b): return math.sin(a**2) * math.exp(b)a = numpy.ones((5,5))b = numpy.ones((5,5))trig(a, b) ######使用vectorize后会使函数转换成numpy中的unfunc,作用于每一个元素 123456789101112131415161718import numpyimport math@vectorize(nopython=True)def trig(a, b): return math.sin(a**2) * math.exp(b)a = numpy.ones((5,5))b = numpy.ones((5,5))trig(a, b)#下面案列同样可使用import numpyimport mathfrom numba import vectorize,int32@vectorize([int32(int32, int32)]) #@vectorize(["int32(int32, int32)"])这样不用从numba导入,且列表里面允许多种类型,传的参数符合一个即可,否则报错def trig(a, b): return math.sin(a**2) * math.exp(b)a = numpy.ones((5,5),dtype=numpy.int32)b = numpy.ones((5,5),dtype=numpy.int32)trig(a, b) ######guvectorize123456789101112131415161718192021222324252627import timeitimport numpy as npfrom numba import jit, guvectorize@guvectorize(["float64[:], float64[:]"], "(n) -&gt; ()", target="parallel", nopython=True)def row_sum_gu(input, output) : output[0] = np.sum(input)@jit(nopython=True)def row_sum_jit(input_array, output_array) : m, n = input_array.shape for i in range(m) : output_array[i] = np.sum(input_array[i,:])rows = int(64)columns = int(1e6)input_array = np.ones((rows, columns))output_array = np.zeros((rows))output_array2 = np.zeros((rows))#the first run includes the compile timerow_sum_jit(input_array, output_array)row_sum_gu(input_array, output_array2)#run each function 100 times and record the timeprint("jit time:", timeit.timeit("row_sum_jit(input_array, output_array)", "from __main__ import row_sum_jit, input_array, output_array", number=100))print("guvectorize time:", timeit.timeit("row_sum_gu(input_array, output_array2)", "from __main__ import row_sum_gu, input_array, output_array2", number=100)) ###### #####Numba中的数据类型 123456789整形 Integer浮点型 Float负数 Complex列表 List字符串 UnicodeType集合 Set元组 UniTuple数组 Array其他.. Numba的两种模式Nopython和Object模式 1234Numba @jit装饰器从根本上以两种编译模式运行，nopython模式和object模式。nopython编译模式，可以完全运行而无需Python解释器的参与。这是使用Numba jit装饰器的推荐和最佳实践方式，因为它可以带来最佳性能。如果编译nopython模式失败（例如出现了字符串处理等numba无法编译的数据），Numba可以使用object模式 。在这种模式下，Numba将识别它可以编译的循环，并将它们编译成在机器代码中运行的函数，并且它将运行解释器中的其余代码。为获得最佳性能，请避免使用此模式。 案例1:斐波那契数列 Numba无法理解Pandas，因此Numba只需通过解释器运行此代码，但会增加Numba内部开销 1234567891011121314151617181920212223#下面代码运行良好from numba import jitimport numpy as npx = np.arange(100).reshape(10, 10)@jit(nopython=True) # Set "nopython" mode for best performance, equivalent to @njitdef go_fast(a): # Function is compiled to machine code when called the first time trace = 0.0 for i in range(a.shape[0]): # Numba likes loops trace += np.tanh(a[i, i]) # Numba likes NumPy functions return a + trace # Numba likes NumPy broadcastingprint(go_fast(x))#下面代码运行较差from numba import jitimport pandas as pdx = &#123;'a': [1, 2, 3], 'b': [20, 30, 40]&#125;@jitdef use_pandas(a): # Function will not benefit from Numba jit df = pd.DataFrame.from_dict(a) # Numba doesn't know about pd.DataFrame df += 1 # Numba doesn't understand what this is return df.cov() # or this!print(use_pandas(x)) 故障排除和技巧http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#numba-troubleshooting 弃用警告12345678910111213141516Numba弃用了列表和集合from numba import njit@njitdef foo(x): x.append(10) return xa = [1, 2, 3]foo(a)NumbaDeprecationWarning或 NumbaPendingDeprecationWarning代表Numba弃用弃用原因(官方解释)首先回想一下，为了使Numba能够在nopython 模式下编译函数，所有变量必须具有通过类型推断确定的具体类型。在简单的情况下，很明显如何反映内部容器的更改nopython模式返回到原始的Python容器。但是，无法快速有效地将具有嵌套容器类型（例如，整数列表的列表）的复杂数据结构反映出来。经过多年的处理此问题的经验后，很明显，提供此行为既困难又常常导致代码性能不佳（所有反映的数据都必须通过特殊的API才能将数据转换为本地数据格式，然后在返回时返回CPython格式）禁止弃用警告:from numba.errors import NumbaDeprecationWarning, NumbaPendingDeprecationWarningimport warningswarnings.simplefilter('ignore', category=NumbaDeprecationWarning)warnings.simplefilter('ignore', category=NumbaPendingDeprecationWarning) Numba无法编译原因1231.普遍的原因是依赖了不受支持的Python功能2.无法确定函数的返回类型,类型统一失败,if--else中返回的类型不同3.列表中类型无法推断(列表中的类型要一致) Numba编译太慢的原因12编译JIT函数缓慢的最常见原因是，在nopython模式下编译失败，并且Numba编译器已退回到对象模式。 与常规的Python解释相比，对象模式目前几乎没有提供任何加速，其主要点是允许进行称为循环提升的内部优化 ：无论哪种代码包围这些内部循环，此优化都将允许以nopython模式编译内部循环要确定函数是否成功进行类型推断，可以使用inspect_types() f.inspect_types() Numba中的cuda(大规模并行运算)———&gt;了解有这个用法1234CUDA: 一种通用并行计算架构,架构使GPU能够解决复杂的计算问题.(CPU与GPU"协同处理")CUDA发展历程: GPU越来越强大，GPU为显示图像做了优化之外，在计算上已经超越了通用的CPU。如此强大的芯片如果只是作为显卡就太浪费了，因此NVidia推出CUDA，让显卡可以用于图像计算以外的目的，也就是超于游戏，使得GPU能够发挥其强大的运算能力。CUDA作用: CUDA具有不同于用于CPU编程的传统顺序模型的执行模型。在CUDA中，您编写的代码将同时由多个线程（通常成百上千个）执行CUDA环境配置: 失败======== 注意12注意:numba装饰的函数在第一次调用的时候会进行编译,会消耗一些时间,再次调用就不会了如果函数无法在nopython模式下编译，则会发出警告\报错，并说明编译失败的原因 swifter =========&gt;优化apply,df.swifter.apply]]></content>
      <categories>
        <category>优化方法</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pp模块]]></title>
    <url>%2F2020%2F07%2F21%2Fpp%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# 大家一起完善import paramikoimport timeimport randomimport pp# 本机iplocal_ip = [x.split(' ')[0] for x in os.popen('hostname -I')][0]host_tuple = (&#123;'ip': '192.168.0.231', 'port': 22, 'username': 'root', 'password': 'e6772fc39c25'&#125;, &#123;'ip': '192.168.0.227', 'port': 22, 'username': 'root', 'password': 'fa2cadd1cc81'&#125;)ssh = paramiko.SSHClient()ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())# 根据内存使用判断进程数def ppserve_1(host_tuple): remote_cpu_nums=[] ports = [] for host in host_tuple: # 内存使用情况 ssh.connect(hostname=host['ip'], port=host['port'], username=host['username'], password=host['password']) print(host['ip']) stdin, stdout, stderr = ssh.exec_command('cat /proc/meminfo') str_out = stdout.read().decode() str_err = stderr.read().decode() if str_err != "": print(str_err) continue str_total = re.search('MemTotal:.*?\n', str_out).group() # print(str_total) totalmem = re.search('\d+', str_total).group() str_free = re.search('MemFree:.*?\n', str_out).group() # print(str_free) freemem = re.search('\d+', str_free).group() mem_free = round(float(freemem) / float(totalmem), 2) print('当前内存空闲率为：' + str(mem_free)) # 本地可以不走分布式，但分布式 debug 模式可以监测在干什么，但增加了数据传输量；减少传输量，传参数过去后再读取或生成比较快。 # if host['ip'] == local_ip: # local_cpu_num = int((mem_free-0.1)/0.0125) # if local_cpu_num&lt;0: # local_cpu_num=0 # else: # 根据内存使用判断进程数：假设保留 10% 内存，剩余内存按 80 个进程平均分配，不适合所有计算 remote_cpu_num = int((mem_free-0.1) / 0.0125) if remote_cpu_num &lt; 0: remote_cpu_num = 0 print('新开进程数为：' + str(remote_cpu_num)) remote_cpu_nums.append(remote_cpu_num) # 随机生成端口号 port = int(random.random()*(65535-1024)+1024) ports.append(port) # nohup 启动不了，不知道 why # cmd = 'cd /usr/bin;nohup python3 ppserver.py -p '+str(port)+' -d -w '+str(remote_cpu_num)+' -P ppserver.pid -t 3600 -k 3600 &gt; ppserver.out 2&gt;&amp;1 &amp;' # 启动命令，进入 ppserver.py 所在文件夹，用 Python3 启动，可以调用 python3 下的包， -p 指定端口，-d debug模式，-w 知道进程数， -P 指定一个文件保留进程号，后面 kill 时候用到 cmd = 'cd /usr/bin;python3 ppserver.py -p '+str(port)+' -d -w '+str(remote_cpu_num)+' -P ppserver.pid -t 3600 -k 3600' # 若在这里执行则后面 ssh.close() 注销掉，否则 close 了就停止（如何 close，在这里分布式，然后分别close？）；而且看不到 debug 模式， # stdin, stdout, stderr = ssh.exec_command(cmd, get_pty=True) print(cmd) ssh.close() return remote_cpu_nums,portsremote_cpu_nums,ports = ppserve_1(host_tuple)# 等待 ppserver 生成完成time.sleep(30)ppservers = tuple([x['ip']+':'+str(port) for x,port in zip(host_tuple,ports)] )# ppservers = tuple([x['ip']+':'+str(port) for x in host_list if x['ip'] != local_ip])# 本地进程数设为0job_server = pp.Server(0 ,ppservers=ppservers,socket_timeout=3600)# 问题：worker数、进程数、cpu数的关系？job_server.get_active_nodes()# job_server.submit() 指令job_server.print_stats()# 关闭 client（python）端job_server.destroy()# kill server（服务器）端 ppserver.py 进程for host in host_tuple: ssh.connect(hostname=host['ip'], port=host['port'], username=host['username'], password=host['password']) ssh.exec_command('cd /usr/bin;kill `cat ppserver.pid`') ssh.close() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import ppimport pandasimport pymysqlimport sqlalchemyimport timeimport jsondb_251_pre2 = &#123; 'host': '192.168.0.251', 'port': 3306, 'user': 'user_rw', 'password': '1a2s3d4f', 'db': 'pre_formal_2', 'charset': 'utf8mb4'&#125;db_222_pre2 = &#123; 'host': '192.168.0.222', 'port': 3306, 'user': 'user_rw', 'password': '1a2s3d4f', 'db': 'pre_formal_2', 'charset': 'utf8mb4'&#125;def df2_mysql(df_data, table_name, **kwargs): """ To wrap a function that insert DataFrame to Mysql :param df_data: DataFrame data :param table_name: The name of mysql table, type is string :param kwargs: The connection of database, type is dict """ engine = sqlalchemy.create_engine( 'mysql+pymysql://&#123;&#125;:&#123;&#125;@&#123;&#125;:&#123;&#125;/&#123;&#125;?charset=&#123;&#125;'.format(kwargs['user'], kwargs['password'], kwargs['host'], kwargs['port'], kwargs['db'], kwargs['charset'])) pandas.io.sql.to_sql(df_data, table_name, engine, index=False, if_exists='append', chunksize=10000) # if_exists: 'replace', 'append' engine.dispose()def main_func(industry_id, db_251_pre2, db_222_pre2, result_tb): print(industry_id, '开始', '********************') # 其他指标 print(f'industry[&#123;industry_id&#125;] reading...') sql1 = f'select * from patent_layout_20200722 where industry_id="&#123;industry_id&#125;"' conn = pymysql.connect(**db_251_pre2) df_data = pandas.read_sql(sql1, conn) conn.close() df_columns = df_data.columns if 'id' in df_columns: # 若有id，就删除id df_data = df_data.drop('id', axis=1) print(f'industry[&#123;industry_id&#125;] insert into mysql...') batch_num = 250 for batch_i in range(0, len(df_data), batch_num): df2_mysql(df_data[batch_i: batch_i + batch_num], result_tb, **db_222_pre2) print(f'industry[&#123;industry_id&#125;] is done! \n', '=' * 50)if __name__ == "__main__": start_time = time.time() with open('industries_idname.txt', 'r', encoding='utf-8') as f: industries = json.load(f) result_tb = 'patent_layout_pptest200727' # 先选择一个服务器作为客户端， # 再找到ppserver.py文件 # 然后，启动文件：python ppserver.py - p 3505 - w 5 - i 192.168.0.231 - s "123456" # 然后在本机中，运行主函数代码 # 注意：主函数代码中，导入模块不能使用import...as，也不能用from ... import，只能用import # 在服务器上，用ps -ef | grep python | grep pp | grep deng 查询pp进程 ppservers = ('192.168.0.231:3505',) # 远程服务端ip和端口号，为空就是本机 job_server = pp.Server(ncpus=5, ppservers=ppservers, secret='123456') # ncpus：本机进程数量 jobs = [job_server.submit(main_func, (industry_id, db_251_pre2, db_222_pre2, result_tb), depfuncs=(df2_mysql,), modules=("pymysql", "time", "sqlalchemy", "pandas")) for industry_id in industries] for job in jobs: job() job_server.destroy()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cython模块]]></title>
    <url>%2F2020%2F07%2F17%2FCython%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[Cython入门12345678910111213141516171819202122232425262728291. 理解Cpython和Cython的区别Cpython:C语言实现的Python解释器,类似Jpython,IronPython,代表标识&gt;&gt;&gt;Cython:用于编写Python的C扩展的语言,用C高效实现某些程序，再给python调用.简单的认为就是给Python加上了静态类型后的语法。2. 使用Cython的优点综合了Python的优势语言简短,写起来方便,和C的速度,编译性语言,将代码编译成机器码形式运行.Python最大的缺点就是运行慢(解释性语言).3. Cython用户指南http://docs.cython.org/en/latest/src/userguide/language_basics.html#cython-file-types4.Cython使用(pycharm\vs等IDE) 1&gt;.创建名为.pyx后缀的文件,文件内写符合C格式的代码(较难) 2&gt;.创建setup.py文件 3&gt;.python setup.py install 生成.c文件 4&gt;.进入cpython,import 模块使用案例:Helloworld 1.创建名为Helloworld.pyx的文件 2.不需要创建setup.py文件,直接在函数里导入 import pyximport pyximport.install() import 模块名 导入函数:模块名.函数名(参数) 编译说明: .pyx文件将要被cython编译成.c文件 .c文件然后被C编译器编译成.pyd文件(可被Python直接调用)5.要点 1&gt;.在 Cython 中，类型标注对于提升速度是至关重要的 2&gt;.C语言在定义函数的时候,参数是要设定类型的,变量使用前需要定义类型 3&gt;.难点在代码转换,如何将Python形式的代码转换成C语言形式 4&gt;在进行整形计算的时候注意可能会溢出,c的int类型由大小限制6.特点 是支持作为语言一部分的可选静态类型声明。将源代码转换为优化的C / C ++代码，并编译为Python扩展模块。 #####Jupyter 中使用Cython121.首先加载cython,%load_ext Cython2.在写函数的时候需要加上%%cython即可对其进行编译 Cython语句和表达式12345678910缺点:需要会C语言相关知识,能够将Python代码通过Python+C实现(重计算的部分\频繁调用)总体速度:纯C&gt;C+Python&gt;纯Python1.Cython的语句和表达式遵循Python的语法2.Cython里没有-&gt;操作符,用·替代;Cython里没有指针的取值操作符,用P[0]替代*p;Cython里取变量的地址操作符&amp;;在Cython里空指针用Null表示,而且Null是保留关键字,不能用0表示3.Cython里用&lt;&gt;进行强制转换,而非() eg: a = &lt;int&gt;b4.Cython的for循环: ①同样支持Python的for in range ②处于性能考虑,Cython对for循环进行了优化,在循环前先定义变量 eg: cdef int i for i in range(10) Cython语法123456789101112131415161718192021222324252627282930313233343536371. 定义一个C变量 cdef int n = 123 声明整形 cdef float score 声明浮点型（单精度） cdef int an[20] 声明一维数组,长度为20,以0填充 cdef int an[10][10] 声明二维数组 cdef list particles 声明列表 cdef dict events 声明字典 cdef set unique_names 声明集合 cdef (double, int, float) d = (1.0, 2, 3.0) cdef bint a = True 或者 cdef bint a = 1 声明bool类型 cdef str mystring = &apos;foo&apos; 声明字符串 cdef char* data = &quot;snfxjh&quot; 声明字符串,当字符串中含有中文时候不适用2. 强制类型转换 &lt;&gt; eg: a = &lt;int&gt;b3. cdef/def/cpdef/ctypedef def在python和cython中均可以调用,cdef在c系列可以调用,cpdef在两者都可以调用,但要知道返回的类型,且丧失了cython的类型安全,不推荐这么做.也就是说我们想要在Python中调用的函数使用def定义,不需要调用的函数使用cdef定义. ctypedef用它来为类型取一个新的名字python程序中看不到cdef函数，python只能直接调用def的函数，cdef的函数只能通过Python调用def的函数来调用4.import 和 cimport cimport可以访问导入模块下的C函数或属性，而import可以访问Python函数或属性 import numpy as np cimport numpy as np #并没有真正导入,只是允许访问模块下的C函数或属性5.cython中创建矩阵 cdef int carr[3][3][3] #创建一个三维矩阵,类型为int cdef np.ndarray h = np.zeros([xmax, ymax], dtype=int) 6.调用C函数 可以在cython包里的cython/includes中找到可以导入哪些包7.边界检查(比如列表的索引) 在函数前添加装饰器 cimport cython @cython.boundscheck(False) @cython.wraparound(False) 去掉边界检查可以提升速度,每次都进行边界检查非常耗时，而且有些代码是不可能产生越界的问题的，所以这个操作并不总是需要被执行,但是如果不进行边界检查，如果碰巧超出了边界，则在最佳情况下将使程序崩溃，在最坏的情况下将破坏数据.8.数组(array) from cpython cimport array import array cdef array.array a = array.array(&apos;i&apos;, [1, 2, 3]) cdef int[:] ca = a #####Cython代码样例 ######1.样例一号(求质数) 1234567891011121314151617181920212223首先,创建.pyx文件,创建prime.pyx,写代码,C特性变量使用前必须先声明def primes(int nb_primes): cdef int n, i, len_p cdef int p[1000] if nb_primes &gt; 1000: nb_primes = 1000 len_p = 0 n = 2 while len_p &lt; nb_primes: for i in p[:len_p]: if n % i == 0: break else: p[len_p] = n len_p += 1 n += 1 result_as_list = [prime for prime in p[:len_p]] return result_as_list然后,在代码里导入模块名import pyximportpyximport.install()import primeprint(prime.primes(100)) 2.样例二号(比较C程度不同的代码速度 https://www.jianshu.com/p/9410db8fbf50 计算沿地球表面两点之间的距离 ​ ​ ​ ​ 速度对比综合:大多数情况下，Python的性能是足够好的，一旦循环、数字运算和Python函数调用上去了，性能就会相应地下降，在这种情况下，建议使用Cython进行优化 3.样例三号(传矩阵参数)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051一.cdef int sum3d(int[:, :, :] arr) nogil: cdef size_t i, j, k, I, J, K cdef int total = 0 I = arr.shape[0] J = arr.shape[1] K = arr.shape[2] for i in range(I): for j in range(J): for k in range(K): total += arr[i, j, k] return total说明:传的arr为三维矩阵,且矩阵内数据为int类型 二.import numpy as npcimport numpy as npDTYPE = np.intctypedef np.int_t DTYPE_tdef naive_convolve(np.ndarray f, np.ndarray g): if g.shape[0] % 2 != 1 or g.shape[1] % 2 != 1: raise ValueError(&quot;Only odd dimensions on filter supported&quot;) assert f.dtype == DTYPE and g.dtype == DTYPE cdef int vmax = f.shape[0] cdef int wmax = f.shape[1] cdef int smax = g.shape[0] cdef int tmax = g.shape[1] cdef int smid = smax // 2 cdef int tmid = tmax // 2 cdef int xmax = vmax + 2 * smid cdef int ymax = wmax + 2 * tmid cdef np.ndarray h = np.zeros([xmax, ymax], dtype=DTYPE) cdef int x, y, s, t, v, w cdef int s_from, s_to, t_from, t_to cdef DTYPE_t value for x in range(xmax): for y in range(ymax): s_from = max(smid - x, -smid) s_to = min((xmax - x) - smid, smid + 1) t_from = max(tmid - y, -tmid) t_to = min((ymax - y) - tmid, tmid + 1) value = 0 for s in range(s_from, s_to): for t in range(t_from, t_to): v = x - smid + s w = y - tmid + t value += g[smid - s, tmid - t] * f[v, w] h[x, y] = value return h或者使用 def naive_convolve(object[DTYPE_t, ndim=2] f,object[DTYPE_t, ndim=2] g)也可以,速度稍微慢点或者使用 def naive_convolve(np.ndarray[DTYPE_t, ndim=2] f,np.ndarray[DTYPE_t, ndim=2] g): 分析Cython程序的好坏1231.cython -a yfxl.pyx (yfxl.pyx为cython文件名,会生成的.html文件2.在jupyter中使用 %%cython --annotate3.通过生成的报告可以看出来转换的程度,白色线条转换为纯C，黄色的为与Python有交互 ######优化(由简单到复杂) 1231. 将数据类型由动态转换成静态(变量使用前先cdef定义一下)-----&gt;这一步就对速度有显著提升2. Python加C混合使用,尽可能多的使用C(如import仍使用Python)3. 完全使用C,比如import math 转换成使用C的标准库实现 ######总结 121. 数据类型必须使用前定义,但不是全部,Cython根据其分配推导局部变量的类型，这也可以减少在任何地方显式指定类型的需要(如循环的i),添加类型过多会使代码的可读性降低，因此请适度使用它们2. 循环和重计算的代码尽量使用Cython代码实现]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>优化方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贪心学院nlp]]></title>
    <url>%2F2020%2F07%2F07%2F%E8%B4%AA%E5%BF%83%E5%AD%A6%E9%99%A2nlp%2F</url>
    <content type="text"><![CDATA[NLP项目流程原始文本—-&gt;分词—–&gt;清洗—–&gt;标准化—–&gt;特征提取——&gt;建模 分词方法1.$\textcolor{red}{分词工具}$ ​ 1.jieba分词 ​ 2.SnowNLP ​ 3.LTP ​ 4.HanNLP 2.$\textcolor{red}{分词方法}$ ​ 1.最大匹配算法 ​ ①前向最大匹配算法]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python优化方法]]></title>
    <url>%2F2020%2F06%2F17%2Fpython%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Python的缺点就是运行特别慢,比其他语言慢2-10倍原因:它是GIL(Global interpreter lock 全局解释器锁)===cpython,其他语言实现的Python可以没有如jpython,为解释型语言,为动态型语言 $\textcolor{red}{静态语言:}$数据类型需要先声明 $\textcolor{red}{动态语言:}$即语言无需声明数据类型直接可以使用的语言 Python写起来方便,简短的代价为执行速度 GIL:使得在同一进程内任何时刻仅有一个线程在执行(Cpython) Python的解析器 ——含有GIL的有：CPython、PyPy、Psyco；没有GIL的有：JPython，IronPython。 multiprocessing库的出现很大程度上是为了弥补thread库因为GIL而低效的缺陷 方便将来$\textcolor{orange}{量化}$或者别的应用场景中的一些$\textcolor{green}{重计算的部分}$单独取出来，然后用Cython改写，独立成模块来提高运算速度 $\textcolor{red}{cpu密集型}$:也叫计算密集型,，指的是系统的硬盘、内存性能相对CPU要好很多，此时，系统运作大部分的状况是CPU Loading 100%，CPU要读/写I/O(硬盘/内存)，I/O在很短的时间就可以完成，而CPU还有许多运算要处理，CPU Loading很高。$\textcolor{red}{io密集型}$:IO密集型指的是系统的CPU性能相对硬盘、内存要好很多，此时，系统运作，大部分的状况是CPU在等I/O (硬盘/内存) 的读/写操作，此时CPU Loading并不高。$\textcolor{red}{Cpython}$ 1234567891011121314151617181920212223242526272829303132333435361.cpython和ipython的区别: CPython是用C语言编写的实现,CPython是使用字节码的解释器，CPython将您的Python代码编译为字节码 cpython就是我们下载完Python后通过命令行Python进入的解释器 CPython用&gt;&gt;&gt;作为提示符，而IPython用In [序号]:作为提示符。 使用: 1.创建名为.pyx后缀的文件,文件内写符合C格式的代码(较难) 2.创建setup.py文件 3.python setup.py install 生成.c文件 4.进入cpython,import 模块 案例:Helloworld 1.创建一个文件helloworld.pyx,内容如下: print('hello world!') 2.创建setup.py文件,内容如下 from distutils.core import setup from Cython.Build import cythonize setup( ext_modules = cythonize("helloworld.pyx)) 3.命令行进入到当前的位置,python setup.py install,生成文件 4.命令行使用Python进入cpython,import helloworld会自动输入helloworld (通常为import fib fib.fib(10)的形式调用函数)2.实现:Cython 一.注意: 1.Cython的扩展名为.pyx 2.引入模块cimport 模块名 3.@cython.boundscheck(False) 和 @cython.wraparound(False) 两个修饰符用来关闭 Cython 的边界检查 4.Cython的函数使用cdef定义 二.实现方法1 同上面的helloworld案例 三.实现方法2 1.创建名为.pyx的文件 2.不需要创建setup.py文件,直接在函数里导入 import pyximport pyximport.install() import 模块名 导入函数:模块名.函数名(参数) $\textcolor{red}{Cython}$ 1231.在 Cython 中，类型标注对于提升速度是至关重要的2.C语言在定义函数的时候,参数是要设定类型的,变量使用前需要定义类型3.难点在代码转换,如何将Python形式的代码转换成C语言形式 $\textcolor{red}{速度快慢}$:纯Python版本&lt;Python+c混合&lt;C实现Python调用&lt;纯c(C实现C调用) https://www.jianshu.com/p/9410db8fbf50 $\textcolor{red}{Cython语句和表达式}$ 123456781.Cython的语句和表达式遵循Python的语法2.Cython里没有-&gt;操作符,用·替代;Cython里没有指针的取值操作符,用P[0]替代*p;Cython里取变量的地址操作符&amp;;在Cython里空指针用Null表示,而且Null是保留关键字,不能用0表示3.Cython里用&lt;&gt;进行强制转换,而非() eg: a = &lt;int&gt;b4.Cython的for循环: ①同样支持Python的for in range ②处于性能考虑,Cython对for循环进行了优化,在循环前先定义变量 eg: cdef int i for i in range(10)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>优化方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络算法]]></title>
    <url>%2F2020%2F06%2F01%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[卷积神经网络1234神经网络的结构:输入层--&gt;隐藏层--&gt;输出层卷积神经网络:卷积神经网络和普通神经网络的关系: 卷积神经网络依旧是层级网络,只是层的功能和形式发生了变化]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[社交网络算法]]></title>
    <url>%2F2020%2F05%2F28%2F%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[社交网络算法12345678910111213使用的包为igraph衡量指标: 1.点度中心性:在网络中共有N个结点,每个节点由k个直接连接的节点,则该点的自由度为deg(x) = k,该点的点度中心性为deg(x)/(N-1),N-1为排除自身总共的节点 2.紧密度中间性:距离d为该点到其余各点带权的最短路径和,各个节点的紧密度中间性为d/(N-1)的结果的倒数.结果越大表示越'紧密' 3.介数中间性(公式见下图): 设节点x 和节点 y之间的最短路径数为σ(x,y)，最短路径中通过节点 v 的路径数为σ(x,y|v) eg:A-B-C-D-E(无向图) 对于A而言经过该节点的路线没有故介数中间性为0 对于B,经过该节点的为A-B-C,A-B-C-D,A-B-C-D-E,每一种情况的σ均为1,所以,B点的介数中心性为3/(4*3/2) 4.特征值中心性 N个节点的无向网络,可以转换成N*N的矩阵,不相连为0,相连则为1 使用特征方程Ix = λx===&gt;(I-λE)x = 0可求解出特征值和特征向量,特征值作为向心度大小,特征向量作为向心度在各个连接的得分值总:紧密度中间性可以降低远距离点的干扰,介数中间性可以找到大佬(公认). igragh包安装12341.https://www.lfd.uci.edu/~gohlke/pythonlibs/ 下载对应python版本的igragh包2.https://www.lfd.uci.edu/~gohlke/pythonlibs/ 下载对应Python版本的pycairo3.pip instal 两个文件的路径4.完成,无法使用的话卸载anaconda重新安装 12g = igraph.Graph([(0,2),(0,3),(3,1),(3,4),(3,5),(1,4)])#创建图igraph.plot(g) 1234567891011121314151617181920212223242526272829303132333435363738import pandas as pdimport pymysqlimport igraphconfig1 = &#123; 'host': '192.168.0.250', 'port': 3306, # MySQL默认端口 'user': 'user_rw', # mysql默认用户名 'password': '1a2s3d4f', 'db': 'pre_formal_1', # 数据库 'charset': 'utf8', &#125;conn1 = pymysql.connect(**config1)sql = "select app_text,inventors from zl_zu_20200526_L where applicant_other = '新疆恒泰艾普能源服务有限公司'"df = pd.read_sql(sql,conn1)ls = []ls1 = []for i in df.inventors: i = i.split(',') i = list(set(i)) ls.extend(i) if len(i)&gt;1: for m in range(len(i)): k = m+1 while k&lt;len(i): ls1.append((i[m],i[k])) k+=1 else: ls1.append((i[0],i[0]))inv_ls = list(set(ls))g = igraph.Graph()vertex = inv_lsg.add_vertices(vertex)g.vs['label'] = g.vs['name']edges = ls1g.add_edges(edges)igraph.plot(g)dict1 = dict(zip(inv_ls,g.degree()))sorted(dict1.items(),key = lambda x:x[1],reverse=True)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>机器学习算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络基础知识]]></title>
    <url>%2F2020%2F05%2F25%2F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[问题总结123456789101.osi7层模型2.TCP/IP四层和五层模型3.简单介绍一下osi各个层及作用4.简单介绍一下tcp/ip协议的三次握手和四次挥手5.为什么连接时候是三次握手而关闭时候是四次6.为什么不能用两次握手建立连接7.TCP和UDP协议的区别8.HTTP和HTTPS的区别9.get请求和post请求的区别10.session和cookie的区别 #####OSI参考模型(开放式系统互联) 定义了网络互联的七层框架,物理层,数据链路层,网络层,传输层,会话层,表示层和应用层 12345678osi七层模型 &lt;-----TCP/IP五层模型 &lt;-------TCP/IP四层模型物理层 物理层 网络接口层数据链路层 数据链路层 网络接口层网络层 网络层 网络层传输层 传输层 传输层会话层 应用层 应用层表示层 应用层 应用层应用层 应用层 应用层 简单介绍osi七层模型中的各层12345678物理层:重要设备为中继器(放大信号)和集线器(放大信号),扩大网络的传输距离. 作用:确保原始数据可在网络媒体上传输数据链路层:重要设备为网桥(连接网段)和交换机(更多连接接口). 作用:为网络层提供可靠的数据传输,数据成帧,流量控制,数据检错,重发. 协议:网络接口协议网络层:重要设备为路由器(连接不同网络,信息传输). 作用:对子网间的数据进行路由选择,拥塞控制,网际互连. 协议:IP/ICMP/ARP/RARP协议传输层:重要设备为网关(网络互连,跟门原理差不多). 作用:将上层数据分段,并提供端到端的,可靠的或不可靠的传输以及端到端的差错控制和流量控制问题 协议:TCP/UDP协议会话层:管理主机间的会话进程表示层:对数据进行变换以保证一个主机应用层信息可以被宁一个主机的应用程序理解应用层:为操作系统或网络应用程序提供访问网络服务的接口会话层、表示层、应用层的协议：FTP(文件传送协议)/Telnet(远程登录协议)/DNS(域名解析协议)/SMTP(邮件传送协议)/POP3 TCP/IP的三次握手和四次挥手♻1234567891011121314151617181920212223241.三次握手（建立连接阶段）： ①客户端发送初始序号x和syn=1请求标志 ②服务器发送请求标志syn，发送确认标志ACK，发送自己的序号seq=y，发送客户端的确认序号ack=x+1 ③客户端发送ACK确认号，发送自己的序号seq=x+1，发送对方的确认号ack=y+1 第一次：客户端发送请求到服务器，服务器知道客户端发送，自己接收正常。SYN=1,seq=x 第二次 ：服务器发给客户端，客户端知道自己发送、接收正常，服务器接收、发送正常。ACK=1,ack=x+1,SYN=1,seq=y 第三次：客户端发给服务器：服务器知道客户端发送，接收正常，自己接收，发送也正常.seq=x+1,ACK=1,ack=y+1 2.四次挥手（连接释放阶段）： ①客户端发出释放FIN=1，自己序列号seq=u，进入FIN-WAIT-1状态 ②服务器收到客户端的后，发出ACK=1确认标志和客户端的确认号ack=u+1，自己的序列号seq=v，进入CLOSE-WAIT状态 ③客户端收到服务器确认结果后，进入FIN-WAIT-2状态。此时服务器发送释放FIN=1信号，确认标志ACK=1，确认序号ack=u+1，自己序号seq=w，服务器进入LAST-ACK（最后确认态） ④第四次挥手：客户端收到回复后，发送确认ACK=1，ack=w+1，自己的seq=u+1，客户端进入TIME-WAIT（时间等待）。客户端经过2个最长报文段寿命后，客户端CLOSE；服务器收到确认后，立刻进入CLOSE状态。 第一次：客户端请求断开FIN,seq=u 第二次：服务器确认客户端的断开请求ACK,ack=u+1,seq=v 第三次：服务器请求断开FIN,seq=w,ACK,ack=u+1 第四次：客户端确认服务器的断开ACK,ack=w+1,seq=u+1 ack:确认序号的标志,ack=1表示序列号有效,ack=0表示报文不含确认序列号信息syn:连接请求序号标志,用于建立连接,syn = 1表示请求连接fin:结束标志,用于释放连接,1表示关闭本方数据流 为什么三次握手和四次挥手12三次握手时,服务器同时把ack和syn放在一起发送到客户端那里四次挥手时,当收到对方的 FIN 报文时，仅仅表示对方不再发送数据了但是还能接收数据，己方是否现在关闭发送数据通道，需要上层应用来决定，因此，己方 ACK 和 FIN 一般都会分开发送。 ######能否两次握手建立连接 13次握手完成两个重要的功能,既要双方做好发送数据的准备工作,也要允许双方就初始序列号进行协商,这个序列号在握手过程中被发送和确认 使用tcp的协议12FTP(文件传输协议) Telnet(远程登录协议) SMTP(简单邮件传输协议) POP3(与SMTP对应用于接收邮件) HTTP协议等 ######TCP和UDP协议的区别 1231.TCP是面向连接的,可靠的字节流服务,UDP是面向无连接的,不可靠的数据流服务2.TCP安全,稳定,但比较慢,对系统资源要求多,且效率较低 HTTP和HTTPS的区别12http是以明文方式发送内容https是在http的基础上使用了SSL/TSL加密 ######Get和Post请求的区别 123456789101112131415161718191、url可见性：get:参数url可见 post:参数url不可见2、数据传输上：get:通过拼接url进行传递参数 post:通过body体传输参数3、缓存性：get请求是可以缓存的 post请求不可以缓存4、后退页面的反应:get请求页面后退时，不产生影响 post请求页面后退时，会重新提交请求5、传输数据的大小get一般传输数据大小不超过2k-4k（根据浏览器不同，限制不一样，但相差不大）post请求传输数据的大小根据配置文件设定，也可以无限大。6.参数的数据类型get只接受ASCII字符 而POST没有限制核心: 首先GET和POST是什么？ 他们是HTTP协议中两种发送请求的方式。HTTP是基于TCP与IP的关于数据在万维网中如何通信的协议。HTTP的底层是TCP/IP，也就是说GET与POST都是TCP链接。GET与POST做的事是一样的，都可以传输数据。因此GET与POST在本质上没有区别，而真正的区别在于TPC链接的不同 原理 POST方式会产生两个TCP数据包。详细的说，对于GET请求，浏览器会把http header和data一并发送出去，服务器响应200ms后返回数据。而POST请求，浏览器会先发送http header服务器响应100-continue(协议)，浏览器再发送data，服务器响应200ms后再返回数据。 session和cookie12345678HTTP协议中,客户端请求服务器端是一种无状态的连接,每次请求都是独立的请求(1)cookie以文本文件格式存储在浏览器中,而session存储在服务器端(2)cookie的存储限制了数据量,只允许4kb,而session是无限量的(3)我们可以轻松访问cookie值但是我们无法轻松访问会话值,因此更安全(4)可以考虑将登录信息等重要信息存放为session,不重要的信息,可以放在cookie中联系:(1)都是用来记录用户的信息以便让服务器分辨不同的用户(2)可以搭配使用 ,都有自己的使用局限,要考虑到安全和性能问题]]></content>
      <categories>
        <category>计算机</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习思路与方向]]></title>
    <url>%2F2020%2F05%2F22%2F%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%B7%AF%E4%B8%8E%E6%96%B9%E5%90%91%2F</url>
    <content type="text"><![CDATA[#多动脑子 勤动手 C语言12345678910111213141516171819202122232425262728293031计算机组成原理---&gt;C语言基础---&gt;看懂项目(开源社区)入门： C语言的基本语法知识： ·环境 ·简单的程序 ·算法概念，简单六成结构，流程图等 ·基本数据类型 ·运算符与表达式 ·简单输入输出函数 ·选择循环分支结构 ·数组 ·函数编程基础 ·指针提高： C语言基本知识框架 ·指针的深刻理解和使用 ·位运算 ·存储管理 ·预处理 ·字符串处理 ·文件的读写操作 ·函数熟练使用 ·常用的数据结构 ·图形图像的简单处理应用： 综合性运用软件的开发能力 ·软件工程基本知识 ·数据库知识 ·数据结构知识 ·程序运维能力 ·工程化思维 Python12python语言基础---&gt;Python语言高级---&gt;全栈前端---&gt;全栈后端---&gt;后端高级---&gt;linux基础---&gt;linux运维自动化开发---&gt;数据分析---&gt;大数据---&gt;机器学习]]></content>
      <categories>
        <category>计划</category>
      </categories>
      <tags>
        <tag>方向</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql底层知识]]></title>
    <url>%2F2020%2F05%2F13%2Fmysql%E5%BA%95%E5%B1%82%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[##基础知识 文章来源:https://www.jianshu.com/p/47664afa249e Mysql的常用引擎1234567891.Innodb(更适合高并发场景) Innodb的存储文件有两个,后缀名分别为.frm和.idb,其中.frm是表的定义文件,而.idb是数据文件 Innodb中存在表锁和行锁,不过行锁是在命中索引的情况下才会起作用 Innodb支持事务,且支持四种隔离级别(读未提交,读已提交,可重复读,序列化),默认为可重读读;而在oracle数据库中,只支持序列化和读已提交这两种级别,默认为读已提交2.Myisam Myisam的存储文件有三个,后缀名分别为.frm,.MYD,.MYI.其中.frm是表的定义文件,MYD是表的数据文件,MYI是索引文件. Myisam只支持表锁,且不支持事务.Mysiam由于有单独的索引文件,在读取数据方面性能更高3.存储结构 Innodb和Myisam都是用B+Tree来存储数据的 Mysql的数据、索引存储结构1234567891011121314151617181.数据存储的原理(硬盘) 信息存储在硬盘里,硬盘是有很多的盘片组成,通过盘片表面的磁性物质来存储数据.盘片表面是凹凸不平的,凸起的地方磁化,代表数字1,凹的地方被磁化,代表数字0,因此硬盘是通过二进制来存储数字,文字等信息的2.数据读写的原理 硬盘在逻辑上被分为磁道,柱面以及扇区. 磁头靠近主轴接触的表面,即线速度最小的地方,是一个特殊的区域,它不存放任何数据,称为启停区或者着陆区,启停区外就是数据区 在最外圈,离主轴最远的地方就是0磁道,硬盘数据的存放就是从最外圈开始的 在硬盘中还有一个叫0磁道检测器的构件,用来完成磁盘的初始定位3.磁盘的读写原理 系统将文件存储到磁盘上时,按柱面、磁头和扇区的方式进行，即最先是第一磁道的第一磁头下的所有扇区，然后是同一柱面的下一个磁头 一个柱面存储满后就推进到下一个柱面，直到把文件内容全部写入磁盘 系统也以相同的顺序读出数据，读出数据时通过告诉磁盘控制器要读出所在扇区所在柱面、磁头号和扇区号进行4.减少i/o的预读原理 由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动消耗的时间。磁盘的存取速度往往是主存的几百分之一，因此为了提高效率尽量减少磁盘的io。 磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。5.mysql的索引 索引是一种用来实现Mysql高效获取数据的数据结构 我们通常所说的在某个字段上建索引，意思就是让mysql对该字段以索引这种数据结构来存储，然后查找时有对应的查找算法。 建立索引的目的是为了查找的优化，一般的查找算法有顺序查找，折半查找，快速查找。每种查找算法都用于特定的数据结构。如顺序查找以来于顺序结构，折半查找通过二叉查找树黑着红黑树实现二分搜索。因此在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这种数据结构以某种方式引用数据，这样就可以在这些数据结果上实现高级查找算法，这种数据结构就是索引。 Mysql中的B+Tree12345678910111213目前大多数的数据库系统及文件系统都采用B-Tree或其变种B+Tree作为索引结构B+树索引是B+树在数据库中的一种实现，是最常见也是数据库中使用最为频繁的一种索引，B表示平衡而非二叉。B+Tree是由平衡二叉树演变而来,B+Tree是由二叉查找树(左子树的键值小于根节点键值,右子树的键值大于根的键值),平衡二叉树(在二叉查找树的条件下,还满足任何节点的两个子树高度最大差为1),平衡多路查找树(为磁盘外存储设备设计的一种平衡查找树)逐步优化过来的系统从磁盘读取数据到内存时是以磁盘块为基本单位的,位于同一磁盘块中的数据会被一次性取出来,而不是按需读取Innodb使用页作为数据读取单位,页是其磁盘管理的最小单位,默认page大小是16k系统中一个磁盘的大小往往没有那么大,因此Innodb每次申请磁盘空间时都会是若干地址连续磁盘块来达到页的大小16kbInnodb在把磁盘数据读入到磁盘时会以页作为基本单位,在查询数据时如果一个页中的每条数据都能助于定位数据记录的位置,这将会减少磁盘I/O的次数,提高查询效率B-Tree结构的数据可以让系统高效的找到数据所在的磁盘块为了描述B-Tree,首先定义一条数据记录为一个二元组[key,data],key为记录的键值,对于不同数据记录,可以是互不相同的,data为数据记录除key外的数据B-Tree的每个节点根据实际情况可以包含大量的关键字信息和分支每个节点占用一个盘块的磁盘空间,一个节点上有两个升序排序的关键字个三个只想子树根节点的指针,指针存储的是子节点所在磁盘块的地址.两个关键词划分成的三个范围域对应三个指针只想的子树的数据的范围域以根节点为例,关键字为17和35,p1指针指向的子树的数据范围为小于17,p2指针指向的子树的数据范围为17-35,p3指针指向的为大于35 ####B-Tree 查找关键字29的过程: ​ 1.根据根节点找到磁盘1,读入内存(磁盘i/o操作一次) ​ 2.根据关键字29在区间(17,35),找到磁盘块1的指针p2 ​ 3.根据p2指针找到磁盘块3,读入内存,磁盘i/o操作第二次 ​ 4.比较关键字29在区间(26,30),找到磁盘块3的指针p2 ​ 5.根据p2指针找到磁盘块8,读入内存,磁盘i/o操作第三次 ​ 6.在磁盘块8的关键字列表中找到关键字29 Mysql的Innodb存储引擎在设计时是将根节点常驻内存的,因此力求达到树的深度不超过3,也就是I/O不需要超过3次 根据上面的过程可以发现需要3次磁盘I/O操作,和3次内存查找工作.由于内存中的关键字是一个有序表结构,可以利用二分法查找提高效率,因而3次磁盘的I/O操作是影响整个B-tree查找效率的决定性因素 B-Tree 相对于平衡二叉树缩减了节点个数,使每次I/O取到内存的数据都发挥了作用,从而提高了查询效率 B+Tree是在B-Tree的基础上的一种优化,使其更适合实现外存储索引结构,Innodb存储引擎就是用B+Tree实现其索引结构 在B-Tree中,每个节点中有key,也有data,而每一个页的存储空间有限,如果data数据较大时会导致每个节点(即一个页)能存储的的key数量很小 在B+Tree中,所有数据记录节点都是按照键值大小顺序存放在同一层的叶子结点上, 而非叶子结点上只存储key值信息,这样可以大大加大每个节点存储的key值数量,降低B+Tree的高度 B+Tree和B-Tree的区别 ​ 1.数据是存在叶子节点上中的 ​ 2.数据节点间是有指针指向的 Myisam中的B+TreeMyisam引擎也是采用的B+Tree结构作为索引结构 由于Myisam中的索引和数据存放在不同的文件,所以在索引树中的叶子结点中存的数据是该索引对应的数据记录的地址,由于数据和索引不在一起,所以Myisam是非聚簇索引 (叶子结点存放对应数据的物理地址) #####Innoodb中的B+Tree Innodb是以ID为索引的数据存储 采用Innodb引擎的数据存储文件有两个,一个定义文件,一个数据文件 Innodb通过B+Tree结构对ID建索引,然后通过叶子结点中存储记录 若建索引的字段不是主键ID,则对该字段建索引,然后在叶子节点中存存的是该记录的主键,然后通过主键索引找到对应记录 #####Mysql的相关优化 1.Mysql性能优化:组成,表的设计​ ①开启查询缓存.避免某些sql函数直接在sql语句中使用,从而导致Mysql缓存失效 ​ ②目的是什么就取什么,能查一条判断的就不要全取 ​ ③建合适的索引,所以要建在合适的地方,合适的对象上,经常操作/比较/判断的字段应该建索引 ​ ④字段大小要适宜.字段的取值是有限而且固定的,这种情况下可以使用enum,ip字段介意用unsigned int来存储 ​ ⑤表的设计.垂直分割表,使得固定表与边长表分割,从而降低表的复杂度和字段的数目 2.SQL语句优化:避免全表扫描​ 1.建索引 一般在where及order by中涉及到的列上建索引,尽量不要对可以重复的字段建索引. ​ 2.尽量避免在where中使用!或or也不要进行null值判断 ​ 3.尽量避免在where中对字段进行函数操作、表达式操作 ​ 4.尽量避免使用llike %，在这种情况下可以进行全文检索 Mysql基准测试原因:基准测试可以观察系统在不同压力下的行为,评估系统的容量,掌握哪些是重要的变化,或者观察系统如何处理不同的数据 ######基准测试的策略: ​ 针对整个系统的测试(集成式full-stack) ​ 单独测试Mysql(单组件式single-component) 测试的指标​ 1.吞吐量,指单位时间内的事务处理数,常用的测试单位是每秒事务级(TPS),或每分钟事务数(TPM) ​ 2.响应时间或者延迟,用于测试任务所需的整体时间,根据具体的应用,测试的时间单位可能是微秒毫秒秒或者分钟.通常使用百分比响应时间来代替最大响应时间 ​ 3.并发性,需要关注的是正在工作中的并发操作,或者是同时工作中的线程数或者连接数.在测试期间记录Mysql数据库的Threads_running状态值 ​ 4.可扩展性,给系统增加一倍的工作,在理想状态下就能获得两倍的效果(即吞吐量增加一倍),对于容量规范非常有用,可以提供其他测试无法提供的信息 基准测试方法1.需要避免的一些常见错误: ​ 使用真实数据的子集而不是全集 ​ 使用错误的数据分布 ​ 使用不真实的分布参数 ​ 在多用户的场景中,只做单用户测试 ​ 在单服务器上测试分布式应用 ​ 与真实用户行为不匹配 ​ 反复执行同一个查询 ​ 没有检查错误 ​ 忽略了系统预热(warm up)过程 ​ 使用默认的服务器配置 ​ 测试时间太短 2,应该建立将参数和结果文档化的规范,每一轮测试都必须进行详细记录 3.基准测试应该运行足够长的时间,需要在稳定状态下测试并观察 4.在执行基准测试时.需要尽可能多地收集被测试系统的信息 5.自动化基准测试可以防止测试人员偶尔遗漏某些步骤,或者误操作,宁外也有助于归档整个测试过程 剖析mysql查询1234561.剖析服务器负载 ·慢查询日志 long_query_time为0可以捕获所有的查询,查询的响应时间单位可以做到微秒级 ·生成剖析报告 pt-query-digest2.剖析单条查询 show profiles show global status 诊断间歇性问题123456789101.尽量不要用试错的方式来解决问题,如果一时无法定位,可能是测量的方式不准确,或者测量的点选择有误,或者使用的工具不合适2.确定单条查询问题还是服务器问题 show global status show processlist 使用查询日志 理解发现的问题,使用gnuplot或R,或其他绘图工具将结果绘制成图形3.捕获诊断数据 诊断触发器:在问题出现时能够捕获数据的基础,有两个常见问题可能导致无法达到预期的结果:误报或者漏检 收集数据:尽可能收集所有能收集的数据,但只在需要的时间段内收集,oprofile,strace,tcpdump,pt-collect,pt-stalk 解释结果数据:pt-mysql-summary输出结果打包.... Schema与数据类型优化 1234567891011schema就是数据库对象的集合，所谓的数据库对象也就是常说的表，索引，视图，存储过程等。在schema之上的，就是数据库的实例，也就是通常create databases获得的东西选择优化的数据类型 1.数据类型的选择原则 更小的通常更好 简单就好 尽量避免null 2.应该尽量只在对小数进行精确计算时才使用decimal,使用int类型通过程序控制单位效果更好 3.使用varchar合适的情况,字符串列的最大长度比平均长度大很多;列的更新很少 4.char适合存储很短的字符串,或者所有值都接近同一个长度,不容易产生碎片,在存储空间上更有效率 5.通常应该尽量使用timestamp,它比datetime空间效率更高 加快alter table操作的速度121.在一台不提供服务的机器上执行alter table操作,然后和提供服务的主库进行切换2.通过影子拷贝,创建一张新表,然后通过重命名和删表操作交换两张表及里面的数据 快速创建Myisam索引,先禁用索引,导入数据,然后重新启用索引 创建高性能的索引1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950一.索引基础 1.索引可以包含一个或多个列的值,如果索引包含多个列,那么列的顺序也十分重要,因为mysql只能高效地使用索引的最左前缀列 2.orm工具能够产生符合逻辑,合法的查询,除非只是生成非常基本的语句,否则很难生成适合索引的查询 3.在mysql中,索引是在存储引擎层而不是服务器层实现的,所以,并没有统一的索引标准;不同存储引擎的索引的工作方式并不一样,也不是所有的存储引擎都支持所有类型的索引 4.B-Tree意味着所有的值都是按顺序存储的,并且每一个叶子页到根的距离相同,能够加快访问数据的速度,从索引的根节点开始搜索,适用于全键值\键值范围或键前缀查找 5.B-Tree索引的限制: 如果不是按照索引的最左列开始查找,则无法使用索引 不能跳过索引中的列 如果查询中有某个列的范围查询,则其右边所有列都无法使用索引优化查找 6.哈希索引基于哈希表实现,只有精确匹配索引所有列的查询才会有效,只有memory引擎支持哈希索引 7.哈希索引的限制: 哈希索引只包含哈希值和行指针,而不存储字段值,所以不能使用索引中的值来避免读取行 哈希索引数据并不是按照索引值顺序存储的,所以也就无法用于排序 哈希索引也不支持部分索引列匹配查找,因为哈希索引始终是使用索引列的全部内容来计算哈希值的 只支持等值比较查询,不支持任何范围查询 访问哈希索引的数据非常快,除非有很多哈希冲突 如果哈希冲突很多的话,一般索引维护的代价也会很高 8.空间数据索引(r-Tree),myisam支持空间索引,可以使用地理数据存储 9.全文索引,试用与Match against操作,而不是普通的where条件操作二.索引的优点 1.三个优点 ①索引大大减少了服务器需要扫描的数据量 ②索引可以帮助服务器避免排序和临时表 ③索引可以将随机I/O变成顺序I/O 2.索引三星系统: ①索引将相关的记录放到一起则获得一星 ②如果索引中的数据顺序和查找中的排序一致则获得二星 ③如果索引中的列包含了查询中需要的全部列则获得三星三.高性能的索引策略 1.独立的列:如果查询中的列不是独立的,则MYSQL不会使用索引.独立的列是指索引列,不能是表达式的一部分,也不能是函数的参数 2前缀索引和索引选择性 ①通常可以索引开始的部分字符,可以大大节约索引空间,但也会降低索引的选择性 ②索引的选择性是指,不重复的索引值和数据表的记录总数的比值.选择性越高则查询效率越高,因为选择性高的索引可以让mysql在查找到时候过滤掉很多的行 ③mysql无法使用最左前缀索引做order by 和group by,也无法做覆盖扫描 3.选择合适的索引列顺序 ①正确的索引列顺序依赖于使用该索引的查询,并且同时需要考虑如何更好地满足排序和分组的需要 ②在一个多列B-Tree索引中,索引列的顺序意味着索引首先按照最左列进行排序,其次是第二列 ③将选择性最高的列放到索引最前列 4.聚簇索引:并不是一种单独的索引类型,而是一种数据存储方式 (最好避免随机的聚簇索引,特别是对于I/O密集型的应用) 5.覆盖索引:如果一个索引包含所有要查询的字段的值,称为覆盖索引 覆盖索引必须要存储索引列的值 6.压缩(前缀)索引,默认值压缩字符串,减少索引的大小,对于cpu密集型应用没因为扫描需要随机查找,压缩索引在myisam上要慢好几倍 7.重复索引是指在相同的列上按照相同的熟悉顺序创建的相同类型的索引,应该尽量避免这样重复的创建索引 8.索引可以让查询锁定更少的行四.维护索引和表 1.check table 表名 检查表是否损坏 2.altet table 表名 engine = Innodb 修复表 3.show index from 表名 查看索引的基数 4.b-tree可能会碎片化,降低查询的效率 五.查询性能优化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566671.为什么查询速度会变慢 ①如果要优化查询,实际上要优化其子任务,要么消除其中一些子任务,要么减少子任务的执行次数,要么让子任务运行的更快 ②查询的生命周期大致可以按照顺序来看:从客户端,到服务器,然后在服务器上进行解析,生成执行计划,执行,并将结果返回给客户端2.慢查询基础:优化数据访问 1.两个分析步骤: ①确认应用程序是否检索大量超过需要的数据 ②确认Mysql服务器层是否在分析大量超过需要的数据行 2.是否向数据库请求了不需要的数据 ①查询了不需要的数据 ②多表关联并返回全部列 ③总是取出全部列 ④重复查询相同的数据 3.mysql是否在扫描额外的记录 1.查询开销三个指标:响应时间,扫描行数,返回的行数 ②响应时间:服务时间和排队时间之和 ③扫描的行数:较短的行的访问速度更快,内存中的行也比磁盘中的行的访问速度要快的多 ④访问的类型:explain中的type列反应了访问类型,通过增加合适的索引. 2.三种方式应用where条件,在索引中使用where条件来过滤不匹配的记录;使用索引覆盖扫描来返回记录,直接从索引中过滤不需要的记录并返回命中结果;从数据表中返回数据,然后过滤不满足条件的记录 3.需要扫描大量数据但只返回少数行的优化技巧:使用索引覆盖扫描,改变库表结构,重写复杂的查询3.重构查询的方式 ①Mysql从设计上让连接和断开连接都很轻量级,再返回一个小的查询结果方面很高效 ②切分查询,将大查询切分成小查询.每个查询功能完全一样,只完成一小部分,每次只返回一小部分查询结果,可以避免锁住很多数据,占满事务日志,耗尽系统资源,阻塞很多小的但重要的查询 ③分解关联查询优势: 1.让缓存的效率更高 2.将查询分解后,执行单个查询可以减小锁的竞争 3.在应用层做关联,可以更容易对数据库进行拆分,更容易做到高性能和可扩展 4.查询效率本身效率也可能会有所提升 5.可以减少冗余记录的查询 6.相当于在应用中实现了哈希关联,而不是使用Mysql的嵌套循环关联 ④分解关联查询的场景 1.当应用能够方便的缓存单个查询的结果的时候 2.当可以将数据分布到不同的MYSQL服务器上的时候 3.当能够使用IN()的方式代替关联查询的时候 4.当查询中使用同一个数据表的时候4.查询执行的基础 ①查询执行的路径 客户端发送一条查询给服务器 服务器先检查查询缓存,如果命中则立刻返回,否则进入下一阶段 服务器端进行sql解析,预处理,再由优化器生成对应的执行计划 mysql根据优化器生成的执行计划,调用存储引擎的api来执行查询 将结果返回给客户端 ②Mysql客户端和服务器之间的通信协议是半双工的,无法将一个消息切成小块独立来发送,没法进行流量控制,一旦一端开始发生消息,宁一端要接受完整个消息才能响应它 ③mysql通常需要等所有的数据都已经发送给客户端才能释放这条查询所占用的资源,所以接收全部结果并缓存通常可以减少服务器的压力 ④语法解析器和预处理,通过关键字将sql语句进行解析,并生成一颗对应解析树,解析器将使用MYSQL语法规则验证和解析查询,预处理器则根据一些mysql规则进一步检查树是否合法 ⑤查询优化器,找到最好的执行计划,使用基本成本的优化器,将尝试预测一个查询使用某种执行计划时的成本,并选择其中成本最小的一个,使用show status like &apos;last_query_cost&apos;;查看需要多少个数据页的随机查找 ⑥导致查询优化器选择错误的原因 1.统计信息不准确,Innodb不能维护一个数据表的行数的精准统计信息 2.执行计划中的成本估算不等同于实际执行的成本 3.Mysql的最优跟你想的最优可能不太一样 4.mysql从不考虑其他并发执行的语句 5.MYSQL也并不任何时候都是基于成本的优化 6.Mysql不会考虑不受控制的操作的成本 7.优化器有时候无法去估算所有可能的执行计划 ⑦mysql能处理的优化类型 1.重新定义关联表的顺序 2.将外链接转换为内链接 3.使用等价变换规则 4.优化count(),min(),max(),在explain可以看到&apos;select tables optimized away&apos; 5.预估并转化为常数表达式,当检测到一个表达式可以转换为常数的时候,就会一直把该表达式作为常数进行优化处理 6.覆盖索引操作,当索引中的列包含所有查询中需要使用的列的时候,就可以使用索引返回需要的数据,而无需查询对应的数据行 7.子查询优化 8.提前终止查询,在发现已经满足查询需求的时候,在发现已经满足查询需求的时候,Mysql总是能够立刻终止查询 9.等值传播,如果两个列的值通过等式关联,那么mysql能够把其中一个列的where条件传递到宁一个列上 10.列表in()的比较,mysql将in()列表中的数据先进行排序,然后通过二分查找的方式来确定列表中的值是否满足条件 ⑧对于union查询,mysql先将一系列的单个查询结果放到一个临时表中,然后再重新读出临时表数据来完成union查询 ⑨无论如何排序都是一个成本很高的操作,所以从性能角度考虑,应尽可能避免排序或者尽可能避免对大量数据进行排序 ⑩当不能使用索引生成排序结果的时候,MYSQL需要自己进行排序,如果数据量小则在内存中进行,如果数据量大则需要使用磁盘,Mysql将这个过程称为文件排序,及时完全是内部排序不需要任何磁盘文件也是如此. 查询优化器1234567891011121314151617181920212223242526272829303132一.Mysql查询优化器的局限性 1.关联子查询:Mysql的子查询实现起来非常糟糕,最糟糕的一类查询是where条件中包含in()的子查询语句.使用group_concat()在in()中构造一个由逗号分隔的列表,或者使用exists()来改写 2.union的限制,有时,Mysql无法将限制条件从外层下推到内层,这使得原本能够限制部分返回结果的条件无法应用到内层查询的优化上 3.Mysql不支持哈希关联,mariadb已经实现了哈希关联 4.MYSQl无法利用多核来进行并行查询 5.Mysql不支持松散索引扫描,5.0后的版本在分组查询中需要找到分组的最大值和最小值时可以使用松散索引扫描 6.对于min和max.mysql优化的并不好二.优化特定类型的查询 1.优化count()查询 ·count()是一个特殊的函数,有两种非常不同的作用:可以统计某个列值的数量,也可以统计行数,在统计列值时要求列值是非空的(不统计null) ·count(*)并不是会像我们猜想的那样扩展成所有的列,实际上,它会忽略所有的列而直接统计所有的行数,当mysql确认括号内的表达式不可能为空时,实际上就是在统计行数 Myisam的count函数只有没有任何where条件下的count(*)才非常快 使用近似值,如explain出来的优化器估算行数 使用索引覆盖 使用汇总表 使用外部缓存系统 2.优化关联查询 确保on或者using子句的列上有索引 确保任何的groupby和order by中的表达式只涉及到一个表中的实例 当升级mysql的时候需要注意:关联语法,运算符优先级等其他可能会发生变化的地方 3.优化子查询,尽可能使用关联查询代替4.优化GROUP BY和DISTINCT使用索引优化当无法使用索引时，GROUP BY使用两种策略来完成：使用临时表或者文件排序来做分组尽可能的将WITH ROLLUP（超级聚合）功能移动应用程序中处理5.优化LIMIT分页最简单的办法是尽可能地使用索引覆盖扫描，而不是查询所有的列，然后根据需要做一次关联操作再返回所需的列，select id,name,…… from table innert join (select id from table order by xxx limit 5000,5) as table1 USING(id);offset会导致MySQL扫描大量不需要的行然后再抛弃掉，如果可以记录上次取数据的位置，下次就可以直接从该记录的位置开始扫描，可以避免使用offset使用预先计算的汇总表，或者关联到一个冗余表6.优化UNION查询 通过创建并填充临时表的方式来执行UNION查询，因此很多优化策略在UNION查询中都没法很好地使用，经常需要手工地将WHERE、LIMIT、ORDER BY等子句下推到UNION的各个子查询中 除非确实需要服务器消除重复的行，否则就一定要使用UNION ALL ######Mysql高级特性 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117一.分区表 1.对用户来说分区表是一个独立的逻辑表,但是底层由多个物理子表组成,实际上是对一组底层表的句柄对象的的封装 2.使用场景: ·表非常大以至于无法全部都放在内存中,或者只在表的最后部分有热点数据,其他均是历史数据 ·分区表的数据更容易维护 ·分区表的数据可以分布在不同的物理设备上,从而高效的利用多个硬件设备 ·可以使用分区表来避免某些特殊的瓶颈 ·如果需要,还可以备份和恢复独立的区 3.使用限制 ·一个表最多只能有1024个区 ·在Mysql5.1中,分区表达式必须是整数,或者返回证书的表达式.在mysql5.5中,某些场景中可以直接使用列来进行分区 ·如果分区字段中有主键或者唯一索引的列,那么所有主键列和唯一索引列都必须包含进来 ·分区表无法使用外键约束 4.使用分区表 当数据量超大的时候,B-Tree索引就无法起作用了,除非是覆盖索引查询,否则数据库服务器需要根据索引扫描返回的结果,查询所有符合条件的记录,如果数据量巨大,将产生大量随机I/O 5.保证大数据量的可扩展性的策略 ·命题扫描数据,无需使用任何索引 ·索引数据并分离热点 6.分区策略的问题 ·NULL值会使分区过滤无效 ·分区列和索引列不匹配 ·选择分区的成本可能会很高 ·打开并锁住所有底层表的成本可能很高 ·维护分区的成本可能很高 ·所有分区都必须使用相同的引擎存储 ·分区函数中可以使用的函数和表达式也有一些限制 ·某些引擎不支持分区 ·对于Myisam的分区表,不能再使用load index into cache操作 ·对于Myisam表,使用分区表时需要打开更多的文件描述符 7.查询优化 很重要的一点是要在where条件中带入分区列 只能在使用分区函数的列本身进行比较时才考虑分区,而不能根据表达式的值去过滤分区,及时这个表达式是分区函数也不行 二.视图 1.视图本身是一个虚拟表,不存放任何数据,返回的数据是从mysql从其他表生成的 2.Mysql使用两种算法:合并算法和临时表算法,会尽可能的使用合并算法 3.如果视图中包含groupby\distinct\任何聚合函数\union\子查询等.只要无法在原表记录和试图记录中建立一一映射的场景中,Mysql都将使用临时表算法来实现视图 4.可更新视图是指可以任何通过更新这个视图来更新视图涉及的相关表,check option表示任何通过视图更新的行,都必须符合视图本身的where条件定义 5.在重构schema的时候可以使用视图,使得在修改底层表结构的时候,应用代码还可能继续不报错的运行 6.mysql中不支持物化视图(指将视图结果数据存放在一个可以查看的表中,并定期从原始表中刷新到这个表中) 7.不会保存视图定义的原始SQL语句 三.外键约束 1.使用外键是有成本的,通常要求每次在修改数据时都要在宁外一张表中多执行一次查找操作 2.如果想确保两个关键表始终有一致的数据,那么使用外键比在应用程序中检查一致性的性能要高得多,在相关数据的删除和更新上,比在应用中维护更高效 3.外键会带来很大的额外消耗四.在Mysql内部存储代码 1.mysql允许通过触发器,存储过程,函数的形式来存储代码,从5.1开始还可以在定时任务中存放代码,这个定时任务称为时间.存储过程和存储函数都被统称为存储程序 2.存储代码的优点: ·他在服务器内部执行,离数据最近,宁外在服务器上执行还可以节省带宽和网络延迟 ·它是一种代码复用,可以方便统一业务规则,保证某些行为总是一致的,所以也可以为应用提供一定的安全性 ·它可以简化代码的维护和版本更新 ·可以帮助提升安全,比如提供细粒度的权限控制 ·服务器端可以缓存存储过程的执行计划,这对于需要反复调用的过程,会大大降低消耗 ·因为是在服务器端部署的,所以可以备份,维护都可以在服务器端完成 ·可以在应用开发和数据库开发人员之间更好地分工 3.存储代码的缺点 ·Mysql本身没有提供好用的开发和调试工具 ·较之应用程序的代码,存储代码效率稍微差点 ·存储代码可能会给应用程序代码的部署带来额外的复杂性 因为存储程序都部署在服务器内,所以可能有安全隐患 存储过程会给数据库服务器增加额外的压力,而数据库服务器的扩展性相比应用服务器要差很多 mysql并没有选项可以控制存储程序的资源消耗,所以在存储过程的一个小错误,可能会直接把服务器拖死 存储代码在mysql中的实现也有很多的限制--执行计划缓存是连接级别的,游标的物化和临时表相同,异常处理也非常困难 调试mysql的存储过程是一件非常困难的事情 他和基于语句的二进投影日志复制合作的并不好 4.存储过程好而寒暑限制 ·优化器无法使用关键字DETERMINISTIC来优化单个查询中多次调用存储函数的情况 ·优化器无法评估存储函数的执行成本 ·每个连接都有独立的存储过程的执行计划缓存 ·存储过程和复制是一种诡异的组合 5.触发器:可以让你在执行insert\update\delete的时候,执行一些特定的操作,可以在mysql中指定是在授权率语句执行前触发还是执行后触发,可以使用触发器实现一些强制限制,或者某些业务逻辑,否则就需要在应用程序中实现逻辑 6.触发器的注意和限制: ·对于每一个表的每一个事件,最多只能定义一个触发器 ·只支持基于行的触发,也就是说,触发器是针对一条记录的,而不是针对整个授权率语句的,如果变更的数据集非常大,效率会很低 ·触发器可以掩盖服务器背后的工作 ·触发器可以掩盖服务器背后的工作,一个简单的sql语句背后可能包含了许多看不见的工作 ·触发器的问题很难排查,如果某个性能问题和触发器相关,会很难分析和定位 ·触发器可能会导致死锁和锁等待 ·触发器并不能保证更新的原子性 7.触发器的用处: ·实现一些约束,系统的维护任务,以及更新反范式化数据的时候 ·记录数据变更日志 8.事件:类似于linux的定时任务,指定mysql在某个时候执行一段sql代码,或者每隔一个时间间隔执行一段sql代码五.全文索引 1.myisam的全文索引作用对象是一个全文集合,这可能是某个数据列的一列,也肯能是多个列 2.可以根据where子句中的match against来区分查询是否使用全文索引 3.在使用全文索引进行排序的时候,Mysql无法在使用索引排序,如果不想使用文件排序的话,就不要在查询中使用order by子句 4.在布尔搜索中,用户可以在查询中自定义某个被搜索的词语的相关性,可以通过一些前缀修饰符来定制搜索 5.全文索引在insert \update\delete 中的操作代价很大 6.全文索引会影响索引选择\where 子句\order by 等六.查询缓存 1.MYSQL查询缓存保存查询返回的完整结果,当查询命中该缓存,mysql会立即返回结果,跳过了解析\优化和执行阶段 2.MYSQL判断缓存命中的方法很简单,缓存放在一个引用表中,通过一个哈希值引用,这个哈希值包括了如下因素,即查询本身,当前要查询的数据库,客户端协议的版本等一些其他可能会影响返回结果的信息 3.当判断缓存是否命中时,mysql不会解析\正规化\或者参数化查询语句,而是直接使用sql语句和客户端发送过来的其他原始信息,任何字符上的不同,例如空格,注释--都会导致缓存的不命中 4.当查询语句中有一些不确定的数据时,则不会被缓存,如包含函数now()等 5.打开查询缓存对读和写都会带来额外的消耗 ·读查询在开始之前必须检查是否命中缓存 ·如果这个读查询可以被缓存,那么当执行完后,MYSQL若发现查询缓存中没有这个查询,会将其结果存入查询缓存,会带来额外的系统消耗 ·当想某个表写入数据的时候,mysql必须将对应表的所有缓存都设置失效,如果查询缓存非常大,或者碎片非常多,这个操作就会带来很大系统消耗 6.对于需要消耗大量资源的查询通常是非常适合缓存的 7.缓存未命中 ·查询语句无法被缓存 ·MYSQL从未处理这个查询 ·查询缓存的内存用完了 ·查询缓存还没有完成预热 ·查询语句之前从未执行过 ·缓存失效操作太多了 8.缓存参数配置 query_cache_type,是否打开查询缓存 query_cache_size 查询缓存使用的总内存空间 query_cache_min_res_unit 在查询缓存中分配内存块时的最小单位,可以帮助减少由碎片导致的内存空间浪费 query_cache_limit mysql能够缓存的最大查询结果 query_cache_wlock_invalidate 如果某个数据表被其他的连接锁住,是否仍然从查询缓存中返回结果 优化服务器设置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136一.mysql配置的工作原理 1.任何打算长期使用的设置都应该写到全局配置文件,而不是在命令行特别指定 2.常用变量和动态修改它们的效果: key_buffer_size:可以一次性为键缓冲区分配所指定的空间 table_cache_size:不会立即生效--将在下次有线程打开表才有效果,如果值大于缓存中表的数量,线程可以把最新打开的表放入缓存,如果比缓存中的表小,京葱缓存中删除不长使用的表 thread_cache_size,不会立即生效,将在下次有连接被关闭时产生效果,检查缓存中是否还有空间在缓存线程,如果有空间,则缓存该线程以备下次连接征用,如果没空间,将销毁该线程而非缓存 query_cache_size:一次性分配并初始化这块内存 read_buffer_size:只有在查询需要使用时才会为该缓存分配内存 read_rnd_buffer_size:只有在查询需要使用到时,才会为该查询分配内存,并且只分配需要的内存大小而不是全部指定的大小 sort_buffer_size:只会在也有查询需要排序的时候才会为该缓存分配内存 3.对应连接级别的设置,不要轻易的在全局级别增加他们的值,除非确认这样做是对的 4.设置变量时请小心,并不是值越大越好,而且如果设置的值太高,可能更容易导致问题:可能会由于内存不足导致服务器内存交换,或者超过地址空间二.什么不该做 1.不要根据比率来调优,例如缓存命中率跟缓存是否过大或者过小无关 2.不要使用调优脚本 3.不要相信很流行的内存消耗公式三.创建mysql配置文件 1.mysql编译的默认设置并不都是靠谱的,其中大部分比较合适 2.从一个比默认值大一点但不是大的很离谱的安全值开始是比较好的,MYSQL的内存利用率并不总是可以预测的,他可能依赖很多的因素,例如查询的复杂性和并发性 3.配置服务器的首选途径,了解它内部做了什么,以及参数之间如何相互影响,然后再决定 4.open_files_limit 在linux上尽可能设置的大一点,如果参数不够大,可能会报错,打开的文件太多 5.每隔60s查看状态变量的增量变化:mysqladmin extended-status ri60四.配置内存使用 1.配置mysql正确使用内存量对高性能至关重要,内存小号分为两类,可以控制的内存和不可以控制的内存 2.配置内存 确定可以使用的内存上限 确定每隔mysql连接需要使用的内存 确定操作系统需要多少内存才够用 吧剩下的内存全部给mysql的缓存 3.mysql保持一个连接只需要少量的内存,他还需要一个基本量的内存来执行任何给定的查询,需要为高峰时期执行的大量查询预留好足够的内存,否则,查询执行可能因为缺乏内存而导致执行效率不佳或执行失败 4.跟查询一样,操作系统也需要保留足够的内存给他工作,如果没有虚拟内存正在交换到磁盘,就是表名操作系统内存足够的最佳迹象 5.如果服务器只运行mysql.所有不需要为操作系统以及查询管理保留的内存都可以用作mysql缓存 6.大部分情况下最重要的缓存: innodb缓冲池 innodb日志文件和myisam数据的操作系统缓存 myisam键缓存 查询缓存 无法手工配置的缓存,例如二进制日志和表定义文件的操作系统缓存 7.innodb缓冲池并不仅仅缓存索引,他还会缓存行的数据,自适应哈希索引,插入缓冲,锁,以及其他内部数据结构,还使用缓冲池来帮助延迟写入,innodb严重依赖缓冲池 8.如果事先知道什么时候需要关闭innodb,可以在运行时修改innodb_max_dirty_pages_pct变量,将值改小,等待刷新纯种清理缓冲池,然后在脏页数量较小时关闭,可以监控the innodb_buffer_pool_pages_dirty状态变量或者使用innotop来监控show innodb status来观察脏页的刷新量 9.Myisam的键缓存也被称为键缓冲,默认只有一个键缓存,但也可以创建多个,MYISAM自身之缓存索引,不换存数据.最重要的配置项是key_buffer_size,不要超过索引的总大小,或者不超过操作系统的缓存保留总内存的25%-50%,以更小的为准 10.了解MyISAM索引实际上占用多少磁盘空间，查询INFORMATION_SCHEMA表的INDEX_LENGTH字段，把它们的值相加，就可以得到索引存储占用空间 11.thread_cache_size变量指定了MySQL可以保持在缓存中的线程数，一般不需要配置这个值，除非服务器会有很多连接请求 12.可以关闭InnoDB的innodb_stats_on_metadata选项来避免耗时的表统计信息刷新 13.如果可以，最好把innodb_open_files的值设置得足够大以使服务器可以保持所有的.ibd文件同时打开五.配置mysql并发 1.InnoDB并发配置 InnoDB有自己的“线程调度器”控制线程怎么进入内核访问数据，以及它们在内核中一次可以做哪些事，最基本的限制并发的方式是使用innodb_thread_concurrency变量，它会限制一次性可以有多少线程进入内核,并发值 = CPU数量 * 磁盘数量 * 2，在实践中使用更小的值会更好一点 2.MyISAM并发配置 尽管MyISAM是表级锁，它依然可以一边读取，一边并发追加新行，这种情况下只能读取到查询开始时的所有数据，新插入的数据是不可见的，这样可以避免不一致读通过设置concurrent_insert这个变量，可以配置MyISAM打开并发插入让INSERT、REPLACE、DELETE、UPDATE语句的优先级比SELECT语句更低，设置low_priority_updates选项就可以六.基于工作负载的配置 1.当服务器满载情况下运行时，请尝试记录所有的查询语句，因为这是最好的方式来查看哪种类型的查询语句占用资源最多，同时创建processlist快照，通过state或者command字段来聚合它们 2.优化BLOB和TEXT场景BLOB有几个限制使得服务器对它的处理跟其他类型不一样，不能在内存临时表中存储BLOB值，效率很低 ·通过SUBSTRING()函数把值转换为VARCHAR ·让临时表更快一些：放在基于内存的文件系统 ·如果使用的是InnoDB，也可以调大InnoDB日志缓冲大小 ·大字段在InnoDB里可能浪费大量空间 ·扩展存储禁用了自适应哈希，因为需要完整地比较列的整个长度，才能发现是不是正确的数据 ·太长的值可能使得查询中作为WHERE条件不能使用索引 ·如果一张表里有很多大字段，最好是把它们组合起来单独存到一个列里面 ·有时候可以把大字段用COMPRESS()压缩后再存为BLOB，或者发送到MySQL前在应用程序中进行压缩 3.优化排序（Filesorts）：当MySQL必须排序BLOG或TEXT字段时，它只会使用前缀，然后忽略剩下部分的值七.完成基本配置 1.tmp_table_size和max_heap_table_size，这两个设置控制使得Memory引擎的内存临时表能使用多大的内存 2.max_connections，这个设置的作用就像一个紧急刹车，以保证服务器不会因应用程序激增的连接而不堪重负，设置得以容纳正常可能达到的负载，并且要足够安全，能保证允许你登录和管理服务器 3.thread_cache_size，可以通过观察服务器一段时间的活动，来计算一个有理有据的值，250的上限是一个不错的估算值 4.table_cache_size，应该被设置得足够大，以避免总是需要重新打开和重新解析表的定义，可能通过观察Open_tables的值及其在一段时间的变化来检查该变量八.安全和稳定的设置 1.expire_logs_days，如果启用了二进制日志，应该打开这个选项，可以让服务器在指定的天数之后清理旧的二进制日志 2.max_allowed_packet，防止服务器发送太大的包，也会控制多大的包可以被接收 3.max_connect_errors，如果知道服务器可以充分抵御蛮力攻击，可以把这个值设得非常大，以有效地禁用主机黑名单 4.skip_name_resolve，禁用了另一个网络相关和鉴权谁相关的陷阱：DNS查找 5.sql_mode，不建议修改 6.sysdate_is_now，可能导致与应用预期向后不兼容的选项 7.read_only，禁止没有特权的用户在备库做变更，只接受从主库传输过来的变更，不接受从应用来的变更，可以把备库设置为只读模式 8.skip_slave_start，阻止MySQL试图自动启动复制 9.slave_net_timeout，控制备库发现跟主库的连接已经失败并且需要重连之前等待的时间，设置为一分钟或更短 10.sync_master_info、sync_relay_log、sync_relay_log_info，5.5以后版本可用，解决了复制中备库长期存在的问题：不把它们的状态文件同步到磁盘，所以服务器崩溃后可能需要人来猜测复制的位置实际上在主库是哪个位置，并且可能在中继日志（Relay Log）里有损坏八.高级InnoDB设置 1.innodb，如果设置为FORCE，只有在InnoDB可以启动时，服务器才会启动 2.innodb_autoinc_lock_mode，控制InnoDB如何生成自增主键值 3.innodb_buffer_pool_instances，在5.5以后，可以把缓冲池切分为多段，在高负载的多核机器上提升MySQL可扩展性的一个重要方式 4.innodb_io_capacity，有时需要把这个设置得相当高，才能稳定地刷新脏页 5.innodb_read_io_threads和innodb_write_io_threads，控制有多少后台线程可以被I/O操作使用 6.innodb_strict_mode，让MySQL在某些条件下把警告改成抛错，尤其是无效的或者可能有风险的CREATE TABLE选项 7.innodb_old_blocks_time，指定一个页面从LRU链表的“年轻”部分转移到“年老”部分之前必须经过的毫秒数，默认为0，设置为1000毫秒（1秒）非常有效九、操作系统和硬件优化 A.什么限制了MySQL的性能 1.当数据可以放在内存中或者可以从磁盘中以足够快的速度读取时，CPU可能出现瓶颈，把大量的数据集完全放到大容量的内存中，以现在的硬件条件完全是可行的 2.I/O瓶颈，一般发生在工作所需的数据远远超过有效内存容量的时候，如果应用程序是分布在网络上的，或者如果有大量的查询和低延迟的要求，瓶颈可能转移到网络上 B.如何为MySQL选择CPU 1.可以通过检查CPU利用率来判断是否是CPU密集型的工作负载，还需要看看CPU使用率和大多数重要的查询的I/O之间的平衡，并注意CPU负载是否分配均匀 2.当遇到CPU密集型的工作时，MySQL通常可以从更快的CPU中获益，但还依赖于负载情况和CPU数量 3.MySQL复制也能在高速CPU下工作得非常好，而多CPU对复制的帮助却不大 4.多CPU在联机事务处理（OLTP）系统的场景中非常有用，在这样的环境中，并发可能成为瓶颈 C.平衡内存和磁盘资源 1.配置大量内存最终目的是避免磁盘I/O，最关键的是平衡磁盘的大小、速度、成本和其他因素，以便为工作负载提供高性能的表现 2.设计良好的数据库缓存（如InnoDB缓冲池），其效率通常超过操作系统的缓存，因为操作系统缓存是为通用任务设计的 3.数据库服务器同时使用顺序和随机I/O，随机I/O从缓存从受益最多 4.每个应用程序都有一个数据的“工作集”——就是这个工作确实需要用到的数据 5.工作集包括数据和索引，所以应该采用缓存单位来计数，一个缓存单位是存储引擎工作的数据最小单位 6.找到一个良好的内存/磁盘比例最好的方式是通过试验和基准测试 7.硬盘选择考虑因素：存储容量、传输速度、访问时间、主轴转速、物理尺寸 8.MySQL如何扩展到多个磁盘上取决于存储引擎和工作负载，InnoDB能很好地扩展到多个硬盘驱动器，然而，MyISAM的表锁限制其写的可扩展性，因此写繁重的工作加在MyISAM上，可能无法从多个驱动器中收益 D.固态存储 1.高质量闪存设备具备： 相比硬盘有更好的随机读写性能 相比硬盘有更好的顺序读写性能 相比硬盘能更好地支持并发 提升随机I/O和并发性 2.闪存的最重要特征是可以迅速完成多次小单位读取，但是写入更有挑战性。闪存不能在没有做擦除操作前改写一个单元（Cell），并且一次必须擦除一个大块。擦除周期是缓慢的，并且最终会磨损整个块 3.垃圾收集对理解闪存很重要。为了保持一些块是干净的并且可以被写入，设备需要回收脏块。这需要设备上有一些空闲空间 4.许多设备被填满后会开始变慢，速度下降是由于没有空闲块时必须等待擦写完成所造成的 5.固态存储最适合使用在任何有着大量随机I/O工作负载的场景下，随机I/O通常是由于数据大于服务器的内存导致的，闪存设备可能大大缓解这种问题 6.单线程工作负载也是另一个闪存的潜在应用场景 7.闪存也可以为服务器整合提供巨大的帮助 8.Flashcache，磁盘和内存技术的结合，适合以读为主的I/O密集型负载，并且工作集太大，用内存优化并不经济的情况 9.优化固态存储上的MySQL 增加InnoDB的I/O容量 让InnoDB日志文件更大 把一些文件从闪存转移到RAID 禁用预读 配置InnoDB刷新算法 禁用双写缓冲的可能 限制插入缓冲大小，插入缓冲设计来用于减少当更新行时不在内存中的非唯一索引引起的随机I/O InnoDB的页大小 优化InnoDB页面校验（Checksum）的替代算法 E.为备库选择硬件 1.通常需要跟主库差不多的配置]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>底层</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jieba中文分词]]></title>
    <url>%2F2020%2F05%2F12%2Fjieba%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%2F</url>
    <content type="text"><![CDATA[jieba是目前最好的中文分词组件,主要有三个特性: 1.支持三种分词模式:精确模式,全模式,搜索引擎模式 2.支持频繁分词 3.支持自定义词典 1234567891011jieba.cut() #返回生成器jieba.lcut() #返回列表'''cut_all:是否使用全模式,默认为False(精准模式:他/来自/安徽省)HMM:是否使用隐马尔可夫模型,默认为True'''jieba.cut_for_search() jieba.lcut_for_search()'''搜索引擎模式HMM:默认为True''' HMM模型 123HMM模型,即隐马尔可夫模型,是一种基于概率的统计分析模型,用来描述一个系统隐性状态的转移和隐形状态的表现概率.在jieba中,对于未登录到词库的词,使用了基于汉字成词能力的HMM模型和Viterbi算法.原理: 采用四个隐含状态,分别表示为单字成词,词组的开头,词组的中间,词组的结尾.通过标注好的分词训练集,可以得到HMM的各个参数,然后使用Viterbi算法来解释测试集,得到分词结果 添加自定义词典 1234567891011词语 词频(可省略) 词性(可省略)云计算 3 创新办 3 i凯特琳 nz#jieba本身自带新词识别能力,但自行添加新词可以保证更高的准确率#载入词典,通常为txt格式文件jieba.load_userdict(file_name)#调整词典jieba.add_word('石墨烯')jieba.add_word('凯特琳',freq=42,tag='nz') # 设置词频和词性jieba.del_word('ada') #删除自定义词语 关键词提取(IF-IDF和TextRank) 1234567891011121314151617IF-IDF是一种统计方法,用以评估一个词语对于一个文件集或一个语料库中的一份文件的重要程度.原理: 一个词语在一篇文章中出现的次数越多,同时在所有文档中出现的次数越少,越能代表该文章公式:TF*IDFTF(term Frequency):词频,某一个给定的词语在改文件中出现的次数 tf = 在某一类中词条w出现的次数/该类中所有的词条数目IDF(inverse document frequency,IDF)逆文件频率,如果包含词条的文件越少,则说明词条具有很好的类别区分能力 idf = log(语料库中的文档总数/(包含词条w的文档数+1))jieba.analyse.extract_tags (基于TF-IDF)'''参数sentense:为待提取的文本topK:为返回几个TF/IDF权重最大的关键词withWeight:是否一并返回关键词权重,默认值为FalseallowPOS:仅包括指定词性的词,默认值为空'''TextRank(基于pagerank)jieba.analyse.textrank 参数和extract_tags一致 自定义语料库 123关键词提取所使用的逆向文件频率(IDF)文本语料库和停用词(stop words)文本语料库可以切换成自定义语料库的路径jieba.analyse.set_stop_words('stop_words.txt')jieba.analyse.set_idf_path('idf.txt.big') 词性标注 1jieba.posseg.cut(text) 返回词语在原文中的起止位置 1jieba.tokenize()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>自然语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLTK]]></title>
    <url>%2F2020%2F04%2F29%2FNLTK%2F</url>
    <content type="text"><![CDATA[12语料库认知:语料库中存放的是在语言的实际使用中真实出现过的语言材料；语料库是以电子计算机为载体承载语言知识的基础资源；真实语料需要经过加工（分析和处理），才能成为有用的资源。 12345678910111213141516171819from nltk.book import *text1.concordance('monstrous') #concordance 一致text1.similar('monstrous') #与monstrous相似上下文的词text1.common_contexts(['monstrous','very']) #研究多个词共同的上下文text1.count('for') #统计某个词出现的次数bigrams(['more', 'is', 'said', 'than', 'done']) #双连词搭配nltk.corpus.gutenberg.fileids() 返回古腾堡项目项目的所有文本信息nltk.corpus.gutenberg.words('austen-emma.txt') #获取古腾堡项目艾玛文本的所有单词from nltk.corpus import brown #布朗语料库:包含500个不同来源的文本brown.categories() #查看布朗语料库所有的类别#查看布朗语料库中类别为新闻的文本中的单词brown.words(categories='news') brown.words(fileids = ['news'])brows.sents(categories=['news','editorial','fiction']) #多个文本#载入自己的语料库from nltk.corpus import PlaintextCorpusReader#处理html === &gt; 对文本进行分词raw = nltk.clean_html(html)tokes = nltk.word_tokensize(raw) 123456789101.nltk.corpus 语料库和词典的标准化接口(获取和处理语料库)2.nltk.tokenize,nltk.stem 分词,句子分解提取主干(字符串处理)3.nltk.collocations t-检验,卡方4.nltk.tag 词性标识符5.nltk.classify,nltk.cluster 决策树,最大熵,贝叶斯,EM等(分类)6.nltk.chunk 正则表达式(分块)7.nltk.parse 图表,基于特征,一致性,概率(解析)8.nltk.metrics 精度,召回率等(指标评测)9.nltk.probability 概率分布,平滑概率分布10.nltk.app nltk.chat 图形化的关键词排序,聊天机器人(应用) nltk自带的语料库(corpus) 123456gutenberg 古典小说语料库webtext 网络广告nps_chat 聊天消息语料库browm 一个百万词级的英语语料库reuters 路透社语料库,新闻文档inaugural 演讲语料库 NLTK词频统计 12345678freq = nltk.FreqDist(数据)B() 返回词典的长度plot(title,cumulative=False) #绘制词频分布图,若cumu为True,则是累计频率分布图 tabulate() 生成频率分布的表格形式most_common() 返回出现次数最频繁的词和频度hapaxes() 返回只出现过一次的词freq.most_common(5) 返回出现频率最高的五个词 NLTK去除停用词(stopwords) 1234567891011在自然语言处理中,无用词(数据)称为停用词from nltk.corpus import stopwordsstopwords.words('english')#例题:从文本中删除停用词from nltk.corpus import stopwordsfrom nltk.tokensize import word_tokenizeexample_sent = 'this is a sample sentence,showing off the stop words filtration'stop_words = set(stopwords.words('english'))word_tokens = word_tokenize(example_sent)filtered_sentence = [w for w in word_tokens if not w in stops_words] nltk的分词(tokensize) 1234567from nltk.tokenize import sent_tokenizefrom nltk.tokenize import word_tokenize#分句sent_tokenize(mytext)sent_tokenize(mytext,'french') #标记非英语语言文本#分词word_tokenize(mytext) nltk词干提取==可能创造不存在的词汇 12345678***************************1********************************from nltk.stem import PorterStemmerporter_stemmer = PorterStemmer()porter_stemmer.stem('working')----------&gt;work****************************2*******************************fromnltk.stem import SnowballStemmerlancaster_stemmer = LancasterStemmet()lancaster_stemmer.stem('working') ---------&gt;work nltk词形还原 == 解决词干提取会出现不存在词汇的问题 1234from nltk.stem import WordNetLemmatizerlemmatizer = WordNetLemmatizer()lemmatizer.lemmatize('increases') ----&gt;increaselemmatizer.lemmatize('palying',post='v')-----&gt;默认还原结果为名词,post还原为动词 nltk词性标注(pos tag) 12345import nltktext = nltk.word_tokenize('what does the fox say')nltk.post_tag(text)#返回[('what', 'WDT'), ('does', 'VBZ'), ('the', 'DT'), ('fox', 'NNS'), ('say', 'VBP')] NLTK中的wordnet 123456789101112131415#获取给定词的定义和例句from nltk.corpus import wordnetsyn = wordnet.synsets('pain') #获取'pain'同义词集syn[0].definition() #pain的解释syn[0].examples() #pain的例句#获取同义词for syn in wordnet.synsets('pain'): for lemma in syn.lemmas(): synonyms.append(lemma.name())#获取反义词antonyms = []for syn in wordnet.synsets("small"): for l in syn.lemmas: if l.antonyms(): antonyms.append(l.antonyms()[0].name())]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习周志华]]></title>
    <url>%2F2020%2F04%2F28%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E5%BF%97%E5%8D%8E%2F</url>
    <content type="text"><![CDATA[基本概念:属性\特征\样本\示例\属性值\属性空间\样本空间\输入空间\数据集\标签\特征向量(一个示例都可以在坐标上表示出来)\训练集\测试集\训练样本\输出空间\标记空间\泛化\过拟合\欠拟合 12345678错误率:分类错误的数据占总样本的比例精度: 1 - 错误率查准率(准确率):TP/(TP+FP)查全率(召回率):TP/(TP+FN)训练误差/经验误差:学习器的实际预测输出与真实输出的差异称为误差,在训练集上的误差称为~泛化误差:新样本上的误差roc/auc/pr曲线/pr图平衡点(BEP):查准点=查全率 y=x与曲线的交点 训练集和测试集产生的方法 123451.留出法:3,7分,2,8分,将训练集分成互斥的两个集合2.交叉验证法:又叫k折交叉验证,先将数据集划分为k个大小相似的互斥子集,每次用k-1个子集作为训练集,余下的作为测试集.这样就可以得到k组训练集\测试集,从而可以进行k次训练和测试,最终返回的是这k个测试结果的均值.通常进行p次k折交叉验证缺点:留出法和交叉验证法,由于保留了一部分样本用于测试,因此实际评估的模型所使用的的训练集比D小,这必然会引入一些因训练样本规模不同而导致的估计偏差3.自助法 以自助采样法为基础.给定包含m个样本的数据集D,我们对它采样产生数据集D1,每次随机从D中挑选一个样本,将其拷贝到D1,然后再将该样本放回初始数据D中,使得该样本在下次采样时仍有可能被采到.这个过程重复执行m次,我们就得到了包含m个样本的数据集D1.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫代码]]></title>
    <url>%2F2020%2F04%2F07%2F%E7%88%AC%E8%99%AB%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[####urllib 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192from urllib import request****************************************基本********************************************#确定url地址base_url = 'http://www.baidu.com/'#发送请求response = request.urlopen(base_url)#获取相应内容html = response.read()print(html.decode('utf-8'))#存储with open('2.html','w',encoding='utf-8') as k: k.write(html.decode('utf-8'))***************************************添加headers********************************************base_url = 'http://www.baidu.com/'headers = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'&#125;req = request.Request(url = base_url,headers=headers)response = request.urlopen(req)html = response.read()print(html.decode('utf-8'))****************************************未登录跳过登录爬取人人网********************************from urllib import request,parsefrom http import cookiejar#实例化cookie管理器cookie = cookiejar.CookieJar()cook_handle = request.HTTPCookieProcessor(cookie)#创建openeropener = request.build_opener(cook_handle)base_url = 'http://www.renren.com/PLogin.do'data = &#123; 'email':'17333119189', 'password':'19960102kuai'&#125;res = parse.urlencode(data)re = request.Request(base_url,data=res.encode('utf-8'))responsd = opener.open(re)******************************************添加代理****************************************from urllib import requestbase_url = 'http://www.66ip.cn/'proxy = &#123; 'http':'alice:123456@120.78.166.84:6666', 'https':'alice:123456@120.78.166.84:6666'&#125;#创建管理器proxy_handler = request.ProxyHandler(proxy)#创建openeropener = request.build_opener(proxy_handler)headers = &#123;'Cookie':' __jsluid=27be3fa08d7aa7c457f79068cd77bc79; Hm_lvt_1761fabf3c988e7f04bec51acd4073f4=1555141241,1555144721,1555152744,1555155158; Hm_lpvt_1761fabf3c988e7f04bec51acd4073f4=1555155531','User-Agent':' Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'&#125;response = opener.open(base_url)print(response.read().decode('gb2312'))import requestsbase_url = 'http://www.baidu.com'#发送请求response = requests.get(base_url)#获取相应内容===1.获取字符串2.获取二进制bytes#指定编码格式print(response)response.encoding='utf-8'print(response.text)#返回bytes格式 contentprint(response.content.decode('utf-8'))****************************************非认证代理****************************************#使用代理ip爬取网页#非认证代理from urllib import requestbase_url = 'http://www.baidu.com'proxy = &#123; 'http':'http://124.205.143.213:41372', 'https':'http://124.205.143.213:41372'&#125;#创建proxy管理器proxy_handler = request.ProxyHandler(proxy)#创建openeropener = request.build_opener(proxy_handler)#发送请求response = opener.open(base_url)print(response.read().decode('utf-8'))*****************************************认证代理*****************************************from urllib import requestbase_url = 'http://www.baidu.com'proxy = &#123; 'http':'http://alice:123456@120.78.166.84:6666', 'https':'https://alice:123456@120.78.166.84:6666'&#125;#创建ip处理器proxy_handler = request.ProxyHandler(proxy)#chuangjianopereropener = request.build_opener(proxy_handler)#发送请求response = opener.open(base_url)print(response.read().decode('utf-8')) requests 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178*********************************************get*****************************************import requestsbase_url = 'https://www.xicidaili.com/'#定义请求头headers = &#123; 'User-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'&#125;res = requests.get(base_url,headers=headers)print(res.text)***********************************************post***************************************import requests,jsonbase_url = 'https://fanyi.baidu.com/sug'data = &#123;'kw': 'hello'&#125;response = requests.post(base_url,data=data)response.encoding='UTF-8'res = json.loads(response.text)# with open('2.json','w',encoding='utf-8')as f:# f.write(json.loads())print(res)******************************************添加请求头**************************************import requestsbase_url = 'http://www.baidu.com'headers = &#123; 'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36'&#125;response = requests.get(base_url,headers=headers)print(response.text)********************************************xpath匹配*************************************from lxml import etreeimport requestsbase_url = 'https://www.mzitu.com/'headers = &#123; 'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36','referer':'https://www.mzitu.com/xinggan/','cookie': 'Hm_lvt_dbc355aef238b6c32b43eacbbf161c3c=1555144582,1555144596,1555144615,1555145046; Hm_lpvt_dbc355aef238b6c32b43eacbbf161c3c=1555145072'&#125;respond = requests.get(base_url,headers = headers)# print(respond.text)html = etree.HTML(respond.text)html_xpath = html.xpath('//a/img[@class="lazy"]/@data-original')# print(html_xpath)for i in html_xpath: respond = requests.get(i,headers=headers) j = i[-14:-4] with open('./pic/&#123;&#125;.jpg'.format(j),'wb') as f: f.write(respond.content)*******************************************添加代理***************************************import requestsbase_url = 'http://www.baidu.com'proxy=&#123; 'http':'http://alice:123456@120.78.166.84:6666', 'https':'https://alice:123456@120.78.166.84:6666'&#125;response = requests.get(base_url,proxies= proxy)response.encoding='utf-8'print(response.text)********************************************综合*******************************************import requests,refrom lxml import etreefrom urllib import parsebase_url = 'http://www.ccdi.gov.cn/'proxy = &#123; 'http':'http://alice:123456@120.78.166.84:6666', 'https':'https://alice:123456@120.78.166.84:6666'&#125;headers = &#123; 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'&#125;response = requests.get(base_url,headers=headers,proxies =proxy)# with open('3.html','wb')as f:# f.write(response.content)html_ele = etree.HTML(response.text)url_list = html_ele.xpath('//nav/span[5]/a/@href')# print(url_list)response_jdbgpage = requests.get(url_list[0],headers = headers,proxies =proxy)# with open('3.html','wb')as f:# # f.write(response_jdbgpage.content)html = etree.HTML(response_jdbgpage.text)url2_list = html.xpath('//ul[@class="menu_list"]/li[position()&gt;1]/a/@href')[0:3]# print(url2_list)for url3 in url2_list: new_url = parse.urljoin(url_list[0],url3) print(new_url) resp = requests.get(new_url,headers = headers,proxies =proxy) # with open('3.html','wb')as f: # f.write(resp.content) pat = re.compile('&lt;li class="on"&gt;&lt;a href="../../(.*?)/"&gt;') res = pat.findall(resp.text)[2:-1] print(res) for i in res: base_url2 = 'http://www.ccdi.gov.cn/special/jdbg3/&#123;&#125;/' # print(base_url2.format(i)) res2 = requests.get(base_url2.format(i),headers=headers) # with open('4.html','wb')as f: # f.write(res2.content) pat = re.compile('&lt;a href="(.*?)"') print(pat.findall(res2.text))******************************************多进程共享数据********************************#爬取中央纪检委举报违纪的实例import requests,refrom lxml import etreefrom day14.day14_2 import Dbfrom multiprocessing import Pool,Manager#爬取基本页信息def First_webpage(url,queue): print('函数1',url) headers = &#123; 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36' &#125; first_webpage_response = requests.get(url,headers=headers,proxies = proxy()) #匹配所有省份 pat = re.compile('&lt;a href="./(.*?)/"&gt;') province_list = pat.findall(first_webpage_response.text) print(province_list) for province in province_list[2:-1]: type_list = ['fjbxgdwt_jdbg3', 'sffbwt_jdbg3', 'sfjds_jdbg3'] for type1 in type_list: second_webpage_url = 'http://www.ccdi.gov.cn/special/jdbg3/&#123;&#125;/&#123;&#125;'.format(province,type1) print(second_webpage_url) queue.put((divoce_page_url,second_webpage_url))# 函数2 http://www.ccdi.gov.cn/special/jdbg3/bt_bgt/sfjds_jdbg3def divoce_page_url(url,queue): print('函数2',url) response = requests.get(url,proxies = proxy()) pat = re.compile('createPageHTML\((.*?), 0, "index", "html"\)') page_count = pat.findall(response.text) for page in range(int(page_count[0])): if page == 0: page_url = url + '/index.html' else: page_url = url + '/index_&#123;&#125;.html'.format(page) response = requests.get(page_url,proxies = proxy()) pat = re.compile('&lt;li class="fixed"&gt;\s+&lt;dl&gt;\s+&lt;dt&gt;\s+&lt;a href="./(.*?)" target="_blank"') url_list = pat.findall(response.text) for url_content in url_list: new_url = url +'/'+ url_content queue.put((get_content_url,new_url))def get_content_url(url): print('函数3:',url) response = requests.get(url,proxies = proxy()) response.encoding='utf-8' html = etree.HTML(response.text) content_list = html.xpath('//div[@class="content"]/div[1]/div[@class="TRS_Editor"]//text()') content = '\n'.join([item.strip() for item in content_list if item.strip() != '']) print(content)def proxy(): proxy = &#123; 'http':'http://alice:123456@120.78.166.84:6666', 'https':'https://alice:123456@120.78.166.84:6666' &#125; return proxyif __name__ == '__main__': pool = Pool(5) url = 'http://www.ccdi.gov.cn/special/jdbg3/index.html' queue = Manager().Queue() queue.put((First_webpage,url)) while True: func,url=queue.get() pool.apply_async(func=func,args=(url,queue))class Db: def __init__(self,database='kl',user='root',password='123456',port=3306,host='localhost'): self.db = pymysql.connect(database=database,port=port,password=password,user=user,host=host,charset='utf8mb4') self.cursor = self.db.cursor() def insert(self,sql,data): self.cursor.execute(sql,data) self.db.commit() def update(self,sql,data): self.cursor.execute(sql,data) self.db.commit() def __del__(self): self.cursor.close() self.db.close() beautifulsoup12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152from bs4 import BeautifulSouptext = """ &lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt; &lt;body&gt; &lt;p class="title p8" id="p1" data="1"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;b&gt;----2b&lt;/b&gt;&lt;/p&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;&lt;span&gt;&lt;b&gt;Elsie&lt;/b&gt;&lt;/span&gt;&lt;/span&gt;--alice&lt;/span&gt;&lt;/a&gt;, &lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and &lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class="story"&gt;...&lt;/p&gt; &lt;div class="item1"&gt;我是div&lt;/div&gt; &lt;div class="item2"&gt;我是div2&lt;/div&gt; 我是body"""#实例化对象soup = BeautifulSoup(text,'lxml')# #格式化# print(soup.prettify())#取第一个元素print(soup.div)#获取标签名print(soup.div.name)#获取文本内容print(soup.div.string)#获取标签中的所有内容包括子标签内的内容print(soup.body.get_text)#获取属性的方法print(soup.p['id'])#获取当前标签的所有的属性和值print(soup.p.attrs)#find_allprint(soup.find_all('div'))#findprint(soup.find('p'))#元素节点print(soup.body.contents)print(soup.body.descendants)#css选择器print(soup.select('p'))print(soup.select('a.sister'))res = soup.select('p&gt;b')print(res)res = soup.select('#p1,.story')print(res) scrapy框架12345流程:1.cd 到想要创建scrapy框架的文件夹里 2.scrapy startproject 框架名 3.生成爬虫程序scrapy genspider 程序名 网址 4.执行爬虫程序 scrapy crawl 爬虫名字 5.调试平台scrapy shell selenium12 db类1234567891011121314151617181920import pymysqlclass MysqlDatabase: def __init__(self): self.db = pymysql.connect(host='localhost',user = 'root',password='123456',port = 3306,database = 'kl',charset = 'utf8mb4') self.cursor = self.db.cursor() def insert(self,sql,data): try: self.cursor.execute(sql,data) self.db.commit() except: print('插入失败') def update(self,sql,data): try: self.cursor.execute(sql, data) self.db.commit() except: print('更新失败') def __del__(self): self.cursor.close() self.db.close() proxy代理类12345678910111213141516171819202122import requests,timeclass ProxyHelper: def __init__(self): self.url = 'http://mvip.piping.mogumiao.com/proxy/api/get_ip_al?appKey=4015b4545c6545d5b21a3b49fec0671c&amp;count=1&amp;expiryDate=0&amp;format=2&amp;newLine=2' self.version = 0 def get_proxy(self): print('获取了一个代理') self.proxy = requests.get(self.url).text.strip() return self.proxy,self.version def update_proxy(self,version): if version == self.version: self.proxy,self.version = self.get_proxy() self.version += 1 print('更新了一个代理:'+self.proxy) return self.proxyif __name__ == '__main__': helper = ProxyHelper() # proxy, version = helper.get_proxy() # print(proxy) # time.sleep(30) helper.update_proxy(0) print(helper.update_proxy(0))]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>代码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[时间复杂度例题]]></title>
    <url>%2F2020%2F04%2F07%2F%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E4%BE%8B%E9%A2%98%2F</url>
    <content type="text"><![CDATA[介绍可理解为程序运行步骤,主要看每一步需要运行的次数 大O表示法表明忽略常数项,只保留最核心,最特征的部分,忽略常量,低阶,系数 例题11234567891011121314151617181920212223242526a + b + c = 1000 and a**2 + b**2 = c**2 假设使用穷举法(枚举法),三层循环循环a,b,c的所有可能,加上两个if判断条件 得到时间复杂读1000*1000*1000*2若 a + b + c = 2000,则时间复杂度为 2000*2000*2000*2 若 a + b + c = N ,则时间复杂度为 N*N*N*2 O(n**3)解:for a in range(0, n): (循环用乘法计算) 这句代码的运行步骤是:O(n) for b in range(0, n): (循环用乘法计算) 这句代码的运行步骤是:O(n) c = n - a -b (基本操作语句为1) 运行步骤为:O(1) if a ^ 2 + b ^ 2 = c ^ 2:print(a,b,c)进入判断语句, 有两个分支(当碰到分支语句的时候,取分支步骤最多的) 分支1执行print(a,b,c) 步骤为1 分支2是判断条件失败,直接跳过了 步骤为0假设令c = N - a - b,使用双层循环,则时间复杂读为N*N *(1+max(0,1)) O(n**2) ######例题2 1234阅读电话簿中每个人的电话号码O(n)在电话簿中根据电话号码找人O(n)阅读电话簿中的以a开头的人的电话号码 O(n)在电话里簿中根据名字查找电话号码 O(logn) 学习思路 1Python数据结构与算法----&gt;机器学习和深度学习和数据挖掘----&gt;大数据-----&gt;NLP自然语言处理-------&gt;linux--------&gt;数据库底层-------&gt;其他底层知识]]></content>
      <categories>
        <category>数据结构与算法设计</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[json存储格式问题]]></title>
    <url>%2F2020%2F03%2F31%2Fjson%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[12345678910111213问题:TypeError: Object of type 'int64' is not JSON serializable (或者float32)class NpEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, np.integer): return int(obj) elif isinstance(obj, np.floating): return float(obj) elif isinstance(obj, np.ndarray): return obj.tolist() else: return super(NpEncoder, self).default(obj)json.dumps(cxy_data, ensure_ascii=False,cls=NpEncoder)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>json存储格式报错</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础指令]]></title>
    <url>%2F2020%2F01%2F07%2FLinux%E5%9F%BA%E7%A1%80%E6%8C%87%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux基础 123456789101112131415161718192021根目录下的文件 /bin 存放着系统必备执行文件 /boot linux启动核心文件 /dev linux的外部设备(device) /etc 配置文件和子目录 /home 用户目录 /lib 类似于windowns里的dll文件,动态连接共享库 /media 将识别到的光驱\u盘等挂载到这个目录下 /misc 存放杂项文件或目录 /mnt 临时挂载别的文件系统的(eg:光驱) /net 存放和网络相关的一些文件 /opt 主机额外安装软件存放的位置 /proc 虚拟目录,系统内存的映射,可获取系统信息 /root 系统管理员的主目录 /run 是系统运行时需要的 /sbin 存放系统管理员使用的系统管理程序 /srv 存放一些启动服务后需要提取的数据 /sys 安装的新文件系统sysfs /tmp 存放临时文件 /usr 等同于windows的program files,存放着很多用户文件和程序 /var 存放着不断扩充的内容,如日志 复制文件或文件夹12cp test1.py test2.py 复制文件cp -r test test1 递归复制整个文件夹 显示当前路径1pwd:显示当前路径 find查找文件12find *.log 找到所有以.log结尾的文件名find kl 找到kl名的文件夹 grep过滤1grep 字符串 文件名 |管道符123在命令之间建立管道,将前面命令的输出作为后面命令的输入通过命令查找tomcat进程：ps -ef | grep tomcat通过命令查找到占用此端口的进程编号：netstat -apn|grep 8080 #####移除文件或目录 123rm test1.pyrm -rf test 递归删除文件夹rmdir test 删除空文件夹 重命名和移动文件12mv test1.py test2.py 重命名mv /temp/movefile /targetFolder 移动文件夹 查看文件内容12345678910cat test1.py 会将所有的内容打印出来cat -n 文件名 列出行号,连同空白行cat -v 文件名 列出一些看不出来的特殊字符cat -b 文件名 列出行号,空白行不标注tac 文件名 从最后一行倒着行数打印more test1.py 查看文件内容,一页一页查看less test1.py 查看文档内容head -n 10 test1.py 查看文件头10行内容tail -n 10 test1.py 查看文件最后10行内容 重定向12ls -l&gt;a.txt 将列表的内容写入a.txt 覆盖写ls -al&gt;a.txt 将列表的内容写入a.txt 追加写 echo12echo 222 &gt;&gt; a.txt 将要显示的内容,存储到文件中echo 变量 显示变量的值 查看历史命令1history 时间日期类型1date +%Y%m%d 用户管理12useradd 用户名passwd atguigu #设置密码 进程线程类123ps -aux 查看系统中所有进程top 查看系统健康状态kill-9 id 终止进程,表示强迫进程立即停止 touch 文件名 创建一个文件 top123456top -u -kuailiang 可以查看自己账户下的进程或直接u后输入账号名称也可top下: shift + m 进程按照内存使用倒叙排 shift + &gt;或&lt; 翻页 shift + p 进程按照cpu使用情况倒叙排 批量kill: ps -ef|grep main_func|grep -v grep|cut -c 9-15|xargs kill -9]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘概念与技术]]></title>
    <url>%2F2019%2F12%2F30%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[必备知识概念 123456789101112131415数据挖掘,最简单的就是统计应用,进一步基于用户的浏览点击,收藏推断用户的年龄,性别等,更深层次的如预测机器学习是实现数据挖掘的一种方法和手段数据的最基本形式是数据库数据、数据仓库数据、和事务数据数据仓库:是一个从多个数据源收集的信息存储库数据挖掘(data mining):旨在从大量的、不完全的、有噪声的、模糊的、随机的数据中，提取隐含在其中的、人们实现不知道、但又是潜在有用的信息和知识数据挖掘的主要任务是关联分析、聚类分析、分类、预测、时序模式和偏差分析关联分析：两个或两个以上变量的取值之间存在的规律性称为关联。数据关联是数据库中存在的一类重要的、可被发现的知识。关联分析分为简单关联、时序关联和因果关联。关联分析的目的是找出数据库中隐藏的关联网。一般用支持度和可信度两个阈值来度量关联规则的相关性,还不断引入兴趣度、相关性等参数，使得挖掘的规则更符合需求。数据挖掘的6个步骤(基本)：1.理解业务：从商业的角度理解项目目标和需求，将其转换为一种数据挖掘的问题定义2.理解数据：收集初步的数据，进行数据的描述、数据探索和数据质量验证3.准备数据：将最初始的原始数据构造成适合建模的数据集4.建模： 选择和应用各种建模技术，并对其参数进行优化5.建模评估：对模型进行较为彻底的评价，并检查构建模型的每个步骤，确定其是否真正实现了预定的商业目的6.模型部署： 数据挖掘的基本技术 12345671.统计学====&gt;即指将数据集合假设一个分布或者概率模型,然后按照模型采用相同的方法来进行挖掘2.聚类分析和模式识别3.决策树分类技术4.人工神经网络和遗传基因算法5.规则归纳6.可视化技术7.关联规则 --&gt;关联可分为简单关联、时序关联、因果关联。目的是找出数据库中的隐藏的关联网 1234数据规范化:将数据按照指定比例进行缩放,映射到指定区域. 常用方法:min-max规范化,z-score规范化,按小数定标规范化数据规范化和数据标准化和数据归一化的区别: 数据规范化是一个大的概念,将不同渠道的数据按照同尺度进行度量,让数据句有可比较性。而数据归一化和数据标准化都是规范化的方式数据归一化：将数据映射到【0，1】的区间范围数据标准化是让规范化的数据呈现正态分布 数据仓库 1234567891011121314151617数据仓库:是一个从多个数据源收集的信息存储,存放在一个一致的模式下,并通常驻留在单个站点数据仓库是一个面向主题的,集成的,随时间变化的,但信息本身相对稳定的数据集合,用于对管理决策过程的支持四个特点: 1.面向主题:数据仓库都是基于某个明确主题,仅需要与主题相关的数据,无关细节数据被排除 2.集成的:从不同的数据源采集数据到同一数据源 3.随时间变化的:关键数据隐式或显式的基于时间变化 4.数据仓库的数据是不可更新的:数据装入后一般只进行查询操作,没有传统数据库的增删改查数据仓库与数据库的区别: 数据仓库是数据哭的升级,从逻辑上来说,数据仓库与数据库没有区别,都是通过数据库软件实现的存放数据的地方,从数据量来说,数据仓库要比数据库更庞大的说.数据仓库主要用于数据挖掘和数据分析,辅助领导者做决策.数据仓库的表结构是依照分析需求,分析维度,分析指标进行设计的数据仓库架构图: ODS层:为临时存储层,是接口数据的临时存储区域,为后一步的数据处理作准备.ods层的表通常包括两类,一类是用于存储当前需要加载的数据,一个用于存储处理完后的历史数据.数据粒度通常是最细的. PDW层:为数据仓库层,pdw层的数据应该是一致的,准确的,干净的数据,即对源系统数据进行了清洗后的数据,数据粒度通常和ods层相同. DM层:为数据集市层,这层数据是面向主题来组织数据的,通常是星型或者雪花结构的数据,从数据粒度来说轻度汇总级的数据. APP层:为应用层,这层数据完全是为了满足具体的分析需求而构建的数据,也是星型或雪花结构的数据.从数据粒度来说是高度汇总的数据.为什么构建分离的数据仓库: 1.提高两个系统的性能,操作数据库是为已知的任务和负载设计的,如使用主关键字索引和散列,检索特定的记录,和优化查询 2.宁一方面数据仓库的查询通常是复杂的,涉及大量数据在汇总级别计算 聚类 12345678聚类是一个把数据对象集划分成多个组或簇的过程,是的簇内的对象具有很高的相似性,但与其他簇的对象很不相似聚类的应用:数据分割和离群点检测(如信用卡欺诈检测)层次聚类:分为凝聚(自下而上)和分裂(自上而下)划分依据:距离(曼哈顿和欧式距离,但是只能聚成球状簇),密度(任意形状的簇,过滤噪声或离群点,将对象集划分成多个互斥的簇或簇的分层结构)聚类最本质最基本的是划分,他把对象组织成多个互斥的组或簇最著名的、最常用的为k均值和k中心点。k-均值（基于形心的划分技术（形心代表该簇），形心可以为该组对象的均值或者 中心点等） k-均值 123456789101112131415161718192021222324252627282930算法:k-均值.用于划分的k-均值算法,其中每个簇的中心都用簇中所有对象的均值来表示输入: k:簇的数目 D:包含N个对象的数据集输出:k个簇的集合方法: 1)从D中任意选择k个对象作为初始簇中心 2)repeat 3)根据簇中对象的均值,将每个对象分配到最相思的簇 4)更新簇均值,即重新计算每个簇对象的均值 5)until不在发生变化不能保证得到的结果为全局最优解,通常情况下都是局部最优解,变种,k-众数怎样提高k-均值算法的可伸缩性?1.在聚类时使用合适规模的的样本2.使用过滤方法,使用空间层次数据索引节省计算均值的开销3.利用微聚类的方法,首先把临近的对象划分到微簇中,然后对这些微簇使用k-均值方法进行聚类K-Means是个简单实用的聚类算法，这里对K-Means的优缺点做一个总结。 ·K-Means的主要优点有： 1）原理比较简单，实现也是很容易，收敛速度快。 2）聚类效果较优。 3）算法的可解释度比较强。 4）主要需要调参的参数仅仅是簇数k。 ·K-Means的主要缺点有： 1）K值的选取不好把握 2）对于不是凸的数据集比较难收敛 3）如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。 4）采用迭代方法，得到的结果只是局部最优。 5）对噪音和异常点比较的敏感。 k-中心点 1234567k-均值算法对离群点敏感,因为存在对象远离大多数数据,因此分配到一个簇时候可能严重影响簇的均值。而k-中心点聚类的基本思想和K-means的思想相同，实质上是对K-means算法的优化和改进。算法步骤：（1）确定聚类的个数K。（2）在所有数据集合中选择K个点作为各个聚簇的中心点。（3）计算其余所有点到K个中心点的距离，并把每个点到K个中心点最短的聚簇作为自己所属的聚簇。（4）在每个聚簇中按照顺序依次选取点，计算该点到当前聚簇中所有点距离之和，最终距离之后最小的点，则视为新的中心点。（5）重复（2），（3）步骤，直到各个聚簇的中心点不再改变。 5种主要聚类算法 123451.k-means聚类(优化:k-medians)原理见上面 ===&gt;优势:速度非常快,只计算了点和群中心的距离 缺点:离群值点较敏感,同时还需要确定划分的组数/类,也会影响最终的结果.2.均值偏移聚类算法 一种基于滑动窗口的算法,试图找到密集的数据点(如:给定一个半径的圆移动直至圆里面点的数量最大),均值偏移是一个爬山算法,他需要在每步中反复地将这个内核移动到一个更高的密度区域,直到收敛. 优势:与k-means相比,均值偏移不需要选择聚类的数量,他会自动的发现. 缺点是:窗口大小和半径的选取非常重要.3.DBSCAN聚类算法:比较代表性的基于密度的聚类算法,类似均值偏移聚类算法 优势:不需要预设定的聚类数量,并且将异常值识别为噪声,同时可以很好的找到任意形状和大小的簇类. 缺点:当聚类具有不同密度的时候,他的性能不像其他聚类算法那样好.当密度变化的时候,距离阈值和识别临近点的minpoints的设置会随着聚类的不同而变化.4.层次聚类算法:分为自下而上和自上而下.自下而上指的是一开始将每个数据点视为一个单一的聚类,然后依次合并(聚集类),知道所有的类合并成一个包含所有数据点的单一聚类,自上而下的层次聚类称为合成聚类或HAC.(选择一个度量两个聚类之间距离的距离变量如平均连接聚类) 优势:无需确定聚类的数量,对距离度量的选择不敏感.(是以低效率为代价的)5.使用高斯混合模型(GMM)和期望最大化(EM)聚类 初始选择聚类的数量,给定每个聚类的高斯分布,计算每个点属于特定簇的概率,离中心越近越可能属于那个聚类.基于这些概率,我们为高斯分布计算一组新的参数,这样我们就能最大程度的利用聚类中的数据点的概率. 文本挖掘 12345文本数据挖掘跟自然语言处理不是一回事,但是有很多的相通之处自然语言处理是计算机科学领域与人工智能领域的一个重要方向.它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法.文本挖掘利用智能算法,分析大量的非结构化文本源,抽取或标记关键字概念,文本挖掘是一个多学科混杂的领域,包括数据挖掘技术、信息检索、机器学习、自然语言处理、计算机语言学、统计数据分析、线性几何、概率论、图论等等。（总结：将结构化数据挖掘技术应用于文本数据）理论上自然语言处理是文本挖掘的基础 谱聚类 12345678910111213141516谱:方阵作为线性算子,它的所有特征值得全体称为方阵的谱.方阵的谱半径为最大的特征值.矩阵A的谱半径为矩阵A转置A的最大特征值谱聚类:谱聚类是一种基于图论的聚类方法,通过对样本数据的拉普拉斯矩阵的特征向量进行聚类的目的.谱聚类可以理解为将高维空间的数据映射到低维,然后在低维空间用其他聚类算法进行聚类.谱聚类算法的描述: 1.输入:n个样本点X=&#123;X1,X2,.....,Xn&#125;和聚类簇的数目k 2.输出:聚类簇A1,A2,...Ak (1)计算n*n的相似度矩阵W (2)计算对角矩阵D (3)计算拉普拉斯矩阵L = D - W (4)计算L的特征值,将特征值从小到大排序,取前K个特征值,并计算前k个特征值的特征向量 (5)将特征向量组成矩阵U (6)使用k-means算法将新样本点Y=&#123;Y1,Y2,....,Yn&#125;聚类成簇C1,C2,....,Ck (7)输出A1,A2,A3....,Ak相似度矩阵: 相似度矩阵就是样本中点的距离,在聚类算法中可以表示为距离近的点他们之间的相似度比较高,而距离远的点他们之间的相似度低,甚至可以忽略.表示相似度矩阵的三种方式:1.KNN 2.伊普西塔-近邻法 3.全连接法 数学概念 12345671.中心趋势度量:均值、中位数、众数截尾均值：丢掉高低极端值后的均值加权算术均值(加权平均)中位数:是有序数据的中间值,对于非对称数据是数据中心跟好的度量极差:最大值与最小值之差, 分位数:取自数据分布的每隔一定间隔上的点,把数据划分为基本上大小相等的连贯集合,2-分位相当于中位数,4-分位相当于三个数据点五数概括:识别可疑离群点的通常规则是,挑选落在第三个四分位数之上标称属性:]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>数据挖掘概念与技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas数据处理]]></title>
    <url>%2F2019%2F12%2F25%2Fpandas%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[###pandas数据算术运算 pandas元素相加 1df1+df2 pandsa和series相减 1df1-s1 apply和applymap 12345#apply操作DataFrame的每一列,而applymap操作DataFrame的每一个元素f1 = lambda x : "%.3f"%xdf1.applymap(f1)f2 = lambda x:x.max() - x.min()df1.apply(f2) pandas数据修改123456789101112131415161718191.数据复制-直接赋值df1 = df.head()df1.iloc[0,0] #原数据df.iloc[0,0] = 10 #对应df1的数据也会变化2.数据复制-copy()函数df1 = df.head().copy() df1.iloc[0,0] #原数据df.iloc[0,0] = 10 #对应df1的数据不会变化3.行列删除 ·删除一列 del df1['sad'] ·删除多列 pd.drop(['a','b','c'],axis=1) ·删除一列并赋值 pd.pop() ·删除行 pd.drop(['a','b','c'],axis=0)4.列增加 ·[],列名方式增加多列 df['s1','s2'] = ·loc,不能新增多列 pd.loc[:,'new_column'] = ·insert 在指定位置 df1.insert(4,'ss',np.linspace(0,1,len(train1 )))5.行增加 ·pd.loc[:'new_index'] = 索引顺序调整12341.同时调整行和列 reindex(index=,columns=[])2.单独调整行或列 reindex_axis() 列格式修改1astype(dtype.copy=True) 数据排序12pd.sort_index(axis,ascending=,inplace)pd.sort_values(by='a',ascending=False,inplace=True) 数据转换12df2 = df.Tdf3 = df.transpose() 重复值12341.重复值查看 ·df.duplicated() 返回的为布尔值 duplicated(subset = None ,keep='first',inplace=False)2.删除重复值 drop_duplicates() #默认所有列去重,可以选择指定列 ####部分值替换 123df1.replace([-99,-100],[NA,0]) #替换多个值为不同值df1.replace(&#123;-99:Na,33:0&#125;) #替换指定数据为指定值df1.replace([1,2],Na) #替换为同一个值 缺失值处理12345#缺失值确认:isnull、notnull#缺失值处理：dropna、fillnadf1.dropna() #删除存在Nan的所有行df1.dropna(axis=1,how="all") #删除所有行数据全为Nan的列 df2.fillna(&#123;1:111,4:444&#125;) #传入字典,将第一列和第四列的所有空值都替换成指定的数据]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>数据处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql进阶知识点汇总]]></title>
    <url>%2F2019%2F12%2F24%2Fmysql%E8%BF%9B%E9%98%B6%E7%9F%A5%E8%AF%86%E7%82%B9%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[mysql引擎 12345水平分表:当一个表的数据太大时效率大大降低,水平分表就是将整个大表拆成不同的小表,每一次查询会判断数据在哪个表中,每一次查询会判断在那个表中,然后对应去查找,提高效率垂直分表:当表中某些字段不常用的时候可以把不常用的字段切到宁一个表中,然后建立关联关系innodb:最常用的引擎,支持事务,行锁,外键等MyISAM:存储效率高,支持分表Memory:速度快 mysql锁机制 12345678910111213141516锁原理:锁是计算机协调多个进程或线程并发访问某一资源的机制分类: 一.对数据操作的类型划分为读锁和写锁 ·读锁(共享锁):针对同一份数据,多个读操作可以同时进行而不会相互影响 ·写锁(排它锁):当前写操作没完成时会阻断其他写锁和读锁 总:读锁会阻塞写，但是不会阻塞读。而写锁则会把读和写都阻塞 二.对数据操作粒度划分为表锁、行锁和页锁 ·Mysiam、memory支持表锁，innodb既支持表锁也支持行锁,innodb与mysiam的最大区别一个是支持事务,宁一个就是支持行锁 三种锁的区别 表锁:开销小,加锁快,不会出现死锁,发生锁冲突的概率最高,并发度最低 行锁:开销大,加锁慢,会出现死锁,发生锁冲突的概率最低,并发度最高 页锁:开销介于表和行锁之间,会出现死锁,并发度一般 mysql行锁变表锁,Innodb只有在通过索引条件检索数据时使用行级锁,否则使用表锁 总结:Innodb的行锁是针对索引加的锁,不是针对记录加的锁.并且该索引不会失效,否则都会从行锁升级为表锁.表锁加锁方式:自动加锁,查询操作,会自动给涉及的所有表加读锁,更新操作(update,delete,insert),会自动给涉及的表加锁行锁加锁方式:自动加锁,对于update,delete和insert,innodb会自动给涉及数据集加排它锁,对于普通select语句,Innodb不会加任何锁,当然我们也可以显示的加锁表锁读锁会阻塞写但不会阻塞读,而写锁则会把读写都阻塞 mysql的索引 1234567mysql中索引的存储类型有两种BTREE和HASH,也就是用树或者hash值来存储该字段Myisam和InnoDB引擎只支持BTREE索引Memory 支持HASH和BTREE索引分类(常用): ·单列索引:只有一列的索引, ·组合索引:在表中的多个字段组合上创建的索引(最左前缀原则) ·全文索引:只在myisam引擎上才能使用]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>进阶知识</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas数据分析书籍内容]]></title>
    <url>%2F2019%2F12%2F18%2Fpandas%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%A6%E7%B1%8D%E5%86%85%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[针对Series和Pandas如果传入的列没有数据则默认全部为nan值 Series方法1234567891011121314s1.valuess1.indexs1.reset_indexs1.isnulls1.notnulls1.sort_valuess1.sort_indexdf.reindex([]) 重造索引,不存在的值为nan,也可以使用fill_value=自己补值,也可以method向前向后填充s1+s1会按照索引相加减,索引不同的值为nans1.index.is_uniques1.rank()s1.unique()s1.value_counts()s1.isin(容器) ###Pandas方法 123456789101112131415df.字段名 == df ['字段名']del 删除列 drop默认删除行,axis=1表示列df.Tdf3.index.namedf3.columns.namedf.reset_index 重置索引df.sort_index() 里面加by可以针对某列效果同sort_valuesdf.sort_values()df.rank(ascending,method)df.columns/index/具体字段名.is_uniquedf.sum\max\min\consum\describedf['one'].unique()df['one].isin(容器)df.fillna()fillna\dropna]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python代码规范]]></title>
    <url>%2F2019%2F12%2F16%2FPython%E4%BB%A3%E7%A0%81%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[函数规范12345678#声明函数的作用以及各个参数的用法def func(a,b,c): ''' :param a: :param b: :param c: :return: ''' 缩进1四个空格或者直接使用tab符号 行最大长度1每行代码最大字符不超过80个 类123类与类之间空两行类中方法与方法空1行,类与方法空一行类命名使用驼峰命名法 注释123单行注释：若注释独占一行，#号顶头，空1格后写注释；若是行尾注释，空2格后#号再空1格写注释多行注释：三对双引号（推荐使用）和三对单引号复杂逻辑一定要写注释 引号123自然语言使用双引号,机器标识使用单引号,代码里多数使用单引号正则表达式使用双引号解释说明文档使用三引号(三单或三双) 空行123模块级函数和类定义之间空两行类成员函数之间空一行函数中可以使用空行分隔出逻辑相关的代码 开头规范123456#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on 2019-11-26 代码简化 多进程@author: lxy""" import语句12345678910111213141516171819import语句应该分开写不推荐一下写两个import语句应该按照顺序排列,每组之间用一个空行分隔模块尽量使用小写命名，首字母保持小写，尽量不要用下划线(除非多个单词，且数量不多的情况)# 正确的模块名import decoderimport html_parser# 不推荐的模块名import Decoder# 正确的写法import osimport sys# 不推荐的写法import sys,os# 正确的写法from subprocess import Popen, PIPE 函数12函数名一律小写,如有多个单词,用下划线隔开私有函数在函数前加一个下划线 变量名命名规则12345变量严格区分大小写变量不能以数字开头变量不能含有特殊符号变量不能含有中文变量命名要有实际意义 编码建议1234561.编码中要考虑其他Python实现的效率问题,比如运算符‘+’在CPython（Python）中效率很高，都是Jython中却非常低，所以应该采用.join()的方式。2.尽可能使用is is not 取代 == 3.异常中try的代码尽可能少4.startswith()和endswith()优于切片前缀或后缀查看5.字符串不要以空格结尾6.二进制数据判断,使用if boolvalue的方式]]></content>
      <categories>
        <category>规范</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python将列表分成很多小列表]]></title>
    <url>%2F2019%2F11%2F26%2Fpython%E5%B0%86%E5%88%97%E8%A1%A8%E5%88%86%E6%88%90%E5%BE%88%E5%A4%9A%E5%B0%8F%E5%88%97%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[方法1:推荐123ls = [i for i in range(15)]n = 3 #大列表中几个数据组成一个小列表ls[i:i+n] for i in range(0,len(ls),n) #####方法2: 1234567def list_of_groups(init_list, childern_list_len): list_of_group = zip(*(iter(init_list),) *childern_list_len) end_list = [list(i) for i in list_of_group] count = len(init_list) % childern_list_len end_list.append(init_list[-count:]) if count !=0 else end_list return end_listlist_of_groups(ls,2)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>经验</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[re模块断言]]></title>
    <url>%2F2019%2F11%2F21%2Fre%E6%A8%A1%E5%9D%97%E6%96%AD%E8%A8%80%2F</url>
    <content type="text"><![CDATA[对于xxx这种格式当我们想要获取title的标题内容的时候,方法一是通过的括号分组实现,宁外一种方法就是断言 pattern = re.compile(‘(?&lt;=title&gt;).*(?=)’)re.findall(pattern,’xxx‘) 断言条件 解释 (?=exp) 匹配exp前面的位置 (?!exp) 匹配后面跟的不是exp的内容 (?&lt;=exp) 匹配exp后面的内容 (?&lt;!exp) 匹配前面不是exp的位置 (?:exp) 匹配exp,不捕获匹配的文本,也不给此分组分配组号 (exp) 匹配exp,并捕获文本到自动命名的组里 匹配exp前面后匹配exp后面的可以一起使用,这样就可以获取到中间的数据了 123456789101112131415. 表示一切字符\ 起转义作用[...] 方括号中的任意字符\d 数字0-9\D 非数字\w 数字,字母,下划线\W 非数字,字母下划线\s 空格换行符等\S 非空格* 前面字符&gt;=0+ &gt;=1? 0或1&#123;m&#125; 匹配m次&#123;m,n&#125; 匹配m到n次&#123;m,&#125; 至少匹配m次 re模块字符串匹配1234567方式1 :re.findall(r&apos;([a-z]):&apos;,s1,re.I) #匹配获得所有括号内的内容,re.I表示不分大小写方式2 :patten = re.compile(r&apos;\d+&apos;) patten.findall(s1)s = &apos;sim@163.com work-da intrest:pingpang&apos;re.split(&apos;[@\-:\s+]&apos;,s)返回结果: [&apos;sim&apos;, &apos;163.com&apos;, &apos;work&apos;, &apos;da&apos;, &apos;intrest&apos;, &apos;pingpang&apos;]re.sub(&apos;1.5[lLtT]&apos;,&apos;1.5L&apos;,s) #将1.5l,1.5t,1.5T全部替换为1.5L]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>re正则</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PCA降维]]></title>
    <url>%2F2019%2F11%2F21%2FPCA%E9%99%8D%E7%BB%B4%2F</url>
    <content type="text"><![CDATA[PCA主成分分析1.降维的概念: 123456降维是一种对高维度数据预处理的方法,保留重要的特征,去除噪声和不重要的特征,从而实现提升数据处理速度的目的.降维的方法:1.SVD奇异值分解2.PCA主成分分析(使用最广泛)3.FA因子分析4.ICA独立成分分析 2.PCA的概念 1思想是将n为特征映射到k维上,k维为全新的正交特征,也被称为主成分.第一个坐标轴为原始数据中方差最大的方向,第二个坐标轴是与第一个坐标轴正交的平面中使得方差最大的,第三个轴是与第一、二个轴正交的平面中方差最大的.后面的坐标轴的方差几乎为0,可以忽略,从而实现降维. 如何获得主成分 1计算数据矩阵的协方差矩阵,得到协方差矩阵的特征值、特征向量，选择特征值最大的k个特征所对应的特征向量组成的矩阵。这样就实现了降维。====获取特征值、特征向量的两种方法，特征值分解和奇异值分解 sklearn调用PCA 12345from sklearn.decomposition import PCAimport numpy as npX = np.array([[-1,2],[-1,2],[1,2]])pca=PCA(n_components=1)pca.fit(X)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>数据预处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[遇到的问题及总结]]></title>
    <url>%2F2019%2F11%2F21%2F%E9%81%87%E5%88%B0%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[1234562.Series重命名s1.name='hello',rename不好使3.方差是协方差的特殊情况，两者公式基本无差别，方差针对一维，协方差针对二维，n-1对应无偏估计4.re模块.想要匹配换行符时候,可以在pattern里面添加re.DOTALL eg:pattern = re.compile('&lt;meta name="description" content="(.*)?_百度百 科',re.DOTALL) re.findall(pattern,response.text)5.re模块断言(?&lt;="description" content=) 匹配"description" content=后面的内容,除了findall能匹配括号内的内容,正常会将条件全部获取到,而断言可以实现只获取指定部分. 问题 出现原因 错误表现 解决方法 mysql死锁 查询和删除同时进行,或者连接未关闭进行删除操作等 Navicat打不开表,一直显示正在加载 首先show processlist查询状态中有lock字样的进程id,然后分别杀死 数据仓库和数据库的区别 看书想到的 概念性 数据仓库(DataWareHouse,DW或DWH)是一种面向主题,集成的,稳定的,反映历史变化的数据集合,用于支持管理决策.面向主题:数据仓库中的数据按照一定的主题域进行组织.集成:原有分散的数据库数据经过系统加工,消除源数据中的不一致性相对稳定:指一旦某个数据进入数据仓库后只需定期的加载和更新反映历史变化:指通过信息,对企业未来趋势定量做出分析预测.数据仓库与数据库区别:1.数据库是面向事务的,而数据仓库是面向主题设计的2.数据库中存储的一般为实时数据,而数据仓库一般为历史数据3.数据库设计尽量避免冗余,而数据仓库是有意引入冗余4.数据库是为了存储数据设计的,而数据仓库是为了分析数据引入的. 内存泄漏 代码出现 指程序中动态分配的堆内存由于某种原因程序未释放或无法释放,造成内存的浪费,导致程序运行缓慢甚至系统崩溃的后果 折旧率 代码出现 折旧率指的是一个物品在使用一段时间后，其市场价格的变化率，例如一部汽车用100万元购入，经过一年如果想脱手，大概只能卖70万，要折价30万才卖得掉，这部车的首年折旧率就是30%，在卖方来说，折旧率低表示使用成本低，当然折旧率越低越好 SSH 想到的 SSH 为 Secure Shell,SSH是一种网络协议，用于计算机之间的加密登录 NULL mysql必知必会 NULL表示无值,与空字符串、空格等均不同。要使用IS NULL或者IS NOT NULL去判断,通常不返回他们,返回整条数据或者其他字段 mysql操作符优先级 mysql必知必会 在无括号情况下,and的优先级高于or mysql模糊匹配区分大小写 mysql必知必会 正常情况不区分大小写,SELECT * FROM kl WHERE BINARY name like ‘C%’ (binary也可以放到like后面,或者在建表的时候字段限制时候使用binary也可以) mysql字段拼接 mysql必知必会 SELECT CONCAT(company,’:’,’参保人数:’,insured_num) FROM kl mysql去除字段左右的空格 mysql必知必会 TRIM()去除左右两边的空格.ltrim()去除左边的空格,rtrim()去除右侧的空格 mysql文本处理函数 mysql必知必会 UPPER()将小写转换为大写,LOWER()将大写转换为小写,LENGTH()返回字 符的长度,一个中文3个字符 时间日期函数 mysql必知必会 DATE()转为日期格式yyyy-mm-dd,year()返回年份,curdate()返回当前日期,curtime()返回当前时间,DATEDIFF()计算两个日期之差,time、day、month、minute、second、hour一样适用 数值处理函数 mysql必知必会 sin、cos、tan、pi、abs、sqrt等函数均支持，以及聚合函数avg、min、max、sum、count，注意：AVG()函数忽略列值为NULL的行。如果指定列名，则指定列的值为空的行被COUNT()函数忽略，但如果COUNT()函数中用的是星号（*），则不忽略。 获取列表\元组对应的索引和具体的内容 代码 enumerate —&gt; for key,value in enumerate() Python有哪些常见的web框架 问题 常用Flask\Django,其中flask属于轻量级适合刚入门的使用,Django需要了解很多的知识,全能型框架. 除此以外,还有性能高的tornado,基本的web.py以及对应二次开发web2.py,以及豆瓣所使用的的quixote框架 代码忽略警告 代码 import warnings warnings.filterwarnings(‘ignore’) .spydata文件读取 001 #####001.spydata文件读取 123456789101112import pickleimport tarfile# open a .spydata filefilename = 'D:\谷歌下载\技术统一表.spydata'tar = tarfile.open(filename, "r")# extract all pickled files to the current working directorytar.extractall()extracted_files = tar.getnames()for f in extracted_files: if f.endswith('.pickle'): with open(f, 'rb') as fdesc: data = pickle.loads(fdesc.read())]]></content>
      <categories>
        <category>day_analyse</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工作常用函数总结]]></title>
    <url>%2F2019%2F11%2F21%2F%E5%B7%A5%E4%BD%9C%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[####将结果存excel 123writer=pd.ExcelWriter(r'C://user/desktop')data.to_excel(writer,sheet_name='heet1')writer.save() 从excel读取数据1pd.read_excel(r'C:\users\adminstrator\desktop',sheet_name='Sheet1') ####从mysql读取数据 12345conn=pymysql.connect(host='localhost',user='root',password='123456',databse='comp,port=3306)sql='SELECT * FROM kl'data = pd.read_sql(sql,conn)conn.close()datasss 将DataFrame存入mysql1234from sqlalchemy import create_engineengine = create_engine('mysql+pymysql://root:123456@localhost:3306/work?charset=UTF8mb4')pd.io.sql.to_sql(df2, 'industry_1', engine, index=False, if_exists='append', chunksize=10000)engine.dispose() 一条一条存入mysql12345678db = pymysql.connect(host='localhost',port=3306,user='root',database='ceshi',password='123456',charset='utf8mb4')cursor = db.cursor()sql = 'insert into kll (name,sex,age) values(%s,%s,%s)'data = ('张三','男',23)cursor.execute(sql,data)db.commit()cursor.close()db.close() 批量插入mysql12345678db = pymysql.connect(host='localhost',port=3306,user='root',database='ceshi',password='123456',charset='utf8mb4')cursor = db.cursor()sql = 'insert into kl (name,sex,age) values(%s,%s,%s)'data = [('张三','男',23),('李四','男',32),('小美','女',18)]cursor.executemany(sql,data)db.commit()cursor.close()db.close() 进程池代码12345678pool = Pool(30)n = 10000count = (len(company_name) // n) + 1for num in range(count): company_list = company_name[num * n:(num + 1) * n] pool.apply_async(func, args=(company_list,))pool.close()pool.join() 将一对多转换成多对多1234con2 = pymysql.connect(host="192.168.0.222", port=3306, user="user_r", password='1q2w3e4r', db='industry_cn_test',charset="utf8")sql_zong = "select `names` as jiqun_name ,ipc_5_list from ipc_split_10000_c_list_v1 "res = pd.read_sql(sql_zong, con2)ipc_group_unique = res.drop('ipc_5_list', axis=1).join(res['ipc_5_list'].str.split(',', expand=True).stack().reset_index(level=1, drop=True).rename('ipcr')) ####将df中某两列互换 123456df = pd.DataFrame([[5,6],[4,3],[7,8]])df.columns = ['one','two']df['three'] = df['one']print(df)df.loc[df.one&gt;df.two,'three'] = df.loc[df.one&gt;df.two,'two']df es返回不符合条件的个数1234567891011es.search(index=['patent_cn_v71'],body=&#123; 'query':&#123; 'bool':&#123; 'must_not':&#123; 'exists':&#123; 'fields':'app_text' &#125; &#125; &#125; &#125;&#125;) 连接250数据(读)1conn1 = pymysql.connect(host='192.168.0.250',user='user_r',password='1q2w3e4r',database='pre_formal_2') ####连接250高权限 1conn1=pymysql.connect(host='192.168.0.250',user='user_rwd_2',password='5d6f7g8h',database='pre_formal_2')]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql必知必会学习]]></title>
    <url>%2F2019%2F11%2F06%2Fmysql%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[1234567891.查看表信息: DESC kl(describe的缩写,为后面的简写版---&gt;) 或者 SHOW COLUMNS FROM kl2.表名筛选 SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME LIKE 'SHEET%';3.查看加锁情况 SHOW OPEN TABLES WHERE IN_USE &gt; 04.通过检查table_locks_waited和table_locks_immediate状态变量分析系统上的表锁定: show status like 'table_locks%']]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python实现栈和队列]]></title>
    <url>%2F2019%2F09%2F30%2Fpython%E5%AE%9E%E7%8E%B0%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[python实现栈12345678910111213141516171819202122232425262728293031class Stack: def __init__(self): self.items = [] def isEmpty(self): return self.items == [] def push(self, item): self.items.append(item) def pop(self): return self.items.pop() def peek(self): return self.items[len(self.items) - 1] def size(self): return len(self.items)#将一个新项添加到栈顶s.push('dog')#从栈返回顶部元素但不会删除s.peek()#返回栈中的item数量s.size()#判断栈是否为空s.isEmpty()#从栈中删除顶部项s.pop()方式二:直接导包(下载)from pythonds.basic.stack import Stack python实现队列1234567891011121314151617181920212223class Queue: def __init__(self): self.items = [] def isEmpty(self): return self.items == [] def enqueue(self, item): self.items.insert(0,item) def dequeue(self): return self.items.pop() def size(self): return len(self.items)#入队enqueue#出队dequeue#查看队列大小size#查看是否为空isEmpty]]></content>
      <categories>
        <category>python数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python数据结构与算法]]></title>
    <url>%2F2019%2F09%2F30%2FPython%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[介绍数据结构与算法对于Python而言是他的核心,但对于Python而言内置了基础的数据结构与算法,弱化了数据结构与算法的使用 数据结构主要涉及，顺序表，链表，堆栈（栈存储的为局部变量，而栈内存存储的为局部变量），队列，树，二叉树，平衡二叉树，红黑树 算法主要涉及排序算法（冒泡排序，选择排序，插入排序，快速排序，希尔排序，归并排序）和查找算法（顺序查找，二分法查找，二叉树查找，哈希查找） 顺序表12Python中的list和tuple都是顺序表结构,list是动态顺序表,支持内部结构变化如增加或者减少元素,而tuple并不支持结构的改变,其他性能和list一致list结构中,append的时间复杂度为O(1),而insert插入函数是在插入位置之后的元素依次向下挪动一个位置,复杂度为O(n),同理pop()删除最后一个位置时,时间复杂度为O(1),当删除指定元素时,则该元素其后的元素依次挪动一个位置,时间复杂度为O(n) 单链表123456789101112131415161718192021222324252627282930313233343536链表是由一个个节点连接而成,节点由两部分构成:元素域、链接域；链接域链接下一个节点 节点对象Class node： def __init__(self,item): self.item = item self.next = None 链表对象class SinglyLinkedList: """链表对象""" def __init__(self): self._head = None def add(self, item): """ 头部添加节点 :param item: 节点值 :return: """ node = Node(item) node.next = self._head self._head = node def append(self, item): """ 尾部添加节点 :param items: :return: """ cur = self._head if not cur: # 判断是否为空链表 self.add(item) else: node = Node(item) while cur.next: cur = cur.next cur.next = node]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法代码练习]]></title>
    <url>%2F2019%2F09%2F27%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[推荐网址:https://blog.csdn.net/weixin_41624982/article/details/89486592 基本概念123456789101112131415161.描述:抽象数据类型(ADT),是计算机领域中被广泛接受的一种思想和方法,也是一种设计和实现程序模块的有效技术.模块通过接口来提供功能所需的信息,并不涉及具体实现细节.2.概念:①数据表示的完全暴露②对象表示和操作对具体表示的依赖性③抽象数据类型提供的操作包括三类,构造操作,解析操作,变动操作.④Python所有的内置类型都是一种抽象数据类型⑤ADT是一种思想也是一种技术数据结构:指相互之间存在一种或多种特定关系的的数据元素的集合数据结构划分:逻辑结构、物理结构、数据运算数据结构的分类：栈（Stack）、队列（Queue）、树（Tree）、堆（Heap）、数组（Array）、链表（Linked List）、图（Graph）、散列（Hash）3.几种排序算法介绍: 1.冒泡排序:就是两两比较,互换位置,每一轮找到一个最大值放在数组右边,多轮循环就排好了.代码主要涉及循环次数和两两互换条件的判断. 优化:当一轮交换后数组顺序没有变化的时候停止排序 2.快速排序(左右指针法,挖坑法):数组中确定一个基准数,将大于基准数的放在右边,小于基准数的放在左边.具体实现是首先确定一个基准数(通常为数组第一个数),从后往前找到小于基准数的第一个数,并记下位置,与基准数互换位置,然后在从前往后找到大于基准数的第一个数,与基准数互换位置,再从后往前(前一次记录的位置开始)找到小于基准数的第一个数,重复,直到从前找的位置和从后找的位置大于等于,左边的全部比基准数小,右边比基准数大.然后两边再分别使用相同方法,循环 优化:①随机选取基准值(否则在一个完全有序的数组里，我们的快速排序就会退化为log（n^2②配合使用插入排序③当大量数据且重复数据较多时,用三路快排(小于,等于,大于)---&gt;推出双路快排为将等于部分分散到左右两端 3.选择排序:首先在数组中找到最大(最小)的元素,然后在剩余数组中找到最大(最小),以此类推,直到排序完成 4.插入排序,类似于排序扑克牌,取出一个数据然后与已经排好序数组的比较,插入到合适的位置,以此类推, 5.归并排序:采用分治法,分为分和治两个阶段,分,指的是将一个数组分成两部分,然后重复不断的分直到只剩一个个单一的元素.然后在对左右两边的数组进行治,将左右两边的数组分别排序好,得到两个排好序的数组.最后进行合并.合并的原理是两个数组第一个数字比较,小的放到新的数组里的第一位,然后小的数的那个数组的第二个数,继续与宁一个数组的第一位数相比较,两者小的放入到新数组第二位,如此循环得到排序好的新数组 6.希尔排序===&gt;插入排序的改进(缩小增量排序):首先选择一个间隔x(小于n)将全部数组分成若干个子序列(相同间隔x的放一起),每一个子序列中进行插入排序,合并后.缩小间隔,重复子序列划分和排序,直到间隔x为1(https://blog.csdn.net/weixin_37818081/article/details/79202115?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase)x每次减半或者1/3都可 7.堆排序:堆是一颗完全二叉树,每个节点的值总是不大于或者不小于父节点的值,由此形成大顶堆和小顶堆.首先将所有数字存储在堆中,按照大顶堆建堆,每次将最大的数取出,剩下的数继续可以找到最大的数,如此可以找到倒叙新的数组,将取出的数字按照相反的顺序进行排列,数字就完成了排序 8.桶排序:根据待排序数组中的最大和最小值(差值和映射规则判断)确定桶的个数,然后遍历每一个元素放在指定的桶中,最后得到排序好的结果,顺序表、链表、堆栈、队列、树、二叉树、平衡二叉树、红黑树；算法将涉及排序算法（冒泡排序、选择排序、插入排序、快速排序、希尔排序、归并排序）、查找算法（顺序查找、二分法查找、二叉树查找、哈希查找） 几种树1234满二叉树:除了叶子结点,每一个节点都有两个子节点(深度k 节点总数为(2^k)-1,则它就是满二叉树)完全二叉树:若二叉树的深度为k,除了k层外,其余各层的结点数都达到最大,第k层所有的节点都连续集中在最左边平衡二叉树:左子树和右子树的的深度之差的绝对值不能超过1,且左右子树都是平衡二叉树最优二叉树(哈夫曼树):树的带权路径长度达到最小, ####python实现链表 输入一个链表，按链表从尾到头的顺序返回一个ArrayList(牛客网)==形如栈 12345678910111213141516# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution:#例如[1,2,3] def printListFromTailToHead(self, listNode): if not listNode: return [] l = [] head = listNode while head: l.insert(0,head.val) head = head.next return l 二分法(仅当列表是有序的时候有用)=====第一种算法123456简单查找(傻找):如1到100中找99,一个个找需要99次.二分查找每次排除一半的数字对于简单查找找到需要n步,而对于二分法查找找到需要log以2为底n的对数low = 0 high = len(ls) - 1mid = (low + high)//2然后根据值对比修改high和low,小了low = mid +1 ,大了high = mid-1 大O表示法(记录的是最糟糕最长的时间情况)12345678910111213用来表示算法的运行时间,以及运行时间随着列表的增长的变化情况:如对数越往后几乎不增长,而线性一直增长很快常见的五种大O表示法O(logn)、O(n)、O(n*logn)、O(n**2)、O(n!)logn优于n优于nlogn优于n**2优于n!·算法的速度并非指的是时间而是操作数的增速·讨论算法的速度时,我们说的是随着输入的增加,其运行时间将以怎样的速度增加·算法的运行时间用大O表示法表示·O(logn)比O(n)快,当需要搜索的元素越多时,前者比后者快的多计算机内存可以看成一个个抽屉,每个抽屉有对应的物理地址常见的大O运行时间: 1.二分查找 O(logn)、简单查找 O(n)、 快速排序 O（平均nlogn，最糟糕n**2）、选择排序O（n**2）、旅行商问题O（n！） ####数组和链表 1234567数组表明所有代办事项在计算机的内存中是相连的,紧靠在一起,当后面位置被占用的时候,就需要整体迁移到有空闲位置的地方(他们必须相连),添加元素速度很慢解决方法: 1.给预留位置,先占内存 缺点:①额外预留的位置用不上 ②待办事项超过指定预留后,还得转移 2.使用链表解决链表:链表中的元素可以存储在内存中的任何地方,链表中的每一个元素都存储了下一个元素的地址,使一系列内存连接在一起.链表中添加元素很简单,只要将后一个元素的地址放到前一个元素中即可 使用链表时根本就不需要移动元素,如为数组分配1000个位置,内存有1000个位置但不在一起,就无法为该数组分配内存.而链表不需要考虑那么多 链表在插入数据上面的优势特别明显(无论结尾插入还是中间插入),数组在随机读取速度的优势特别明显,链表删除数据也比较方便。不同如插入，删除操作一定能成功，插入操作当内存空间不够时，插入操作可能会失败。删除操作任何情况都能成功 12链表读取时间O(n),存储时间O(1),删除时间O(1)数组读取时间O(1),存储时间O(n),删除时间O(n) 1234为什么数组都是以0为开头的索引?从0开始让基于数组的代码编写起来更容易,因此程序员都坚持这么做.通常情况下数组用的多,数组支持随机访问,而链表只支持顺序访问 选择排序=====第二种算法12如一堆数中按照大小进行排序,每次的时间复杂读为O(n),执行n次,总的为O(n**2)每次取一个最大,依次排 python实现选择排序123456789101112131415def func(ls,data_max): for i in range(len(ls)): if ls[i]&gt;data_max: data_max = ls[i] return data_maxls = [1,21,15,20,3,5,6,2,13]ls2 = []data_max = ls[0]def func2(): for i in range(len(ls)): max_data = func(ls,data_max) ls2.append(max_data) ls.remove(max_data) return ls2func2() 快速排序(D&amp;C算法)=====第三种算法(基础:递归)123456789101112快排比选择排序快很多,是一个优雅的算法(并非一种解决问题的算法,而是一种解决问题的思路)工作原理:1.找出简单的基线条件 2.确定如何缩小问题的规模,使其符合基线条件有时候循环可以简单的完成任务,为何还要使用递归方式呢,因为函数式编程语言(如Haskell)没有循环,只能使用递归来编写这样的函数快速排序思想为通过一趟排序将要排序的数据分割成独立的两部分,其中一部分的所有数据都比宁一部分的所有数据都要小,然后按此方法对两部分数据进行快速排序,整个排序过程可以递归进行快排流程: 1.首先设定一个分界值,通过该分界值将数组分成左右两部分(随机选取通常为数组的第一个值) 2.将大于或等于分界值的数据集中到数组右边,小于分界值的数据集中到数组的左边.使得左边的数值都小于等于分界值.右边的元素都大于或等于分界值 3.然后左边和右边的数据可以独立排序.左边的数据又可以取一个分界值,将此部分数据局变成左右两部分,同样在左边放置最小值,右边放置最大值.右侧的数据做类似的处理 4.重复上述步骤.递归进行. 图地址:https://bkimg.cdn.bcebos.com/pic/b7003af33a87e950707fdf2110385343fbf2b416?x-bce-process=image/resize,m_lfit,w_220,h_220,limit_1 ####合并排序(归并排序) 12345合并排序的时间总是O(nlogn),而快排平均时间为O(nlogn),最糟糕时候为n**2流程: 1.将待排序的元素分成大小大致相同的两个字序列 2.将两个字序列进行合并排序 3.将排序好的有序序列进行合并,得到最终的有序序列 图地址:https://upload-images.jianshu.io/upload_images/13629684-ba3a964b7c17782c.png?imageMogr2/auto-orient/strip|imageView2/2/w/487/format/webp 插入排序1基本思想:把待排序的记录按照其关键码值的大小逐个插入到一个已经排好序的有序序列中,知道所有的记录插完为止,得到一个新的有序序列 散列表1234567891011121314151617181920散列表也称为散列映射、映射、字典和关联数组。散列表：散列函数和数组共同创建的一种数据结构。散列表使用数组来存储数据①散列函数总是将同样的输入映射到相同的索引。②散列函数将不同的输入映射到不同的函数。③散列函数知道数组有多大，只返回有效的索引。散列表适用于:①模拟映射关系 ②防止重复 ③缓存\记住数据,以免服务器通过处理来生成它们冲突:当键已经存在时,再赋给它新值,就会覆盖原来的值,这种情况称为冲突对比: 散列表查找花费的时间为常量O(1),简单查找的运行时间为线性时间,二分查找的花费的时间为对数时间 小结: 1.可以结合散列函数和数组来创建散列 2.冲突很糟糕,应该使用可以最大程度的减小冲突的散列函数 3.散列表的查找、插入和删除速度都非常快 4.散列表适合用于模拟映射关系 5.一旦填装因子超过0.7，就该调整散列表的长度 6.散列表可用于调整长度 7.散列表非常适合用于防止重复 广度优先搜索(通过队列实现,顺序添加,顺序查找)1234567891011121314广度优先搜索是用来解决最短路径问题的,是一种用于图的查找算法,主要解决两类问题 ·从节点A出发,有前往节点B的路径吗 ·从节点A出发,前往节点B的哪条路径最短基础(图相关知识): 图由节点和边组成,一个节点可能与众多节点直接相连,这些节点被称为邻居 有向图:有箭头指向自己但没有指向别人.rama-&gt;adit 无向图:没有箭头,直接相连的节点互为邻居.ross-lilei一度关系胜过二度关系,二度胜过三度,关系以此类推.因而先在一度中搜索,然后二度...一直到搜索到为止.队列是先进先出的,栈是后进后出的,你需要按加入顺序检查搜索列表中的人,否则找到的就不是最短路径,因此搜索列表必须为队列对于检查过的人务必不要再去检查,否则会陷入无限循环运行时间：时间至少为O（边数）。将每一个添加到队列的时间是O(1),因此广度搜索的运行时间O(人数加边数) 狄克斯特拉算法1234567891011加权图:带权重的图非加权图:不带权重的图获取加权图的最短路径使用狄克斯特拉算法,获取非加权图的最短路径使用广度优先搜索算法(狄克斯特拉算法只适用于有向无环图)当权重为正时狄克斯特拉算法才管用如果包含负权边,请使用贝尔曼-福德算法基本思路: 1.找出最便宜的节点,即可在最短时间内到达的节点 2.更新该节点的邻居的开销 3.重复这个过程,直到对图中的每个节点都这样做了 4.计算最短路径 第一步:找出最便宜的节点.使用散列表,把它当成一个列表 父节点 节点 耗时 起点 A 6 起点 B 2 起点 终点 未知 找出最短耗时2,节点B是最近的 第二步:计算经节点B前往其各个邻居所需的时间 父节点 节点 耗时 B A 5(2+3) B B 2 B 终点 7(2+5) 第三步:重复操作 贪婪算法(贪心算法)===局部最优解,企图以这种方式获得全局最优解123456789101112指的是,在对问题求解时,总是做出当前看来最好的选择,也就是说,不从整体最优上加以考虑,做出的是某种意义的局部最优解常见的问题: 1.背包问题:有一个背包，背包容量是M=150kg。有7个物品，物品不可以分割成任意大小。要求尽可能让装入背包中的物品总价值最大，但不能超过总容量(每次挑选最大价值\每次挑选重量最大\每次挑选单位价值最大)===不一定最优,可能局部最优 2.马踏棋盘:在8×8方格的棋盘上，从任意指定方格出发，为马寻找一条走遍棋盘每一格并且只经过一次的一条路径. 3.集合覆盖问题：假设存在需要付费的广播台，以及广播台信号可以覆盖的地区。 如何选择最少的广播台，让所有的地区都可以接收到信号。基本思路: 1.建立数学模型来描述问题 2.把求解的问题分解成若干子问题 3.对每一个子问题求解,得到子问题的局部最优解 4.把子问题对应的局部最优解合成原来整个问题的一个近似最优解优点:贪婪算法易于实现、运行速度快，是不错的近似算法 动态规划(DP=dynamic programming)123456789101112131415161718192021222324这是一种解决棘手问题的方法,将问题分解成若干个小问题,先着手解决这些小问题动态规划原理: 编号动态规划:最大不下降子序列 划分动态规划:矩阵链乘、凸多边形三角剖分 数轴动态规划:0-1背包 前缀动态规划:最长公共子序列 树形动态规划:最优二分搜索树 基本思想:问题的最优解如果可以由子问题的最优解推导得到,则可以先求子问题的最优解,在构造原问题的最优解,若子问题的有较多的重复出现,则可以自底向上从最终子问题向原问题逐步求解使用条件:可分为多个相关子问题,子问题的解被重复使用 动态规划算法的设计步骤: 1.分析优化解的结构 2.递归的定义最优解的代价 3.自底向上地计算优化解的代价保存,并获取构造最优解的信息 4.根据构造最优解的信息构造最优解 动态规划的特点: 1.把原始问题分成一系列子问题 2.求解每个子问题,并将结果保存在一个表里,以后用到时直接存取,不重复计算,节省计算时间 3.自底向上的计算 4.整体问题最优解取决于子问题的最优解DP核心思想:尽可能缩小解空间 树算法123456789101112131415161718192021222324树:树是一种管理有像树干、树枝、树叶一样关系的数据得数据结构 树由节点（顶点）和边（枝）构成，并且有一个节点作为起始点。这个起始点作为树的根节点。由此形成父节点、子节点、叶子节点节点的子树的根称为节点的孩子，相应的该节点称为孩子的双亲，同一双亲的孩子之间互称为兄弟，节点的祖先是从该节点所经分支上的所有结点基本概念：节点的度：一个节点含有的子树的个数称为该节点的度叶节点或终端节点：度为0的节点称为叶节点非终端节点或分支节点：度不为0的节点双亲结点或父节点：若一个节点含有子节点，则这个节点称为其子节点的父节点孩子节点或子节点:一个节点含有的子树的根节点称为该节点的子节点兄弟节点:具有相同父节点的节点互称为兄弟节点树的度:一棵树中,最大的节点的度称为树的度节点的层次:从根开始定义起,跟为第一层,根的子节点为第二层树的高度或深度:树中节点的最大层次堂兄弟节点:双亲在同一层的节点称为堂兄弟节点节点的祖先:从跟到该节点所经分支上的所有节点子孙:以某节点为根的子树中任一节点都称为该节点的子孙森林:由m棵互不相交的树的集合称为森林在计算机中数据的存储有两种结构,顺序存储和链式存储,对树而言,顺序存储结构显然是不行的,而链式存储结构也是有缺点的第一种:链式存储结构中的节点需含有子节点的引用和指针,但在树中子节点的不确定性导致无法固定具体节点中有几个引用或指针第一种(改): 对于上面的存储结构会过多的浪费空间,那么在结点中声明一个动态链表Nodes来存放可能的子节点node第二种:使用数组+链表结合的方式表示树 图地址:https://img2018.cnblogs.com/blog/1244002/201809/1244002-20180921160023094-1488648937.png 各种树介绍12345678#有序树与无序树有序树:树中任意节点的子节点之间有顺序关系无序树:树中任意节点的子节点之间没有顺序关系,这种树称为无序树,也称自由树#二叉树完全二叉树:叶结点只能出现在最底层的两层,且最底层叶节点均处于次底层叶节点的左侧.满二叉树:除了叶子结点外每一个节点都有左右子叶,且叶子结点都处在最底层的二叉树(最标准那种)平衡二叉树:又被称为avl树(非avl算法),且具有以下性质,它是一颗空树或它的左右两个子树的高度差的绝对值不超过1,并且左右两个子树都是一颗平衡二叉树,平衡二叉树的常用实现方法有红黑树、AVL、替罪羊树、Treap、伸展树等。红黑树:是一种自平衡二叉查找树,典型的用途是实现关联数组 堆123456789堆就是用数组实现的二叉树,所以它没有使用父指针或者子指针,堆根据堆属性来排序,堆属性决定了树中节点的位置堆的常用方法: ·构建优先队列 ·支持堆排序 ·快速找出一个集合中的最小值(或者最大值)堆分为最大堆和最小堆最大堆:父节点的值比每一个子节点的值都要大.最小堆:父节点中的值比每一个子节点的值都要小堆属性非常有用,常常被当做优先队列使用,因为可以快速访问到最重要的元素]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP学习]]></title>
    <url>%2F2019%2F09%2F26%2FNLP%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[TF-IDF算法:代码及原理 https://blog.csdn.net/asialee_bird/article/details/81486700 stop_words 停用词 12概念:搜索的时候会自动忽略这些词作用:有效的提高关键词密度 自然语言处理 12345678概念:是研究人与人交际以及人与计算机交际中的语言问题的一门学科,涉及语言学\计算机科学\数学\自动化等不同学科.自然语言处理的基本理论:语言模型,隐马尔可夫模型(HMM) ·语言模型:根据语言客观事实而进行语言抽象数学建模,是一种对应关系 ·隐马尔可夫模型:用来描述一个含有隐含未知参数的马尔可夫过程语言资源:语料库和词汇知识库自然语言处理研究的内容:1.机器翻译 2.自动文摘 3.信息检索 4.文档分类 5.问答系统 6.信息过滤 7.信息抽取 8.文本挖掘 9.舆情分析 10.隐喻计算 11.文字编辑和自动校对 12.作文自动评分 13.语音识别 14.文语转换 15.人声识别/认证/验证自然语言处理的困难:①自然语言大量的歧义现象②对于特定的系统可能会出现未知的词汇,未知的结构等未知情况自然语言处理的用处:为了让众多的非结构化数据(文本,语音,视频等数据)能够进行分析和利用 12分词:单词组成句子,句子之间由空格隔开中文:字、词、句、段、篇 NLP训练营12345678910111213141516171819202122232425262728NLP = NLU + NLG natural language processingNLU:语音/文本---&gt;转换成理解 natural language understanding 五大难点: 语言的多样性 语言的歧义性 语言的鲁棒性 语言的知识依赖 语言的上下文NLG:理解----&gt;转化为语音/文本 natural language generator NLG 的6个步骤： 内容确定 – Content Determination 文本结构 – Text Structuring 句子聚合 – Sentence Aggregation 语法化 – Lexicalisation参考表达式生成 – Referring Expression Generation|REG语言实现 – Linguistic RealisationNLP的面临的及格问题: 1.一个意思多种表达方法 2.一词多义(苹果,包袱等)cv computer vision 计算机视觉,所见即所得 因而较nlp简单机器翻译:基于语料库的统计翻译以及概率--&gt;发展模型语料库的统计翻译缺点:慢,未考虑语境,未区分一词多义,未考虑上下文,语法问题等翻译步骤:(如中译英) 中文---&gt;分词---&gt;TM(transalation Model)----&gt;broken English----&gt;Language Model----&gt;输出 TM:将每个中文/词翻译成对应的英文(step 1) LM:将英文排列组合找到最符合人话的句子(概率最高的即为输出)(step 2)缺点:时间复杂度高,比如10个词,排列组合为10!复杂度为指数级别,np hard问题优化:step1 和 step2合并到一起===&gt;经典算法 decoding Algorithm (viterbi Algorithm)语音识别经典算法(使用贝叶斯公式)p(e/c) NLP基础理解1234567891011121314151617181920212223242526272829303132333435363738394041424344454647NLP的2个核心任务： 自然语言理解 – NLU 自然语言生成 – NLG NLP 的5个难点： 语言是没有规律的，或者说规律是错综复杂的。 语言是可以自由组合的，可以组合复杂的语言表达。 语言是一个开放集合，我们可以任意的发明创造一些新的表达方式。 语言需要联系到实践知识，有一定的知识依赖。 语言的使用要基于环境和上下文。 NLP 的4个典型应用： 情感分析 聊天机器人 语音识别 机器翻译 NLP 的6个实现步骤： 分词-tokenization 次干提取-stemming 词形还原-lemmatization 词性标注-pos tags 命名实体识别-ner 分块-chunking 中文分词工具 下面排名根据 GitHub 上的 star 数排名： Hanlp Stanford 分词 ansj 分词器 哈工大 LTP KCWS分词器 jieba IK 清华大学THULAC ICTCLAS 英文分词工具 Keras Spacy Gensim NLTK 1、中文分词规则分词123正向最大匹配逆向最大匹配双向最大匹配 统计分词121.建立语言统计模型:根据上下文的相关特性建立数学模型,核心是判断一个句子在文本中出现的概率2.对句子进行单词划分,对划分结果进行概率计算,获得概率最大的分词方式.===&gt;统计学习算法:HMM,CRF 2、词性标注和命名实体识别12词性标注:标注动词,名词等命名实体:常用的人名,地名,日期,组织,货币等等 3、关键词提取12常用方法:IF-IDF,textrank算法,主题模型主题模型:是以非监督学习的方法,对文集中的隐含语义结构进行聚类 3.1TF-IDF算法—&gt;统计算法—&gt;词频–逆文档频次算法123TF算法:统计一个词在文档中的频次IDF算法:统计词在文档集中多少个文档出现的频次以上为传统的,tf-idf也有很多变种的加权方法 3.2Text_Rank123优点:脱离语料库只对单文档分析即可提取文档关键词用法:最早用于自动摘要,思想来源于谷歌的pagerank,pagerank思想:一个网页被越多的网页链接说明越重要,或者一个网页被一个越高权值的网页链接,也能表明网页重要]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python实现迁表]]></title>
    <url>%2F2019%2F09%2F25%2FPython%E5%AE%9E%E7%8E%B0%E8%BF%81%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[python实现mysql的迁表注意事项: ​ 1.表结构问题 ​ 2.权限问题(无删除权限时,不能truncate表重置索引) ​ 3.思路问题(读表结构,修改表结构数据,读表数据,在新服务器库里建表,插入数据) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import pymysqlimport pandas as pdfrom multiprocessing import Poolfrom sqlalchemy import create_engineimport re#获取pre_formal_2中所有以expect开头的表名称db1 = pymysql.connect(host='192.168.0.222',port=3306,user='user_r_2',database='pre_formal_2',password='4f5g6h7j',charset='utf8mb4')cursor1 = db1.cursor()db2 = pymysql.connect(host='localhost', port=3306, user='root', database='ceshi', password='123456', charset='UTF8mb4')cursor2 = db2.cursor()sql = "SELECT distinct TABLE_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME LIKE 'expect%'"cursor1.execute(sql)data = cursor1.fetchall()data = pd.DataFrame(list(data))data = data[0].tolist()def func(table_name_list): for table_name in table_name_list: # 查看建表语句 sql = f"show create table &#123;table_name&#125;" cursor1.execute(sql) data = cursor1.fetchone() # 重置自增值 try: patten = re.compile('AUTO_INCREMENT=(.*) DEFAULT') result = patten.findall(data[1]) data1 = data[1].replace(result[0], '0') except: data1 = data[1] # 获取数据内容 engine = create_engine('mysql+pymysql://user_r_2:4f5g6h7j@192.168.0.222:3306/pre_formal_2') sql = f'select * from &#123;table_name&#125;' df = pd.read_sql_query(sql, engine) print(df) # 将表数据存入数据库 print(data1) sql_1 = f"&#123;data1&#125;" cursor2.execute(sql_1) engine = create_engine('mysql+pymysql://root:123456@localhost:3306/ceshi?charset=UTF8mb4') pd.io.sql.to_sql(df, f'&#123;table_name&#125;', engine, index=False, if_exists='append', chunksize=10000, ) engine.dispose()if __name__ == '__main__': pool = Pool(5) n = 300 count = (len(data) // n) + 1 for num in range(count): table_name_list = data[num * n:(num + 1) * n] print(table_name_list) #参数是指各个不同进程使用的参数本能当成传递使用 pool.apply_async(func, args=(table_name_list,)) pool.close() pool.join()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[es分页三种方式比较]]></title>
    <url>%2F2019%2F09%2F20%2Fes%E5%88%86%E9%A1%B5%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[(1) from / size : 该查询的实现原理类似于mysql中的limit，比如查询第10001条数据，那么需要将前面的10000条都拿出来，进行过滤，最终才得到数据。(性能较差，实现简单，适用于少量数据，数据量不超过10w)。(2) scroll：该查询实现类似于消息消费的机制，首次查询的时候会在内存中保存一个历史快照以及游标(scroll_id)，记录当前消息查询的终止位置，下次查询的时候将基于游标进行消费(性能良好，维护成本高，在游标失效前，不会更新数据，不够灵活，一旦游标创建size就不可改变，适用于大量数据导出或者索引重建)(3) search_after: 性能优秀，类似于优化后的分页查询，历史条件过滤掉数据。 scroll 1234567891011121314151617181920#scroll深度分页result = es.search(index=['patent_cn_v71'],body= &#123; 'query':&#123; 'match_all':&#123;&#125; &#125;, 'sort':&#123; 'app_text':'asc' &#125; &#125;,scroll='1m',size=10000)mdata = result.get("hits").get("hits")scroll_id = result["_scroll_id"]total = result["hits"]["total"]j = 1for i in range(total//10): res = es.scroll(scroll_id=scroll_id, scroll='1m') #scroll参数必须指定否则会报错 mdata += res["hits"]["hits"] breakfor result in mdata: print(result['_source']['app_text']) search_after 123456789101112131415161718192021222324252627282930313233343536373839404142from elasticsearch import Elasticsearches = Elasticsearch(hosts=&#123; '192.168.0.220', '192.168.0.225', '192.168.0.221', &#125;, timeout=3600)i = 5000next_id = 0while i == 5000: result_2 = es.search(index=['patent_cn_v71'], body=&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "sort": [ &#123;"app_text": "asc"&#125; ], "search_after": [next_id], &#125;, filter_path=[ 'hits.hits._source.ipcr_text', 'hits.hits._source.ipc_text', 'hits.hits._source.id', 'hits.hits._source.app_text', ], size=5000, ) try: result_2 = result_2['hits']['hits'] except: break for result in result_2: app_text = result['_source']['app_text'] i = len(result_2) from size 12345678910111213141516171819i = 10j = 0while i == 10: result = es.search(index=['patent_cn_v71'],body= &#123; 'query':&#123; 'match_all':&#123;&#125; &#125;, 'sort':[&#123; 'app_text':'asc' &#125;], 'from':j, 'size':10 &#125;, filter_path = ['hits.hits._source.app_text'], ) j+=10 i = len(result['hits']['hits']) print(result)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas常用方法总结]]></title>
    <url>%2F2019%2F09%2F19%2Fpandas%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[to_dict(将dataframe转换成字典格式)===对应的为json格式(十分常用)常用的有records和index12345678910111213141516import pandas as pdimport numpy as npdf = pd.DataFrame(np.arange(12).reshape(3,4),columns=['A','B','C','D'])df A B C D0 0 1 2 31 4 5 6 72 8 9 10 11df.to_dict('records') # 使用频繁[&#123;'A': 0, 'B': 1, 'C': 2, 'D': 3&#125;, &#123;'A': 4, 'B': 5, 'C': 6, 'D': 7&#125;, &#123;'A': 8, 'B': 9, 'C': 10, 'D': 11&#125;]df.to_dict('index') # 使用频繁，一般报表的横坐标为日期，将date作为Index&#123;0: &#123;'A': 0, 'B': 1, 'C': 2, 'D': 3&#125;, 1: &#123;'A': 4, 'B': 5, 'C': 6, 'D': 7&#125;, 2: &#123;'A': 8, 'B': 9, 'C': 10, 'D': 11&#125;&#125; 次要的有dict(默认)\list\series\split都必须掌握123456789101112131415161718192021222324252627282930313233343536import pandas as pdimport numpy as npdf = pd.DataFrame(np.arange(12).reshape(3,4),columns=['A','B','C','D'])df A B C D0 0 1 2 31 4 5 6 72 8 9 10 11df.to_dict('dict') # 默认是dict&#123;'A': &#123;0: 0, 1: 4, 2: 8&#125;, 'B': &#123;0: 1, 1: 5, 2: 9&#125;, 'C': &#123;0: 2, 1: 6, 2: 10&#125;, 'D': &#123;0: 3, 1: 7, 2: 11&#125;&#125;df.to_dict('list') &#123;'A': [0, 4, 8], 'B': [1, 5, 9], 'C': [2, 6, 10], 'D': [3, 7, 11]&#125;df.to_dict('series') &#123;'A': 0 0 1 4 2 8 Name: A, dtype: int64, 'B': 0 1 1 5 2 9 Name: B, dtype: int64, 'C': 0 2 1 6 2 10 Name: C, dtype: int64, 'D': 0 3 1 7 2 11 Name: D, dtype: int64&#125;df.to_dict('split')&#123;'index': [0, 1, 2], 'columns': ['A', 'B', 'C', 'D'], 'data': [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]]&#125; astype(强制数据类型转换)1df['A'] = df['A'].astype(float) group_by(分组)123456789101112(如果groupby后并sum,如果存在数字求和,则其他类型组合会清除,如果不存在数字求和,则字符串会出现拼接效果,如下所示)====通常的group都是与apply一起使用的,需要进一步了解df = pd.DataFrame([['0',1,'boy','3'],['0','5','girl','7']], columns=['A','B','C','D'])df A B C D0 0 1 boy 31 0 5 girl 7df.groupby('A').sum()df C DA 0 boygirl 37 map\applymap\apply用法123456789101112131415161718192021222324252627282930map===将series中的每一个元素通过函数进行处理(常用于dataframe中数据某一个字段整体的清洗)df1 = pd.DataFrame([['a','b','c'],['d','e','f']],columns=['A','B','C'])df1 A B C0 a b c1 d e fdf1['A'] = df1['A'].map(lambda x:x+'号')df1 A B C0 a号 b c1 d号 e fapplymap对于dataframe中的每一个元素进行操作df1.applymap(lambda x:x+'球') A B C0 a号球 b球 c球1 d号球 e球 f球apply方法可选定对于dataframe中行或者列进行操作df1.apply(lambda x:x.sum(),axis=0)A a号d号B beC cfdtype: objectdf1.apply(lambda x:x.sum(),axis=1)0 a号bc1 d号efdtype: object dataframe索引操作reset_index 1234567891011121314重置行索引(若果不加drop=True,则原来的索引成为新的列,加上则替换原索引)df1.index = ['s','b']df1 A B Cs a号 b cb d号 e fdf1.reset_index(drop=True) A B C0 a号 b c1 d号 e fdf1.reset_index() index A B C0 s a号 b c1 b d号 e f reindex 123456789101112131415重构或者任意重排索引,没有的数据会显示nandf = pd.DataFrame(np.random.randint(1,100,(5,5)))df 0 1 2 3 40 94 14 86 30 91 94 19 7 29 402 53 7 45 57 183 71 75 56 53 364 96 11 92 97 53df.reindex([4,2,3,5]) 0 1 2 3 44 96.0 11.0 92.0 97.0 53.02 53.0 7.0 45.0 57.0 18.03 71.0 75.0 56.0 53.0 36.05 NaN NaN NaN NaN NaN rename 1df.rename(columns=lambda x: x + 1)：批量更改列名 数据处理相关fillna补值 12345678910#直接替换原数据的nan为0,不加inplace不对原数据操作df.fillna(0,inplace=True)#通过字典对不同的列填充不同的常数df1.fillna(&#123;0:10,1:20,2:30&#125;)#用前面的值来填充df2.fillna(method='ffill')#用后面的值来填充df2.bfill(method='bfill')#limit限制填充的个数,axis修改填充的方向df2.fillna(method="ffill",limit=1,axis=1) dropna删除空值 123456789101112131415df.dropna(how='') all删除全为空值那一行,any删除存在空值那一行(默认0表示行,1代表列)DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)参数说明：axis:axis=0: 删除包含缺失值的行axis=1: 删除包含缺失值的列how: 与axis配合使用how=‘any’ :只要有缺失值出现，就删除该行货列how=‘all’: 所有的值都缺失，才删除行或列thresh： axis中至少有thresh个非缺失值，否则删除比如 axis=0，thresh=10：标识如果该行中非缺失值的数量小于10，将删除改行subset: list在哪些列中查看是否有缺失值inplace: 是否在原数据上操作。如果为真，返回None否则返回新的copy，去掉了缺失值 unique去重(针对series的去重) 1df[&apos;A&apos;].unique() drop_duplicates(针对dataframe去重也可series)====实质针对的是所有的列字段 1234全部字段df.drop_duplicates()指定字段df.drop_duplicates([&apos;colA&apos;, &apos;colB&apos;]) replace替换指定字符 12df.replace(np.nan,0,inplace=True)将所有的空值替换成0 describe\info 123456789101112131415161718192021222324查看汇总统计df.describe() 0 1 2 3 4count 4.000000 4.000000 4.000000 4.000000 4.00000mean 55.000000 23.250000 48.250000 51.750000 26.75000std 40.685788 34.798228 37.915476 39.811012 22.85279min 0.000000 0.000000 0.000000 0.000000 0.0000025% 39.750000 5.250000 33.750000 39.750000 13.5000050% 62.000000 9.000000 50.500000 55.000000 27.0000075% 77.250000 27.000000 65.000000 67.000000 40.25000max 96.000000 75.000000 92.000000 97.000000 53.00000查看索引、数据类型和内存信息df.info()&lt;class 'pandas.core.frame.DataFrame'&gt;Int64Index: 4 entries, 4 to 6Data columns (total 5 columns):0 4 non-null float641 4 non-null float642 4 non-null float643 4 non-null float644 4 non-null float64dtypes: float64(5)memory usage: 192.0 bytes 数据合并merge\concat\join 1234pd.merge(df1, df2, on=&apos;key&apos;, how=&apos;left&apos;)pd.concat([df1,df2],join=&quot;inner&quot;,axis=1)===不会去重pd.concat([df1,df2],ignore_index=True).drop_duplicates() ====会去重df1.join(df2,how=&apos;inner&apos;) date_range 创建时间数据 12345pd.date_range(&apos;2018-09-10&apos;,periods=8,freq=&apos;m&apos;)DatetimeIndex([&apos;2018-09-30&apos;, &apos;2018-10-31&apos;, &apos;2018-11-30&apos;, &apos;2018-12-31&apos;, &apos;2019-01-31&apos;, &apos;2019-02-28&apos;, &apos;2019-03-31&apos;, &apos;2019-04-30&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;M&apos;) value_counts数据频数统计 12345678对某一行或者某一列数据进行频数统计df.loc[2].value_counts()7.0 1额外请问 145.0 157.0 118.0 1Name: 2, dtype: int64 数据统计 12345678df.describe()：查看数据值列的汇总统计df.mean()：返回所有列的均值df.corr()：返回列与列之间的相关系数df.count()：返回每一列中的非空值的个数df.max()：返回每一列的最大值df.min()：返回每一列的最小值df.median()：返回每一列的中位数df.std()：返回每一列的标准差 数据选取(isin和str.contains)isin(df和series都可用) 1234567891011判断是否含所有某个数据(完全匹配)等于多个数值或者字符串时 内容为列表格式--符合任意一个都会返回df 0 1 2 3 44 委屈翁 11.0 92.0 97.0 53.02 额外请问 7.0 45.0 57.0 18.03 问问 75.0 56.0 53.0 36.06 委屈翁群 0.0 0.0 0.0 0.0df[0][df[0].isin(['委屈翁','问问'])]4 委屈翁3 问问Name: 0, dtype: object str.contains(针对的为series) 12345678910111213141516171819202122判断某个数据中是否包含指定字符(字符串的模糊筛选)df[0][df[0].str.contains('委屈')]4 委屈翁6 委屈翁群Name: 0, dtype: object多条件筛选用|符号df[0][df[0].str.contains('翁|问')]4 委屈翁2 额外请问3 问问6 委屈翁群Name: 0, dtype: object #参数解释 Series.str.contains( pat, # 要查询的字符串、要查询的或者正则表达式 case=True, # 是否对大小写敏感 flags=0, # 用来传给正则模块的参数，比如 flags=re.INGNORECASE 等价于 case=False na=nan, # 默认对空值不处理，即输出结果还是 NaN regex=True # 即第一个参数 pat部分 要不要按照正则表达式的规则。 #所以针对特殊符号，默认情况下我们必须使用转义符，或者设置 regex=False ) set_index 和 reset_index12345678将df1中A这一列设为索引,并且不删除原来的A那一列df1.set_index('A',drop=False)将df1中A这一列设为索引,并且删除原来的A那一列df1.set_index('A',drop=True)将df1中的索引还原,并且添加一列新的列(原来的索引列)df1.reset_index('A',drop=False)将df1中的索引重置,索引那一列直接删除df1.reset_index('A',drop=True) rank12345df.rank() #默认按照列排序,返回的结果为对应各自排名的矩阵df.rank(axis=1) #按照行进行排序df.rank(axis=1,method='average') #数据相同的排名全部取对应排名的平均值df.rank(axis=1,method='max') #取最大排名df.rank(axis=1,method='first') 原始数据中出现的顺序进行排名 12sorted(cxy_data, key=lambda i: i['num'], reverse=True) 按照字典中的某个值倒叙sorted(dict1.items(),key=lambda i:i[1]) 按照字典的值排序 #####to_excel123writer = pd.ExcelWriter(r'C:\Users\Administrator\Desktop\ceshe.xlsx')data.to_excel(writer,sheet_name='Sheet1')writer.save() #直接将df导出到excel中 #####stack和unstack pandas分行取top多少12345678Series.nlargest([n, 保持])返回最大的n 个元素。Series.nsmallest([n, 保持])返回最小的n 个元素。 Python操作excel方法 对比 优化方法1234567df.eval( &quot;&quot;&quot; c = a + b d = a + b + c a = 1 &quot;&quot;&quot;,inplace=False)df.query(&quot;strings == &apos;a&apos; and nums == 1&quot;) 等长列表两两元素操作1list(map(lambda x :x[0]+x[1] ,zip(a,b))) 传参123outer_params = [company_table,zl_zu_table,replace_rate_group_table,techonology_life_replace_table,zhibiao]outer_params = &apos; &apos;.join(outer_params)os.system(f&quot;python3 tech_life_grant_ceased_num_2.py &#123;outer_params&#125;&quot;) dataframe插入clickhouse1234client = Client(host=&apos;192.168.0.170&apos;, port=&apos;9000&apos;, user=&apos;algorithm&apos;, password=&apos;1a2s3d4f&apos;, database=&apos;algorithm_dis&apos;,settings=&#123;&apos;use_numpy&apos;: True&#125;)sql_insert = &apos;insert into industryid_name_20211208 values&apos;client.insert_dataframe(sql_insert,df) transform12345超级好用,支持在原有基础上新增一列,支持groupby后的操作hold_df.groupby([&apos;PORT_ID&apos;,&apos;SEC_ID&apos;,&apos;VALID_DATE&apos;]).REAL_SEC_ID.transform(&apos;count&apos;)transform相当于新增了一列,好东西 shift12整体向下平移,支持groupby后shift,搭配assigh使用group.assign(PRE_NAV=(group.groupby( [&apos;GROUP_ID&apos;] + [&apos;GROUP_NAME&apos;] + [&apos;SEC_ID&apos;] + [&apos;SEC_ABBR&apos;])[&apos;ADJ_NAV&apos;].shift(1))) python离线安装第三方whl文件1234567pip download pandas -d E:\光大POC\fin_indicators# download onlinepip download package_name -d target_dir# install offlinepip install --no-index -f target_dir -r requirements.txt 内存回收123数据使用的进程不销毁,占用的内存不会减少,因而可以通过创建子进程实现回收with concurrent.futures.ProcessPoolExecutor(max_workers=1) as executor: result = executor.submit(func).result() 转换数类型神器df.convert_dtypes 补充:其他的常用方 法总结https://www.cnblogs.com/rexyan/p/7975707.html]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[接口获取数据]]></title>
    <url>%2F2019%2F09%2F19%2F%E6%8E%A5%E5%8F%A3%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[注意json格式的转换和data中[]符号不能缺 方式一: 12345import requests,jsonurl = 'http://192.168.0.226:9557/api/internal/findQxbInfo'data = &#123;"company":["华为技术有限公司"]&#125;result = requests.post(url,json.dumps(data))result.text 方式二:(一的提升) 1234567import requests,jsonurl = 'http://192.168.0.226:9557/api/internal/findQxbInfo'data = &#123;'company':["华为技术有限公司"]&#125;headers = &#123;'Content-type':'application/json'&#125;data = json.dumps(data)result = requests.post(url,data,headers)json.loads(result.text) 获取公司曾用名和现用名接口操作 123456zl_list = []for applicant in applicant_list: url = f'http://192.168.0.226:9666/api/search/patentStatisticData?companyName=&#123;applicant&#125;' result = requests.get(url) inventionNum = json.loads(result.text)['result']['inventionNum'] zl_list.append([applicant,inventionNum]) python flask写接口 1234567891011121314151617181920212223242526272829303132from flask import request, Flask, jsonifyfrom http import clientclient.HTTPConnection._http_vsn = 10client.HTTPConnection._http_vsn_str = 'HTTP/1.0'app = Flask(__name__)app.config['JSON_AS_ASCII'] = False@app.route('/test', methods=['POST'])def post_Data(): try: info = request.json app_texts = info['app_texts'] except: result = &#123;'FlagCode':0,'message':'格式错误'&#125; return jsonify(result) if type(app_texts) != list: result = &#123;'FlagCode':0,'message':'格式错误'&#125; return jsonify(result) if app_texts == []: result = &#123;'FlagCode':0,'message':'缺少数据'&#125; return jsonify(result) #查数据写成对应格式 result = &#123;'FlagCode':1,'message':'操作成功','result':res&#125; return jsonify(result)if __name__ == '__main__': app.run(debug=True, host='0.0.0.0', port=9525)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas读取mysql]]></title>
    <url>%2F2019%2F09%2F18%2Fpandas%E8%AF%BB%E5%8F%96mysql%2F</url>
    <content type="text"><![CDATA[#读取 方式一:(元组嵌套格式)12345db=pymysql.connect(host='localhost',port=3306,user='root',database='comp',password='123456',charset='utf8mb4')cursor = db.cursor()sql = 'select * from company_message 'cursor.execute(sql)company_data = cursor.fetchall() 方式二:(dataframe格式)1234conn=pymysql.connect(host='localhost',port=3306,user='root',database='comp',password='123456',charset='utf8mb4')sql = 'select * from company_message 'datasss = pd.read_sql(sql,conn)datasss 方式三:(dataframe格式)12345from sqlalchemy import create_engineengine = create_engine('mysql+pymysql://root:123456@localhost:3306/comp')sql = 'select * from company_message 'df = pd.read_sql_query(sql,engine)print(df) #存储 方式一:直接dataframe保存到mysql12345from sqlalchemy import create_engineimport pandas as pdengine = create_engine('mysql+pymysql://root:123456@localhost:3306/comp?charset=UTF8')pd.io.sql.to_sql(app_times,'app_times', engine, index=False, if_exists='append', chunksize=10000,) engine.dispose() ###方式二:正常存储123456789db = pymysql.connect(host='localhost', port=3306, user='root',database='comp',password='123456', charset='utf8mb4')cursor = db.cursor()data = (data[0], data[1])sql = 'insert into patent_apply_date (app_text,apply_msg) values (%s,%s)'cursor.execute(sql, data)db.commit()cursor.close()db.close()]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jupyter修改字体颜色大小]]></title>
    <url>%2F2019%2F09%2F17%2F%E4%BF%AE%E6%94%B9%E5%AD%97%E4%BD%93%E9%A2%9C%E8%89%B2%E5%A4%A7%E5%B0%8F%2F</url>
    <content type="text"><![CDATA[原文转载:https://blog.csdn.net/AlanGuoo/article/details/86318381 改变字体颜色(color)红色+加粗红色+加粗 蓝色+斜体蓝色+斜体或蓝色+斜体 绿色+加粗+斜体绿色+加粗+斜体或绿色+加粗+斜体或绿色+加粗+斜体 改变字体(face)、大小(size)5号雅黑 5号雅黑 可以看出来 color, size, font 的顺序可以随意调整，且参数不需要加引号文字进行高亮显示通过标签黄色高亮 改变高亮颜色改变高亮颜色 叠加字体的变化叠加字体的变化]]></content>
      <tags>
        <tag>jupyter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[selenium爬取企查查案例]]></title>
    <url>%2F2019%2F09%2F12%2Fselenium%E7%88%AC%E5%8F%96%E4%BC%81%E6%9F%A5%E6%9F%A5%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354chrome_options = Options() chrome_options.add_argument('--headless') chrome_options.add_argument('--disable-gpu') # 驱动路径 path = 'C:\Program Files (x86)\Google\Chrome\Application\chromedriver.exe' # 创建浏览器对象 driver = webdriver.Chrome(executable_path=path, chrome_options=chrome_options) url = 'https://www.qichacha.com/' driver = webdriver.Chrome() driver.get(url) driver.find_element_by_xpath('//input[@id="searchkey"]').send_keys(f'&#123;company_name&#125;') driver.find_element_by_xpath("//input[@value ='查一下']").click() driver.find_element_by_xpath("//a[@class ='ma_h1']").click() cookies = driver.get_cookies() cookies_list= [] for cookie_dict in cookies: cookie =cookie_dict['name']+'='+cookie_dict['value'] cookies_list.append(cookie) header_cookie = ';'.join(cookies_list) print(header_cookie) headers2 = &#123; 'cookie':header_cookie, 'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36' &#125; fin_url = driver.current_url response = requests.get(fin_url,headers=headers2) mainhandle=driver.current_window_handle #主页面句柄 每个浏览器标签页都有一个句柄 # print(response.text) handles = driver.window_handles for handle in handles:# 轮流得出标签页的句柄 切换窗口 因为只有两个标签页实际是假for循环 if handle!=mainhandle: driver.switch_to_window(handle) #获得数据 try: raw=driver.find_element_by_xpath("//table[@class='ntable']") print (raw.text) for data1 in raw.text.split('\n'): if '-' in data1: continue #流通市值 ltsz = data1.split(' ')[3] #市盈率 syl = data1.split(' ')[1] #市净率 sjl = data1.split(' ')[3] print(ltsz,syl,sjl) except Exception as e: print("无该数据")]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python实现排序算法]]></title>
    <url>%2F2019%2F09%2F12%2Fpython%E5%AE%9E%E7%8E%B0%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[##冒泡排序123456def bubble(nums): for i in range(len(nums)-1): for j in range(len(nums)-i-1): if nums[j] &gt; nums[j+1]: nums[j],nums[j+1] = nums[j+1],nums[j] return nums ##插入排序12345678def insert_sort(arr): for i in range(1,len(arr)): key = arr[i] j = i-1 while j&gt;0 and key &lt; arr[j]: arr[j+1] = arr[j] j -= 1 arr[j+1] = key ##归并排序1234567891011121314151617181920def merge_sort(alist): if len(alist) &lt;= 1: return alist nums = len(alist)/2 left = merge_sort(alist[:num]) right = merge_sort(alist[num:] return merge(left,right)def merge(left,right): l,r = 0,0 result = [] while l&lt;len(left) and r&lt;len(right): if left[1] &lt; right[r] result.append(left[1]) l+=1 else: result.append(right[r]) r+=1 result += left[l:] result += rifht[r:] return result ##快速排序12345678910111213141516def quick_sort(alist,start,end): if start &gt;= end: return mid = alist[start] low = start high = end while low &lt; high: while low &lt; high and alist[high] &gt;= mid: high -= 1 alist[low] = alist[high] while low &lt; high and alist[low] &lt; mid: low+=1 alist[high] = alist[low] alist[low] = mid quick_sort(alist,start,low-1) quick_sort(alist,low+1,end)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>几种常见的排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python操作es查询]]></title>
    <url>%2F2019%2F09%2F12%2Fpython%E6%93%8D%E4%BD%9Ces%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[#es查询相关 1234567891011121314151617181920212223aggs聚合中terms和cardinality的区别: terms:聚合分组，类似于sql中group by，结果为每个单位出现的次数，需要给定size值，不然默认最大为10 举例: "aggs": &#123; "classid": &#123; "terms": &#123; "field": "classid", "size": 10 &#125; &#125; &#125; 返回结果为classid去重后对应每个classid出现的次数 cardinality: 去重，类似于sql中distinct ,结果为单位数量， 举例: "aggs": &#123; "classid": &#123; "cardinality": &#123; "field": "classid" &#125; &#125; &#125; 返回结果为classid去重后的个数 –match–123456789101112131415161718192021222324query==match==query实现模糊查询 分词查询含有的会被找出(如下含有学校或者公司的会被找出)===自动分词或者自己设计result_list = es.search(index=['patent_cn_v7'], body=&#123; "query": &#123; "match": &#123; 'applicant_address_other': &#123; 'query':'公司' &#125;, # 检索条件 &#125;,# 'match':&#123;# 'app_text':'CN01316971'# &#125; &#125; &#125;, filter_path=[ 'hits.hits._source.applicant_address_other', 'hits.hits._source.app_text', 'hits.hits._source.applicant_other' ], # 数据量 size=10 )result_list –match_phrase123456789101112131415161718192021query==match_phase_query 必须符合全部分词才会被找出来添加slop可以放宽条件result_list = es.search(index=['patent_cn_v7'], body=&#123; "query": &#123; "match_phrase": &#123; 'applicant_address_other': &#123; 'query':'山东学校', 'slop': 1 &#125;, # 检索条件 &#125; &#125; &#125;, filter_path=[ 'hits.hits._source.applicant_address_other', 'hits.hits._source.app_text', ], # 数据量 )result_list –multi_match–1234567891011121314151617181920multi_match 多字段匹配其中一个满足即可========也是自动分词result_list = es.search(index=['patent_cn_v7'], body=&#123; "query": &#123; "multi_match": &#123; 'query':'山东电学校', 'fields':['applicant_address_other','applicant_other'] , # 检索条件 &#125; &#125; &#125;, filter_path=[ 'hits.hits._source.applicant_address_other', 'hits.hits._source.applicant_other', 'hits.hits._source.app_text', ], # 数据量 )result_list –term–12345678910111213141516term表示完全匹配不会经过分词器result_list = es.search(index=['patent_cn_v7'], body=&#123; "query": &#123; "term": &#123; 'applicant_other.keyword':'山东省电力学校', &#125; &#125; &#125;, filter_path=[ 'hits.hits._source.applicant_address_other', 'hits.hits._source.applicant_other', 'hits.hits._source.app_text', ], # 数据量 )result_list –terms–1234567891011121314151617terms多条件精准匹配result_list = es.search(index=['patent_cn_v7'], body=&#123; "query": &#123; "terms": &#123; 'applicant_other.keyword':['孙向柔','吕百顺'], &#125; &#125; &#125;, filter_path=[ 'hits.hits._source.applicant_address_other', 'hits.hits._source.applicant_other', 'hits.hits._source.app_text', ], size=1000# 数据量 )result_list –bool filter–12345678910111213141516171819202122bool====filter双重过滤(相当于mysql中的where)result_list = es.search(index=['patent_cn_v7'], body=&#123; "query": &#123;'bool':&#123; 'must':&#123;"match": &#123; 'applicant_other.keyword':'吕百顺' &#125;&#125;, 'filter':&#123; 'match':&#123; 'app_text':'CN89215264' &#125; &#125; &#125;&#125;&#125;, filter_path=['hits.hits._source.applicant_other'] )result_list –prefix–1234567891011121314151617prefix以什么指定开头result_list = es.search(index=['patent_cn_v7'], body=&#123; "query": &#123; "prefix": &#123; 'applicant_other.keyword':'三江瓦力', &#125; &#125; &#125;, filter_path=[ 'hits.hits._source.applicant_address_other', 'hits.hits._source.applicant_other', 'hits.hits._source.app_text', ], size=1000# 数据量 )result_list –regexp–1234567891011121314151617正则表达式检索result_list = es.search(index=['patent_cn_v7'], body=&#123; "query": &#123; "regexp": &#123; 'applicant_other.keyword':'C.*', &#125; &#125; &#125;, filter_path=[ 'hits.hits._source.applicant_address_other', 'hits.hits._source.applicant_other', 'hits.hits._source.app_text', ], size=4000# 数据量 )result_list –wildcard–1234567891011121314151617通配符查询(?和*)result_list = es.search(index=['patent_cn_v7'], body=&#123; "query": &#123; "wildcard": &#123; 'applicant_other.keyword':'CDT??有限公司', &#125; &#125; &#125;, filter_path=[ 'hits.hits._source.applicant_address_other', 'hits.hits._source.applicant_other', 'hits.hits._source.app_text', ], size=4000# 数据量 )result_list –聚合查询–(avg\sum\min\max\stats)12345678910111213141516result_list = es.search(index=['patent_cn_v7'], body=&#123;'size':0, "query": &#123; "match_all": &#123; &#125; &#125; , 'aggs':&#123; 'agg_terms':&#123; 'stats':&#123; 'field':'country_id' &#125; &#125; &#125; &#125; )result_list –聚合分组–1234567891011121314151617result_list = es.search(index=['patent_cn_v7'], body=&#123;'size':0, "query": &#123; "match": &#123;'applicant_other':'CJ第一制糖株式会社' &#125; &#125; , #分组 'aggs':&#123; 'agg_terms':&#123; 'terms':&#123; 'field':'ipcr_text' &#125; &#125; &#125; &#125; )result_list –聚合分组后计算–12345678910111213141516171819202122232425result_list = es.search(index=['patent_cn_v7'], body=&#123;'size':0, "query": &#123; "match": &#123;'applicant_other':'CJ第一制糖株式会社' &#125; &#125; , #分组 'aggs':&#123; 'agg_terms':&#123; 'terms':&#123; 'field':'ipcr_text' &#125;, 'aggs':&#123; 'avg_id':&#123; 'stats':&#123; 'field':'country_id' &#125; &#125; &#125; &#125; &#125; &#125; )result_list —返回聚合分组后的某一组数据(post_filter)—1234567891011121314151617181920返回聚合分组后的某一组数据(post_filter)result = es.search(index=[&apos;patent_cn_v71&apos;],body=&#123; &apos;query&apos;:&#123; &apos;match_all&apos;:&#123;&#125; &#125;, &apos;aggs&apos;:&#123; &apos;ipcr&apos;:&#123; &apos;terms&apos;:&#123; &apos;field&apos;: &quot;applicant_type&quot; &#125;, &#125; &#125;, &apos;post_filter&apos;:&#123; &apos;term&apos;:&#123;&apos;applicant_type&apos;:&apos;individual&apos;&#125; &#125;&#125;, filter_path=[&apos;hits.hits._source.app_text&apos;])result —聚合嵌套-(注意写的位置)–1234567891011121314151617181920212223result = es.search(index=[&apos;patent_cn_v71&apos;],body=&#123; &apos;query&apos;:&#123; &apos;match_all&apos;:&#123;&#125; &#125;, &apos;aggs&apos;:&#123; &apos;ipcr&apos;:&#123; &apos;terms&apos;:&#123; &apos;field&apos;:&apos;ipcr_text&apos; &#125;, &apos;aggs&apos;:&#123; &apos;ipcrs&apos;:&#123; &apos;terms&apos;:&#123; &apos;field&apos;:&apos;ipcr_text&apos; &#125; &#125; &#125; &#125; &#125; &#125;)result —聚合返回拼接字段聚合(script)—123456789101112131415result = es.search(index=[&apos;patent_cn_v71&apos;],body=&#123; &apos;query&apos;:&#123; &apos;match_all&apos;:&#123;&#125; &#125;, &apos;aggs&apos;:&#123; &apos;kl&apos;:&#123; &apos;terms&apos;:&#123; &apos;script&apos;:&#123; &apos;inline&apos;:&quot;doc[&apos;app_country&apos;].value+&apos; &apos;+doc[&apos;app_date&apos;].value+&apos; &apos;+doc[&apos;id&apos;].value&quot;, &#125;, &apos;size&apos;:&apos;1000&apos; &#125; &#125; &#125;&#125;)result –聚合结果排序– 12345678910111213141516171819result = es.search(index=['patent_cn_v71'],body=&#123; 'query':&#123; 'match_all': &#123;&#125; &#125; , 'aggs':&#123; 'company_name':&#123; 'terms':&#123; 'field':'ipcr_text', 'order':[&#123; '_count':'asc' &#125;] &#125; &#125;, &#125; &#125; )result –对聚合结果排序顺序倒叙– 1234567891011121314151617181920result = es.search(index=['patent_us_v71'],body=&#123;'size':0, 'query':&#123; 'match_all': &#123;&#125; &#125; , 'aggs':&#123; 'company_name':&#123; 'terms':&#123; 'field':'app_text', 'order':[&#123; '_count':'desc' &#125;] , 'size':'200000'&#125; &#125;, &#125; &#125; )result –nested嵌套查询– 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#例1es = Elasticsearch(hosts=&#123;'192.168.0.220', '192.168.0.225', '192.168.0.221', &#125;, timeout=3600)result_2 = es.search(index='patent_cn_v71', body=&#123; "query": &#123; "bool": &#123; "must": [ &#123;"terms": &#123;"patent_type": [1, 2]&#125;&#125;, &#123; "nested": &#123; "path": "applicants", "query": &#123; "bool": &#123; "must": [ &#123;"term": &#123; "applicants.name_other.keyword": '宝洁公司'&#125;&#125;, ] &#125; &#125; &#125;&#125; ] &#125;&#125;, #三层三次聚合,一次进里面.二次再进一步,三次进入到inventors中 "aggs": &#123; "aggs": &#123; "aggs": &#123;"de_inventor": &#123; "terms": &#123;"field": "inventors.name_other.keyword", "size": 200000&#125;&#125;&#125;, "nested": &#123;"path": "inventors"&#125; &#125; &#125; &#125;, )result2_list#例2result_2 = es.search(index=['patent_cn_v7'], body=&#123; "query": &#123; "nested": &#123; "path": "applicants", 'query':&#123; 'term':&#123; 'applicants.name_other.keyword': 'CJ第一制糖株式会社', &#125; &#125;, &#125; &#125;, "aggs": &#123; "aggs": &#123; "aggs": &#123;"de_inventor": &#123; "terms": &#123;"field": "inventors.name_other.keyword", "size": 200000&#125;&#125;&#125;, "nested": &#123;"path": "inventors"&#125; &#125; &#125; &#125; )result_2['aggregations']['aggs'] –复合查询bool–12345678910111213141516171819202122232425262728result = es.search(index = ['patent_cn_v7'], body=&#123; 'query':&#123; 'bool':&#123; 'must':[&#123; 'term':&#123; 'applicant_other.keyword':'孙长友' &#125; &#125; ], 'must_not': &#123; 'term':&#123; 'app_text':'CN201320765434' &#125;, &#125;, &#125; &#125; &#125;, filter_path=[ 'hits.hits._source.applicant_address_other', 'hits.hits._source.applicant_other', 'hits.hits._source.app_text', ], size=10)result –切片式查询from size–12345678910111213141516171819202122232425262728result = es.search(index = ['patent_cn_v7'], body=&#123; 'query':&#123; 'bool':&#123; 'must':[&#123; 'term':&#123; 'applicant_other.keyword':'孙长友' &#125; &#125; ], 'must_not': &#123; 'term':&#123; 'app_text':'CN201320765434' &#125;, &#125;, &#125; &#125; &#125;, filter_path=[ 'hits.hits._source.applicant_address_other', 'hits.hits._source.applicant_other', 'hits.hits._source.app_text', ], size=10)result –范围查询range–1234567891011121314151617result = es.search(index = ['patent_cn_v7'],body=&#123; 'query':&#123; 'range':&#123; 'id':&#123; 'gte':1000, 'lte':10000 &#125; &#125; &#125;&#125;, filter_path = [ 'hits.hits._source.applicant_address_other', 'hits.hits._source.applicant_other', 'hits.hits._source.app_text', 'hits.hits._source.id', ], ) –sort排序–1234567891011121314151617result= es.search(index=['patent_cn_v7'], body=&#123; 'query':&#123; 'match_all':&#123;&#125; &#125;, 'sort':&#123; 'app_text':&#123; 'order':'asc' &#125; &#125; &#125;, filter_path=[ 'hits.hits._source.applicant_address_other', 'hits.hits._source.applicant_other', 'hits.hits._source.app_text', 'hits.hits._source.id', ]) –keyword与text的区别是分词与不分词的区别 123456789101112result = es.search(index=['patent_cn_v71'],body=&#123; 'query':&#123; 'nested':&#123; 'path':'applicants', 'query':&#123; 'term':&#123; 'applicants.name_en.keyword':'ELECTROLUX APPLIANCES' &#125; &#125; &#125; &#125;&#125;)result –count–获取匹配到的数量 123456789result = es.count(index=['patent_cn_v7'], body=&#123; 'query':&#123; 'match':&#123; 'applicant_other.keyword':'蒋子刚' &#125; &#125; &#125;) –es_scroll进行深度分页you表获取内容– 12345678910111213141516171819202122# list1 = []result = es.search(index=['patent_cn_v71'],body= &#123; 'query':&#123; 'match_all':&#123;&#125; &#125;, 'sort':&#123; 'app_text':'asc' &#125; &#125;,scroll='1m',size=10)mdata = result.get("hits").get("hits")scroll_id = result["_scroll_id"]total = result["hits"]["total"]j = 1for i in range(total//100): res = es.scroll(scroll_id=scroll_id, scroll='1m') #scroll参数必须指定否则会报错 mdata += res["hits"]["hits"] if j==2: break j+=1for result in mdata: print(result['_source']['app_text']) –是否存在某个字段exists– 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#方法1result = es.search(index=['patent_cn_v7'], body=&#123; 'query':&#123; 'exists':&#123; 'field':'applicant_other' &#125; &#125;&#125; )#方法2es.search(index=['patent_jp_v71'], body=&#123; "query": &#123; "bool":&#123; "must_not":&#123; "nested":&#123; "path": "prime_notified", "query":&#123; "match_all":&#123;&#125; &#125;&#125;&#125; &#125; &#125;,# 'aggs':&#123;# 'agg_terms':&#123;# 'terms':&#123;# 'field':'app_date',# 'size':'10000'# &#125; &#125;&#125; &#125; ,filter_path=['hits.total', 'hits.hits._source.app_text'],size=300)#方法3es.search(index=['patent_jp_v71'], body=&#123; "query": &#123; "nested": &#123; "path": "inventors", 'query':&#123; "bool":&#123; 'must':[ &#123; 'exists':&#123; 'field':'inventors.name_other' &#125;&#125;, &#123; 'exists':&#123; 'field':'inventors.name_en' &#125;&#125;, ] &#125; &#125;, &#125; &#125;,# "aggs": &#123;# "aggs": &#123;# "aggs": &#123;"de_inventor": &#123;# "terms": &#123;"field": "inventors.name_other.keyword", "size": 200000&#125;&#125;&#125;,# "nested": &#123;"path": "inventors"&#125;# &#125;# &#125; &#125; ,filter_path=['hits.total', 'hits.hits._source.app_text'],size=300) –深度理解filter– 12345678910result = es.search(index=['patent_cn_v71'], body=&#123; 'query':&#123; 'bool':&#123;'must':&#123;'term':&#123;'id':'17640553'&#125;&#125;, 'filter':&#123;'term':&#123;'applicant_other.keyword':'王永民'&#125;&#125; &#125; &#125;&#125;, filter_path=['hits.hits._source.app_text'])print(result) –案例一filter放在bool下的等级等同于must 123456789101112131415import times1 = time.time()result = es.search(index=['patent_cn_v71'], body=&#123; 'query':&#123; 'bool':&#123; 'must':&#123;'match':&#123;'applicant_address_other':&#123;'query':'北京市'&#125;&#125;&#125;, 'filter':&#123;'term':&#123;'applicant_other.keyword':'王永民'&#125;&#125; &#125; &#125; &#125;, filter_path =['hits.hits._source.app_text'])print(result)print(time.time()-s1) filter常与bool一起使用,等级同must(里面要加match\term之类的) 123456789101112result = es.search(index=['patent_cn_v71'], body=&#123; 'query':&#123; 'bool':&#123; 'filter':&#123;'term':&#123;'applicant_other.keyword':'王永民'&#125;&#125;, 'must':&#123;'match':&#123;'applicant_address_other':&#123;'query':'北京市'&#125;&#125;&#125;, &#125; &#125; &#125;, filter_path =['hits.hits._source.app_text'])print(result) 1234567注意事项: 1.中文只能通过关键字查询 2.match会系统自动分词,而term表示完全匹配不进行分词 3.match对应模糊查找,term对应精确查找 4.gte大于等于 gt大于 lte小于等于 lt小于 5.terms表示符合其中任意一个条件即可分词:如我是中国人 会分成我 中国人类似,可以使用系统默认的亦可以自己设置]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>es服务器查询api</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[beautifulsoup4]]></title>
    <url>%2F2019%2F06%2F21%2Fbeautifulsoup4%2F</url>
    <content type="text"><![CDATA[CSS 选择器：BeautifulSoup4Beautiful Soup 也是一个HTML/XML的解析器，主要的功能也是如何解析和提取 HTML/XML 数据。 抓取工具 速度 使用难度 安装难度 正则 最快 困难 无（内置） BeautifulSoup 慢 最简单 简单 lxml 快 简单 一般 lxml 只会局部遍历，而Beautiful Soup 是基于HTML DOM的，会载入整个文档，解析整个DOM树，因此时间和内存开销都会大很多，所以性能要低于lxml。 官方文档：http://beautifulsoup.readthedocs.io/zh_CN/v4.4.0 使用安装： pip install beautifulsoup4 使用： from bs4 import BeautifulSoup text = “”” The Dormouse’s story The Dormouse’s story—-2b ​ Once upon a time there were three little sisters; and their names were​ Elsie–alice,​ Lacie and​ Tillie;​ and they lived at the bottom of a well. ​ …​ 我是div​ “”” # 创建beautifulsoup对象soup = BeautifulSoup(text,’lxml’) # 格式化输出soup.prettify()实例化对象html_bs4 = BeautifulSoup(text,’lxml’)格式化print(html_bs4.prettify()) 获取元素 只能获取第一个print(html_bs4.div,type(html_bs4.div))print(html_bs4.div[‘class’])获取标签名print(html_bs4.div.name) 获取文本内容print(html_bs4.div.string,type(html_bs4.div.string))print(html_bs4.p.string)获取标签中的素有内容包括字标签内的内容print(html_bs4.body.get_text) 获取属性的方法print(html_bs4.p[‘id’])获取当前标签所有的属性和值print(html_bs4.p.attrs) find()find_all() 返回的是列表print(html_bs4.find(‘p’))print(html_bs4.find(id=’p1’))print(html_bs4.find_all(‘p’))print(html_bs4.find_all(id=’p1’))print(html_bs4.find_all(data=”1”)) 元素节点print(html_bs4.body.contents)chil = html_bs4.body.childrenfor item in chil: print(item)print(html_bs4.body.descendants) css选择器 标签选择器print(html_bs4.select(‘p’))所有含类选择器title的print(html_bs4.select(‘.title’)) 同时获取id=p1的元素和 class=story 的标签res = html_bs4.select(‘#p1,.story’)print(res) 获取class属性为sister的按标签print(html_bs4.select(‘a.sister’))print(type(html_bs4.select(‘a.sister’)[0])) 查找只要满足选择器条件的 b 标签==组合查找,同时满足res = html_bs4.select(‘p b’)print(res)获取 页面中所有p标签中的 直接子元素b标签==子类选择器res = html_bs4.select(‘p&gt;b’)print(res) 查找包含指定属性的标签print(soup.select(‘[name]’)) 获取文本内容 text属性get(属性名) 获取指定属性的值res = html_bs4.select(‘a’)print(res)for i in res: print(i.text) print(i.get(‘href’)) 获取标签和标签属性 tag = soup.p 获取标签名 tag.name 获取标签属性 tag = soup.p[‘class’] #返回指定属性的值 tag = soup.p.attrs # 返回当前标签的所有属性 获取所有的子节点soup.body.contents # 返回列表形式soup.body.children # 返回迭代器类型soup.body.denscendants #返回所有后代元素 find_all()soup.find_all(‘p’) # 获取所有的p元素soup.find_all(‘p’，attrs = {‘class’:’title’}) # 获取所有class=title的p元素 选择器的用法 ***soup.select(‘p.title’) # 获取class=title的素有p标签soup.select(‘p .title’) # 获取p里面class=title的素有元素soup.select(‘#p1’) # 获取id=p1的元素soup.select(‘p &gt; a’) # 获取p的子元素asoup.select(‘p,div’)soup.select(‘a[class=”item1”]’)soup.select(‘a[class]’)soup.select(‘a[class*=”o”]’) #模糊查询soup.select(‘a[class^=”o”]’) # 限制开头soup.select(‘a[class$=”0”]’) # 限制结尾 获取文本和属性的方法.text().get(src) 练习 使用bs4获取腾讯招聘的信息]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jsonpath]]></title>
    <url>%2F2019%2F06%2F21%2Fjsonpath%2F</url>
    <content type="text"><![CDATA[jsonpathJsonPath 是一种信息抽取类库，是从JSON文档中抽取指定信息的工具，提供多种语言实现版本，包括：Javascript, Python， PHP 和 Java。 官方文档：http://goessner.net/articles/JsonPath JSONPath 描述 $ 根节点@ 现行节点.or[] 取子节点.. 就是不管位置，选择所有符合条件的条件 匹配所有元素节点[] 迭代器标示（可以在里边做简单的迭代操作，如数组下标，根据内容选值等）[,] 支持迭代器中做多选。?() 支持过滤操作.() 支持表达式计算 根据要求获取下面数据中的内容 1.将字符串转换成 python可操作性的数据类型dic = json.loads(strs)print(dic,type(dic))2.实例化jsonpath对象 获取根路径下的所有的数据json_obj = jsonpath.jsonpath(dic,’$..*’)获取根节点下的store的所有数据json_obj = jsonpath.jsonpath(dic,’$.store’)1.获取所有书籍的作者json_obj = jsonpath.jsonpath(dic,’$..author’)获取第二本书的详细信息 []里写的是索引值json_obj = jsonpath.jsonpath(dic,’$..book[1]’)获取第二本书和第三本书的详细信息json_obj = jsonpath.jsonpath(dic,’$..book[1,3]’)获取前两本书json_obj = jsonpath.jsonpath(dic,’$..book[:2]’)获取后两本书json_obj = jsonpath.jsonpath(dic,’$..book[-2:]’)获取所有的book信息json_obj = jsonpath.jsonpath(dic,’$..book’)获取价格大于10的数据json_obj = jsonpath.jsonpath(dic,’$..book[?(@.price&gt;10)]’)找价格等于22.99的书籍 是否相等 ==json_obj = jsonpath.jsonpath(dic,’$..book[?(@.price==22.99)]’)找有书号的书籍信息json_obj = jsonpath.jsonpath(dic,’$..book[?(@.isbn)]’)查看所有书籍的价格json_obj = jsonpath.jsonpath(dic,’$..book..price’)取个 价格 &gt;8 并且小于20 的 书看下json_obj = jsonpath.jsonpath(dic,’$..book[?(@.price8.99)]’)$..book[?(8&lt;@.price&lt;20)json_obj = jsonpath.jsonpath(dic,’$..book[?(8&lt;@.price&lt;20)]’)print(json_obj)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spss]]></title>
    <url>%2F2019%2F06%2F19%2Fspss%2F</url>
    <content type="text"><![CDATA[源文件:有道云笔记 链接http://note.youdao.com/noteshare?id=4a5d306357317ee80a5415af19e00d0a spss概括1.1 Spss历史 SPSS是世界上最早的统计分析软件，由美国斯坦福大学的三位研究生Norman H. Nie、C. Hadlai (Tex) Hull 和 Dale H. Bent于1968年研究开发成功，同时成立了SPSS公司，并于1975年成立法人组织、在芝加哥组建了SPSS总部。 2009年7月28日，IBM公司宣布将用12亿美元现金收购统计分析软件提供商SPSS公司。如今SPSS的最新版本为25，而且更名为IBM SPSS Statistics。迄今，SPSS公司已有40余年的成长历史。（百度） 1.2 Spss特点 操作简便 界面非常友好，类似于windows的风格界面，类似于Excel布局的数据视图给初学者一种天然的亲切感。除了数据录入及部分命令程序等少数输入工作需要键盘键入外，大多数操作可通过鼠标拖曳、点击“菜单”、“按钮”和“对话框”来完成。可以说只要了解统计分析的原理，无需通晓统计方法的各种算法，即可得到需要的统计分析结果。 编程方便 强大的编程互动性，常见的统计方法，SPSS的命令语句、子命令及选择项的选择绝大部分由“对话框”的操作完成。因此，用户无需花大量时间记忆大量的命令、过程、选择项。 功能强大 涵盖主要的数据分析流程，例如数据获取、数据处理、数据分析、数据展现等数据分析流程环节能够快速实现。尤其是数据分析环节，SPSS提供了从简单的统计描述到复杂的多因素统计分析方法，比如数据的探索性分析、统计描述、列联表分析、二维相关、秩相关、方差分析、多元回归、生存分析、协方差分析、聚类分析、Logistic回归等。 数据兼容 能够读取及输出多种格式的文件。比如SPSS可以读取文本、Excel、Stata、SAS；把表格、图像导出 为word、excel、powerpoint、txt文本、pdf、html等格式文件。 模块组合 SPSS for Windows软件分为若干功能模块。用户可以根据自己的分析需要和计算机的实际配置情况灵活选择。 1.3 Spss安装 助教帮助安装 1.4 认识Spss 一、数据窗口 我们，Spss窗口和Excel窗口貌似很类似，但细细看来，还是有很大不同。整体来看，Spss窗口主要有两部分构成： 菜单栏 主要包括 “文件”、“编辑”、“查看”、“数据”、“转换”、“分析”、“直销”、“图形”、“实用程序”、“窗口”、“帮助” 11个菜单。其中 “数据”、“转换”、“分析”三个菜单最常用。 数据视图 顾名思义就是输入、编辑和显示数据的视图窗口。在Spss中每一行数据被称为一条记录，被称为个案，每一列代表一个特征，在Spss中被称为变量 变量视图 用于设置、定义变量属性的窗口。 可以设置或者查看变量的属性，例如“名称”、“类型”、“宽度”等信息 注意：在设置变量属性时，注意数据类型、数据测量和数据角色的设置，因为这三个很容易设置错。一旦设置错误，将会造成无法进行数据处理、或者数据分析；更可怕的是有时会导致出现错误的分析结果。整个数据窗口可以被保存，默认格式为sav。 二、输出窗口 输出窗口又叫结果查看器，主要用于输出数据分析结果或绘制的相关图表。 输出窗口分为两个部分，左边是由目录组成的导航窗口，右边为内容区，所显示内容与右边目录一一对应。我们可以对输出结果进行复制、编辑等操作。 输出窗口可以被保存，可以把分析结果或者图表保存以备随时查看。保存格式默认为sav。 spss数据处理1.1 Spss历史 SPSS是世界上最早的统计分析软件，由美国斯坦福大学的三位研究生Norman H. Nie、C. Hadlai (Tex) Hull 和 Dale H. Bent于1968年研究开发成功，同时成立了SPSS公司，并于1975年成立法人组织、在芝加哥组建了SPSS总部。 2009年7月28日，IBM公司宣布将用12亿美元现金收购统计分析软件提供商SPSS公司。如今SPSS的最新版本为25，而且更名为IBM SPSS Statistics。迄今，SPSS公司已有40余年的成长历史。（百度） 1.2 Spss特点 操作简便 界面非常友好，类似于windows的风格界面，类似于Excel布局的数据视图给初学者一种天然的亲切感。除了数据录入及部分命令程序等少数输入工作需要键盘键入外，大多数操作可通过鼠标拖曳、点击“菜单”、“按钮”和“对话框”来完成。可以说只要了解统计分析的原理，无需通晓统计方法的各种算法，即可得到需要的统计分析结果。 编程方便 强大的编程互动性，常见的统计方法，SPSS的命令语句、子命令及选择项的选择绝大部分由“对话框”的操作完成。因此，用户无需花大量时间记忆大量的命令、过程、选择项。 功能强大 涵盖主要的数据分析流程，例如数据获取、数据处理、数据分析、数据展现等数据分析流程环节能够快速实现。尤其是数据分析环节，SPSS提供了从简单的统计描述到复杂的多因素统计分析方法，比如数据的探索性分析、统计描述、列联表分析、二维相关、秩相关、方差分析、多元回归、生存分析、协方差分析、聚类分析、Logistic回归等。 数据兼容 能够读取及输出多种格式的文件。比如SPSS可以读取文本、Excel、Stata、SAS；把表格、图像导出 为word、excel、powerpoint、txt文本、pdf、html等格式文件。 模块组合 SPSS for Windows软件分为若干功能模块。用户可以根据自己的分析需要和计算机的实际配置情况灵活选择。 1.3 Spss安装 助教帮助安装 1.4 认识Spss 一、数据窗口 我们，Spss窗口和Excel窗口貌似很类似，但细细看来，还是有很大不同。整体来看，Spss窗口主要有两部分构成： 菜单栏 主要包括 “文件”、“编辑”、“查看”、“数据”、“转换”、“分析”、“直销”、“图形”、“实用程序”、“窗口”、“帮助” 11个菜单。其中 “数据”、“转换”、“分析”三个菜单最常用。 数据视图 顾名思义就是输入、编辑和显示数据的视图窗口。在Spss中每一行数据被称为一条记录，被称为个案，每一列代表一个特征，在Spss中被称为变量 变量视图 用于设置、定义变量属性的窗口。 可以设置或者查看变量的属性，例如“名称”、“类型”、“宽度”等信息 注意：在设置变量属性时，注意数据类型、数据测量和数据角色的设置，因为这三个很容易设置错。一旦设置错误，将会造成无法进行数据处理、或者数据分析；更可怕的是有时会导致出现错误的分析结果。整个数据窗口可以被保存，默认格式为sav。 二、输出窗口 输出窗口又叫结果查看器，主要用于输出数据分析结果或绘制的相关图表。 输出窗口分为两个部分，左边是由目录组成的导航窗口，右边为内容区，所显示内容与右边目录一一对应。我们可以对输出结果进行复制、编辑等操作。 输出窗口可以被保存，可以把分析结果或者图表保存以备随时查看。保存格式默认为sav 描述性分析常见数据分析的分类： \1. 描述性数据分析 对所收集的数据进行分析，得出反映客观现象的各种数量特征的一种分析方法，主要分析方向为集中趋势分析、离散程度分析、频数分析等。描述性分析是所有继续数据分析的基础。 \2. 探索性数据分析 通过某些分析方法从大量的数据中发现未知有价值信息的过程。特点是不受分析模型和研究假设的限制，尽可能地寻找变量间的关联性，常见的分析方法有聚类、因子、对应分析等。 3.推断数据分析 通过样本数量特征对研究总体数量特征进行推断的过程。常见分析有相关分析、回归分析、假设检验等。 我们的课程主要集中于描述性数据分析和部分推断数据分析。 3.1 频率分析 主要分为两类： 3.1.1 分类变量频率分析 读入‘问卷调查.csv’文件，点击‘分析’菜单，选择‘描述统计’，此时右侧弹出子菜单，选择并点击‘频率’按钮，跳出如下对话框： 在左选择需要进行频率栏中的分类变量，然后点击转换按钮，待分析变量会自动跑到右边框内，然后点击确定就会得到分析结果： 如何理解有效百分比： |有效值|/|总体| 还有很多其他变量，留作大家课下练着玩吧！ 3.1.2 连续变量频率分析 step1: 完全和分类变量频率分析一致。 step2: 在‘频率’对话框中点击‘统计’按钮，弹出以下对话框： 在‘频率:统计对话框中，选择你所需的任意统计量，点击‘继续’。回到‘频率’对话框。点击‘确定’。生成下列图表。 接下来，让我们稍停步伐，看一看在‘频率：统计’对话框中的四个功能区，他们分别是‘百分位值‘、’集中趋势‘、’离散趋势‘、’分布特征‘；接下来我们逐一讲解： 百分位值： 有三个功能项，分别是四分位数、分割点、和百分位数。四分位数意思是用三个数据点把整体数据分成四等份，百分位数是用99个数据点把整体数据分成100等份。分割点功能可以实现整体数据的任意等分。 集中趋势： 主要有四个功能项，平均值、中位数、众数、总和。我们这里主要关注平均值、中位数、和众数的区别。 离散趋势： 主要功能项有，方差、标准差和范围。 范围就是数据中最大值与最小值得差，计算简单，容易理解。但是对极端值敏感，无法正确反映离散情况。相对而言，方差和标准差能够反映数据的离散情况，对极端值有一定忍受程度，但是极端值过大，也会造成对数据的离散程度描述失真。 分布特征： 对于较大数据，人们通常希望了解他的分布状况，峰度和偏度是描述其分布的两个重要的统计参数。它们的值都依赖于标准正态分布，描述的是与标准正态分布的偏离程度。 让我们再次回答频率对话框，此时我们已经选好了所想展示的统计参数，如果我们不去直接按‘确定‘，而是单击第二个按钮’图表‘，然后跳出’频率：图标‘对话框： 在’频率：图表‘对话框中主要的功能项有’条形图‘，’饼图‘，’直方图‘。我们可以根据数据的类型和分析目的来选择合适的图表功能，比如对于离散数据，如果想了解其分布状态，通常选择条形图，如果想了解数据结构，通常选择饼图。对于连续数据，可通过直方图了解其分布状态。因此，这里我们选择直方图来描述连续变量Q3的分布状态。 从上图可以发现，变量Q3是近似正态，并且右偏的分布 3.2 描述分析 单击’分析菜单‘，在下拉框中选择’描述统计‘，在右边下拉框中找到并点击’描述分析‘，弹出下列对话框： 在左栏选择变量’Q3‘，点击转换按钮，Q3自动转换到右栏。点击’选项‘按钮，弹出’描述：选项‘。 在该对话框中选择所需的统计参数，点击继续，重新回到’描述‘对话框，然后点击确定。产生下面的描述统计表。 描述统计分析的结果和频率分析结果没有任何区别，甚至在某些方面还不如频率分析。但是，它也有自己的独到之处，比如利用描述分析对话框，可直接进行数据标准化。 3.3 交叉表分析 交叉表是一种行列交叉的分类汇总表格，行和列上至少各有一个分类变量，行和列的交叉处可以对数据进行多种汇总计算。 交叉表分析一般分析分类变量，以交叉表格的形式对两个或两个以上分类变量的关系进行多角度对比分析。 单击’分析菜单‘，在下拉框中选择’描述统计‘，在右边下拉框中找到并点击’交叉表‘，然后将会弹出下面的对话框： 利用一贯使用的方法，分别把’Q3‘和’Q13‘转移到’行‘功能框里，把Q2转移到’列‘功能框里，然后点击’单元格‘按钮’，弹出‘交叉表：单元格显示’对话框： 我们在‘计数’和‘百分比’与‘非整数权重’复选功能框内选取合适的功能项，然后点击继续，又回到‘交叉表’对话框，点击‘确定’按钮，返回结果。 3.4 多分类定义 分类数据可以通过二分类与多分类进行录入。二分类数据录入非常容易，通过一个带有变量值‘0’和‘1’的变量来完成即可，多分类数据录入也没有想象中的那么难。只要用合适的数字表示就可以了。接下来我们举例说明： 假设有这样一个问卷，有四个选项: 我们可以通过四个二分类变量的设置来保存问卷结果： ID 有钱 有权 长的帅 有钱有权长得丑 妹子1 0 1 0 0 妹子2 1 0 0 0 妹子3 0 0 1 0 妹子4 0 0 1 0 妹子5 0 0 0 1 妹子6 0 0 0 1 妹子7 1 0 0 0 妹子8 1 0 0 0 妹子9 0 1 0 0 也可以通过一个分类变量来保存问卷结果，如果我们把‘妹子的择偶标准’的四个分类分别赋予1，2，3，4四个值： 问卷结果的单变量多分类存储方式： ID 选择结果 妹子1 2 妹子2 1 妹子3 3 妹子4 3 妹子5 4 妹子6 4 妹子7 1 妹子8 1 妹子9 2 如果是多选，我们可以根据多选的数量来设置变量数量，比如可以多选，最多选三个： 多重分类法来存储多选结果： ID 选择结果1 选择结果2 选择结果3 妹子1 2 3 4 妹子2 1 2 4 妹子3 1 2 3 妹子4 1 2 3 妹子5 2 3 4 妹子6 2 3 4 妹子7 1 2 4 妹子8 1 2 3 妹子9 2 3 4 二分类法存储多选结果： ID 有钱 有权 长的帅 有钱有权长得丑 妹子1 0 1 1 1 妹子2 1 1 0 1 妹子3 1 1 1 0 妹子4 1 1 1 0 妹子5 0 1 1 1 妹子6 0 1 1 1 妹子7 1 1 0 1 妹子8 1 1 1 0 妹子9 0 1 1 1 无论是二分法还是多重分类法，只要使用多变量存储多选结果，那么无论如何就要对这些变量进行定义。否则spss将无法分析数据。因为这些变量其实归属于一个特征，我们必须把它们集中成一个集合，定义成一个被我们统称之为‘多重响应集’的集合。其本质含义就是使用多变量记录结果，其中每条数据（每个个案）包含多个选择结果。 单击‘分析’菜单，选择‘定制表’，在弹出的右侧子菜单里选择并点击‘多重响应集’，则弹出’定义多重响应集’对话框： Q5明显是二分类多变量记录存储，因此必须把它们合并成一个多重响应集。 把对话框所有Q5变量的分类项全部转移到右框中，选择二分法，计数值填写‘1’也就是说按类标签‘1’计数。在‘类别标签来源’处，选择功能项‘变量标签’才有意义。 Q8明显是多分类多变量存储记录，因此必须把它们合并成一个多重响应集。其方法和上面雷同，仅仅在‘变量编码’处选择‘类别’，与上面不同。 相关分析第四章: 相关分析 相关分析分为两种：线性相关关系和非线性相关关系，线性相关又叫直线相关，非线性相关通常指的是曲线相关。 我们这里主要关注于‘皮尔森线性相关’ 接下来我们首先进行散点图绘制。单击‘图形’菜单，选择‘旧对话框’，此时在右侧出现的菜单中选择并点击‘散点图/电图’，然后弹出下面的对话框： 选择‘简单散点图’然后点击‘定义’按钮。 然后把右边栏目中的合适变量分别通过转换按钮转移到’Y轴‘和’X轴‘功能框里。单击’确定‘按钮，即可得到下面的散点图： 很明显，变量‘Season’和‘Retail Sales’之间存在明显的线性正相关关系，‘Retail Sales’随着变量‘Season’单调增变化。 散点图帮助我们实现了对变量间关系的可视化直观判断，若想量化这种关系，就必须通过相关分析来实现了。 单击‘分析’菜单，选择‘相关’，在右侧弹出子菜单，单击‘双变量’，弹出‘双变量相关性’对话框。 回归分析第五章: 回归分析 定义:回归分析是统计分析方法，旨在模拟一个因变量与一个或多个自变量之间的关系。主要用来定量描述相关性或者对因变量的值进行预测。 换成通俗语言：研究自变量与因变量之间数量变化关系的一种分析方法，它主要是通过确定因变量Y与影响它的自变量Xi之间的函数关系，衡量自变量Xi对因变量Y的影响能力，进而可以预测因变量Y的发展趋势。 回归分析包括线性和非线性回归，非线性回归可以通过适合的数学变换转化为线性回归。这里我们主要着力于线性回归。 5.1 线性回归分析流程 \1. 根据预测目标选择合适的自变量和因变量 \2. 对于简单回归分析，可以绘制散点图，直观地观察因变量是否随自变量呈线性发展趋势 \3. 利用最小二法进行模型参数估计，确定回归模型 \4. 对回归模型进行检验 \5. 利用回归模型进行预测 线性回归的理论依据是最小二算法，这在下一阶段的算法课上会有介绍。 下面我们通过一个例子来学习下用Spss处理线性回归问题。 1，数据聚合，数据初判断 打开文件’Sales5000.xls’，单击‘数据’，选择并单击‘汇总’后，跳出下面的对话框： 通过转换按钮把选定的分组变量从左框中转移到‘分界变量’功能框里；把待聚合变量转移到‘汇总变量’功能框里，具体内容见下表： 按下‘确定’按钮，就会产生一个新的承载聚合结果的数据集‘New_data’。此时，数据量从原始的50000条数据聚合到2766条数据。 为了进一步聚合，我们通过变量计算把变量‘OrderDate’转换成只包含年月的字符串，然后按转换后的变量聚合。 按转换后的变量‘Orderdata_new1’继续聚合： 最终得到下面的结果： 聚合后的数据减少到91行 2，回归分析 我们确定Orderdata_new1为自变量，UnitsSold_sum_sum为因变量，然后对二者做散点图： 同理，我们也可以对‘TotalRevenue_sum_sum’作图。 我们发现其中并没有很强规律性，当然还是可以做回归分析的，只不过效果不会太好。尽管无法实施我们的最终目的，但是从中我们可以总结出，对于单变量或双变量回归，通过作图，在一定程度上可以判断数据是否具有回归价值。 打开数据‘推广效果数据’，画出散点图，方法如上： 点击‘确定’后，产生下面的图像： 很明显，上面图像告诉我们，广告费用与购买用户数存在明显正相关。两者之间的皮尔森相关系数为：0.952。下面我们对其建立线性回归模型： 单击‘分析’，在下拉菜单中选择‘回归’，在右边的下拉菜单中点击‘线性’，则跳出‘线性回归’对话框： 参数设置： \1. 设置‘因变量’与‘自变量’ \2. 回归系数与拟合度设置 单击‘统计’按钮，弹出‘线性回归：统计’对话框。 在‘回归系数’功能框中选择‘估算值’功能框,返回回归系数。 在该框的右边选择‘模型拟合’返回拟合优度R的值。 \3. 自变量步进标准及常数项设置 点击‘线性回归’对话框中的‘选项’按钮，跳出‘线性回归：选项’对话框 步进法条件实际就是模型输入变量和移去变量的条件，具体如下： 准则: F-to-enter 的概率 &lt;= .050，F-to-remove 的概率 &gt;= .100，F指的是F-检验，它通常来检验正态假定下两个变量（自变量与因变量）之间的相关性。F值对应的概率越小，自变量与因变量的关系越强，也就意味着越有资格进入。这样的设置避免了经验缺乏的新手不知道如何通过F值作变量存移。 另外，还需在‘在方程中包括常量’前面的方框里打上对号。 点击‘继续’回到线性回归对话框。单机确定，完成回归分析。 \4. 模型检验，运行结果分析： 这个表格没什么可说的。 这个表格中最重要的参数是R方，因为简单的线性回归主要采用R方来考量模型模拟效果，而调整后R方用于修正因自变量个数的增加而导致的模型过拟合问题，因此多用于衡量多重线性回归模型的拟合效果。 对于这个表格我们只需关注显著性（p值）即可。也即直接与显著性水平a（0.01,或0.05）比较得出结果。表格中的显著性=0.000&lt;=0.01,说明0假设不合理，对立假设成立。也即自变量和因变量间存在的线性关系具有极其显著性的统计学意义。如果0.01&lt;显著性（p值）&lt;=0.05，则结果具有显著的统计学意义；如果显著性（p值）&lt;=0.01，则结果具有极其显著的统计学意义。 上面的表格是线性回归模型回归系数表。这里重要的信息是‘B’列数据和‘显著性’列数据。‘B’列数据描述的是回归系数；‘显著性’列是回归系数显著性检验的结果，即研究回归模型中的每个自变量与因变量之间是否存在显著的线性关系。p值越小，两者之间的线性关系就越显著；反之，则越不显著。如果检验不显著，则其对应的回归系数要从模型中删去。 根据‘B’列我们可以确定该线性回归模型： Y=77.687x+1835.016 \5. 回归模型进行预测 点击‘分析’ 按钮，在下拉框中选择‘回归’标签，然后在右边的下拉框中选择并点击‘线性‘，在弹出的对话框中点击保存： 在弹出的对话框中按下图设置参数： 点击‘继续’再次回到’线性回归‘对话框中。然后点击确定，就会发现一个新列产生，该列就是预测值。]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>Spss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据的加载,存储,清洗,转换合并]]></title>
    <url>%2F2019%2F06%2F17%2F%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8A%A0%E8%BD%BD-%E5%AD%98%E5%82%A8-%E6%B8%85%E6%B4%97-%E8%BD%AC%E6%8D%A2%E5%90%88%E5%B9%B6%2F</url>
    <content type="text"><![CDATA[拼接 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849以下皆是以3x3的矩阵进行拼接np.hstack((a,b)) 水平拼接 行数相等如两个3x3拼接 [[ 7, 34, 37, 70, 11, 67], [56, 12, 35, 66, 38, 85], [84, 80, 89, 49, 23, 88]]np.vstack((a,b)) 垂直拼接 列数相等如两个3x3拼接 [[21, 62, 2], [41, 8, 48], [46, 37, 91], [ 1, 62, 72], [82, 84, 4], [ 4, 88, 81]]concatenate((a,b),axis=0) 可拼接多个,axis默认垂直拼接=0(主要用于大数据的处理)axis=0默认的 [[80, 45, 33], [18, 39, 41], [10, 80, 32], [71, 23, 95], [67, 58, 21], [ 2, 42, 31]]axis=1水平的 [[78, 81, 99, 58, 76, 46], [30, 47, 52, 22, 66, 42], [21, 89, 84, 2, 56, 31]]column_stack((a,b)) 水平拼接 列拼接 [[86, 10, 90, 27, 23, 28], [20, 23, 45, 37, 82, 34], [14, 57, 86, 72, 6, 27]]row_stack(a,b) 垂直拼接 行拼接 [[51, 90, 24], [63, 90, 37], [27, 79, 91], [90, 2, 45], [74, 4, 48], [54, 70, 11]]merge(a,b,on=&apos;对应键a&apos;) 按照对应键进行笛卡尔乘积(内连接)键相同的保留,合并的连个数据框需要列名相同(pandas的函数) a b_x c_x b_y c_y 0 9 3 9 1 2其中merge中有一个参数修改连接方式(how=&apos;left&apos;左连接,how=&apos;right&apos;右连接),若果键名不同通过left_on\right_on实现join 同merge,默认使用左连接,how = &apos;right&apos;右连接 两个dataframe的列索引不能有一样的concat((a1,a2),axis=0) 默认垂直拼接,axis=1变成水平拼接axis=1 a b c a b c0 9 3 9 9 1 21 8 1 5 7 1 12 1 8 7 2 8 7]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas.read_csv参数]]></title>
    <url>%2F2019%2F06%2F14%2Fpandas-read-csv%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[读取csv文件：pd.read_csv(),写入csv文件:pd.to_csv()pandas还可以读取一下文件：read_csv,read_excel,read_hdf,read_sql,read_json,read_msgpack (experimental),read_html,read_gbq (experimental),read_stata,read_sas,read_clipboard,read_pickle;相应的写入：to_csv,to_excel,to_hdf,to_sql,to_json,to_msgpack (experimental),to_html,to_gbq (experimental),to_stata,to_clipboard,to_pickle. 常用参数的读取csv文件header=None时，即指明原始文件数据没有列索引，这样read_csv为自动加上列索引，除非你给定列索引的名字。 header=0，表示文件第0行（即第一行，python，索引从0开始）为列索引，这样加names会替换原来的列索引。 index_col为指定数据中哪一列作为Dataframe的行索引，也可以可指定多列，形成层次索引，默认为None,即不指定行索引，这样系统会自动加上行索引（0-） usecols:可以指定原数据集中，所使用的列。在本例中，共有4列，当usecols=[0,1,2,3]时，即选中所有列，之后令第一列为行索引，当usecols=[1,2,3]时，即从第二列开始，之后令原始数据集的第二列为行索引。 nrows：可以给出从原始数据集中的所读取的行数，目前只能从第一行开始到nrows行。 sep 指定分隔符 sep=’\s+’ delimiter : 定界符，备选分隔符（如果指定该参数，则sep参数失效）squeeze : boolean, default False 如果文件值包含一列，则返回一个Series prefix : str, default None 在没有列标题时，给列添加前缀。例如：添加‘X’ 成为 X0, X1, … mangle_dupe_cols : boolean, default True 重复的列，将‘X’…’X’表示为‘X.0’…’X.N’。如果设定为false则会将所有重名列覆盖。 dtype : Type name or dict of column -&gt; type, default None 每列数据的数据类型。例如 {‘a’: np.float64, ‘b’: np.int32} converters : dict, default None 列转换函数的字典。key可以是列名或者列的序号 skipinitialspace : boolean, default False 忽略分隔符后的空白（默认为False，即不忽略） skiprows : list-like or integer, default None 需要忽略的行数（从文件开始处算起），或需要跳过的行号列表（从0开始） skipfooter : int, default 0 从文件尾部开始忽略。 (c引擎不支持) skip_footer : int, default 0 不推荐使用：建议使用skipfooter ，功能一样 keep_default_na : bool, default True 如果指定na_values参数，并且keep_default_na=False，那么默认的NaN将被覆盖，否则添加。 na_filter : boolean, default True 是否检查丢失值（空字符串或者是空值）。对于大文件来说数据集中没有空值，设定na_filter=False可以提升读取速度 verbose : boolean, default False 是否打印各种解析器的输出信息，例如：“非数值列中缺失值的数量”等。 skip_blank_lines : boolean, default True 如果为True，则跳过空行；否则记为NaN。 keep_date_col : boolean, default False 如果连接多列解析日期，则保持参与连接的列。默认为False。 dayfirst : boolean, default False DD/MM格式的日期类型 chunksize : int, default None 文件块的大小 thousands : str, default None 千分位分割符，如“，”或者“.” decimal : str, default ‘.’ 字符中的小数点 (例如：欧洲数据使用’，‘). quotechar : str (length 1), optional 引号，用作标识开始和解释的字符，引号内的分割符将被忽略。 doublequote : boolean, default True 双引号，当单引号已经被定义，并且quoting 参数不是QUOTE_NONE的时候，使用双引号表示引号内的元素作为一个元素使用。 escapechar : str (length 1), default None 当quoting 为QUOTE_NONE时，指定一个字符使的不受分隔符限值。 error_bad_lines : boolean, default True 如果一行包含太多的列，那么默认不会返回DataFrame ，如果设置成false，那么会将改行剔除（只能在C解析器下使用）。]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率论]]></title>
    <url>%2F2019%2F06%2F12%2F%E6%A6%82%E7%8E%87%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[条件概率分布函数]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas和绘图课件]]></title>
    <url>%2F2019%2F06%2F11%2Fpandas%E5%92%8C%E7%BB%98%E5%9B%BE%E8%AF%BE%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Pandas的数据结构Series:Series数据的建立 -&gt;In [1]: import numpy as np In [2]: import pandas as pd In [3]: pddata_1=pd.Series([1,2,3]) In [4]: pddata_1Out[4]:0 11 22 3dtype: int64数据取值和获取搜索索引In [7]: pddata_1.valuesOut[7]: array([1, 2, 3], dtype=int64)In [8]: pddata_1.indexOut[8]: RangeIndex(start=0, stop=3, step=1) 例子： In [9]: pddata_2=pd.Series([‘this’,’is’,’pandas’]) In [10]: pddata_2.valuesOut[10]: array([‘this’, ‘is’, ‘pandas’], dtype=object) In [11]: pddata_2.indexOut[11]: RangeIndex(start=0, stop=3, step=1) 带有独特索引的Series数据： In [15]: pddata_3=pd.Series([344,33,-456],index=[‘andereas’,’hachenberger’,’dieter’])In [16]: pddata_3Out[16]:andereas 344hachenberger 33dieter -456dtype: int64In [17]: pddata_3.index,pddata_3.valuesOut[17]:(Index([‘andereas’, ‘hachenberger’, ‘dieter’], dtype=’object’),array([ 344, 33, -456], dtype=int64)) 例子：In [21]: testa=[245,788,90]In [22]: pddata_4=pd.Series(testa,index=[‘wir’,’sind’,’Menschen’])In [23]: pddata_4Out[23]:wir 245sind 788Menschen 90dtype: int64In [25]: pddata_4[‘wir’]Out[25]: 245In [27]: pddata_4[[‘wir’,’sind’,’Menschen’]]Out[27]:wir 245sind 788Menschen 90dtype: int64 例子：In [31]: seriesdata_1=pd.Series([‘Profession’,’student’,’bechelor’,’professor’,’informatics developer’],index=[‘Name’,’Linker’,’Hamacher’,’Schuhmacher’,’Heintze’]) In [32]: seriesdata_1Out[32]:Name ProfessionLinker studentHamacher bechelorSchuhmacher professorHeintze informatics developerdtype: object Series数据中的过滤、计算。 In [33]: seriesdata_2=pd.Series([34,-4,-45,-37,32,9,1,3],index=[‘a’,’d’,’e’,’y’,’f’,’t’,’u’,’o’]) In [34]: seriesdata_2Out[34]:a 34d -4e -45y -37f 32t 9u 1o 3dtype: int64 In [35]: seriesdata_2[seriesdata_2&gt;1]Out[35]:a 34f 32t 9o 3dtype: int64 In [36]: seriesdata_2[seriesdata_2&gt;0]Out[36]:a 34f 32t 9u 1o 3dtype: int64 In [37]: seriesdata_2*3Out[37]:a 102d -12e -135y -111f 96t 27u 3o 9dtype: int64 In [38]: np.sin(seriesdata_2)Out[38]:a 0.529083d 0.756802e -0.850904y 0.643538f 0.551427t 0.412118u 0.841471o 0.141120dtype: float64 Serie 和字典十分相似，因此原本字典的函数也可以用： 我们先复习下字典：例子： In [39]: dic={‘Winne’:178,’Johanis’:189,’Banach’:186} In [40]: dic[‘Li’]=176 In [41]: dicOut[41]: {‘Banach’: 186, ‘Johanis’: 189, ‘Li’: 176, ‘Winne’: 178} 例子：In [19]: list11=[‘zhang’,’wang’,’li’] In [20]: list22=range(3) In [21]: indirect=zip(list22,list11) In [22]: diction={index:value for index,value in indirect} In [23]: dictionOut[23]: {0: ‘zhang’, 1: ‘wang’, 2: ‘li’} 例3： In [26]: list11=[‘a’,’b’,’c’] In [27]: list22=[1,2,3] In [28]: dfgeg=zip(list11,list22) In [29]: sss=dict((index,value) for index,value in dfgeg) In [30]: sssOut[30]: {‘a’: 1, ‘b’: 2, ‘c’: 3} 应用字典函数“in”和“Update”到Series 例子In [34]: datas_pys=pd.Series(range(4),index=[‘i’,’want’,’to’,’do’]) In [35]: datas_pysOut[35]:i 0want 1to 2do 3dtype: int32 In [36]: ‘want’ in datas_pysOut[36]: True例子：In [56]: datas_pys.update(pd.Series([2,3,4],index=[‘want’,’to’,’do’])) In [57]: datas_pysOut[57]:i 0want 2to 3do 4dtype: int32 In [58]: s1 = pd.Series([1, 2, 3]) In [59]: s2 = pd.Series([4, 5, 6]) In [60]: s3 = pd.Series([4, 5, 6], index=[3,4,5]) In [61]: s1.append(s2)Out[61]:0 11 22 30 41 52 6dtype: int64 In [62]: s1.append(s3)Out[62]:0 11 22 33 44 55 6dtype: int64 In [63]: s1.append(s2, ignore_index=True)Out[63]:0 11 22 33 44 55 6dtype: int64重新编号 字典可以直接转化为Serie In [68]: dicttna={1:’foo’,3:’drt’,8:’tyue’} In [69]: serie_12=pd.Series(dicttna) In [70]: serie_12Out[70]:1 foo3 drt8 tyuedtype: object isnull和notnull函数可用于检测数据缺失。 In [79]: dit_113={‘lin’:139,’zhang’:134,’wang’:173,’tan’:None} In [80]: serie_123=pd.Series(dit_113) In [81]: serie_123Out[81]:lin 139.0tan NaNwang 173.0zhang 134.0dtype: float64 In [82]: pd.isnull(serie_123)Out[82]:lin Falsetan Truewang Falsezhang Falsedtype: bool In [83]: pd.notnull(serie_123)Out[83]:lin Truetan Falsewang Truezhang Truedtype: bool serie_123.notnull()Out[85]:lin Truetan Falsewang Truezhang Truedtype: bool Series 的索引可以修改：In [87]: serie_123.indexOut[87]: Index([‘lin’, ‘tan’, ‘wang’, ‘zhang’], dtype=’object’)In [88]: serie_123.index=[‘lin’, ‘tan’, ‘shan’, ‘zhang’]In [90]: serie_123Out[90]:lin 139.0tan NaNshan 173.0zhang 134.0dtype: float64 DataFrame简单数据框的构成：由字典直接形成 In [93]: sales=[12365,34563,45673,23461,89034] In [94]: seller=[‘zhanghui’,’dongyibo’,’yangqian’,’liujuntao’,’zhangshanshan’] In [95]: sales_quantity=[213,305,452,302,190] In [96]: table_sales={‘Seller’:seller,’Sales’:sales,’SalesQuantity’:sales_quantity} In [97]: framesample_1=pd.DataFrame(table_sales) In [98]: framesample_1Out[98]:Sales SalesQuantity Seller0 12365 213 zhanghui1 34563 305 dongyibo2 45673 452 yangqian3 23461 302 liujuntao4 89034 190 zhangshanshan 与Series一样，索引会自动加上。 我们还可以指定列序列的左右顺序。 In [99]: framesample_1=pd.DataFrame(table_sales,columns=[‘SalesQuantity’,’Seller’,’Sales’]) In [100]: framesample_1Out[100]:SalesQuantity Seller Sales0 213 zhanghui 123651 305 dongyibo 345632 452 yangqian 456733 302 liujuntao 234614 190 zhangshanshan 89034 如果数据框中不包含所要找的值，自动返回NaN In [101]: framesample_1=pd.DataFrame(table_sales,columns=[‘SalesQuantity’,’Seller’,’Sales’,’Profit’]) In [102]: framesample_1Out[102]:SalesQuantity Seller Sales Profit0 213 zhanghui 12365 NaN1 305 dongyibo 34563 NaN2 452 yangqian 45673 NaN3 302 liujuntao 23461 NaN4 190 zhangshanshan 89034 NaN 索引DataFrame 的数据。类似字典 列数据索引In [104]: framesample_1[‘Seller’]Out[104]:0 zhanghui1 dongyibo2 yangqian3 liujuntao4 zhangshanshanName: Seller, dtype: object 行数据索引： In [106]: framesample_1.ix[2]Out[106]:SalesQuantity 452Seller yangqianSales 45673Prodit NaNName: 2, dtype: object Dateframe的索引也可以改变。 In [108]: framesample_1.index=[5,6,7,8,9] In [109]: framesample_1Out[109]:SalesQuantity Seller Sales Prodit5 213 zhanghui 12365 NaN6 305 dongyibo 34563 NaN7 452 yangqian 45673 NaN8 302 liujuntao 23461 NaN9 190 zhangshanshan 89034 NaN NAN值可以通过赋值语句替换。In [13]: framesample_1[‘Profit’]=[278,967,654,234,432]In [14]: framesample_1Out[14]:SalesQuantity Seller Sales Profit5 213 zhanghui 12365 2786 305 dongyibo 34563 9677 452 yangqian 45673 6548 302 liujuntao 23461 2349 190 zhangshanshan 89034 432 当用Series赋值时，可以精确地赋值到行列交叉位，没有指定的行列交叉位不会被赋值，将会以NAN的形式显示。In [17]: val_Series=pd.Series([0,1,1],index=[6,7,9])In [19]: framesample_1[‘Profit’]=val_SeriesIn [20]: framesample_1Out[20]:SalesQuantity Seller Sales Profit5 213 zhanghui 12365 NaN6 305 dongyibo 34563 0.07 452 yangqian 45673 1.08 302 liujuntao 23461 NaN9 190 zhangshanshan 89034 1.0 为不存在的列赋值会产生新的列。In [21]: framesample_1[‘loss’]=pd.Series([165,0,0,34,0],index=range(5,10,1))In [22]: framesample_1Out[22]:SalesQuantity Seller Sales Profit loss5 213 zhanghui 12365 NaN 1656 305 dongyibo 34563 0.0 07 452 yangqian 45673 1.0 08 302 liujuntao 23461 NaN 349 190 zhangshanshan 89034 1.0 0 可用del删除列 In [25]: del framesample_1[‘Profit’] In [26]: framesample_1Out[26]:SalesQuantity Seller Sales loss5 213 zhanghui 12365 1656 305 dongyibo 34563 07 452 yangqian 45673 08 302 liujuntao 23461 349 190 zhangshanshan 89034 0 嵌套字典：In [28]: Qtditc={‘TeacherLiu’:{‘height’:172,’weight’:67,’age’:34},’TeacherHuang’:{‘height’:182,’weight’:77,’age’:36},’TeacherTao’:{‘height’:192,’weight’:98,’age’:56} } In [29]: QtditcOut[29]:{‘TeacherHuang’: {‘age’: 36, ‘height’: 182, ‘weight’: 77},‘TeacherLiu’: {‘age’: 34, ‘height’: 172, ‘weight’: 67},‘TeacherTao’: {‘age’: 56, ‘height’: 192, ‘weight’: 98}} 如果把嵌套字典传给数据框，构造数据框，外键为列，内键为行索引。In [30]: GetNewFrame=pd.DataFrame(Qtditc) In [31]: GetNewFrameOut[31]:TeacherHuang TeacherLiu TeacherTaoage 36 34 56height 182 172 192weight 77 67 98 数据框的转置 In [32]: GetNewFrame.TOut[32]:age height weightTeacherHuang 36 182 77TeacherLiu 34 172 67TeacherTao 56 192 98 Series组成的字典也可以直接转化为数据框： In [33]: Popdic={‘LinFeng’:pd.Series([23,45,165],index=[‘age’,’weight’,’height’]),’Zhangduoli’:pd.Series([43,75,175],index=[‘age’,’weight’,’height’]),’JinChang’:pd.Series([51,46,185],index=[‘age’,’weight’,’height’])} In [35]: Getnumerpop=pd.DataFrame(Popdic) In [36]: GetnumerpopOut[36]:JinChang LinFeng Zhangduoliage 51 23 43weight 46 45 75height 185 165 175 数据框名字的添加： In [45]: Getnumerpop.columns.name=’Name’ In [46]: Getnumerpop.index.name=’personal information’ In [47]: GetnumerpopOut[47]:Name JinChang LinFeng Zhangduolipersonal informationage 51 23 43weight 46 45 75height 185 165 175 数据框的值：In [53]: Getnumerpop.valuesOut[53]:array([[ 51, 23, 43],[ 46, 45, 75],[185, 165, 175]], dtype=int64) 数据框的index： 例1： Getnumerpop.index=[‘p_age’,’p_weight’,’p_height’] Getnumerpop.indexOut[57]: Index([‘p_age’, ‘p_weight’, ‘p_height’], dtype=’object’) GetnumerpopOut[58]:Name JinChang LinFeng Zhangduolip_age 51 23 43p_weight 46 45 75p_height 185 165 175 In [64]:index1=Getnumerpop.indexIn [65]:index2=GetNewFrame.index In [66]:index1.append(index2)Out[66]: Index([‘p_age’, ‘p_weight’, ‘p_height’, ‘age’, ‘height’, ‘weight’], dtype=’object’) 例2index2.intersection(index1)Out[82]: Index([], dtype=’object’) 例3：index2.union(index1)Out[86]: Index([‘age’, ‘height’, ‘p_age’, ‘p_height’, ‘p_weight’, ‘weight’], dtype=’object’) 例4index1.delete(1)Out[94]: Index([‘p_age’, ‘p_height’], dtype=’object’)22例5index1.insert(1,’pheight’)Out[97]: Index([‘p_age’, ‘pheight’, ‘p_weight’, ‘p_height’], dtype=’object’)index1.is_monotonicOut[104]: Falseindex1.is_uniqueOut[105]: Trueindex1.unique()Out[107]: Index([‘p_age’, ‘p_weight’, ‘p_height’], dtype=’object’)Series索引重建In [2]: import numpy as npIn [3]: import pandas as pdIn [4]: Seriestest_1=pd.Series([10,-34,-89,36,50],index=[‘a’,’b’,’c’,’d’,’e’]) In [5]: Seriestest_1Out[5]:a 10b -34c -89d 36e 50dtype: int64 In [7]: Seriestest_2=Seriestest_1.reindex([‘c’,’b’,’d’,’a’,’e’,’f’]) In [8]: Seriestest_2Out[8]:c -89.0b -34.0d 36.0a 10.0e 50.0f NaNdtype: float64 NAN值可以被替换 In [10]: Seriestest_2=Seriestest_1.reindex([‘c’,’b’,’d’,’a’,’e’,’f’],fill_value=0) In [11]: Seriestest_2Out[11]:c -89b -34d 36a 10e 50f 0dtype: int64 使用ffill 和 bfill函数可以自动向前和向后补充缺失索引。（缺失值补充）In [6]: testarray1=pd.Series([23,22,34],index=[3,7,9])In [10]: test_12=testarray1.reindex(range(11),method=’ffill’) In [11]: test_12Out[11]:0 NaN1 NaN2 NaN3 23.04 23.05 23.06 23.07 22.08 22.09 34.010 34.0dtype: float64 In [12]: test_12=testarray1.reindex(range(11),method=’bfill’) In [13]: test_12Out[13]:0 23.01 23.02 23.03 23.04 22.05 22.06 22.07 22.08 34.09 34.010 NaNdtype: float64 数据框的索引修改：（reindex可以任意删除，添加，交换行列） 对于数据框，reindex可以修改行索引和列，或两个都更改。 In [14]: #这里我用一个新的方法构造数据框In [18]: frame_1=pd.DataFrame(np.arange(9).reshape(3,3),index=[‘row1’,’row2’,’row3’],columns=[‘one’,’two’,’three’]) In [19]: frame_1Out[19]:one two threerow1 0 1 2row2 3 4 5row3 6 7 8 In [20]: frame2=frame_1.reindex([‘row0’,’row1’,’row2’,’row3’]) In [21]: frame2Out[21]:one two threerow0 NaN NaN NaNrow1 0.0 1.0 2.0row2 3.0 4.0 5.0row3 6.0 7.0 8.0 In [22]: #reindex 修改列In [28]: frame3=frame2.reindex(columns=[‘four’,’three’]) In [29]: frame3Out[29]:four threerow0 NaN NaNrow1 NaN 2.0row2 NaN 5.0row3 NaN 8.0 In [30]: #同时修改列和行索引 In [31]: frame4=frame_1.reindex([‘row1’,’row2’,’row3’,’row4’],columns=[‘five’,’three’,’six’]) In [32]: frame_1Out[32]:one two threerow1 0 1 2row2 3 4 5row3 6 7 8 In [33]: frame4Out[33]:five three sixrow1 NaN 2.0 NaNrow2 NaN 5.0 NaNrow3 NaN 8.0 NaNrow4 NaN NaN NaN 利用ix函数修改数据框行索引与列，快捷整洁，节约时间。但是Ix已经几乎被启用，现在用loc函数。In [35]: frame5=frame_1.reindex(columns=[‘one’,’four’,’two’,’three’]) In [36]: frame5Out[36]:one four two threerow1 0 NaN 1 2row2 3 NaN 4 5row3 6 NaN 7 8 In [40]: frame5[‘four’]=pd.Series([34,56,78],index=[‘row1’,’row2’,’row3’]) In [41]: frame5Out[41]:one four two threerow1 0 34 1 2row2 3 56 4 5row3 6 78 7 8In [42]: frame5.ix[[‘row2’,’row1’,’row3’],[‘one’,’two’,’four’,’three’]]C:\Users\dongfeng\Anaconda3\lib\site-packages\ipykernel_launcher.py:1: DeprecationWarning:.ix is deprecated. Please use.loc for label based indexing or.iloc for positional indexing See the documentation here:http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated“””Entry point for launching an IPython kernel.Out[42]:one two four threerow2 3 4 56 5row1 0 1 34 2row3 6 7 78 8 #现在用loc的比较多，ix已经不再推荐使用。 In [44]: frame5.loc[[‘row2’,’row1’,’row3’],[‘one’,’two’,’four’,’three’]]Out[44]:one two four threerow2 3 4 56 5row1 0 1 34 2row3 6 7 78 8 指定轴上项目的丢弃： Drop 函数应用到“Series”In [46]: Seriestest_1=pd.Series([233,356,997],index=[‘as’,’a’,’sample’])In [47]: Seriestest_1Out[47]:as 233a 356sample 997dtype: int64 Seriestest_1.drop(‘a’)Out[95]:as 233sample 997dtype: int64 In [48]: #drop函数或者说方法返回的是一个在指定轴上删除指定值的新对象 Drop函数应用到“DataFrame”Drop函数删除指定值时必须指定轴，如果不指定轴，默认零轴。也就是说删除“1”轴上的指定项时必须指定轴号。删除“0”轴上的指定项时不需要指定轴号。In [53]: Dataframe_1_test=pd.DataFrame(np.floor(np.random.randn(4,4)),index=[‘xu’,’liu’,’zhang’,’feng’],columns=[‘stufe’,’klasse’,’degree’,’group’]) In [54]: Dataframe_1_testOut[54]:stufe klasse degree groupxu -1.0 0.0 -1.0 1.0liu 0.0 -2.0 -1.0 -1.0zhang 0.0 -3.0 -2.0 1.0feng -1.0 0.0 -1.0 0.0 In [57]: Dataframe_1_test.drop([‘stufe’,’klasse’],axis=1)Out[57]:degree groupxu -1.0 1.0liu -1.0 -1.0zhang -2.0 1.0feng -1.0 0.0 In [59]: Dataframe_1_test.drop([‘xu’,’feng’])Out[59]:stufe klasse degree groupliu 0.0 -2.0 -1.0 -1.0zhang 0.0 -3.0 -2.0 1.0 In [60]: Dataframe_1_test.drop([‘liu’])Out[60]:stufe klasse degree groupxu -1.0 0.0 -1.0 1.0zhang 0.0 -3.0 -2.0 1.0feng -1.0 0.0 -1.0 0.0 索引与选取 Series的索引和字典没有太大不同,唯一的区别是Series不仅可以通过设置的索引进行检索，也可以用默认的索引进行检索。另外Series也可以做切片，下面我们先看几个例子：In [6]: Testser_1=pd.Series([23,43,789,674,90,65],index=[‘apple’,’pear’,’persimmon’,’watermelon’,’strawberry’,’orange’]) In [7]: Testser_1Out[7]:apple 23pear 43persimmon 789watermelon 674strawberry 90orange 65dtype: int64 #按默认索引检索In [9]: Testser_1[2:4]Out[9]:persimmon 789watermelon 674dtype: int64 #按给定索引检索In [25]: Testser_1[‘apple’:’watermelon’]Out[25]:apple 23pear 43persimmon 789watermelon 674dtype: int64 #注意，按给定标签（也叫索引）检索切片末端不会减一末端是被包含的，这就是说末端是封闭区间。（是’]’，不是‘）’）In [26]: Testser_1[[‘apple’,’watermelon’]]Out[26]:apple 23watermelon 674dtype: int64 #注意上面两个例子的区别 #也可单个索引，按默认和按给定索引（也叫标签）均可检索In [27]: Testser_1[‘apple’]Out[27]: 23 In [28]: Testser_1[0]Out[28]: 23 #也可以按照bool值进行索引，此时检索不再按照给定的默认索引，而是与Series中的数据进行比较。 In [30]: Testser_1[Testser_1&gt;100]Out[30]:persimmon 789watermelon 674dtype: int64 给Series赋值：In [31]: Testser_1[‘apple’:’watermelon’]=[112,22,32,42] In [32]: Testser_1Out[32]:apple 112pear 22persimmon 32watermelon 42strawberry 90orange 65dtype: int64In [34]: Testser_1[‘apple’:’watermelon’]=56 In [35]: Testser_1Out[35]:apple 56pear 56persimmon 56watermelon 56strawberry 90orange 65dtype: int64例子：In [37]: Testser_1[[2,3]]=34 In [38]: Testser_1Out[38]:apple 56pear 56persimmon 34watermelon 34strawberry 90orange 65dtype: int64 In [39]: Testser_1[‘apple’]=34 In [40]: Testser_1Out[40]:apple 34pear 56persimmon 34watermelon 34strawberry 90orange 65dtype: int64 DataFrame的索引 In [5]: Data=pd.DataFrame(np.arange(16).reshape(4,4),index=(‘one’,’two’,’three’,’four’),columns=(‘wir’,’sie’,’ihr’,’ich’)) In [6]: DataOut[6]:wir sie ihr ichone 0 1 2 3two 4 5 6 7three 8 9 10 11four 12 13 14 15 In [7]: Data[‘wir’]Out[7]:one 0two 4three 8four 12Name: wir, dtype: int32 #单个索引时只能索引列，因此用“bool”值来检索时只能先检索出列数据，然后再与设定值比较，得出bool值Series，见下例：In [22]: Data[Data[‘ihr’]&gt;6]Out[22]:wir sie ihr ichthree 8 9 10 11four 12 13 14 15 Bool Series是可以当做索引的，见下Data[pd.Series([True,False,True,False],index=[‘one’,’two’,’three’,’four’])]Out[44]: wir sie ihr ichone 0 1 2 3three 8 9 10 11但这种索引容易出错，往往结果与我们的目的不一致，比如：ata=pd.DataFrame([[1,2,3,4],[0,4,8,2],[0.3,0.6,2.3,4],[3,8,9,0]],index=(‘one’,’two’,’three’,’four’),columns=(‘wir’,’sie’,’ihr’,’ich’)) ataOut[54]: wir sie ihr ichone 1.0 2.0 3.0 4two 0.0 4.0 8.0 2three 0.3 0.6 2.3 4four 3.0 8.0 9.0 0 ata[ata[‘ihr’]&gt;3]Out[55]: wir sie ihr ichtwo 0.0 4.0 8.0 2four 3.0 8.0 9.0 0 我们想选出大于‘3’，而结果并非如此。 In [8]: Data[[‘wir’,’ihr’]]Out[8]:wir ihrone 0 2two 4 6three 8 10four 12 14In [13]: #DataFrame 切片只能依靠行索引 In [16]: Data[‘one’:’three’]Out[16]:wir sie ihr ichone 0 1 2 3two 4 5 6 7three 8 9 10 11 In [17]: Data[1:3]Out[17]:wir sie ihr ichtwo 4 5 6 7three 8 9 10 11 bool值索引：In [23]: Data&gt;8Out[23]:wir sie ihr ichone False False False Falsetwo False False False Falsethree False True True Truefour True True True TrueIn [25]: Data[Data&gt;8]=0 In [26]: DataOut[26]:wir sie ihr ichone 0 1 2 3two 4 5 6 7three 8 0 0 0four 0 0 0 0上面我们已经讲过单个索引时我们只能索引列，为了解决这个问题numpy创造了ix方法。但是ix函数或许太老了，目前已经有新的函数loc来替代它。通过这个函数你几乎可以为所欲为。loc函数只能用给定行列索引进行检索 In [31]: Data.loc[‘two’]Out[31]:wir 4sie 5ihr 6ich 7Name: two, dtype: int32In [33]: Data.loc[‘four’,’wir’:’ihr’]Out[33]:wir 12sie 13ihr 14Name: four, dtype: int32 In [34]: Data.loc[‘four’,[‘wir’,’ihr’]]Out[34]:wir 12ihr 14Name: four, dtype: int32 Data.loc[[‘one’,’four’],[‘wir’,’ich’]]Out[136]: wir ichone 0 3four 12 15 In [35]: Data.loc[‘one’:’three’,’ich’]Out[35]:one 3two 7three 11Name: ich, dtype: int32In [38]: Data.loc[‘one’:,’wir’]Out[38]:one 0two 4three 8four 12Name: wir, dtype: int32 bool值索引In [45]: Data.loc[Data.wir&gt;5,:’ich’]Out[45]:wir sie ihr ichthree 8 9 10 11four 12 13 14 15 Reindex 方法： In [50]: DataOut[50]:wir sie ihr ichone 0 1 2 3two 4 5 6 7three 8 9 10 11four 12 13 14 15 In [51]: Data.reindex(index=[‘one’,’three’,’four’,’two’])Out[51]:wir sie ihr ichone 0 1 2 3three 8 9 10 11four 12 13 14 15two 4 5 6 7 In [52]: Data.reindex(columns=[‘wir’,’ihr’,’sie’,’ich’])Out[52]:wir ihr sie ichone 0 2 1 3two 4 6 5 7three 8 10 9 11four 12 14 13 15 Xs方法：根据标签选取单行和单列，返回‘Series’ In [66]: Data.xs(‘one’)Out[66]:wir 0sie 1ihr 2ich 3Name: one, dtype: int32 In [69]: Data.xs(‘sie’,axis=1)Out[69]:one 1two 5three 9four 13Name: sie, dtype: int32 Get_value 方法得到数据框内单个值： Data.get_value(‘three’,’ihr’)Out[72]: 10 简单的算数运算和数据对齐 加法Series数据相加必须按照索引一一相加，如果索引不能配对，将返回空值。In [1]: import numpy as np In [2]: import pandas as pd In [3]: Se1=pd.Series([2.3,78,2.5,3.8],index=[‘as’,’a’,’pupil’,’and’]) In [4]: Se2=pd.Series([2,7.7,2.1,3.3,9,12],index=[‘as’,’a’,’studen’,’teacher’,’and’,’or’])In [6]: tesseq_1=Se1+Se2 In [7]: tesseq_1Out[7]:a 85.7and 12.8as 4.3or NaNpupil NaNstuden NaNteacher NaNdtype: float64与Series相同，DataFrame数据相加必须按照行索引和列一一相加，如果索引和列不能配对（也就是两个数据的行列标签不相等时），将返回空值。（先行索引匹配，然后在进行列匹配）In [22]: add1=pd.DataFrame(np.arange(9).reshape(3,3),index=[‘ein’,’zwei’,’drei’],columns=list(‘bde’)) In [23]: add2=pd.DataFrame(np.arange(12).reshape(4,3),index=[‘zwei’,’sechs’,’drei’,’seven’],columns=list(‘bef’))In [26]: add1Out[26]:b d eein 0 1 2zwei 3 4 5drei 6 7 8 In [27]: add2Out[27]:b e fzwei 0 1 2sechs 3 4 5drei 6 7 8seven 9 10 11 In [24]: Test_add=add1+add2 In [25]: Test_addOut[25]:b d e fdrei 12.0 NaN 15.0 NaNein NaN NaN NaN NaNsechs NaN NaN NaN NaNseven NaN NaN NaN NaNzwei 3.0 NaN 6.0 NaN这里我们发现如果我们没有把两个数据框的行标签和列完全一一对应或者说一一相等的话，就会出现很多空值。空值的重新赋值比较麻烦，所以尽量不要出现这种情况。接下来我们尝试给这个数据框赋值。也就是说消除空值. 首先我们根据数据缺失的实际状况构造出一个新的数据框： In [9]: Newframe=pd.DataFrame(np.array([[2,45,11,30],[-34,45,89,63],[44,90,36,27]]),index=[‘ein’,’sechs’,’seven’],columns=[‘b’,’d’,’e’,’f’])In [10]: Test_add.add(Newframe,fill_value=0)Out[10]:b d e fdrei 12.0 NaN 15.0 NaNein 2.0 45.0 11.0 30.0sechs -34.0 45.0 89.0 63.0seven 44.0 90.0 36.0 27.0zwei 3.0 NaN 6.0 NaN In [12]: Diframe=Test_add.add(Newframe,fill_value=0) In [13]: DiframeOut[13]:b d e fdrei 12.0 NaN 15.0 NaNein 2.0 45.0 11.0 30.0sechs -34.0 45.0 89.0 63.0seven 44.0 90.0 36.0 27.0zwei 3.0 NaN 6.0 NaNIn [26]: Diframe.loc[‘drei’,’d’]=36In [28]: Diframe.loc[‘zwei’,’d’]=0 In [29]: Diframe.loc[‘drei’,’f’]=0 In [30]: Diframe.loc[‘zwei’,’f’]=0 In [31]: DiframeOut[31]:b d e fdrei 12.0 36.0 15.0 0.0ein 2.0 45.0 11.0 30.0sechs -34.0 45.0 89.0 63.0seven 44.0 90.0 36.0 27.0zwei 3.0 0.0 6.0 0.0 减法乘法和除法In [32]: S1=pd.Series([3,6,9])In [34]: S2=pd.Series([4,3.2,3])S3=S1-S2 S3Out[38]:0 -1.01 2.82 6.0dtype: float64 S4=S1*S2 S4Out[40]:0 12.01 19.22 27.0dtype: float64 S5=S1/S2 S5Out[42]:0 0.7501 1.8752 3.000dtype: float64 In [43]: Frame1=pd.DataFrame([[2,2,1],[1,1,3],[0,2,3]],index=[‘Nr1’,’Nr2’,’Nr3’],columns=[‘mon’,’tue’,’wen’]) In [44]: Frame2=pd.DataFrame([[1,3,1],[0,1,2],[2,0,1]],index=[‘Nr1’,’Nr2’,’Nr3’],columns=[‘mon’,’tue’,’wen’]) In [45]: Frame3=Frame1-Frame2 In [46]: Frame3Out[46]:mon tue wenNr1 1 -1 0Nr2 1 0 1Nr3 -2 2 2 In [47]: Frame4=Frame1*Frame2 In [48]: Frame4Out[48]:mon tue wenNr1 2 6 1Nr2 0 1 6Nr3 0 0 3 In [49]: Frame5=Frame1/Frame2 In [50]: Frame5Out[50]:mon tue wenNr1 2.000000 0.666667 1.0Nr2 inf 1.000000 1.5Nr3 0.000000 inf 3.0 DataFrame 和 Series之间运算 数据框与Serie是之间的加减运算分为沿行和沿列运算的。 匹配列索引，沿行运算 In [1]: import numpy as np In [2]: import pandas as pd In [3]: Frame1=pd.DataFrame([[2,2,1],[1,1,3],[0,2,3]],index=[‘Nr1’,’Nr2’,’Nr3’],columns=[‘mon’,’tue’,’wen’]) In [4]: Frame1Out[4]:mon tue wenNr1 2 2 1Nr2 1 1 3Nr3 0 2 3In [7]: S1=pd.Series([2,2,1],index=[‘mon’,’tue’,’wen’]) In [8]: Frame1-S1Out[8]:mon tue wenNr1 0 0 0Nr2 -1 -1 2Nr3 -2 0 2这里我们要注意到数据框沿着行一行行的被Series减去。这种计算方法在Python里被称作广播。 如果某个索引不出现在在数据框的列索引或Series的索引里就会出现NaN。这对数据分析也是不利的，因此要求我们在设置索引时一定要一一对应。 In [9]: S1=pd.Series([2,2,1],index=[‘mon’,’wen’,’fri’]) In [10]: Frame1-S1Out[10]:fri mon tue wenNr1 NaN 0.0 NaN -1.0Nr2 NaN -1.0 NaN 1.0Nr3 NaN -2.0 NaN 1.0 匹配行索引，沿列运算(广播）In [11]: S2=pd.Series([2,1,0],index=[‘Nr1’,’Nr2’,’Nr3’])In [15]: Frame1.sub(S2,axis=0)Out[15]:mon tue wenNr1 0 0 -1Nr2 0 0 2Nr3 0 2 3(乘法和除法可以按照.Multiply 和.devide方法就行求解！！！） 函数如何应用到数据框 适合数组的方法与函数，也可应用到pandas的数据结构上。 In [17]: frame12=pd.DataFrame(np.arange(12).reshape(4,3),index=[‘r1’,’r2’,’r3’,’r4’],columns=[‘c1’,’c2’,’c3’]) In [18]: frame12Out[18]:c1 c2 c3r1 0 1 2r2 3 4 5r3 6 7 8r4 9 10 11In [20]: frame13=frame12*-1 In [21]: frame13Out[21]:c1 c2 c3r1 0 -1 -2r2 -3 -4 -5r3 -6 -7 -8r4 -9 -10 -11 In [22]: np.abs(frame13)Out[22]:c1 c2 c3r1 0 1 2r2 3 4 5r3 6 7 8r4 9 10 11 In [23]: f=lambda x:x.max()-x.min() In [24]: frame12.apply(f)Out[24]:c1 9c2 9c3 9dtype: int64 In [25]: frame12.apply(f,axis=1)Out[25]:r1 2r2 2r3 2r4 2dtype: int64 .Min（.max）方法和min（max）函数在计算上没有区别，只是方法有更多的选择性。 In [26]: frame12.min()Out[26]:c1 0c2 1c3 2dtype: int32 #不做声明，默认轴为‘0’意思为沿行标签操作 #更多选择性In [37]: frame12.min(0)Out[37]:c1 0c2 1c3 2dtype: int32 In [38]: frame12.min(1)Out[38]:r1 0r2 3r3 6r4 9dtype: int32 In [28]: np.min(frame12)Out[28]:c1 0c2 1c3 2dtype: int32 Lamda 函数应用： In [23]: f=lambda x:x.max()-x.min() In [24]: frame12.apply(f)Out[24]:c1 9c2 9c3 9dtype: int64 In [25]: frame12.apply(f,axis=1)Out[25]:r1 2r2 2r3 2r4 2dtype: int64 #应用apply方法可以得到更为整齐的结果，试比较例1和例2In [6]: frame11=pd.DataFrame(np.arange(12).reshape(3,4),index=[‘r1’,’r2’,’r3’],columns=[‘c1’,’c2’,’c3’,’c4’]) In [13]: def table_extrem (x): …: return pd.Series([x.min(),x.max()],index=[‘min’,’max’]) …: 例子1：In [14]: table_extrem(frame11)Out[14]:min c1 0c2 1c3 2c4 3dtype: int32max c1 8c2 9c3 10c4 11dtype: int32dtype: object例子2.In [15]: frame11.apply(table_extrem)Out[15]:c1 c2 c3 c4min 0 1 2 3max 8 9 10 11 DataFrame 的格式化（十分有用）In [16]: frame11=pd.DataFrame(np.random.randn(3,4),index=[‘r1’,’r2’,’r3’],columns=[‘c1’,’c2’,’c3’,’c4’]) In [17]: frame11Out[17]:c1 c2 c3 c4r1 -2.061714 0.584658 -0.540976 0.090904r2 -0.517271 -0.077818 0.163807 0.418174r3 0.321513 0.769480 2.131075 -0.535560 In [18]: formatierung=lambda x:’%.2f’ % x In [19]: frame11.applymap(formatierung)Out[19]:c1 c2 c3 c4r1 -2.06 0.58 -0.54 0.09r2 -0.52 -0.08 0.16 0.42r3 0.32 0.77 2.13 -0.54 排序和排名Series 排序：In [118]: import numpy as np In [119]: import pandas as pd In [120]: project_1=pd.Series(np.arange(1,5),index=[‘a’,’d’,’e’,’f’]) In [121]: project_1Out[121]:a 1d 2e 3f 4dtype: int32 In [122]: project_1.sort_index()Out[122]:a 1d 2e 3f 4dtype: int32 DataFrame 排序：In [124]: DataFrame_12=pd.DataFrame(np.arange(8).reshape(2,4),index=[‘r1’,’r2’],columns=[‘ted’,’lie’,’qiu’,’send’]) In [125]: DataFrame_12.sort_index()Out[125]:ted lie qiu sendr1 0 1 2 3r2 4 5 6 7 #给行标签排序In [126]: DataFrame_12.sort_index(axis=1)Out[126]:lie qiu send tedr1 1 2 3 0r2 5 6 7 4 #给列标签排序 上面的程序行标签和列标签（也叫列索引）都是按升序进行排列。我们也可以按降序排列数据框。 In [7]: DataFrame_12.sort_index(axis=1,ascending=False)Out[7]:ted send qiu lier1 0 3 2 1r2 4 7 6 5 我们上面都是按标签（索引）进行排序，其实我们还可以用sort_values方法对Series按值进行排序：In [15]: Test_series_1=pd.Series([3,6,2,9,1,0],index=[‘r1’,’r2’,’r3’,’r4’,’r5’,’r6’]) In [16]: Test_series_1Out[16]:r1 3r2 6r3 2r4 9r5 1r6 0dtype: int64 In [17]: Test_series_1.sort_values()Out[17]:r6 0r5 1r3 2r1 3r2 6r4 9dtype: int64 我们也可以对DataFrame进行按值排序 In [18]: Dataffdic=pd.DataFrame({‘liu’:[-1,0,2,-3],’wang’:[3,2,-5,7],’jiang’:[1,0,1,5]}) In [19]: DataffdicOut[19]:jiang liu wang0 1 -1 31 0 0 22 1 2 -53 5 -3 7 In [20]: Dataffdic.sort_index(by=’liu’)#对’liu’列进行排序C:\Users\dongfeng\Anaconda3\lib\site-packages\ipykernel_launcher.py:1: FutureWarning: by argument to sort_index is deprecated, pls use .sort_values(by=…)“””Entry point for launching an IPython kernel.Out[20]:jiang liu wang3 5 -3 70 1 -1 31 0 0 22 1 2 -5 In [21]: #目前sort_index方法对数据框某列的值进行排序已经落后，现在用新的方法sort_values 对列值进行排序。 In [22]: Dataffdic.sort_values(by=’liu’)Out[22]:jiang liu wang3 5 -3 70 1 -1 31 0 0 22 1 2 -5 #由于按‘liu’列进行排序，所以我们看到‘liu’列的数值排序是正确的。由于排序是整行移动，所以在对‘liu’列进行排序时，‘jiang’列和‘wang’列数值顺序一定会随其变得杂乱无章。 下面我们讲讲排名，排名和排序从结果上讲是完全不同的，排序是把一个没有顺序的标签或数据按照符号的自然排序规则（比如如果升序排列字符串“as”在顺序排位上应该在“at”之前）或者数值的太小进行排列。排名是通过打破数据的平级关系，从而产生一种有级别差的顺序。通俗的解释。一组没有排名的数据他们在级别上等级的，然后我们通过某种运算规则，使每个数据变成一个名次数值。 另外，排序可以对数据值本身和标签进行操作。而排名通常对数据本身操作。 In [1]: import numpy as np In [2]: import pandas as pd In [3]: Series_1=pd.Series(np.array([3,8,34,33]),index=[‘r1’,’r2’,’r3’,’r4’]) In [4]: Frame_1=pd.DataFrame(np.array([[3,2,5,0],[7,4,12,13],[1,0,1,-3],[3,6,2,0]]),index=[‘r1’,’r2’,’r3’,’r4’],columns=[‘a’,’b’,’c’,’d’]) Series排名的四种方法1.‘average’法In [6]: Series_1.rank(method=’average’)Out[6]:r1 1.0r2 2.0r3 4.0r4 3.0dtype: float64 2.‘min’法In [8]: Series_1.rank(method=’min’)Out[8]:r1 1.0r2 2.0r3 4.0r4 3.0dtype: float64 3.‘max’法In [7]: Series_1.rank(method=’max’)Out[7]:r1 1.0r2 2.0r3 4.0r4 3.0dtype: float64 4.‘first’法In [9]: Series_1.rank(method=’first’)Out[9]:r1 1.0r2 2.0r3 4.0r4 3.0dtype: float64上面所有的方法都是产生升序排名的结果，他们也可以产生降序的效果，如果我们填加“ascending”方法。 Series_1.rank(ascending=False, method=’first’)Out[19]:r1 4.0r2 3.0r3 1.0r4 2.0dtype: float64 DataFrame的排名在Series排名上，我们没有发现这四种方法的区别，他们一定有区别，只是我们这个Series的排名结果就是这样，没有显现出区别。我们下面通过DataFrame来区别这四种方法。 In [14]: Frame_1.rank(method=’first’)Out[14]:a b c dr1 2.0 2.0 3.0 2.0r2 4.0 3.0 4.0 4.0r3 1.0 1.0 1.0 1.0r4 3.0 4.0 2.0 3.0 In [15]: Frame_1.rank(method=’average’)Out[15]:a b c dr1 2.5 2.0 3.0 2.5r2 4.0 3.0 4.0 4.0r3 1.0 1.0 1.0 1.0r4 2.5 4.0 2.0 2.5 In [16]: Frame_1.rank(method=’min’)Out[16]:a b c dr1 2.0 2.0 3.0 2.0r2 4.0 3.0 4.0 4.0r3 1.0 1.0 1.0 1.0r4 2.0 4.0 2.0 2.0 In [17]: Frame_1.rank(method=’max’)Out[17]:a b c dr1 3.0 2.0 3.0 3.0r2 4.0 3.0 4.0 4.0r3 1.0 1.0 1.0 1.0r4 3.0 4.0 2.0 3.0 我们发现这四种排名的方式是不一样的，尽管是不一样的，但是排名这个目的都十分正确的实现了。仅仅是穿了不同外壳。 上面都是逐行排名，实际上我们还可以逐列排名。 In [20]: Frame_1.rank(axis=1,method=’max’)Out[20]:a b c dr1 3.0 2.0 4.0 1.0r2 2.0 1.0 3.0 4.0r3 4.0 2.0 4.0 1.0r4 3.0 4.0 2.0 1.0 In [21]: Frame_1.rank(axis=1,method=’first’)Out[21]:a b c dr1 3.0 2.0 4.0 1.0r2 2.0 1.0 3.0 4.0r3 3.0 2.0 4.0 1.0r4 3.0 4.0 2.0 1.0 上面就是逐列排名。个人认为‘first’法排名最为规整，且符合我们的习惯思维。 名词解释：逐列排名（或排序）实际上对一行数据进行排名。逐行排名（或排序）实际上对一列数据进行排名。计算机默认逐行排名。 带有重复值的轴索引 对于数据框来说，轴索引就是指的行标签（或者叫行索引）或列标签（或者叫列索引），也就是我们Excel里的行表头和列表头。如下图 很多时候我们要求轴索引是唯一的，但这个要求并不是强制性的，很多时候它可以重复，重复的轴索引对我们数据分析人员来说，不见得是坏事。 例子:In [1]: import numpy as np In [2]: import pandas as pd In [3]: framne_repeat=pd.DataFrame(np.arange(16).reshape(4,4),index=[‘r1’,’r2’,’r3’,’r1’],columns=[‘a’,’b’,’b’,’c’]) In [4]: framne_repeatOut[4]:a b b cr1 0 1 2 3r2 4 5 6 7r3 8 9 10 11r1 12 13 14 15 In [5]: framne_repeat[‘b’]Out[5]:b br1 1 2r2 5 6r3 9 10r1 13 14In [8]: framne_repeat.loc[‘r1’]Out[8]:a b b cr1 0 1 2 3r1 12 13 14 15 In [12]: framne_repeat.loc[(‘r1’)]Out[12]:a b b cr1 0 1 2 3r1 12 13 14 15上面两个例子可以看出framne_repeat.loc[‘r1’]和framne_repeat.loc[(‘r1’)]运行结果是一样的。也就是说加不加括号都无所谓。 带有重复轴索引的SeriesIn [7]: testserie_1=pd.Series(np.array([1,2,3,9,10,3]),index=[‘a’,’b’,’a’,’d’,’e’,’a’]) In [8]: testserie_1Out[8]:a 1b 2a 3d 9e 10a 3dtype: int32 In [9]: testserie_1[‘a’]Out[9]:a 1a 3a 3dtype: int32 可以通过is_unique 函数来判断是否存在重复轴索引。 In [1]: import pandas as pd In [2]: import numpy as np In [3]: testserie_1=pd.Series(np.array([1,2,3,9,10,3]),index=[‘a’,’b’,’a’,’d’,’e’,’a’]) In [4]: testserie_1.is_uniqueOut[4]: False In [5]: #False值说明有重复轴索引 In [6]: #如果对没有重复值的索引进行索引时，返回一个标量。 In [7]: testserie_1[‘b’]Out[7]: 2 Serie 和 Dataframe 的计算 dataFrame_1=pd.DataFrame([range(4),[np.nan,2,3.6,0.9],[1.2,4,6,7],[3.4,7.9,0.4,8]],index=[‘r1’,’r2’,’r3’,’r4’],columns=[‘c1’,’c2’,’c3’,’c4’])dataFrame_1Out[9]: c1 c2 c3 c4r1 0.0 1.0 2.0 3.0r2 NaN 2.0 3.6 0.9r3 1.2 4.0 6.0 7.0r4 3.4 7.9 0.4 8.0dataFrame_1.sum()Out[12]:c1 4.6c2 14.9c3 12.0c4 18.9dtype: float64 dataFrame_1.sum(1)Out[13]:r1 6.0r2 6.5r3 18.2r4 19.7dtype: float64 只要不是整行都是NaN值，python自动排除NAN值然后进行计算。通过skipna可以禁止该功能。 dataFrame_1.sum(axis=1,skipna=False)Out[22]:r1 6.0r2 NaNr3 18.2r4 19.7dtype: float64 dataFrame_1.idxmax()Out[23]:c1 r4c2 r4c3 r3c4 r4dtype: object #求出每列最大值dataFrame_1.idxmax(1)Out[7]:r1 c4r2 c3r3 c4r4 c4dtype: object #求出每行最大值dataFrame_1.idxmin(1)Out[9]:r1 c1r2 c4r3 c1r4 c3dtype: object求出每行最小值，空值不参与比较。 dataFrame_1.idxmin(axis=1,skipna=False)Out[10]:r1 c1r2 NaNr3 c1r4 c3dtype: object #修改skipna的默认值”True”,可以修改NaN值自动忽略功能。 #累计加 按列累计加，遇到NaN值自动忽略，不参与运算。dataFrame_1.cumsum()Out[8]: c1 c2 c3 c4r1 0.0 1.0 2.0 3.0r2 NaN 3.0 5.6 3.9r3 1.2 7.0 11.6 10.9r4 4.6 14.9 12.0 18.9 dataFrame_1.cumsum(1)Out[10]: c1 c2 c3 c4r1 0.0 1.0 3.0 6.0r2 NaN 2.0 5.6 6.5r3 1.2 5.2 11.2 18.2r4 3.4 11.3 11.7 19.7 dataFrame_1.describe()Out[12]: c1 c2 c3 c4count 3.000000 4.000 4.000000 4.00000mean 1.533333 3.725 3.000000 4.72500std 1.724336 3.050 2.388863 3.34203min 0.000000 1.000 0.400000 0.9000025% 0.600000 1.750 1.600000 2.4750050% 1.200000 3.000 2.800000 5.0000075% 2.300000 4.975 4.200000 7.25000max 3.400000 7.900 6.000000 8.00000 #上面的分位数我们后面会有介绍 去重，值计数以及值资格判断 series_3=pd.Series([‘c’,’d’,’we’,’a’,’c’,’d’,’e’,’f’,’we’])series_3.unique()Out[25]: array([‘c’, ‘d’, ‘we’, ‘a’, ‘e’, ‘f’], dtype=object) #通过uique方法的调用，我们立即可以去除Series中的重复的项目serie_unique=series_3.unique()serie_unique.sort()serie_uniqueOut[40]: array([‘a’, ‘c’, ‘d’, ‘e’, ‘f’, ‘we’], dtype=object) #通常去重后要进行排序 通过value_counts()方法对Series中的值进行计数series_3.value_counts()Out[43]:we 2c 2d 2a 1f 1e 1dtype: int64 我们可以通过这种方法求出指定元素出现的次数。 serie_nr=series_3.value_counts()serie_nr[‘we’]Out[46]: 2 上面我们看到都是value_counts()作为方法在应用，其实它也可以作为函数（或者叫Pandas的方法）单独应用。 pd.value_counts(series_3.values)Out[47]:we 2c 2d 2a 1f 1e 1dtype: int64也可以不把计算内容做降序处理：pd.value_counts(series_3.values,sort=False)Out[49]:e 1d 2f 1a 1c 2we 2dtype: int64 value_counts()作为单独函数使用时，是一种超级牛的函数，他可以使用到任何序列和数组。 看下面例子： list_1=[1,3,4,5,2,6,8,4,3,8,9] pd.value_counts(list_1)Out[51]:8 24 23 29 16 15 12 11 1dtype: int64 tuple_1=1,2,4,3,2,5,6,3,9 pd.value_counts(tuple_1)Out[54]:3 22 29 16 15 14 11 1dtype: int64 array_1=[1,2,3,4,52,3,2] pd.value_counts(array_1)Out[56]:3 22 24 11 152 1dtype: int64series_3Out[60]:0 c1 d2 we3 a4 c5 d6 e7 f8 wedtype: object series_3.isin([‘a’,’c’])Out[61]:0 True1 False2 False3 True4 True5 False6 False7 False8 Falsedtype: bool series_3.isin([‘a’])Out[63]:0 False1 False2 False3 True4 False5 False6 False7 False8 Falsedtype: bool #这是一个布尔Series，我们可以把他作为角码选出我们指定值在Series里的所有信息。boolmatrix_2=series_3.isin([‘a’,’c’]) series_3[boolmatrix_2]Out[68]:0 c3 a4 cdtype: object 我们也可以把value_counts()用到数据框，会得到一个你预料不到的结果。尽管这个结果很奇怪，但他却是巧妙的导出频数分布图。dataFrame_1Out[69]: c1 c2 c3 c4r1 0.0 1.0 2.0 3.0r2 NaN 2.0 3.6 0.9r3 1.2 4.0 6.0 7.0r4 3.4 7.9 0.4 8.0 dataFrame_1.fillna(0)Out[71]: c1 c2 c3 c4r1 0.0 1.0 2.0 3.0r2 0.0 2.0 3.6 0.9r3 1.2 4.0 6.0 7.0r4 3.4 7.9 0.4 8.0 dataFrame_2=dataFrame_1.fillna(0) result=dataFrame_2.apply(pd.value_counts).fillna(0) resultOut[74]: c1 c2 c3 c40.0 2.0 0.0 0.0 0.00.4 0.0 0.0 1.0 0.00.9 0.0 0.0 0.0 1.01.0 0.0 1.0 0.0 0.01.2 1.0 0.0 0.0 0.02.0 0.0 1.0 1.0 0.03.0 0.0 0.0 0.0 1.03.4 1.0 0.0 0.0 0.03.6 0.0 0.0 1.0 0.04.0 0.0 1.0 0.0 0.06.0 0.0 0.0 1.0 0.07.0 0.0 0.0 0.0 1.07.9 0.0 1.0 0.0 0.08.0 0.0 0.0 0.0 1.0 NAN数据处理， 在数据处理工作中，我们经常遇到缺失数据，它们的处理往往很费时间。Pandas设计之初，就已经考虑到这种情况。可以说，快速轻松地处理缺失数据是pandas最大优点之一。 #NaN值和None值都可以被当做空值处理：series_1=pd.Series([1,np.nan,3,None,2.3,6]) series_1Out[5]:0 1.01 NaN2 3.03 NaN4 2.35 6.0dtype: float64 缺失值得快速补充法 frame_123=pd.DataFrame([[1,np.nan,np.nan,2,3],[1,2,3,None,8],[6,2.3,8,None,3],[2,3,2,3,2],[3.2,8.9,3,4.5,3],[2,3,0.45,None,8]]) frame_123Out[8]: 0 1 2 3 40 1.0 NaN NaN 2.0 31 1.0 2.0 3.00 NaN 82 6.0 2.3 8.00 NaN 33 2.0 3.0 2.00 3.0 24 3.2 8.9 3.00 4.5 35 2.0 3.0 0.45 NaN 8 pd.isnull(frame_123)Out[9]: 0 1 2 3 40 False True True False False1 False False False True False2 False False False True False3 False False False False False4 False False False False False5 False False False True False boolmatrix_1=pd.isnull(frame_123)frame_123[boolmatrix_1]=0frame_123Out[13]: 0 1 2 3 40 1.0 0.0 0.00 2.0 31 1.0 2.0 3.00 0.0 82 6.0 2.3 8.00 0.0 33 2.0 3.0 2.00 3.0 24 3.2 8.9 3.00 4.5 35 2.0 3.0 0.45 0.0 8 Dropna过滤NaN数据series_23=pd.Series([1,4,3,np.nan,4.5,None,3.4,4.5,6,np.nan,4,5,3]) series_23Out[20]:0 1.01 4.02 3.03 NaN4 4.55 NaN6 3.47 4.58 6.09 NaN10 4.011 5.012 3.0dtype: float64 series_23.dropna()Out[21]:0 1.01 4.02 3.04 4.56 3.47 4.58 6.010 4.011 5.012 3.0dtype: float64 也可以通过Notnull series_23[series_23.notnull()]Out[22]:0 1.01 4.02 3.04 4.56 3.47 4.58 6.010 4.011 5.012 3.0dtype: float64 Dropna 用于DataFrame和用于Series稍有不同。用于DataFrame，含有NaN值的行会被删去。 frame_123=pd.DataFrame([[1,np.nan,np.nan,2,3],[1,2,3,None,8],[6,2.3,8,None,3],[2,3,2,3,2],[3.2,8.9,3,4.5,3],[2,3,0.45,None,8]]) frame_123Out[28]: 0 1 2 3 40 1.0 NaN NaN 2.0 31 1.0 2.0 3.00 NaN 82 6.0 2.3 8.00 NaN 33 2.0 3.0 2.00 3.0 24 3.2 8.9 3.00 4.5 35 2.0 3.0 0.45 NaN 8 frame_123.dropna()Out[27]: 0 1 2 3 43 2.0 3.0 2.0 3.0 24 3.2 8.9 3.0 4.5 3 使用how=’all’只会消除全为NaN的行：matrix_2=frame_123.dropna(how=’all’) matrix_2Out[32]: 0 1 2 3 40 1.0 NaN NaN 2.0 31 1.0 2.0 3.00 NaN 82 6.0 2.3 8.00 NaN 33 2.0 3.0 2.00 3.0 24 3.2 8.9 3.00 4.5 35 2.0 3.0 0.45 NaN 8 列行的添加和丢弃frame_123Out[47]: 0 1 2 3 40 1.0 NaN NaN 2.0 31 1.0 2.0 3.00 NaN 82 6.0 2.3 8.00 NaN 33 2.0 3.0 2.00 3.0 24 3.2 8.9 3.00 4.5 35 2.0 3.0 0.45 NaN 8 frame_123[5]=[1,3,56,32,np.nan,0.7] frame_123Out[49]: 0 1 2 3 4 50 1.0 NaN NaN 2.0 3 1.01 1.0 2.0 3.00 NaN 8 3.02 6.0 2.3 8.00 NaN 3 56.03 2.0 3.0 2.00 3.0 2 32.04 3.2 8.9 3.00 4.5 3 NaN5 2.0 3.0 0.45 NaN 8 0.7 #列的添加frame_123Out[47]: 0 1 2 3 40 1.0 NaN NaN 2.0 31 1.0 2.0 3.00 NaN 82 6.0 2.3 8.00 NaN 33 2.0 3.0 2.00 3.0 24 3.2 8.9 3.00 4.5 35 2.0 3.0 0.45 NaN 8 frame_123.loc[6,:]=[np.nan,2,2.4,5,6] frame_123Out[53]: 0 1 2 3 40 1.0 NaN NaN 2.0 3.01 1.0 2.0 3.00 NaN 8.02 6.0 2.3 8.00 NaN 3.03 2.0 3.0 2.00 3.0 2.04 3.2 8.9 3.00 4.5 3.05 2.0 3.0 0.45 NaN 8.06 NaN 2.0 2.40 5.0 6.0 #行的添加frame_123Out[55]: 0 1 2 3 40 1.0 NaN NaN 2.0 31 1.0 2.0 3.00 NaN 82 6.0 2.3 8.00 NaN 33 2.0 3.0 2.00 3.0 24 3.2 8.9 3.00 4.5 35 2.0 3.0 0.45 NaN 8 frame_123[5]=np.nan frame_123Out[57]: 0 1 2 3 4 50 1.0 NaN NaN 2.0 3 NaN1 1.0 2.0 3.00 NaN 8 NaN2 6.0 2.3 8.00 NaN 3 NaN3 2.0 3.0 2.00 3.0 2 NaN4 3.2 8.9 3.00 4.5 3 NaN5 2.0 3.0 0.45 NaN 8 NaN frame_123[5]=np.nan frame_123Out[57]: 0 1 2 3 4 50 1.0 NaN NaN 2.0 3 NaN1 1.0 2.0 3.00 NaN 8 NaN2 6.0 2.3 8.00 NaN 3 NaN3 2.0 3.0 2.00 3.0 2 NaN4 3.2 8.9 3.00 4.5 3 NaN5 2.0 3.0 0.45 NaN 8 NaN frame_123.dropna(axis=1,how=’all’)Out[60]: 0 1 2 3 40 1.0 NaN NaN 2.0 31 1.0 2.0 3.00 NaN 82 6.0 2.3 8.00 NaN 33 2.0 3.0 2.00 3.0 24 3.2 8.9 3.00 4.5 35 2.0 3.0 0.45 NaN 8 frame_1=pd.DataFrame(np.random.randn(7,4)) frame_1Out[65]: 0 1 2 30 -0.668146 0.034772 0.482339 1.4441381 -1.167959 -0.703595 0.641404 -1.1007712 -1.657068 -2.038607 0.141572 2.5258313 -1.869547 -0.291923 0.275511 -0.4597394 -0.287525 -0.966589 2.145633 0.7037355 0.499207 -0.385792 -1.192131 -1.6798056 -0.529885 2.053872 0.970785 -0.733382 frame_1.loc[:4,1]=np.nan;frame_1.loc[:2,2]=np.nan frame_1Out[67]: 0 1 2 30 -0.668146 NaN NaN 1.4441381 -1.167959 NaN NaN -1.1007712 -1.657068 NaN NaN 2.5258313 -1.869547 NaN 0.275511 -0.4597394 -0.287525 NaN 2.145633 0.7037355 0.499207 -0.385792 -1.192131 -1.6798056 -0.529885 2.053872 0.970785 -0.733382 [:4,1]注意这里切片是切到“4”，不是到“3” frame_1.dropna()Out[72]: 0 1 2 35 0.499207 -0.385792 -1.192131 -1.6798056 -0.529885 2.053872 0.970785 -0.733382 #把不想看的数据行部分赋值NaN，然后在通过dropna语句把含有NaN的行删除掉。 缺失数据的补充： 通过fillna指令我们可以更快地补充缺失数据。这与通过isnull把数据框转化为bool数据框，然后把bool数据框作为索引来赋值‘0’要快的多。（前面已经讲过） 但是这个指令只能赋一个值。通过字典，我们可以给每一列赋不同的值。 frame_1.fillna({1:5,2:4})Out[7]: 0 1 2 30 1.651360 5.000000 4.000000 -0.6009971 -0.020649 5.000000 4.000000 -0.5298432 -0.850476 5.000000 4.000000 -0.8938133 0.812226 5.000000 0.614715 0.1125094 0.074320 5.000000 1.145727 0.1687185 -0.516371 -1.022050 0.774645 0.7058476 0.617606 0.881843 -0.619870 -1.527961 _=frame_1.fillna(0,inplace=True) frame_1Out[13]: 0 1 2 30 1.651360 0.000000 0.000000 -0.6009971 -0.020649 0.000000 0.000000 -0.5298432 -0.850476 0.000000 0.000000 -0.8938133 0.812226 0.000000 0.614715 0.1125094 0.074320 0.000000 1.145727 0.1687185 -0.516371 -1.022050 0.774645 0.7058476 0.617606 0.881843 -0.619870 -1.527961 #可以直接对数据框修改，也就是说可以直接改变源数据，不产生副本。 frame_1=pd.DataFrame(np.random.randn(7,4));frame_1.loc[:4,1]=np.nan;frame_1.loc[:2,2]=np.nan frame_1Out[107]: 0 1 2 30 -0.748195 NaN NaN -0.5392461 -2.360077 NaN NaN -0.0610782 -1.743350 NaN NaN -0.2330603 -2.100371 NaN 0.187509 0.8933214 -0.477253 NaN 1.876733 0.2083895 0.770411 -2.262797 0.321646 -2.4902156 1.265209 -0.453140 0.045817 0.951077 frame_1.fillna({1:pd.Series([2,3,7,9,0],index=[0,1,2,3,4]),2:pd.Series([1,9,8],index=[0,1,2])})Out[108]: 0 1 2 30 -0.748195 2.000000 1.000000 -0.5392461 -2.360077 3.000000 9.000000 -0.0610782 -1.743350 7.000000 8.000000 -0.2330603 -2.100371 9.000000 0.187509 0.8933214 -0.477253 0.000000 1.876733 0.2083895 0.770411 -2.262797 0.321646 -2.4902156 1.265209 -0.453140 0.045817 0.951077 层次化索引： series_1=pd.Series(np.random.randn(12),index=[[‘r1’,’r1’,’r1’,’t1’,’t1’,’s1’,’s1’,’s1’,’s1’,’p1’,’q1’,’q1’],[9,8,7,6,5,4,3,2,1,0,12,11]]) series_1Out[8]:r1 9 1.536272 8 0.378965 7 -0.292986t1 6 -0.254990 5 0.765191s1 4 2.204928 3 0.662662 2 -0.595029 1 -1.905753p1 0 -1.029524q1 12 -1.038748 11 -1.192589dtype: float64 series_1.indexOut[9]:MultiIndex(levels=[[‘p1’, ‘q1’, ‘r1’, ‘s1’, ‘t1’], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12]], labels=[[2, 2, 2, 4, 4, 3, 3, 3, 3, 0, 1, 1], [9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 11, 10]]) 可以直接索引外层标签，也就是第一层标签。series_1[‘s1’]Out[6]:4 -0.0123063 0.8006792 -0.8629861 0.048458dtype: float64 也可以做切片。但是外层标签必须是1.经过排序的，必须！2.每个外层标签的第一个字母不能不一致，要大写，全大写，要小写全小写。 Series_123=pd.Series(np.random.randint(10),index=[[‘apple’,’apple’,’apple’,’apricot’,’apricot’,’apricot’,’banana’,’banana’,’blackberry’,’cherry’],[3,2,5,6,0,4,9,11,21,17]]) Series_123[‘apple’]Out[4]:3 92 95 9dtype: int64 Series_123[‘apple’:’banana’]Out[64]:apple 3 5 2 5 5 5apricot 6 5 0 5 4 5banana 9 5 11 5dtype: int64 Series_123[‘apple’:’apricot’]Out[65]:apple 3 5 2 5 5 5apricot 6 5 0 5 4 5dtype: int64 Series_123[‘apple’:’blackberry’]Out[67]:apple 3 5 2 5 5 5apricot 6 5 0 5 4 5banana 9 5 11 5blackberry 21 5dtype: int64 Series_67=pd.Series(np.random.randint(10),index=[[‘apple’,’apple’,’apple’,’apricot’,’apricot’,’apricot’,’banana’,’banana’,’blackberry’,’cherry’],[3,2,5,6,5,4,9,5,21,17]]) Series_67Out[7]:apple 3 4 2 4 5 4apricot 6 4 5 4 4 4banana 9 4 5 4blackberry 21 4cherry 17 4dtype: int64 Series_67[:,5]Out[8]:apple 4apricot 4banana 4dtype: int64 #对于Series，中括号的第一个位置是代表外层标签，第二位置代表内存标签 例子： Series_67[‘apple’,5]Out[10]: 4 带有层次化索引的Series可以立马转化为数据框，反之，数据框也可以转化为带有层次化索引的SeriesSeries_67.unstack()Out[20]: 2 3 4 5 6 9 17 21apple 4.0 4.0 NaN 4.0 NaN NaN NaN NaNapricot NaN NaN 4.0 4.0 4.0 NaN NaN NaNbanana NaN NaN NaN 4.0 NaN 4.0 NaN NaNblackberry NaN NaN NaN NaN NaN NaN NaN 4.0cherry NaN NaN NaN NaN NaN NaN 4.0 NaN #Series的内层索引变化为列标签，外层索引转化为行标签,并且重复索引删除，索引保持唯一性。 Series_67.unstack(0)Out[22]: apple apricot banana blackberry cherry2 4.0 NaN NaN NaN NaN3 4.0 NaN NaN NaN NaN4 NaN 4.0 NaN NaN NaN5 4.0 4.0 4.0 NaN NaN6 NaN 4.0 NaN NaN NaN9 NaN NaN 4.0 NaN NaN17 NaN NaN NaN NaN 4.021 NaN NaN NaN 4.0 NaN #axis=0时，Series的内层索引变化为行标签，外层索引转化为列标签 Series_67.unstack(1)Out[24]: 2 3 4 5 6 9 17 21apple 4.0 4.0 NaN 4.0 NaN NaN NaN NaNapricot NaN NaN 4.0 4.0 4.0 NaN NaN NaNbanana NaN NaN NaN 4.0 NaN 4.0 NaN NaNblackberry NaN NaN NaN NaN NaN NaN NaN 4.0cherry NaN NaN NaN NaN NaN NaN 4.0 NaN Series_67.unstack().stack()Out[29]:apple 2 4.0 3 4.0 5 4.0apricot 4 4.0 5 4.0 6 4.0banana 5 4.0 9 4.0blackberry 21 4.0cherry 17 4.0dtype: float64 Gamedata=pd.DataFrame(np.array([[31,27,24,60],[26,28,13,29],[27,17,12,27],[29,9,5,18],[31,12,4,70],[45,11,12,12]]),index=[[‘Morning’,’Morning’,’Morning’,’Afternoon’,’Afternoon’,’Afternoon’],[1,2,3,1,2,3]],columns=[[‘Junior’,’Junior’,’Youth’,’Youth’],[‘Zhang’,’Wang’,’Li’,’Zhao’]]) GamedataOut[35]: Junior Youth Zhang Wang Li ZhaoMorning 1 31 27 24 60 2 26 28 13 29 3 27 17 12 27Afternoon 1 29 9 5 18 2 31 12 4 70 3 45 11 12 12 Gamedata=pd.DataFrame(np.array([[31,27,24,60],[26,28,13,29],[27,17,12,27],[29,9,5,18],[31,12,4,70],[45,11,12,12]]),index=[[‘A_Morning’,’A_Morning’,’A_Morning’,’B_Afternoon’,’B_Afternoon’,’B_Afternoon’],[1,2,3,1,2,3]],columns=[[‘Junior’,’Junior’,’Youth’,’Youth’],[‘Zhang’,’Wang’,’Li’,’Zhao’]]) Gamedata.index.names=[‘time’,’sequence’] #因为行有双层索引，因此需要两个名字 #列也是如此，也是两个名字 Gamedata.columns.names=[‘age’,’name’] GamedataOut[50]:age Junior Youthname Zhang Wang Li Zhaotime sequenceA_Morning 1 31 27 24 60 2 26 28 13 29 3 27 17 12 27B_Afternoon 1 29 9 5 18 2 31 12 4 70 3 45 11 12 12 Gamedata[‘Junior’]Out[51]:name Zhang Wangtime sequenceA_Morning 1 31 27 2 26 28 3 27 17B_Afternoon 1 29 9 2 31 12 3 45 11 Gamedata.loc[‘Morning’]Out[31]:Age Junior YouthName Zhang Wang Li Zhaoseries_nr1 31 27 24 602 26 28 13 293 27 17 12 27 Gamedata.loc[‘Morning’,2]Out[32]:Age NameJunior Zhang 26 Wang 28Youth Li 13 Zhao 29Name: (Morning, 2), dtype: int32 多重标签的顺序互换： Gamedata.swaplevel(‘sequence’,’time’)Out[52]:age Junior Youthname Zhang Wang Li Zhaosequence time1 A_Morning 31 27 24 602 A_Morning 26 28 13 293 A_Morning 27 17 12 271 B_Afternoon 29 9 5 182 B_Afternoon 31 12 4 703 B_Afternoon 45 11 12 12 Gamedata.swaplevel(‘age’,’name’,axis=1)Out[60]:name Zhang Wang Li Zhaoage Junior Junior Youth Youthtime sequenceA_Morning 1 31 27 24 60 2 26 28 13 29 3 27 17 12 27B_Afternoon 1 29 9 5 18 2 31 12 4 70 3 45 11 12 12 Gamedata.sortlevel(1)C:\Users\dongfeng\Anaconda3\lib\site-packages\ipykernel_launcher.py:1: FutureWarning: sortlevel is deprecated, use sort_index(level= …) “””Entry point for launching an IPython kernel.Out[55]:age Junior Youthname Zhang Wang Li Zhaotime sequenceA_Morning 1 31 27 24 60B_Afternoon 1 29 9 5 18A_Morning 2 26 28 13 29B_Afternoon 2 31 12 4 70A_Morning 3 27 17 12 27B_Afternoon 3 45 11 12 12 #sortlevel则根据单个级别的值进行排序，上例根据sequence列数据。 Gamedata.swaplevel(‘age’,’name’,axis=1).sortlevel(0)C:\Users\dongfeng\Anaconda3\lib\site-packages\ipykernel_launcher.py:1: FutureWarning: sortlevel is deprecated, use sort_index(level= …) “””Entry point for launching an IPython kernel.Out[61]:name Zhang Wang Li Zhaoage Junior Junior Youth Youthtime sequenceA_Morning 1 31 27 24 60 2 26 28 13 29 3 27 17 12 27B_Afternoon 1 29 9 5 18 2 31 12 4 70 3 45 11 12 12 #交换两层列标签，然后对第二行列标签排序GamedataOut[62]:age Junior Youthname Zhang Wang Li Zhaotime sequenceA_Morning 1 31 27 24 60 2 26 28 13 29 3 27 17 12 27B_Afternoon 1 29 9 5 18 2 31 12 4 70 3 45 11 12 12 Gamedata.swaplevel(‘age’,’name’,axis=1).sortlevel(1)C:\Users\dongfeng\Anaconda3\lib\site-packages\ipykernel_launcher.py:1: FutureWarning: sortlevel is deprecated, use sort_index(level= …) “””Entry point for launching an IPython kernel.Out[63]:name Zhang Wang Li Zhaoage Junior Junior Youth Youthtime sequenceA_Morning 1 31 27 24 60B_Afternoon 1 29 9 5 18A_Morning 2 26 28 13 29B_Afternoon 2 31 12 4 70A_Morning 3 27 17 12 27B_Afternoon 3 45 11 12 12 #交换两层行标签，然后对第二列行标签排序 根据级别计算： GamedataOut[62]:age Junior Youthname Zhang Wang Li Zhaotime sequenceA_Morning 1 31 27 24 60 2 26 28 13 29 3 27 17 12 27B_Afternoon 1 29 9 5 18 2 31 12 4 70 3 45 11 12 12 Gamedata.sum(level=’sequence’)Out[64]:age Junior Youthname Zhang Wang Li Zhaosequence1 60 36 29 782 57 40 17 993 72 28 24 39 #每个级别包含两个元素，（因为此级别标签分别重复两次。）Gamedata.sum(level=’name’,axis=1)Out[68]:name Li Wang Zhang Zhaotime sequenceA_Morning 1 24 27 31 60 2 13 28 26 29 3 12 17 27 27B_Afternoon 1 5 9 29 18 2 4 12 31 70 3 12 11 45 12 每个级别只包含一个元素，因此无法相加，只能排序。 Gamedata.sum(level=’age’,axis=1)Out[69]:age Junior Youthtime sequenceA_Morning 1 58 84 2 54 42 3 44 39B_Afternoon 1 38 23 2 43 74 3 56 24 #每个级别包含两个元素，分别两两相加。 Gamedata.sum(level=’time’)Out[71]:age Junior Youthname Zhang Wang Li ZhaotimeA_Morning 84 72 49 116B_Afternoon 105 32 21 100 每个级别包含三个元素，分别相加即可 把数据框的列转化为行索引： frame_1123=pd.DataFrame({‘a’:range(4),’b’:range(4,0,-1),’c’:[‘one’,’one’,’two’,’two’],’d’:[0,1,2,3]}) frame_1123Out[73]: a b c d0 0 4 one 01 1 3 one 12 2 2 two 23 3 1 two 3 frame_1224=frame_1123.set_index([‘c’,’d’]) frame_1224Out[76]: a bc done 0 0 4 1 1 3two 2 2 2 3 3 1 #我们发现列从数据框中消失，变成了双重行索引。 然而而这些列也可以不消失。 frame_1123.set_index([‘c’,’d’],drop=False)Out[79]: a b c dc done 0 0 4 one 0 1 1 3 one 1two 2 2 2 two 23 3 1 two 3 我们可以把双重索引再转化到数据框的数据区域。 frame_1123.reset_index()Out[82]: index a b c d0 0 0 4 one 01 1 1 3 one 12 2 2 2 two 23 3 3 1 two 3 面板数据： Panel实质上是一个三维的数据框，3维说明其有三个轴，每个轴的含义如下： items: 0轴， 每个项目对应其中的一个DataFramemajor_axis（主轴）: 1轴，它是每个DataFrame的indexminor_axis（副轴）: 2轴，它是每个DataFrame的column创建panel：A.数组创建,三维数组 Dataframe_3 = pd.Panel(np.random.randn(2,5,4),items=[‘Item1’,’Item2’],major_axis=pd.date_range(‘2000-01-01’,’2000-01-05’),minor_axis=[‘A’,’B’,’C’,’D’]) Dataframe_4=pd.Panel(np.array([[[1,4,67,45],[34,56,2,0],[78,90,3,4],[4,23,67,5],[34,89,67,1]],[[67,90,64,7],[789,345,6,2],[33,89,467,8],[43,93,2,8],[33,74,89,6]]]),items=[‘Item1’,’Item2’],major_axis=pd.date_range(‘2000-01-01’,’2000-01-05’),minor_axis=[‘A’,’B’,’C’,’D’]) Dataframe_3Out[107]: Dimensions: 2 (items) x 5 (major_axis) x 4 (minor_axis)Items axis: Item1 to Item2Major_axis axis: 2000-01-01 00:00:00 to 2000-01-05 00:00:00Minor_axis axis: A to D Dataframe_4Out[108]: Dimensions: 2 (items) x 5 (major_axis) x 4 (minor_axis)Items axis: Item1 to Item2Major_axis axis: 2000-01-01 00:00:00 to 2000-01-05 00:00:00Minor_axis axis: A to D B.通过字典创建data = {‘Item1’ : pd.DataFrame(np.random.randn(4, 3),index=pd.date_range(‘2017-09-05’,’2017-09-08’),columns=[‘X’,’D’,’F’]),’Item2’ : pd.DataFrame(np.random.randn(4, 2),index=pd.date_range(‘2017-09-05’,’2017-09-08’),columns=[‘X’,’D’])} panel_1=pd.Panel(data) panel_1Out[115]: Dimensions: 2 (items) x 4 (major_axis) x 3 (minor_axis)Items axis: Item1 to Item2Major_axis axis: 2017-09-05 00:00:00 to 2017-09-08 00:00:00Minor_axis axis: D to X C.通过数据框创建 midx = pd.MultiIndex(levels=[[‘one’, ‘two’], [‘x’,’y’]], labels=[[1,1,0,0],[1,0,1,0]])midxOut[117]:MultiIndex(levels=[[‘one’, ‘two’], [‘x’, ‘y’]], labels=[[1, 1, 0, 0], [1, 0, 1, 0]]) 第一层行标是[‘one’, ‘two’]，labels[1, 1, 0, 0]；第二层行标是 [‘x’,’y’]，labels[1, 0, 1, 0]，注意’one’对应‘0’，’two’对应‘1’ df = pd.DataFrame({‘A’:[1,2,3,4],’B’:[5,6,7,8]},index=midx) dfOut[119]: A Btwo y 1 5 x 2 6one y 3 7 x 4 8df.to_panel() 面板操作A.选取Dataframe_3[‘Item1’]Out[124]: Dataframe_3 A B C D2000-01-01 0.836830 -1.856472 0.345340 0.5767712000-01-02 -1.529758 -1.646630 0.635996 -0.3374082000-01-03 0.451765 0.156648 -1.225328 -0.1776412000-01-04 1.123645 0.364549 0.684536 1.5588842000-01-05 -0.082263 1.472391 -0.379373 2.410845 Dataframe_3.major_axisOut[128]:DatetimeIndex([‘2000-01-01’, ‘2000-01-02’, ‘2000-01-03’, ‘2000-01-04’, ‘2000-01-05’], dtype=’datetime64[ns]’, freq=’D’) Dataframe_3[‘Item1’]Out[131]: A B C D2000-01-01 0.836830 -1.856472 0.345340 0.5767712000-01-02 -1.529758 -1.646630 0.635996 -0.3374082000-01-03 0.451765 0.156648 -1.225328 -0.1776412000-01-04 1.123645 0.364549 0.684536 1.5588842000-01-05 -0.082263 1.472391 -0.379373 2.410845 Dataframe_3.major_xs(Dataframe_3.major_axis[2]) Dataframe_3.minor_axisOut[134]: Index([‘A’, ‘B’, ‘C’, ‘D’], dtype=’object’) Dataframe_3.minor_xs(Dataframe_3.minor_axis[3])Out[137]: Item1 Item22000-01-01 0.576771 1.3871762000-01-02 -0.337408 -0.1593942000-01-03 -0.177641 -1.6220842000-01-04 1.558884 0.3938002000-01-05 2.410845 -0.249931 转置： Dataframe_3=pd.Panel(np.random.randn(2,5,4),items=[‘Item1’,’Item2’],major_axis=pd.date_range(‘2000-01-01’,’2000-01-05’),minor_axis=[‘A’,’B’,’C’,’D’]) Dataframe_3Out[5]: Dimensions: 2 (items) x 5 (major_axis) x 4 (minor_axis)Items axis: Item1 to Item2Major_axis axis: 2000-01-01 00:00:00 to 2000-01-05 00:00:00Minor_axis axis: A to D Dataframe_3[‘Item1’,’2000-01-03’,’B’]Out[9]: 2.3125077493899666 #转置前的元素定位查询Dataframe_3.transpose(0,2,1)Out[10]: Dimensions: 2 (items) x 4 (major_axis) x 5 (minor_axis)Items axis: Item1 to Item2Major_axis axis: A to DMinor_axis axis: 2000-01-01 00:00:00 to 2000-01-05 00:00:00 Data_transpose=Dataframe_3.transpose(0,2,1) Data_transpose[‘Item1’,’B’,’2000-01-03’]Out[12]: 2.3125077493899666 #转置后的同一元素查询，两者查询结果相同，说明转置成功。 面板转化为分层索引数据框Dataframe_3.to_frame()Out[16]: Item1 Item2major minor2000-01-01 A 1.345174 -1.719012 B -1.075240 1.258681 C 0.366470 -0.004046 D -0.149155 -0.4052952000-01-02 A 0.005256 0.204166 B -0.032269 -0.667655 C 1.825649 1.050139 D -1.505179 0.5348742000-01-03 A 0.876495 -0.414982 B 2.312508 -0.731893 C -0.129701 -1.470191 D -0.637856 -0.0831882000-01-04 A -0.287276 0.633456 B -0.623472 -0.229308 C 0.530747 -0.896306 D 0.229674 0.2880642000-01-05 A 0.163915 -1.836235 B 0.618230 1.353955 C 0.260995 0.808007 D 1.673127 -0.275785切片： Dataframe_3[‘Item2’,:,’B’]Out[18]:2000-01-01 1.2586812000-01-02 -0.6676552000-01-03 -0.7318932000-01-04 -0.2293082000-01-05 1.353955Freq: D, Name: B, dtype: float64 Dataframe_3[‘Item2’,’2000-01-02’:’2000-01-04’,’B’]Out[19]:2000-01-02 -0.6676552000-01-03 -0.7318932000-01-04 -0.229308Freq: D, Name: B, dtype: float64 Dataframe_3[‘Item2’,’2000-01-02’]Out[20]:A 0.204166B -0.667655C 1.050139D 0.534874Name: 2000-01-02 00:00:00, dtype: float64 Dataframe_3[‘Item2’,’2000-01-02’:’2000-01-04’]Out[21]: A B C D2000-01-02 0.204166 -0.667655 1.050139 0.5348742000-01-03 -0.414982 -0.731893 -1.470191 -0.0831882000-01-04 0.633456 -0.229308 -0.896306 0.288064 Dataframe_3[‘Item1’:’Item2’,’2000-01-02’:’2000-01-04’,’C’]Out[22]: Item1 Item22000-01-02 1.825649 1.0501392000-01-03 -0.129701 -1.4701912000-01-04 0.530747 -0.896306 Dataframe_3[:,’2000-01-02’:’2000-01-04’,’C’:’D’]Out[28]: Dimensions: 2 (items) x 3 (major_axis) x 2 (minor_axis)Items axis: Item1 to Item2Major_axis axis: 2000-01-02 00:00:00 to 2000-01-04 00:00:00Minor_axis axis: C to D #切出新面板。 数据加载，存储，清理，转换，合并与重塑 数据的加载与存储Python在文本文件的加载与存储方面极其方便，这是它成为深受大家喜爱语言的原因之一。 Pandas 提供了一些直接将表格文件读取为DataFrame对象的函数下面我们一一讲解：A, read_csvimport pandas as pd!type example_1.txt系统找不到指定的文件。我们做一下简单修改!type Desktop\example_1.txtLiu,Zhang,Wang,Li,Class23,34,78,32,’primary’77,32,89,66,’intermediate’99,34,78,66,’senior’66,34,6,33,’intermediate’也可以直接读取‘CSV’文件!type Desktop\example_1.csvLiu,Zhang,Wang,Li,Class23,34,78,32,’primary’77,32,89,66,’intermediate’99,34,78,66,’senior’66,34,6,33,’intermediate’直接读取的数据相当于源数据，即没有转化为数据框的数据。 下面我们直接把CSV文件读成数据框：frame_1=pd.read_csv(‘Desktop\example_1.csv’) frame_1Out[10]: Liu Zhang Wang Li Class0 23 34 78 32 ‘primary’1 77 32 89 66 ‘intermediate’2 99 34 78 66 ‘senior’3 66 34 6 33 ‘intermediate’ #我们发现我们得到一个完美的数据框下面我们尝试用‘read_table’frame_1=pd.read_table(‘Desktop\example_1.csv’) frame_1Out[15]: Liu,Zhang,Wang,Li,Class0 23,34,78,32,’primary’1 77,32,89,66,’intermediate’2 99,34,78,66,’senior’3 66,34,6,33,’intermediate’ 我们发现被读成的DataFrame排列很混乱，下面我们加上分割符号‘，’再运行一次。frame_1=pd.read_table(‘Desktop\example_1.csv’,sep=’,’) frame_1Out[13]: Liu Zhang Wang Li Class0 23 34 78 32 ‘primary’1 77 32 89 66 ‘intermediate’2 99 34 78 66 ‘senior’3 66 34 6 33 ‘intermediate’ 总结：用read_table一定要加分割符号。上面的例子中的csv数据我们都加了列名，如果没有列名会怎么样呢?frame_2=pd.read_table(‘Desktop\example_2.csv’,sep=’,’) frame_2Out[19]: 23 34 78 32 ‘primary’0 77 32 89 66 ‘intermediate’1 99 34 78 66 ‘senior’2 66 34 6 33 ‘intermediate’ 我们发现计算机自动把第一行数据当做列名，这样的话我们的源数据就遭到破坏。 为了避免这种错误，我们加入‘header’属性。让计算机自动加列名或者自己显性指定： frame_3=pd.read_csv(‘Desktop\example_2.csv’,header=None) frame_3Out[21]: 0 1 2 3 40 23 34 78 32 ‘primary’1 77 32 89 66 ‘intermediate’2 99 34 78 66 ‘senior’3 66 34 6 33 ‘intermediate’ frame_3=pd.read_csv(‘Desktop\example_2.csv’,names=[‘a’,’b’,’c’,’d’,’class’]) frame_3Out[23]: a b c d class0 23 34 78 32 ‘primary’1 77 32 89 66 ‘intermediate’2 99 34 78 66 ‘senior’3 66 34 6 33 ‘intermediate’源文件中去掉双引号，这里双引号也会消失！我们可以把最右边的列直接转化成行索引列。 frame_3=pd.read_csv(‘Desktop\example_2.csv’,names=[‘a’,’b’,’c’,’d’,’class’],index_col=’class’) frame_3Out[28]: a b c dclass‘primary’ 23 34 78 32‘intermediate’ 77 32 89 66‘senior’ 99 34 78 66‘intermediate’ 66 34 6 33 除了可以读取为普通的数据框，还可以读取为带有重索引的数据框： pd.read_csv(‘Desktop\example_5.csv’,index_col=[‘group’,’games’])Out[45]: a b c a.1 b.1 c.1 a.2 b.2 c.2group gamesone Att 4 3 4 2 4 3 4 2 1 Cgt 4 3 4 2 4 3 4 2 1 Aer 4 3 4 2 4 3 4 2 1two Att 4 3 4 2 4 3 4 2 1 Cgt 4 3 4 2 4 3 4 2 1 Aer 4 3 4 2 4 3 4 2 1 对于那些用数量不等空格或者字符串隔开的数据，我们可以通过正则操作符一步处理到位，把数据处理整齐： !type Desktop\exampel_6.csv A B C’aaa -3.45 2.36 8.90’bbb 0.334 0.457 -4.5’ccc 0.76 -7.34 -8.99’ddd 0.37 -7.8 -4.45’ pd.read_csv(‘Desktop\exampel_6.csv’,sep=’\s+’)Out[63]: A B C’aaa -3.450 2.360 8.90’bbb 0.334 0.457 -4.5’ccc 0.760 -7.340 -8.99’ddd 0.370 -7.800 -4.45’ 如果这里的源数据没有引号的话，这里引号也会自动消失。利用skiprows语句可以跳过任意无意义行信息。!type Desktop\example_9.csvtitle:Good BoyLittle Robert asked his mother for two cents. “What did you do with the money I gave you yesterday?”a,b,c,d,name“I gave it to a poor old woman,” he answered.“You’re a good boy,” said the mother proudly. “Here are two cents more. But why are you so interested in the old woman?”group1,12,2,2,3,robertgroup2,10,19,18,17,linz“She is the one who sells the candy.”group3,29,28,27,27,hansgroup4,34,35,35,36,manfried 整理后数据 pd.read_csv(‘Desktop\example_9.csv’,skiprows=[0,1,3,4,7])Out[69]: a b c d namegroup1 12 2 2 3 robertgroup2 10 19 18 17 linzgroup3 29 28 27 27 hansgroup4 34 35 35 36 manfried 缺失数据要么没有，要么用某个标记值表示。请看下面两个例子： !type Desktop\example_10.csvLiu,Zhang,Wang,Li,Class23,34,78,32,NA77,32,,66,’intermediate’99,34,78,66,’senior’66,34,6,33,’intermediate’ pd.read_csv(‘Desktop\example_10.csv’)Out[83]: Liu Zhang Wang Li Class0 23 34 78.0 32 NaN1 77 32 NaN 66 ‘intermediate’2 99 34 78.0 66 ‘senior’3 66 34 6.0 33 ‘intermediate’ #上面例子产生NAN值，（直接在数字处空一位，就会产生NaN，比如数据1,2,4对应列标签，A,B,C,D） !type Desktop\example_11.csv23,34,78,32,’ ‘77,32,89,66,’intermediate’99,34, ,66,’senior’66,34,6,33,’intermediate’ pd.read_csv(‘Desktop\example_11.csv’)Out[86]: 23 34 78 32 ‘ ‘0 77 32 89 66 ‘intermediate’1 99 34 66 ‘senior’2 66 34 6 33 ‘intermediate’ #上面的例子什么都不显示（只有空字符串和空格）通过na_values python可以用字符串标识NAN值。但要注意源数据中字符串上不能有引号。 !type Desktop\example_12.csvA,B,C,D1,2,3,45,me,7,88,10,11,me12,13,14,16 pd.read_table(‘Desktop\example_12.csv’,na_values=[‘me’],sep=’,’)Out[14]: A B C D0 1 2.0 3 4.01 5 NaN 7 8.02 8 10.0 11 NaN3 12 13.0 14 16.0 可以用字符串字典的形式标识多空值数据框!type Desktop\example_13.csvA,B,C,D1,2,3,4too,to,56,3345,dee,78,6912,21,34,43 pd.read_table(‘Desktop\example_13.csv’,na_values={‘A’:’too’,’B’:[‘to’,’dee’]},sep=’,’)Out[18]: A B C D0 1.0 2.0 3 41 NaN NaN 56 332 45.0 NaN 78 693 12.0 21.0 34 43 爬虫数据例子：df=pd.read_csv(‘Desktop\pachong.csv’,encoding=’gbk’,sep=’,’,header=None, skiprows=[14,47,49,50,51,59,68,83,104,125,127,128,136,150,173,184])用这个指令可以处理中文数据 Pd.read_csv读取excel数据： import pandas as pd pd.read_excel(r’Desktop\1.xls’ ,sheetname=[0,1])Out[2]:OrderedDict([(0, 1 2 3 4 0 2 3 4 5 1 3 4 5 6 2 4 5 6 7 3 5 6 7 8 4 6 7 8 9 5 7 8 9 10 6 8 9 10 11 7 9 10 11 12 8 10 11 12 13 9 11 12 13 14 10 12 13 14 15 11 13 14 15 16 12 14 15 16 17 13 15 16 17 18 14 16 17 18 19 15 17 18 19 20 16 18 19 20 21 17 19 20 21 22 18 20 21 22 23 19 21 22 23 24), (1, 23 22 34 55 67 0 24 23 35 56 68 1 25 24 36 57 69 2 26 25 37 58 70 3 27 26 38 59 71 4 28 27 39 60 72 5 29 28 40 61 73 6 30 29 41 62 74 7 31 30 42 63 75 8 32 31 43 64 76 9 33 32 44 65 77 10 34 33 45 66 78 11 35 34 46 67 79 12 36 35 47 68 80 13 37 36 48 69 81 14 38 37 49 70 82 15 39 38 50 71 83 16 40 39 51 72 84 17 41 40 52 73 85 18 42 41 53 74 86 19 43 42 54 75 87 20 44 43 55 76 88 21 45 44 56 77 89 22 46 45 57 78 90)]) #sheetname=表格时，返回多个指定的在一个workbook里面的表格，sheetname=none，将返回一个workbook里的所有表格。sheetname=int时，int指的是表格索引号，sheetname=’sheet1‘返回指定表格。 pd.read_excel(r’Desktop\1.xls’ ,sheetname=None)Out[4]:OrderedDict([(‘Sheet1’, 1 2 3 4 0 2 3 4 5 1 3 4 5 6 2 4 5 6 7 3 5 6 7 8 4 6 7 8 9 5 7 8 9 10 6 8 9 10 11 7 9 10 11 12 8 10 11 12 13 9 11 12 13 14 10 12 13 14 15 11 13 14 15 16 12 14 15 16 17 13 15 16 17 18 14 16 17 18 19 15 17 18 19 20 16 18 19 20 21 17 19 20 21 22 18 20 21 22 23 19 21 22 23 24), (‘Sheet2’, 23 22 34 55 67 0 24 23 35 56 68 1 25 24 36 57 69 2 26 25 37 58 70 3 27 26 38 59 71 4 28 27 39 60 72 5 29 28 40 61 73 6 30 29 41 62 74 7 31 30 42 63 75 8 32 31 43 64 76 9 33 32 44 65 77 10 34 33 45 66 78 11 35 34 46 67 79 12 36 35 47 68 80 13 37 36 48 69 81 14 38 37 49 70 82 15 39 38 50 71 83 16 40 39 51 72 84 17 41 40 52 73 85 18 42 41 53 74 86 19 43 42 54 75 87 20 44 43 55 76 88 21 45 44 56 77 89 22 46 45 57 78 90), (‘Sheet3’, 234 22 44 78 990 0 235 23 45 79 991 1 236 24 46 80 992 2 237 25 47 81 993 3 238 26 48 82 994 4 239 27 49 83 995 5 240 28 50 84 996 6 241 29 51 85 997 7 242 30 52 86 998 8 243 31 53 87 999 9 244 32 54 88 1000 10 245 33 55 89 1001 11 246 34 56 90 1002 12 247 35 57 91 1003 13 248 36 58 92 1004 14 249 37 59 93 1005 15 250 38 60 94 1006 16 251 39 61 95 1007 17 252 40 62 96 1008 18 253 41 63 97 1009 19 254 42 64 98 1010 20 255 43 65 99 1011)]) pd.read_excel(r’Desktop\1.xls’ ,sheetname=2)Out[5]: 234 22 44 78 9900 235 23 45 79 9911 236 24 46 80 9922 237 25 47 81 9933 238 26 48 82 9944 239 27 49 83 9955 240 28 50 84 9966 241 29 51 85 9977 242 30 52 86 9988 243 31 53 87 9999 244 32 54 88 100010 245 33 55 89 100111 246 34 56 90 100212 247 35 57 91 100313 248 36 58 92 100414 249 37 59 93 100515 250 38 60 94 100616 251 39 61 95 100717 252 40 62 96 100818 253 41 63 97 100919 254 42 64 98 101020 255 43 65 99 1011 pd.read_excel(r’Desktop\1.xls’ ,sheetname=0)Out[6]: 1 2 3 40 2 3 4 51 3 4 5 62 4 5 6 73 5 6 7 84 6 7 8 95 7 8 9 106 8 9 10 117 9 10 11 128 10 11 12 139 11 12 13 1410 12 13 14 1511 13 14 15 1612 14 15 16 1713 15 16 17 1814 16 17 18 1915 17 18 19 2016 18 19 20 2117 19 20 21 2218 20 21 22 2319 21 22 23 24 #sheetname=整数时，整数表示表格编号，编号从0开始。 pd.read_excel(r’Desktop\1.xls’ ,sheetname=’Sheet3’)Out[9]: 234 22 44 78 9900 235 23 45 79 9911 236 24 46 80 9922 237 25 47 81 9933 238 26 48 82 9944 239 27 49 83 9955 240 28 50 84 9966 241 29 51 85 9977 242 30 52 86 9988 243 31 53 87 9999 244 32 54 88 100010 245 33 55 89 100111 246 34 56 90 100212 247 35 57 91 100313 248 36 58 92 100414 249 37 59 93 100515 250 38 60 94 100616 251 39 61 95 100717 252 40 62 96 100818 253 41 63 97 100919 254 42 64 98 101020 255 43 65 99 1011 #上面是用字符串‘Sheet3’来调用表格三数据 #header : int, list of ints, default 0 指定列索引行，默认0，即取第一行，即数据第一行被自动读取为列索引，这一点我们并不希望，因为会丢失掉第一行数据，因此通常设定 header = None，这种情况下，计算机自动为整个数据添加新的列索引。 pd.read_excel(r’Desktop\1.xls’ ,sheetname=’Sheet3’,header=None)Out[10]: 0 1 2 3 40 234 22 44 78 9901 235 23 45 79 9912 236 24 46 80 9923 237 25 47 81 9934 238 26 48 82 9945 239 27 49 83 9956 240 28 50 84 9967 241 29 51 85 9978 242 30 52 86 9989 243 31 53 87 99910 244 32 54 88 100011 245 33 55 89 100112 246 34 56 90 100213 247 35 57 91 100314 248 36 58 92 100415 249 37 59 93 100516 250 38 60 94 100617 251 39 61 95 100718 252 40 62 96 100819 253 41 63 97 100920 254 42 64 98 101021 255 43 65 99 1011 #上例中我们发现计算机没有破坏原数据，额外为我们添加了一行列索引。 #skiprows : list-like，跳过指定行数的数据pd.read_excel(r’Desktop\1.xls’ ,sheetname=’Sheet3’,header=None,skiprows=[1,5,19,10])Out[11]: 0 1 2 3 40 234 22 44 78 9901 236 24 46 80 9922 237 25 47 81 9933 238 26 48 82 9944 240 28 50 84 9965 241 29 51 85 9976 242 30 52 86 9987 243 31 53 87 9998 245 33 55 89 10019 246 34 56 90 100210 247 35 57 91 100311 248 36 58 92 100412 249 37 59 93 100513 250 38 60 94 100614 251 39 61 95 100715 252 40 62 96 100816 254 42 64 98 101017 255 43 65 99 1011 #我们发现数据少了4行，原因是我们分别跳过了4行不同行 #skip_footer : int,default 0, 省略从尾部数的整数行数据 pd.read_excel(r’Desktop\1.xls’ ,sheetname=’Sheet3’,header=None,skip_footer=8)Out[13]: 0 1 2 3 40 234 22 44 78 9901 235 23 45 79 9912 236 24 46 80 9923 237 25 47 81 9934 238 26 48 82 9945 239 27 49 83 9956 240 28 50 84 9967 241 29 51 85 9978 242 30 52 86 9989 243 31 53 87 99910 244 32 54 88 100011 245 33 55 89 100112 246 34 56 90 100213 247 35 57 91 1003 #从下向上数8行，然后跳过这8行 #index_col : int, list of ints, default None指定某列为索引列pd.read_excel(r’Desktop\1.xls’ ,sheetname=’Sheet3’,header=None,index_col=4)Out[15]: 0 1 2 34990 234 22 44 78991 235 23 45 79992 236 24 46 80993 237 25 47 81994 238 26 48 82995 239 27 49 83996 240 28 50 84997 241 29 51 85998 242 30 52 86999 243 31 53 871000 244 32 54 881001 245 33 55 891002 246 34 56 901003 247 35 57 911004 248 36 58 921005 249 37 59 931006 250 38 60 941007 251 39 61 951008 252 40 62 961009 253 41 63 971010 254 42 64 981011 255 43 65 99 #上面指定第四列为索引列 #names : array-like, default None, 给所有列索引重新命名。 pd.read_excel(r’Desktop\1.xls’ ,sheetname=’Sheet3’,names=[‘col_1’,’col_2’,’col_3’,’col_4’,’col_5’])Out[17]: col_1 col_2 col_3 col_4 col_50 235 23 45 79 9911 236 24 46 80 9922 237 25 47 81 9933 238 26 48 82 9944 239 27 49 83 9955 240 28 50 84 9966 241 29 51 85 9977 242 30 52 86 9988 243 31 53 87 9999 244 32 54 88 100010 245 33 55 89 100111 246 34 56 90 100212 247 35 57 91 100313 248 36 58 92 100414 249 37 59 93 100515 250 38 60 94 100616 251 39 61 95 100717 252 40 62 96 100818 253 41 63 97 100919 254 42 64 98 101020 255 43 65 99 1011 #上面的列索引都被更换。 文本文件的块读取 如果只想读取数据一部分，或者数据过大，我们想逐步读取，我们可以通过附加属性函数来实现。 通过nrows属性函数可以选择所需要的行数!type Desktop\example_14.csvone,two,three,four0,1,2,34,5,6,78,9,10,1112,13,14,1516,17,18,1920,21,22,2312,13,14,1516,17,18,1920,21,22,2312,13,14,1516,17,18,1920,21,22,2312,13,14,1516,17,18,1920,21,22,230,1,2,34,5,6,78,9,10,1112,13,14,1516,17,18,1920,21,22,2312,13,14,1516,17,18,19 pd.read_table(‘Desktop\example_14.csv’,sep=’,’,nrows=8)Out[21]: one two three four0 0 1 2 31 4 5 6 72 8 9 10 113 12 13 14 154 16 17 18 195 20 21 22 236 12 13 14 157 16 17 18 19 实验项目：数据的分块读取： 原始数据的部分截取： block_2=pd.read_csv(‘Desktop\lockreading.csv’,chunksize=59)key_quantity=pd.Series([]) #block_2是对整个数据分块后的返回值，可看做一个三维数据框，key_quantity是一个存储行编号数量的series for block_var in block_2:key_quantity=key_quantity.add(block_var[‘Key’].value_counts(),fill_value=0) {千万注意：这里fill_value很重要，因为在循环过程中，由于相加的两个Seires的行标签不可能每一次都完全匹配，行标签不完全匹配的两个Series相加如果会产生空值，这种情况下会影响下次循环series的相加，因为空值+实数会继续产生空值，从而计数错误。为了消除这种现象的产生，最好的办法是每一次相加前的空值立马填充为‘0’值，这样杜绝空值产生，不会影响下一步两个series的相加。看下面例子：import pandasimport pandas as pdserie_1=pd.Series([1,2,3],index=list(‘abc’))serie_2=pd.Series([4,5,8],index=list(‘abd’))serie_3=serie_1+serie_2serie_3Out[6]:a 5.0b 7.0c NaNd NaNdtype: float64 #假如我们添加’fill_value=0‘serie_1.add(serie_2,fill_value=0)Out[8]:a 5.0b 7.0c 3.0d 8.0dtype: float64 } 接下来我们继续我们的项目：key_quantity=key_quantity.sort_values(ascending=False)key_quantity Out[16]:Nr.36 10.0Nr.30 10.0Nr.32 10.0Nr.33 10.0Nr.34 10.0Nr.35 10.0Nr.37 10.0Nr.38 10.0Nr.39 10.0Nr.29 10.0Nr.40 10.0Nr.41 10.0Nr.28 10.0Nr.27 10.0Nr.26 10.0Nr.25 10.0Nr.24 10.0Nr.23 10.0Nr.31 10.0Nr.42 9.0Nr.21 7.0Nr.20 7.0Nr.2 7.0Nr.19 7.0Nr.18 7.0Nr.17 7.0Nr.22 7.0Nr.15 7.0Nr.14 7.0Nr.3 7.0Nr.13 7.0Nr.12 7.0Nr.11 7.0Nr.10 7.0Nr.16 7.0Nr.9 7.0Nr.8 7.0Nr.52 7.0Nr.7 7.0Nr.6 7.0Nr.59 7.0Nr.58 7.0Nr.57 7.0Nr.56 7.0Nr.55 7.0Nr.54 7.0Nr.53 7.0Nr.51 7.0Nr.4 7.0Nr.50 7.0Nr.5 7.0Nr.49 7.0Nr.48 7.0Nr.47 7.0Nr.46 7.0Nr.45 7.0Nr.44 7.0Nr.43 7.0Nr.1 7.0dtype: float64 #上面这个Series左边是行编号，（注意不是行索引，行编号是key列中的值，它是每一行的编号），右边是每个编号在整个数据里出现的次数。 通过一个小函数我们能够瞬间遍查每个数据块寻找任何一个给定的行编号。 def search_rowindizies(str_test): list_1=[] for block_var in block_2: boll=block_var[‘Key’]==str_test st=block_var.loc[boll,:’A24’].values lst=st.astype(np.float32) #alst=list(lst) list_1.append(lst) return list_1 block_2=pd.read_csv(‘Desktop\lockreading.csv’,chunksize=59) search_rowindizies(‘Nr.42’)Out[19]:[array([[ 4.57160000e+04, 2.33750000e+04, 2.23410000e+04, 1.60100000e+03, 6.03000000e+02, 9.98000000e+02, 2.27580000e+04, 1.01930000e+04, 1.25650000e+04, 1.37070000e+04, 7.65300000e+03, 6.05400000e+03, 4.13700000e+03, 2.58400000e+03, 1.55300000e+03, 2.97900000e+03, 2.01500000e+03, 9.64000000e+02, 5.19000000e+02, 3.16000000e+02, 2.03000000e+02, 1.50000000e+01, 1.10000000e+01, 4.00000000e+00]], dtype=float32), array([[ 4.57160000e+04, 2.33750000e+04, 2.23410000e+04, 1.60100000e+03, 6.03000000e+02, 9.98000000e+02, 2.27580000e+04, 1.01930000e+04, 1.25650000e+04, 1.37070000e+04, 7.65300000e+03, 6.05400000e+03, 4.13700000e+03, 2.58400000e+03, 1.55300000e+03, 2.97900000e+03, 2.01500000e+03, 9.64000000e+02, 5.19000000e+02, 3.16000000e+02, 2.03000000e+02, 1.50000000e+01, 1.10000000e+01, 4.00000000e+00]], dtype=float32), array([[ 4.57160000e+04, 2.33750000e+04, 2.23410000e+04, 1.60100000e+03, 6.03000000e+02, 9.98000000e+02, 2.27580000e+04, 1.01930000e+04, 1.25650000e+04, 1.37070000e+04, 7.65300000e+03, 6.05400000e+03, 4.13700000e+03, 2.58400000e+03, 1.55300000e+03, 2.97900000e+03, 2.01500000e+03, 9.64000000e+02, 5.19000000e+02, 3.16000000e+02, 2.03000000e+02, 1.50000000e+01, 1.10000000e+01, 4.00000000e+00]], dtype=float32), array([[ 4.57160000e+04, 2.33750000e+04, 2.23410000e+04, 1.60100000e+03, 6.03000000e+02, 9.98000000e+02, 2.27580000e+04, 1.01930000e+04, 1.25650000e+04, 1.37070000e+04, 7.65300000e+03, 6.05400000e+03, 4.13700000e+03, 2.58400000e+03, 1.55300000e+03, 2.97900000e+03, 2.01500000e+03, 9.64000000e+02, 5.19000000e+02, 3.16000000e+02, 2.03000000e+02, 1.50000000e+01, 1.10000000e+01, 4.00000000e+00]], dtype=float32), array([[ 4.57160000e+04, 2.33750000e+04, 2.23410000e+04, 1.60100000e+03, 6.03000000e+02, 9.98000000e+02, 2.27580000e+04, 1.01930000e+04, 1.25650000e+04, 1.37070000e+04, 7.65300000e+03, 6.05400000e+03, 4.13700000e+03, 2.58400000e+03, 1.55300000e+03, 2.97900000e+03, 2.01500000e+03, 9.64000000e+02, 5.19000000e+02, 3.16000000e+02, 2.03000000e+02, 1.50000000e+01, 1.10000000e+01, 4.00000000e+00]], dtype=float32), array([[ 4.57160000e+04, 2.33750000e+04, 2.23410000e+04, 1.60100000e+03, 6.03000000e+02, 9.98000000e+02, 2.27580000e+04, 1.01930000e+04, 1.25650000e+04, 1.37070000e+04, 7.65300000e+03, 6.05400000e+03, 4.13700000e+03, 2.58400000e+03, 1.55300000e+03, 2.97900000e+03, 2.01500000e+03, 9.64000000e+02, 5.19000000e+02, 3.16000000e+02, 2.03000000e+02, 1.50000000e+01, 1.10000000e+01, 4.00000000e+00]], dtype=float32), array([[ 4.57160000e+04, 2.33750000e+04, 2.23410000e+04, 1.60100000e+03, 6.03000000e+02, 9.98000000e+02, 2.27580000e+04, 1.01930000e+04, 1.25650000e+04, 1.37070000e+04, 7.65300000e+03, 6.05400000e+03, 4.13700000e+03, 2.58400000e+03, 1.55300000e+03, 2.97900000e+03, 2.01500000e+03, 9.64000000e+02, 5.19000000e+02, 3.16000000e+02, 2.03000000e+02, 1.50000000e+01, 1.10000000e+01, 4.00000000e+00]], dtype=float32), array([[ 4.57160000e+04, 2.33750000e+04, 2.23410000e+04, 1.60100000e+03, 6.03000000e+02, 9.98000000e+02, 2.27580000e+04, 1.01930000e+04, 1.25650000e+04, 1.37070000e+04, 7.65300000e+03, 6.05400000e+03, 4.13700000e+03, 2.58400000e+03, 1.55300000e+03, 2.97900000e+03, 2.01500000e+03, 9.64000000e+02, 5.19000000e+02, 3.16000000e+02, 2.03000000e+02, 1.50000000e+01, 1.10000000e+01, 4.00000000e+00], [ 4.57160000e+04, 2.33750000e+04, 2.23410000e+04, 1.60100000e+03, 6.03000000e+02, 9.98000000e+02, 2.27580000e+04, 1.01930000e+04, 1.25650000e+04, 1.37070000e+04, 7.65300000e+03, 6.05400000e+03, 4.13700000e+03, 2.58400000e+03, 1.55300000e+03, 2.97900000e+03, 2.01500000e+03, 9.64000000e+02, 5.19000000e+02, 3.16000000e+02, 2.03000000e+02, 1.50000000e+01, 1.10000000e+01, 4.00000000e+00]], dtype=float32)] #每次执行函数都要调用一次block_2,因为数据用完一次后自动与源数据断开，Block_2自动清零消失到此为止，我们的项目结束！！ 把数据下载到csv 文件DataFrame_1=pd.DataFrame(np.arange(16).reshape(4,4),index=list(‘asdt’),columns=list(‘sgdt’)) DataFrame_1Out[5]: s g d ta 0 1 2 3s 4 5 6 7d 8 9 10 11t 12 13 14 15 DataFrame_1.to_csv(‘Desktop\DataFrame_1.csv’) #数据储存到桌面，文件名是DataFrame_1。默认情况下，To_csv方法只会把数据存为用逗号隔开的csv数据。我们可以通过!type函数直接调用这个已存到桌面上的文件，就会发现它的确如此，如下： !type Desktop\DataFrame_1.csv,s,g,d,ta,0,1,2,3s,4,5,6,7d,8,9,10,11t,12,13,14,15 #上面是没有转化为数据框的源数据格式，也就是csv文件的直接调取。当然，我们也可以直接将数据直接存其它分割方式，例如下面的例子 DataFrame_1.to_csv(‘Desktop\DataFrame_2.csv’,sep=’#’) !type Desktop\DataFrame_2.csv #s#g#d#ta#0#1#2#3s#4#5#6#7d#8#9#10#11t#12#13#14#15 DataFrame_2=DataFrame_1.copy() DataFrame_2.loc[‘a’,’s’]=np.nan DataFrame_2Out[32]: s g d ta NaN 1 2 3s 4.0 5 6 7d 8.0 9 10 11t 12.0 13 14 15 DataFrame_2.to_csv(‘Desktop\DataFrame_3.csv’) !type Desktop\DataFrame_3.csv,s,g,d,ta,,1,2,3s,4.0,5,6,7d,8.0,9,10,11t,12.0,13,14,15 #再重新通过！type指令调回已存储的文件后，发现数据框原有的NaN值变成空位置，然而，空位置在csv文件中并不好辨认，因此我们需要把含有nan值的数据框存储为nan值被指定的容易辨认的符号替代的csv文件。 DataFrame_2Out[50]: s g d ta NaN 1 2 3s 4.0 5 6 7d 8.0 9 10 11t 12.0 13 14 15 DataFrame_2.to_csv(‘Desktop\DataFrame_4.csv’,na_rep=’Cat’)!type Desktop\DataFrame_4.csv,s,g,d,ta,Cat,1,2,3s,4.0,5,6,7d,8.0,9,10,11t,12.0,13,14,15 #用!type调用储存在桌面的文件DataFrame_4.csv，我们会发现原来在数据框的NaN值在存储后被‘Cat’替代 这里我们要注意空位置与空字符串不一样试比较： pd.read_csv(‘Desktop\example_16.csv’)Out[42]: a b c d0 ‘ ‘ 123 456 891 22 65 ‘ ‘ 122 345 89 0 63 23 ‘ ‘ 33 66 !type Desktop\example_16.csva,b,c,d‘ ‘,123,456,8922,65,’ ‘,12345,89,0,623,’ ‘,33,66 上面是空字符串 pd.read_csv(‘Desktop\example_17.csv’)Out[44]: a b c d0 NaN 123.0 456.0 891 22.0 65.0 NaN 122 345.0 89.0 0.0 63 23.0 NaN 33.0 66 !type Desktop\example_17.csva,b,c,d,123,456,8922,65,,12345,89,0,623,,33,66 上面是空位置，空位置再读取成数据框时会产生NaN值。 如果没有特殊指定，数据框行和列的标签会随数据一起存储为csv文件，当然我们也可以不这样。如下： DataFrame_1Out[51]: s g d ta 0 1 2 3s 4 5 6 7d 8 9 10 11t 12 13 14 15 DataFrame_1.to_csv(‘Desktop\Dataframe_12.csv’,index=False,header=False) !type Desktop\Dataframe_12.csv0,1,2,34,5,6,78,9,10,1112,13,14,15 我们发现行标签和列标签都不见了。 我们还可以只存储数据框的部分列，并指定顺序： DataFrame_1.to_csv(‘Desktop\Dataframe_13.csv’,index=False,columns=[‘s’,’t’,’d’]) 调用存储文件，测试！！ !type Desktop\Dataframe_13.csvs,t,d0,3,24,7,68,11,1012,15,14 我们发现只有部分列被存储，且列的顺序按照指定。 Series 也可以通过to_csv来存储信息到桌面。dates_1=pd.date_range(‘01/06/2017’,periods=14) dates_1Out[60]:DatetimeIndex([‘2017-01-06’, ‘2017-01-07’, ‘2017-01-08’, ‘2017-01-09’, ‘2017-01-10’, ‘2017-01-11’, ‘2017-01-12’, ‘2017-01-13’, ‘2017-01-14’, ‘2017-01-15’, ‘2017-01-16’, ‘2017-01-17’, ‘2017-01-18’, ‘2017-01-19’], dtype=’datetime64[ns]’, freq=’D’)series_12=pd.Series(np.arange(14),index=dates_1)series_12.to_csv(‘Desktop\series_123.csv’)!type Desktop\series_123.csv2017-01-06,02017-01-07,12017-01-08,22017-01-09,32017-01-10,42017-01-11,52017-01-12,62017-01-13,72017-01-14,82017-01-15,92017-01-16,102017-01-17,112017-01-18,122017-01-19,13 csv文件也可以从桌面直接被读取成Series：pd.Series.from_csv(‘Desktop\series_123.csv’)Out[71]:2017-01-06 02017-01-07 12017-01-08 22017-01-09 32017-01-10 42017-01-11 52017-01-12 62017-01-13 72017-01-14 82017-01-15 92017-01-16 102017-01-17 112017-01-18 122017-01-19 13dtype: int64 注：1.Read_csv可以读取文件，url，文件型对象，但被加载文件必须有分割符，默认的分割符为逗号。2.Read_table可以读取文件，url，文件型对象，但被加载文件必须有分割符，默认的分割符为制表符‘\t’。这一点与read_csv不同，烦请再加载有逗号的数据时用seq属性注明，也即seq=‘，’！3.Read_fwf读取（或称加载）没有分隔符数据，但是各数据之间间距要恒定。4.Read_clipboard 通常用来读取网络数据,在使用前，必须把网页内容先复制到粘贴板上，例如： pd.read_clipboard(sep=’\s+’)Out[73]: 北 京 220956 209468 11488 697.02 264.30 253555 44087 701.420 天 津 93162 90080 3082 330.68 126.98 106063 15983 340.101 河 北 209740 200012 9728 963.39 327.27 262396 62384 982.312 山 西 131802 114466 17336 628.55 192.27 214625 100159 669.393 内蒙古 71196 65627 5569 306.82 110.15 101829 36202 308.104 NaN5 辽 宁 211502 199611 11891 838.09 305.58 268741 69130 846.25 上面函数常用关键字列表 filepath_or_buffer: 文件系统位置，url，文件型对象的字符串delimiter或sep 源文件各数据间分隔符或正则表达式header 上载数据成数据框时，数据框的列名，默认为上载数据的第一行。即‘0’行。如果不需要列标签，那么使header=Noneindex_col 就是给行层次化索引命名，行层次索引一般有两列，内列和外列。skiprows 需要忽略的行数，跳过无用行na_values 实质就是把源数据中的指定字符串转化为空值。converters 由列名或者说列号或者说列标签和函数组成字典，例如{‘A’:f}说明f函数应用到’A’列中的每一个数据。nrows 需要读取的数据行数skip_footer 需要忽略的行数，注意从源数据最后一行向上数encoding 用于指明unicode文本文件的文本编码格式Squeeze 如果数据仅有一列，自动返回seriesthousands 千分位分隔符，如’.’或’,’ converters的应用： 1.数据框的元素作为字符穿处理In [1]: import pandas as pd In [2]: import numpy as np In [3]: Data_1=pd.DataFrame([[1,2,1],[3,4,2],[3,5,4]],columns=[‘A’,’B’,’C’],index=[‘r1’,’r2’,’r3’]) In [4]: Data_1Out[4]:A B Cr1 1 2 1r2 3 4 2r3 3 5 4 In [7]: Data_1.to_csv(r’Desktop\dong1234.csv’,sep=’,’) In [8]: pd.read_csv(‘Desktop\dong1234.csv’,delimiter=’,’,usecols=[‘A’,’B’,’C’])Out[8]:A B C0 1 2 11 3 4 22 3 5 4 In [10]: pd.read_csv(‘Desktop\dong123.csv’,delimiter=’,’,usecols=[‘A’,’B’,’C’],converters={‘B’:lambda x:x*2})Out[10]:A B C0 1 22 11 3 44 22 3 55 4 In [11]: pd.read_csv(‘Desktop\dong123.csv’,delimiter=’,’,usecols=[‘A’,’B’,’C’],converters={‘B’:lambda x:x*2+’1’})Out[11]:A B C0 1 221 11 3 441 22 3 551 4 In [12]: pd.read_csv(‘Desktop\dong123.csv’,delimiter=’,’,usecols=[‘A’,’B’,’C’],converters={‘B’:lambda x:x23+’1’}) …:Out[12]:A B C0 1 2222221 11 3 4444441 22 3 5555551 42.数据框的元素作为数据处理 In [14]: pd.read_csv(‘Desktop\dong123.csv’,delimiter=’,’,usecols=[‘A’,’B’,’C’],converters={‘B’:lambda x:float(x)*3+2float(x)})Out[14]:A B C0 1 12.0 11 3 72.0 22 3 135.0 4 In [15]: import mathIn [21]: converters_1={i: lambda x:float(x)*3+math.sin((np.pi/6)float(x)) for i in range(3)} In [22]: converters_1Out[22]:{0: &lt;function main..&gt;,1: &lt;function main..&gt;,2: &lt;function main..&gt;} In [23]: pd.read_csv(‘Desktop\dong123.csv’,delimiter=’,’,usecols=[‘A’,’B’,’C’],converters=converters_1)Out[23]:A B C0 1.5 8.866025 11 28.0 64.866025 22 28.0 125.500000 4 分隔符格式的手工处理通常用read_table或者read_csv直接加载csv文件或者TXT文件都是没有问题的，但有时csv文件十分混乱，以致无法加载，也是十分常见的，这就要求我们在加载前要对原始csv文件进行处理，使其能够被顺利加载。 我们先看一个单字符分隔符文件import pandas as pd import numpy as np !type Desktop\example_20.csv“f”,”g”,”h”,”d”“1”,”1”,”4”,”5”“7”,”9”,”0”,”4”“2”,”8”,”11”,0import csv调用内置csv模块，将已打开文件直接传给csv.readerf=open(‘Desktop\example_20.csv’)getting_1=csv.reader(f)getting_1Out[25]: &lt;_csv.reader at 0x3077b74320&gt; #我们无法看到getting_1的庐山真面目，怎么办？通过for循环，我们看到getting_1中的每个元素。这是一个十分好的方法，请大家务必注意：for subgetting in getting_1: print(subgetting) [‘f’, ‘g’, ‘h’, ‘d’][‘1’, ‘1’, ‘4’, ‘5’][‘7’, ‘9’, ‘0’, ‘4’][‘2’, ‘8’, ‘11’, ‘0’]我下面再举一个具体的例子： range(1,9,1)Out[27]: range(1, 9)for subrange in range(1,9,1): print(subrange) 12345678当然这里我们还有一个终极解决方案，也就是通过list。Gate_1=list(csv.reader(open(‘Desktop\example_20.csv’)))Gate_1Out[32]:[[‘f’, ‘g’, ‘h’, ‘d’], [‘1’, ‘1’, ‘4’, ‘5’], [‘7’, ‘9’, ‘0’, ‘4’], [‘2’, ‘8’, ‘11’, ‘0’]] #直到这里我们终于看到了csv.reader()的庐山真面目。 header,values=Gate_1[0],Gate_1[1:] dic_new_1={u:v for u,v in zip(header,zip(*values))} dic_new_1Out[36]:{‘d’: (‘5’, ‘4’, ‘0’), ‘f’: (‘1’, ‘7’, ‘2’), ‘g’: (‘1’, ‘9’, ‘8’), ‘h’: (‘4’, ‘0’, ‘11’)} 最终我们把相对杂乱无章的数据转化为规整的字典。 解释：zip（2dim-Array）这个操作相当于把嵌套列表每一行一一对应先组成元组，然后组成列表：c=[[1,2,3],[4,7,0]]zip(c)Out[40]: list(zip(*c))Out[41]: [(1, 4), (2, 7), (3, 0)] 我们可以自定义csv文件的读取或写入格式：Import csvclass mydialect(csv.Dialect): lineterminator = ‘\n’ #用于写操作时的行结束符，读操作时将忽略此项，也就是读操作时无需在文件数据行尾处加‘\n’ delimiter = ‘#’ #可以读取用‘#’分割的字段（或者数据）quotechar=’”‘ #用于带有特殊字符的字段（通常指的是带有单引号的字符串）的引用符号，例如源数据这样的字段’12#3’通过csv.reader函数读取到python程序页面后会变成”’12”,”3’”。quoting=csv.QUOTE_ALL #读取所有字段（和数据）skipinitialspace=True #或略分隔符后面的空格doublequote=True #如果读取的字段含有引用符号，则整个字段加双引号。 下面是一个比较简单的例子：Import csvclass mydialect(csv.Dialect): lineterminator = ‘\n’ delimiter = ‘#’ quotechar=’”‘ quoting=csv.QUOTE_ALL skipinitialspace=True doublequote=True myfile_1=open(‘Desktop\example_223.csv’) content_11=csv.reader(myfile_1,dialect=mydialect) for contents in content_11: print(contents) [‘At’, ‘bee’, ‘cotton’, ‘death’, “common’hero’tree”][‘1’, ‘2’, ‘3’, ‘4’][‘5’, ‘6’, ‘7’, ‘8’, ‘9’][‘9’, ‘10’, ‘11’, ‘12’, “‘12”, “3’”] !type Desktop\example_223.csvAt#bee#cotton# death#common’hero’tree1#2# 3#45#6#7#8#99#10#11#12#’12#3’ 我们上面一直在讨论通过定义csv.Dialect的一个子类来读取独有格式的csv，下面我们谈谈写入 class mydialect(csv.Dialect): lineterminator = ‘\n’ delimiter = ‘#’ quotechar=’”‘ quoting=csv.QUOTE_ALL skipinitialspace=Truedoublequote=True with open(‘Desktop\example_25.csv’,’w’) as newsetting_1: #首先open()函数打开文件’Desktop\example_25.csv’，如果不存在这个文件，则创建它，返回文件对象newsetting_1。 writer_object=csv.writer(newsetting_1,dialect=mydialect) #创建一个写对象’writer_object’，可理解成给打开文件添加独有编码风格“mydialect”。然后返回一个带有独有编码风格的新文件对象 writer_object.writerow([“‘one’”,0, 8]) #可通过列表把内容一步步地写入到打开的文件 writer_object.writerow([‘two’,62,43]) mylist=[[‘three’,46,88],[‘four’, 3,12]] writer_object.writerows(mylist) !type Desktop\example_25.csvone’#”0”#”8”two#”62”#”43”three#”46”#”88”four#”3”#”12” JSON数据JSON是一种数据格式，它是通过HTTP请求在web浏览器与其他应用程序之间数据传送格式之一。 例如：Json_documents=”””{“Name”:”Wes”,”places_lived”:[“United States”,”Spain”,”Germany”,”Japan”],”pet”:null,”siblings”:[{“name”:”Scott”,”age”:25,”pet”:”Zuko”,”weight”:56,”height”:172},{“name”:”Katie”,”age”:33,”pet”:”Cisco”,”weight”:89,”height”:189}]}“””这是一个Json数据格式字符串数据对象，可以通过python标准库中的json.loads函数即可将JSON字符串数据转化为python字符串数据。 import json getting_1=json.loads(Json_documents) getting_1Out[36]:{‘Name’: ‘Wes’, ‘pet’: None, ‘places_lived’: [‘United States’, ‘Spain’, ‘Germany’, ‘Japan’], ‘siblings’: [{‘age’: 25, ‘height’: 172, ‘name’: ‘Scott’, ‘pet’: ‘Zuko’, ‘weight’: 56}, {‘age’: 33, ‘height’: 189, ‘name’: ‘Katie’, ‘pet’: ‘Cisco’, ‘weight’: 89}]} 试比较这两种数据结构，我们会发现：A.JSON格式的字符串数据对象的字符串没有排序，确切的说字典或者说JSON对象的键和其所对应的子字典内容都没有任何排序。而python格式的字典缺有严格工整的排序，无论键还是键所对应的字典都有严格排序。B.所有字符串的双引号都变成单引号C.JSON格式的字典最前面和最后面都有‘”””’（也可用“’”）标识，转化成Python格式后所有标识全部消失。D.JSON的空值用null表达，Python用None表达。 有一点我们必须要注意，JSON格式的对象（JSON有对象（即字典）、数组、字符串、数字，bool值以及null等数据结构）的键必须是字符串。如果不是字符串将会在转换时出现错误提示:Json_documents=”””{23:”Wes”,”places_lived”:[“United States”,”Spain”,”Germany”,”Japan”],”pet”:null,”siblings”:[{“name”:”Scott”,”age”:25,”pet”:”Zuko”,”weight”:56,”height”:172},{“name”:”Katie”,”age”:33,”pet”:”Cisco”,”weight”:89,”height”:189}]}“”” getting_1=json.loads(Json_documents)JSONDecodeError Traceback (most recent call last) in ()—-&gt; 1 getting_1=json.loads(Json_documents) ~\Anaconda3\lib\json__init__.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw) 352 parse_int is None and parse_float is None and... 357 raise JSONDecodeError(“Expecting value”, s, err.value) from None JSONDecodeError: Expecting property name enclosed in double quotes: line 2 column 2 (char 2) 我们当然也可以把python格式的数据对象转化为JSON格式，使用python的函数json.dumps可以实现这一愿望：例子： python_format_1={‘Name’: ‘Wes’, ‘pet’: None, ‘places_lived’: [‘United States’, ‘Spain’, ‘Germany’, ‘Japan’], ‘siblings’: [{‘age’: 25, ‘height’: 172, ‘name’: ‘Scott’, ‘pet’: ‘Zuko’, ‘weight’: 56}, {‘age’: 33, ‘height’: 189, ‘name’: ‘Katie’, ‘pet’: ‘Cisco’, ‘weight’: 89}]} json_format_1=pd.json.dumps(python_format_1) json_format_1Out[6]:’{“Name”:”Wes”,”pet”:null,”places_lived”:[“United States”,”Spain”,”Germany”,”Japan”],”siblings”:[{“age”:25,”height”:172,”name”:”Scott”,”pet”:”Zuko”,”weight”:56},{“age”:33,”height”:189,”name”:”Katie”,”pet”:”Cisco”,”weight”:89}]}’ 再把json格式数据转化为python数据格式后，我们需要把数据作进一步处理，以便我们更好的处理数据。最常用的做法是把数据继续转化为数据框： newDataFrame=pd.DataFrame(python_format_1[‘siblings’]) newDataFrameOut[8]: age height name pet weight0 25 172 Scott Zuko 561 33 189 Katie Cisco 89python_format_1[‘siblings’]表示的是一个由字典组成的列表，通过它可以直接生成一个数据框，这是一个很好的数据框创建方法。我们下面一起做一些用这种模式创建数据框的练习： 例子1：dict_1={‘c1’:[1,2,3,4,5],’c2’:[21,65,32,90,0],’c3’:[90,67,5,6,3],’c4’:[1,2,1,2,0],’c5’:[9,0,8,7,0]} dict_2={‘c1’:[1,3,3,4,5],’c2’:[21,5,32,90,3],’c3’:[90,7,5,6,3],’c4’:[1,2,155,2,0],’c5’:[9,450,8,7,0]}newframe_0=pd.DataFrame(dict_1) newframe_0Out[19]: c1 c2 c3 c4 c50 1 21 90 1 91 2 65 67 2 02 3 32 5 1 83 4 90 6 2 74 5 0 3 0 0 例子2：newframe_0=pd.DataFrame([dict_1])newframe_0Out[17]: c1 c2 c3 c4 \0 [1, 2, 3, 4, 5] [21, 65, 32, 90, 0] [90, 67, 5, 6, 3] [1, 2, 1, 2, 0] c5 0 [9, 0, 8, 7, 0] 例子3：newframe_1=pd.DataFrame([dict_1,dict_2]):newframe_1Out[15]: c1 c2 c3 c4 \0 [1, 2, 3, 4, 5] [21, 65, 32, 90, 0] [90, 67, 5, 6, 3] [1, 2, 1, 2, 0]1 [1, 3, 3, 4, 5] [21, 5, 32, 90, 3] [90, 7, 5, 6, 3] [1, 2, 155, 2, 0] c5 0 [9, 0, 8, 7, 0]1 [9, 450, 8, 7, 0]例子4dic_3={‘c6’:[1,3,3,4,5],’c7’:[21,5,32,90,3],’c8’:[90,7,5,6,3],’c9’:[1,2,155,2,0],’c10’:[9,450,8,7,0]}newFrame_4=pd.DataFrame([dict_1,dic_3])newFrame_4Out[22]: c1 c10 c2 c3 \0 [1, 2, 3, 4, 5] NaN [21, 65, 32, 90, 0] [90, 67, 5, 6, 3]1 NaN [9, 450, 8, 7, 0] NaN NaN c4 c5 c6 c7 \ 0 [1, 2, 1, 2, 0] [9, 0, 8, 7, 0] NaN NaN1 NaN NaN [1, 3, 3, 4, 5] [21, 5, 32, 90, 3] c8 c9 0 NaN NaN1 [90, 7, 5, 6, 3] [1, 2, 155, 2, 0] #数据框的列索引由字典的键确定，字典列表中的有多少字典键就有多少列，譬如字典列表的第一个字典（0位置字典）有5个键，第二个字典（1位置字典）有5个键，那么生成的数据框就有10个列。字典列表的长度代表了行的数量，就如例子中字典列表的长度是2，那么数据框就有两行，0行和1行。 通过columns我们可以指定需要显示的数据列： newframe_5=pd.DataFrame([dict_1,dict_2],columns=[‘c2’,’c3’]) newframe_5Out[24]: c2 c30 [21, 65, 32, 90, 0] [90, 67, 5, 6, 3]1 [21, 5, 32, 90, 3] [90, 7, 5, 6, 3] 数据的二进制格式存储 定义：将对象转换为可通过网络传输或可以存储到本地磁盘的数据格式的过程称为序列化；反之，则称为反序列化。Python内置的pickle序列化用于实现Python数据类型与Python特定二进制格式之间的转换。import pandas as pdframe_1=pd.read_csv(‘Desktop\exampel_6.csv’)frame_1=pd.read_csv(‘Desktop\exampel_6.csv’,sep=’\s+’) frame_1Out[6]: A B C’aaa -3.450 2.360 8.90’bbb 0.334 0.457 -4.5’ccc 0.760 -7.340 -8.99’ddd 0.370 -7.800 -4.45’ frame_1.to_pickle(‘Desktop\frame_pickle’) #上面我们把frame_1存储成pickle格式我们也可以把数据读回到python。 import pickleload_file=open(‘Desktop\Frame_pickle’,’rb’) pickle.load(load_file)Out[17]: A B C’aaa -3.450 2.360 8.90’bbb 0.334 0.457 -4.5’ccc 0.760 -7.340 -8.99’ddd 0.370 -7.800 -4.45’ HDF5的存取 Hierarchical Data Format(HDF)是一种针对大量数据进行组织和存储的文件格式。它包含了数据模型，库，和文件格式标准。以其便捷有效，移植性强，灵活可扩展的特点受到了广泛的关注和应用。 对大数据的组织和存储得益于HDF文件的系统式节点结构，这样的结构不仅支持元数据，而且使多数据集存储成为现实。 Python中的HDF5库有两个接口Pytables和h5py。 我们先看一下h5py，h5py是python的一种工具包，它提供了一种直接而高级的HDF5 API访问接口。 import h5pyimport numpy as np #HDF5的写入Intentity_12=np.ones((10,12,300,305))data_h5py_1=h5py.File(‘Desktop\example_29-1.h5’,’w’) #创建一个h5文件data_h5py_1[‘data’]=Intentity_12 #把数据写入到data_h5py_1主键‘data’中data_h5py_1[‘labels’]=range(120) #把数据写入到data_h5py_1主键‘labels’中data_h5py_1.close() #HDF5的读取data_h5py_2=h5py.File(‘Desktop\example_29-1.h5’,’r’) #h5文件的读取key_content=data_h5py_2.keys() #所有键的查看abc_123=data_h5py_2[‘data’][:] #查看主键‘data’的内容data_h5py_2[‘labels’][:] #查看主键‘labels’的内容 我们再看另一种接口Pytables，Pytables也是python的一种工具包，它抽像了许多HDF5的细节以提供多种灵活数据容器，及实现表索引和查询功能。 import pandas as pdstore=pd.HDFStore(‘Desktop\example_30.h5’,’w’) #通过HDFStore创建一个h5文件getframe_1=pd.DataFrame(np.arange(16).reshape(4,4),index=[‘r1’,’r2’,’r3’,’r4’],columns=[‘c1’,’c2’,’c3’,’c4’])store[‘layer_1’]=getframe_1store[‘layer_1_1’]=getframe_1[‘c2’]storeOut[50]: File path: Desktop\example_30.h5/layer_1 frame (shape-&gt;[4,4])/layer_1_1 series (shape-&gt;[4])store[‘layer_1’][‘c4’]Out[51]:r1 3r2 7r3 11r4 15Name: c4, dtype: int32 注意：1.HDF5最好一次写入，不要多次写入,以免破坏文件2.创建H5文件时，各种文件类型。r：只能读r+：可读可写，不会创建不存在的文件。如果直接写文件，则从顶部开始写，覆盖之前此位置的内容，如果先读后写，则会在文件最后追加内容。w：只能写，覆盖整个文件，文件不存在则创建a：只能写，从文件底部添加内容，文件不存在则创建 数据的合并常用的数据合并函数有pandas.merge和pandas.contact以及combine_first。它们分别有着各自的应用条件。我们根据实际情况选取合适的函数来进行数据合并。 1.pandas.merge通过merge函数可以实现数据框的合并，但这种合并要经过一个或多个键的衔接，这就要求我们必须在数据框中专门添加一个‘Key’列用于衔接。例1：Frame_1=pd.DataFrame({‘schluessel’:[‘Nr.1’,’Nr.2’,’Nr.3’,’Nr.4’,’Nr.5’],’values_1’:[1,2,2,8,9],’values_2’:[4,7,9,0,1]}) Frame_2=pd.DataFrame({‘schluessel’:[‘Nr.1’,’Nr.1’,’Nr.3’,’Nr.3’,’Nr.5’],’values_1’:[1,2,2,8,9],’values_2’:[4,7,9,0,1]}) Frame_1Out[28]: schluessel values_1 values_20 Nr.1 1 41 Nr.2 2 72 Nr.3 2 93 Nr.4 8 04 Nr.5 9 1 Frame_2Out[29]: schluessel values_1 values_20 Nr.1 1 41 Nr.1 2 72 Nr.3 2 93 Nr.3 8 04 Nr.5 9 1 pd.merge(Frame_2,Frame_1,on=’schluessel’)Out[27]: schluessel values_1_x values_2_x values_1_y values_2_y0 Nr.1 1 4 1 41 Nr.1 2 7 1 42 Nr.3 2 9 2 93 Nr.3 8 0 2 94 Nr.5 9 1 9 1 多对一（重复键列对不重复键列）内连接注意事项每次合并前都要指定作为键的列，比如上面我们指定“schluessel”作为键的列如有重复键要保留，不要去重，看另一个待合并数据框有没有这个键，如果有，把相同的键合并到一起。如果某些键在各自数据框中都没有重复现象，但是两个数据框相比这些键相同，那么要把两个键合并到一起。如果某些键在各自数据框中都没有重复现象，且两个数据框相比这些键也不相同，那么这些键将不出现在合并后的键列里 注意：默认是内连接 如果键名不同，我们可以通过关键字“left_on”和“right_on”分别安置：frame_2=pd.DataFrame({‘rkey’:[‘a’,’c’,’d’],’data_2’:[3,2,1]}) frame_1=pd.DataFrame({‘lkey’:[‘a’,’a’,’c’,’a’,’b’,’c’,’d’,’b’],’data_1’:[2,6,9,3,2,1,0,5]}) pd.merge(frame_1,frame_2,left_on=’lkey’,right_on=’rkey’)Out[5]: data_1 lkey data_2 rkey0 2 a 3 a1 6 a 3 a2 3 a 3 a3 9 c 2 c4 1 c 2 c5 0 d 1 d 在默认的条件下，合并后的键是两个数据框的键的交集，例如上面的例子均是如此，这种键的合并方式被称作内连接，除了内连接还有左连接、右连接以及外连接。下满我们分别比较这几种方式： frame_2=pd.DataFrame({‘sl’:[‘a’,’c’,’d’,’f’],’data_2’:[3,2,1,7]})frame_1=pd.DataFrame({‘sl’:[‘a’,’a’,’c’,’a’,’b’,’c’,’d’,’b’],’data_1’:[2,6,9,3,2,1,0,5]}) pd.merge(frame_1,frame_2,on=’sl’,how=’inner’)Out[16]: data_1 sl data_20 2 a 31 6 a 32 3 a 33 9 c 24 1 c 25 0 d 1 #内连接是键的交集pd.merge(frame_1,frame_2,on=’sl’,how=’left’)Out[17]: data_1 sl data_20 2 a 3.01 6 a 3.02 9 c 2.03 3 a 3.04 2 b NaN5 1 c 2.06 0 d 1.07 5 b NaN pd.merge(frame_2,frame_1,on=’sl’,how=’left’)Out[6]: data_2 sl data_10 3 a 2.01 3 a 6.02 3 a 3.03 2 c 9.04 2 c 1.05 1 d 0.06 7 f NaN #左连接以第一个数据框的键列为合并参考键列，观察参考键列中的键是否在两个待合并数据框中有重复，有则在合并时也要重复。pd.merge(frame_1,frame_2,on=’sl’,how=’right’)Out[18]: data_1 sl data_20 2.0 a 31 6.0 a 32 3.0 a 33 9.0 c 24 1.0 c 25 0.0 d 16 NaN f 7 #右连接以第二个数据框的键列为合并参考键列，观察参考键列中的键是否在两个待合并数据框中有重复，有则在合并时也要重复。 pd.merge(frame_1,frame_2,on=’sl’)Out[19]: data_1 sl data_20 2 a 31 6 a 32 3 a 33 9 c 24 1 c 25 0 d 1 我们可以看到，“how=’inner’”和how在缺失的状况下运行结果是一样的。 下面我们讨论多对多键的操作： 内连接frame_2=pd.DataFrame({‘sl’:[‘a’,’c’,’a’,’d’,’f’],’data_2’:[3,2,1,7,6]})frame_1=pd.DataFrame({‘sl’:[‘a’,’a’,’c’,’a’,’b’,’c’,’d’,’b’],’data_1’:[2,6,9,3,2,1,0,5]}) pd.merge(frame_2,frame_1,on=’sl’)Out[9]: data_2 sl data_10 3 a 21 3 a 62 3 a 33 1 a 24 1 a 65 1 a 36 2 c 97 2 c 18 7 d 0 多对多键合并注意事项（内连接）: #指定合并键列 #作为键的列如有重复键要保留，不要去重，看另一个待合并数据框有没有这个键，如果有，把相同的键合并到一起并重复。如果在一个数据框重复的键在另一个数据框中也重复出现，那么合并后它重复的次数符合笛卡尔积。如果某些键在各自数据框中都没有重复现象，但是两个数据框相比这些键相同，那么要把两个键合并到一起。如果某些键在各自数据框中都没有重复现象，且两个数据框相比这些键也不相同，那么这些键将不出现在合并后的键列里 frame_2Out[17]: data_2 sl0 3 a1 2 c2 1 a3 7 d4 6 f frame_1Out[18]: data_1 sl0 2 a1 6 a2 9 c3 3 a4 2 b5 1 c6 0 d7 5 b pd.merge(frame_2,frame_1,on=’sl’,how=’left’)Out[16]: data_2 sl data_10 3 a 2.01 3 a 6.02 3 a 3.03 2 c 9.04 2 c 1.05 1 a 2.06 1 a 6.07 1 a 3.08 7 d 0.09 6 f NaN #左连接以第一个数据框的键列为合并参考键列，观察参考键列中的键是否在两个待合并数据框中有重复，有则在合并时也要重复。如果在一个数据框重复的键在另一个数据框中也重复出现，那么合并后它重复的次数符合笛卡尔积。pd.merge(frame_2,frame_1,on=’sl’,how=’right’)Out[19]: data_2 sl data_10 3.0 a 21 1.0 a 22 3.0 a 63 1.0 a 64 3.0 a 35 1.0 a 36 2.0 c 97 2.0 c 18 7.0 d 09 NaN b 210 NaN b 5 #右连接以第二个数据框的键列为合并参考键列，观察参考键列中的键是否在两个待合并数据框中有重复，有则在合并时也要重复。如果在一个数据框重复的键在另一个数据框中也重复出现，那么合并后它重复的次数符合笛卡尔积。 外连接组合了左连接和右连接的效果。pd.merge(frame_2,frame_1,on=’sl’,how=’outer’)Out[20]: data_2 sl data_10 3.0 a 2.01 3.0 a 6.02 3.0 a 3.03 1.0 a 2.04 1.0 a 6.05 1.0 a 3.06 2.0 c 9.07 2.0 c 1.08 7.0 d 0.09 6.0 f NaN10 NaN b 2.011 NaN b 5.0 通过上面的一系列例子，我们发现，两个数据框的键列如果不完全一致，或者键不唯一，将是一件很麻烦的事，因此，我建议同学们除非万不得已还是应该给两个待合并的数据框设置带有唯一键的完全相同的键列。 frame_2=pd.DataFrame({‘sl’:[‘a’,’b’,’c’,’d’,’f’],’data_2’:[3,2,1,7,6]})frame_1=pd.DataFrame({‘sl’:[‘a’,’b’,’c’,’d’,’f’],’data_1’:[2,6,9,3,2]}) pd.merge(frame_2,frame_1,on=’sl’)Out[22]: data_2 sl data_10 3 a 21 2 b 62 1 c 93 7 d 34 6 f 2 indirect_Data=pd.merge(frame_2,frame_1,on=’sl’)indirect_Data.reindex(columns=[‘data_1’,’data_2’,’sl’])Out[27]: data_1 data_2 sl0 2 3 a1 6 2 b2 9 1 c3 3 7 d4 2 6 f 带有双关键字列的数据框的合并 frame_doppel_1=pd.DataFrame({‘key1’:list(‘wwerrt’),’key2’:list(‘onoodn’),’data1’:[1,9,76,9,5,6]}) frame_doppel_2=pd.DataFrame({‘key1’:list(‘wwt’),’key2’:list(‘onn’),’data1’:[1,9,76]}) frame_doppel_2Out[31]: data1 key1 key20 1 w o1 9 w n2 76 t n frame_doppel_1Out[32]: data1 key1 key20 1 w o1 9 w n2 76 e o3 9 r o4 5 r d5 6 t n pd.merge(frame_doppel_1,frame_doppel_2,on=[‘key1’,’key2’],how=’outer’)Out[33]: data1_x key1 key2 data1_y0 1 w o 1.01 9 w n 9.02 76 e o NaN3 9 r o NaN4 5 r d NaN5 6 t n 76.0 pd.merge(frame_doppel_1,frame_doppel_2,on=[‘key1’,’key2’],how=’left’)Out[34]: data1_x key1 key2 data1_y0 1 w o 1.01 9 w n 9.02 76 e o NaN3 9 r o NaN4 5 r d NaN5 6 t n 76.0 pd.merge(frame_doppel_1,frame_doppel_2,on=[‘key1’,’key2’],how=’right’)Out[35]: data1_x key1 key2 data1_y0 1 w o 11 9 w n 92 6 t n 76 pd.merge(frame_doppel_1,frame_doppel_2,on=[‘key1’,’key2’])Out[36]: data1_x key1 key2 data1_y0 1 w o 11 9 w n 92 6 t n 76 #对于有双从键列的数据框，我们先在各自的数据框内把两个键列组成一个元组序列键列，然后按单键列处理即可。（当然，真正的实际原理并非如此）。 frame_123=pd.DataFrame({‘data’:[1,2,8,9,0,98,6,56],’key1’:list(‘aacabbed’)})frame_456=pd.DataFrame({‘data_1’:[2,67,1]},index=[‘a’,’b’,’e’]) frame_456Out[39]: data_1a 2b 67e 1 pd.merge(frame_123,frame_456,left_on=’key1’,right_index=True)Out[40]: data key1 data_10 1 a 21 2 a 23 9 a 24 0 b 675 98 b 676 6 e 1 pd.merge(frame_123,frame_456,left_on=’key1’,right_index=True,how=’outer’)Out[42]: data key1 data_10 1 a 2.01 2 a 2.03 9 a 2.02 8 c NaN4 0 b 67.05 98 b 67.06 6 e 1.07 56 d NaN firstar_1=pd.DataFrame({‘sl_1’:[‘doctor’,’doctor’,’doctor’,’patient’,’patient’],’sl_2’:[4,2,4,20,32],’data_1’:np.arange(5)})secondar_1=pd.DataFrame(np.arange(12).reshape(4,3),index=[[‘doctor’,’doctor’,’patient’,’patient’],[4,4,20,20]],columns=[‘day’,’hour’,’second’]) secondar_1Out[7]: day hour seconddoctor 4 0 1 2 4 3 4 5patient 20 6 7 8 20 9 10 11 firstar_1Out[8]: data_1 sl_1 sl_20 0 doctor 41 1 doctor 22 2 doctor 43 3 patient 204 4 patient 32 combination_1=pd.merge(firstar_1,secondar_1,left_on=[‘sl_1’,’sl_2’],right_index=True) combination_1Out[11]: data_1 sl_1 sl_2 day hour second0 0 doctor 4 0 1 20 0 doctor 4 3 4 52 2 doctor 4 0 1 22 2 doctor 4 3 4 53 3 patient 20 6 7 83 3 patient 20 9 10 11 对于带有重索引的数据框合并时，要先把重索引组成一个个元组对，然后与另一个数据框两个键列组成的元组队进行比较。上面的例子选出两个数据框元组对的交集。多对多相同时符合笛卡尔积 #下面是左连接效果pd.merge(firstar_1,secondar_1,left_on=[‘sl_1’,’sl_2’],right_index=True,how=’left’)Out[5]: data_1 sl_1 sl_2 day hour second0 0 doctor 4 0.0 1.0 2.00 0 doctor 4 3.0 4.0 5.01 1 doctor 2 NaN NaN NaN2 2 doctor 4 0.0 1.0 2.02 2 doctor 4 3.0 4.0 5.03 3 patient 20 6.0 7.0 8.03 3 patient 20 9.0 10.0 11.04 4 patient 32 NaN NaN NaN #下面是左连接效果pd.merge(firstar_1,secondar_1,left_on=[‘sl_1’,’sl_2’],right_index=True,how=’right’)Out[6]: data_1 sl_1 sl_2 day hour second0 0 doctor 4 0 1 22 2 doctor 4 0 1 20 0 doctor 4 3 4 52 2 doctor 4 3 4 53 3 patient 20 6 7 83 3 patient 20 9 10 11 #下面是外连接效果pd.merge(firstar_1,secondar_1,left_on=[‘sl_1’,’sl_2’],right_index=True,how=’outer’)Out[7]: data_1 sl_1 sl_2 day hour second0 0 doctor 4 0.0 1.0 2.00 0 doctor 4 3.0 4.0 5.02 2 doctor 4 0.0 1.0 2.02 2 doctor 4 3.0 4.0 5.01 1 doctor 2 NaN NaN NaN3 3 patient 20 6.0 7.0 8.03 3 patient 20 9.0 10.0 11.04 4 patient 32 NaN NaN NaN 同时使用双方索引直接合并也是没有问题的，我们看下例： DataFrame_1=pd.DataFrame([[2,89,0],[7,9,34],[3.4,5.6,8.9]],index=[‘a’,’g’,’f’],columns=[‘c1’,’c2’,’c3’]) DataFrame_2=pd.DataFrame([[8,45,6,8],[4,5,3,23],[45,90,6.9,0.7],[4.5,6.8,9.2,7.6]],index=[‘g’,’g’,’a’,’f’],columns=[‘c3’,’c4’,’c5’,’c6’])pd.merge(DataFrame_1,DataFrame_2,left_index=True,right_index=True,how=’outer’)Out[12]: c1 c2 c3_x c3_y c4 c5 c6a 2.0 89.0 0.0 45.0 90.0 6.9 0.7f 3.4 5.6 8.9 4.5 6.8 9.2 7.6g 7.0 9.0 34.0 8.0 45.0 6.0 8.0g 7.0 9.0 34.0 4.0 5.0 3.0 23.0 下面我们用join函数来实现按索引的合并：利用join按行索引合并时，两个待合并数据框的列索引不能有重复。DataFrame_2_cor=pd.DataFrame([[8,45,6,8],[4,5,3,23],[45,90,6.9,0.7],[4.5,6.8,9.2,7.6]],index=[‘g’,’g’,’c’,’b’],columns=[‘c7’,’c4’,’c5’,’c6’]) DataFrame_2_corOut[22]: c7 c4 c5 c6g 8.0 45.0 6.0 8.0g 4.0 5.0 3.0 23.0c 45.0 90.0 6.9 0.7b 4.5 6.8 9.2 7.6 DataFrame_1Out[23]: c1 c2 c3a 2.0 89.0 0.0g 7.0 9.0 34.0f 3.4 5.6 8.9 DataFrame_2_cor.join(DataFrame_1)Out[24]: c7 c4 c5 c6 c1 c2 c3b 4.5 6.8 9.2 7.6 NaN NaN NaNc 45.0 90.0 6.9 0.7 NaN NaN NaNg 8.0 45.0 6.0 8.0 7.0 9.0 34.0g 4.0 5.0 3.0 23.0 7.0 9.0 34.0 DataFrame_2_cor.join(DataFrame_1,how=’inner’)Out[25]: c7 c4 c5 c6 c1 c2 c3g 8.0 45.0 6.0 8.0 7.0 9.0 34.0g 4.0 5.0 3.0 23.0 7.0 9.0 34.0 DataFrame_2_cor.join(DataFrame_1,how=’right’)Out[26]: c7 c4 c5 c6 c1 c2 c3a NaN NaN NaN NaN 2.0 89.0 0.0f NaN NaN NaN NaN 3.4 5.6 8.9g 8.0 45.0 6.0 8.0 7.0 9.0 34.0g 4.0 5.0 3.0 23.0 7.0 9.0 34.0 DataFrame_2_cor.join(DataFrame_1,how=’outer’)Out[28]: c7 c4 c5 c6 c1 c2 c3a NaN NaN NaN NaN 2.0 89.0 0.0b 4.5 6.8 9.2 7.6 NaN NaN NaNc 45.0 90.0 6.9 0.7 NaN NaN NaNf NaN NaN NaN NaN 3.4 5.6 8.9g 8.0 45.0 6.0 8.0 7.0 9.0 34.0g 4.0 5.0 3.0 23.0 7.0 9.0 34.0 #其运行结果完全按照merge.只不过默认是how=’left’。 轴向连接在Numpy阶段我们讲过数组的合并，比如我们用concatenate连接两个数组： arr=np.arange(12).reshape(3,4) arrOut[3]:array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) np.concatenate((arr,arr),axis=0)Out[4]:array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) np.concatenate((arr,arr),axis=1)Out[5]:array([[ 0, 1, 2, 3, 0, 1, 2, 3], [ 4, 5, 6, 7, 4, 5, 6, 7], [ 8, 9, 10, 11, 8, 9, 10, 11]]) 接下来我们看一下两个或两个以上的Series是如何合并的。ps_1=pd.Series([0,1],index=[‘a’,’b’]) ps_2=pd.Series([6,2],index=[‘c’,’d’]) ps_3=pd.Series([7,0,8,6,0],index=[‘e’,’f’,’g’,’h’,’k’]) pd.concat((ps_1,ps_2,ps_3))Out[10]:a 0b 1c 6d 2e 7f 0g 8h 6k 0dtype: int64接下来我们使 ’axis=1’，看看结果如何（注意：通常情况下，对于Series的轴是不允许其为“1”的）：pd.concat((ps_1,ps_2,ps_3),axis=1)Out[11]: 0 1 2a 0.0 NaN NaNb 1.0 NaN NaNc NaN 6.0 NaNd NaN 2.0 NaNe NaN NaN 7.0f NaN NaN 0.0g NaN NaN 8.0h NaN NaN 6.0k NaN NaN 0.0 pd.concat((ps_2,ps_4))Out[16]:c 6d 2a 0b 8c 6d 2dtype: int64 pd.concat((ps_2,ps_4),axis=1)Out[17]: 0 1a NaN 0b NaN 8c 6.0 6d 2.0 2 pd.concat((ps_2,ps_4),axis=1,join=’inner’)Out[18]: 0 1c 6 6d 2 2 我们还可以给合并后的Series指定索引名：pd.concat((ps_2,ps_4),axis=1,join_axes=[[‘a’,’b’,’u’,’v’]])Out[19]: 0 1a NaN 0.0b NaN 8.0u NaN NaNv NaN NaN 通过key参数可以实现把Series合并成层次化Series：pd.concat((ps_1,ps_2*3,ps_3),keys=[‘ein’,’drei’,’fuenf’])Out[6]:ein a 0 b 1drei c 18 d 6fuenf e 7 f 0 g 8 h 6 k 0dtype: int64这里如果我们添加axis=1，结果会如何呢？ ein drei fuenfa 0.0 NaN NaNb 1.0 NaN NaNc NaN 18.0 NaNd NaN 6.0 NaNe NaN NaN 7.0f NaN NaN 0.0g NaN NaN 8.0h NaN NaN 6.0k NaN NaN 0.0 我们发现此时原本Series的外层索引变成列索引。 对于有重复行索引的Series合并后结果会如何呢？ 很不幸，无法运行！ 到此，我们需要做一个用concat合并Series的总结：总结：待合并的每个series中不能出现重复的行标签对于axis=0，Series合并后的效果是行标签的直接组合。对于axis=1，要视合并模式而定，默认合并模式join=“outer”，按照单个的Seires，一列一列地排下去，join=“inner”模式意味着在outer模式的基础上只保留个Series共有部分，其他全删除。 下面我们把contact应用到数据框 import pandas as pdimport numpy as npframe_1=pd.DataFrame([[1000,2000],[279,1123],[721,877]],index=[‘salary’,’expenditure’,’surplus’],columns=[‘mattias’,’jennifer’])frame_2=pd.DataFrame(np.array([[1000,2000,3000],[279,1123,2000],[721,877,1000]])*12,index=[‘salary per annum’,’expenditure per annum’,’surplus per annum’],columns=[‘mattias’,’jennifer’,’jocker’]) pd.concat((frame_1,frame_2),axis=1,keys=[‘works’,’boss’])Out[2]: works boss mattias jennifer mattias jennifer jockerexpenditure 279.0 1123.0 NaN NaN NaNexpenditure per annum NaN NaN 3348.0 13476.0 24000.0salary 1000.0 2000.0 NaN NaN NaNsalary per annum NaN NaN 12000.0 24000.0 36000.0surplus 721.0 877.0 NaN NaN NaNsurplus per annum NaN NaN 8652.0 10524.0 12000.0 pd.concat((frame_1,frame_2),axis=0,keys=[‘works’,’boss’])Out[3]: jennifer jocker mattiasworks salary 2000 NaN 1000 expenditure 1123 NaN 279 surplus 877 NaN 721boss salary per annum 24000 36000.0 12000 expenditure per annum 13476 24000.0 3348 surplus per annum 10524 12000.0 8652视axis=1和 =0 而定，Keys可以给数据框的列或行加上重索引 Keys所实现的功能也可以通过字典来实现： pd.concat({‘works’:frame_1,’boss’:frame_2},axis=1)Out[6]: boss works mattias jennifer jocker mattias jenniferexpenditure NaN NaN NaN 279.0 1123.0expenditure per annum 3348.0 13476.0 24000.0 NaN NaNsalary NaN NaN NaN 1000.0 2000.0salary per annum 12000.0 24000.0 36000.0 NaN NaNsurplus NaN NaN NaN 721.0 877.0surplus per annum 8652.0 10524.0 12000.0 NaN NaN通过names可以给重索引进行命名：pd.concat({‘works’:frame_1,’boss’:frame_2},axis=1,names=[‘Identity’,’Names’])Out[7]:Identity boss worksNames mattias jennifer jocker mattias jenniferexpenditure NaN NaN NaN 279.0 1123.0expenditure per annum 3348.0 13476.0 24000.0 NaN NaNsalary NaN NaN NaN 1000.0 2000.0salary per annum 12000.0 24000.0 36000.0 NaN NaNsurplus NaN NaN NaN 721.0 877.0surplus per annum 8652.0 10524.0 12000.0 NaN NaN 通过Ignore_index=Ture可以把自行设置的行或列索引转变成python自动配置的行或列索引。pd.concat((frame_1,frame_2),axis=0,ignore_index=True)Out[11]: jennifer jocker mattias0 2000 NaN 10001 1123 NaN 2792 877 NaN 7213 24000 36000.0 120004 13476 24000.0 33485 10524 12000.0 8652 pd.concat((frame_1,frame_2),axis=1,ignore_index=True)Out[12]: 0 1 2 3 4expenditure 279.0 1123.0 NaN NaN NaNexpenditure per annum NaN NaN 3348.0 13476.0 24000.0salary 1000.0 2000.0 NaN NaN NaNsalary per annum NaN NaN 12000.0 24000.0 36000.0surplus 721.0 877.0 NaN NaN NaNsurplus per annum NaN NaN 8652.0 10524.0 12000.0 如果axis=1，那么join是按0轴操作；同理，axis=1，那么join是按1轴操作。 pd.concat((frame_1,frame_2),axis=1,join=’outer’)Out[16]: mattias jennifer mattias jennifer jockerexpenditure 279.0 1123.0 NaN NaN NaNexpenditure per annum NaN NaN 3348.0 13476.0 24000.0salary 1000.0 2000.0 NaN NaN NaNsalary per annum NaN NaN 12000.0 24000.0 36000.0surplus 721.0 877.0 NaN NaN NaNsurplus per annum NaN NaN 8652.0 10524.0 12000.0 pd.concat((frame_1,frame_2),axis=0,join=’inner’)Out[17]: mattias jennifersalary 1000 2000expenditure 279 1123surplus 721 877salary per annum 12000 24000expenditure per annum 3348 13476surplus per annum 8652 10524 ag_1=pd.DataFrame(np.floor(np.random.randn(4,4)),index=list(‘gfas’),columns=list(‘xcvb’)) ag_2=pd.DataFrame(np.floor(np.random.randint(2,8,12).reshape(4,3)),index=[‘xu’,’de’,’jiang’,’hu’],columns=list(‘xvb’)) pd.concat((ag_1,ag_2),keys=[‘Gr’,’A’],axis=1,join_axes=[ag_1.index]) Out[64]: Gr A x c v b x v bg -2.0 0.0 0.0 1.0 NaN NaN NaNf -2.0 -1.0 0.0 -1.0 NaN NaN NaNa -2.0 0.0 0.0 2.0 NaN NaN NaNs 0.0 -2.0 -1.0 -2.0 NaN NaN NaN ag_1=pd.DataFrame(np.floor(np.random.randn(4,4)),index=list(‘gfas’),columns=list(‘xcvb’)) ag_2=pd.DataFrame(np.floor(np.random.randint(2,8,12).reshape(4,3)),index=[‘xu’,’de’,’jiang’,’hu’],columns=list(‘xvb’)) pd.concat((ag_1,ag_2),keys=[‘Gr’,’A’],axis=1,join_axes=[ag_2.index]) Out[65]: Gr A x c v b x v bxu NaN NaN NaN NaN 3.0 3.0 4.0de NaN NaN NaN NaN 5.0 2.0 6.0jiang NaN NaN NaN NaN 7.0 2.0 3.0hu NaN NaN NaN NaN 7.0 2.0 6.0 ag_1=pd.DataFrame(np.floor(np.random.randn(4,4)),index=list(‘gfas’),columns=list(‘xcvb’)) ag_2=pd.DataFrame(np.floor(np.random.randint(2,8,12).reshape(4,3)),index=[‘xu’,’de’,’jiang’,’hu’],columns=list(‘xvb’)) pd.concat((ag_1,ag_2),keys=[‘Gr’,’A’],axis=0,join_axes=[ag_2.columns]) Out[67]: x v bGr g -1.0 -1.0 0.0 f -1.0 -1.0 -1.0 a 1.0 -1.0 1.0 s 0.0 -1.0 -1.0A xu 7.0 3.0 3.0 de 6.0 6.0 3.0 jiang 2.0 7.0 5.0 hu 7.0 5.0 7.0 ag_1=pd.DataFrame(np.floor(np.random.randn(4,4)),index=list(‘gfas’),columns=list(‘xcvb’)) ag_2=pd.DataFrame(np.floor(np.random.randint(2,8,12).reshape(4,3)),index=[‘xu’,’de’,’jiang’,’hu’],columns=list(‘xvb’)) pd.concat((ag_1,ag_2),keys=[‘Gr’,’A’],axis=0,join_axes=[ag_1.columns]) Out[68]: x c v bGr g 0.0 0.0 -1.0 2.0 f 0.0 -1.0 0.0 -2.0 a 0.0 -1.0 -2.0 0.0 s -1.0 -2.0 0.0 -1.0A xu 2.0 NaN 2.0 6.0 de 6.0 NaN 4.0 2.0 jiang 7.0 NaN 6.0 5.0 hu 4.0 NaN 7.0 6.0 Concat的参数详解：objs 参与连接的pandas对象的列表或者元组或者字典。是唯一必需的参数。axis 指明按那个轴进行连接，默认为0join 指明连接方式，“inner”或”outer”，默认“outer”。指明其他轴向上的索引是按交集（“inner”）还是并集(“outer”)进行合并。keys 与连接对象有关的值，用于形成连接轴向上的层次化索引。可以是任意值的列表或数组、元组数组、数组列表等。join_axes 指定根据那个数据框的索引来对齐数据，这个索引不参与并\交运算Names 给层索引每一层命名Ignore_index 不保留连接轴上的索引，产生一组新索引 给数据打补丁：尽管我们学习各种各种的数据合并与连接（merge、join、concatenation等）。然而他们都仅仅是对索引的直接处理。很难对Series或者数据框所包含的数据直接处理。通过np.where,可以实现数值的补空合并。 ser_1=pd.Series([np.nan,45,np.nan,3.9,90,np.nan],index=list(‘abcdef’))ser_2=pd.Series(np.arange(len(ser_1)),index=list(‘abcdef’)) ser_1Out[73]:a NaNb 45.0c NaNd 3.9e 90.0f NaNdtype: float64 ser_2Out[74]:a 0b 1c 2d 3e 4f 5dtype: int32 pd.Series(np.where(pd.isnull(ser_1),ser_2,ser_1),index=list(‘abcdef’))Out[79]:a 0.0b 45.0c 2.0d 3.9e 90.0f 5.0dtype: float64 通过combine_first来实现相同功能：ser_1.combine_first(ser_2)Out[80]:a 0.0b 45.0c 2.0d 3.9e 90.0f 5.0dtype: float64 #拿fra_2值补fra_1 fra_1=pd.DataFrame({‘a’:[1.,np.nan,5.,np.nan],’b’:[np.nan,2.,np.nan,6.],’c’:range(2,9,2)}) fra_2=pd.DataFrame({‘a’:[3,np.nan,6,3,8],’b’:[np.nan,np.nan,2,6,8]}) fra_1Out[84]: a b c0 1.0 NaN 21 NaN 2.0 42 5.0 NaN 63 NaN 6.0 8 fra_2Out[85]: a b0 3.0 NaN1 NaN NaN2 6.0 2.03 3.0 6.04 8.0 8.0 fra_1.combine_first(fra_2)Out[86]: a b c0 1.0 NaN 2.01 NaN 2.0 4.02 5.0 2.0 6.03 3.0 6.0 8.04 8.0 8.0 NaN #拿fra_2值补fra_1数据的重塑（仔细读注解额）数据的重塑其本质就是对数据表格进行重排。所用到的函数通常被称为重塑函数或轴向旋转函数。 重塑层次化索引： 带有层次化索引数据框的重塑主要是通过以下函数来实现的：Stack：将列索引旋转为行索引Unstack：将行索引旋转为列索引注意：以上只适合重索引数据框我们先从一个简单的数据框开始。data_1=pd.DataFrame(np.arange(6).reshape(2,3),index=pd.Index([‘XDL_1’,’XDL_2’],name=’education_group’),columns=pd.Index([‘one’,’three’,’two’],name=’nr.’)) data_1Out[4]:nr. one three twoeducation_groupXDL_1 0 1 2XDL_2 3 4 5 data_1.stack()Out[5]:education_group nr.XDL_1 one 0 three 1 two 2XDL_2 one 3 three 4 two 5dtype: int32 #对于没有重索引的数据框，用stack方法会实现列索引成为内层行索引data_1.unstack()Out[14]:nr. education_groupone XDL_1 0 XDL_2 3three XDL_1 1 XDL_2 4two XDL_1 2 XDL_2 5dtype: int32 #对于没有重索引的数据框，用unstack方法会实现列索引成为外层行索引data_double_ser=data_1.stack() #生成重索引seriesdata_double_serOut[21]:education_group nr.XDL_1 one 0 three 1 two 2XDL_2 one 3 three 4 two 5dtype: int32data_double_ser.unstack()Out[6]:nr. one three twoeducation_groupXDL_1 0 1 2XDL_2 3 4 5 #通过unstack把内层行索引旋转到列索引，默认旋转内层级行索引到列索引。data_1.stack().unstack(0)Out[10]:education_group XDL_1 XDL_2nr.one 0 3three 1 4two 2 5 #默认按内层索引操作，我们可以指定操作级别。这里的‘0’代表外层级别。也可以用外层索引名来代替‘0’data_1.stack().unstack(‘education_group’)Out[13]:education_group XDL_1 XDL_2nr.one 0 3three 1 4two 2 5 #指定重索引行外层级，并通过unstack把外层行索引旋转到列索引 s1=pd.Series([0,3,4,-3],index=list(‘abcd’)) s2=pd.Series([6,2,9],index=list(‘bca’)) data_ser_double=pd.concat((s1,s2),keys=[‘one’,’two’]) data_ser_doubleOut[25]:one a 0 b 3 c 4 d -3two b 6 c 2 a 9dtype: int64 data_ser_double.unstack()Out[26]: a b c done 0.0 3.0 4.0 -3.0two 9.0 6.0 2.0 NaN #如果S1和S2的索引数目不等或者数目相等但索引内容不同都会产生空值，因此我们要求S1和S2的索引必须完全一致，才能在运算中不产生空值！！ #产生空值的情况下，如果我们设置dropna=True（默认）那么上面的运算可逆。如果我们设置dropna=False的情况下，上面的运算时不可逆的。 data_ser_double.unstack().stack()Out[28]:one a 0.0 b 3.0 c 4.0 d -3.0two a 9.0 b 6.0 c 2.0dtype: float64 data_ser_double.unstack().stack(dropna=False)Out[29]:one a 0.0 b 3.0 c 4.0 d -3.0two a 9.0 b 6.0 c 2.0 d NaNdtype: float64 移除重复数据data=pd.DataFrame({‘k1’:[‘one’]3+[‘two’]4,’k2’:[1,2,3,4,5,6,7]}) dataOut[8]: k1 k20 one 11 one 22 one 33 two 44 two 55 two 66 two 7 利用duplicated可以判断重复行 #默认k1和k2列必须都重复才算重复行data.duplicated()Out[5]:0 False1 False2 False3 False4 False5 False6 Falsedtype: bool data=pd.DataFrame({‘k1’:[‘one’]3+[‘two’]4,’k2’:[2,3,2,3,6,6,6]}) dataOut[8]: k1 k20 one 21 one 32 one 23 two 34 two 65 two 66 two 6 data.duplicated()Out[9]:0 False1 False2 True3 False4 False5 True6 Truedtype: bool #注意，原项不会判断为True，只有真正的重复项才会，如上例子，红色不会判断为True，蓝色才会判断为True。 #通过drop_duplicates方法，可以移除重复行，返回一个没有重复行的DataFrame。data.drop_duplicates()Out[11]: k1 k20 one 21 one 33 two 34 two 6 #可以指定某一列或几列中行重复就算重复行data[‘k3’],data[‘k4’],data[‘k5’]=[0,0,0,0,3,6,7],[4,8,6,7,6,7,7],[2,2,4,4,6,6,8]dataOut[20]: k1 k2 k3 k4 k50 one 2 0 4 21 one 3 0 8 22 one 2 0 6 43 two 3 0 7 44 two 6 3 6 65 two 6 6 7 66 two 6 7 7 8 data.drop_duplicates([‘k1’])Out[22]: k1 k2 k3 k4 k50 one 2 0 4 23 two 3 0 7 4 data.drop_duplicates([‘k1’,’k2’])Out[24]: k1 k2 k3 k4 k50 one 2 0 4 21 one 3 0 8 23 two 3 0 7 44 two 6 3 6 6 dataOut[27]: k1 k2 k3 k4 k50 one 2 0 4 21 one 3 0 8 22 one 2 0 6 43 two 3 0 7 44 two 6 3 6 65 two 6 6 7 66 two 6 7 7 8 data.drop_duplicates([‘k1’],keep=’last’)Out[29]: k1 k2 k3 k4 k52 one 2 0 6 46 two 6 7 7 8 data.duplicated([‘k1’],keep=’last’)Out[34]:0 True1 True2 False3 True4 True5 True6 Falsedtype: bool #默认保留重复行的第一行，也可以设置保留最后一个，keep=’last’,删除其他所有重复行。 利用字典映射对数据进行操作： data_1=pd.DataFrame({‘food’:[‘bacon火腿’,’sausage红肠’,’pulled pork手撕肉’,’sirloin牛里脊肉’,’beef jerky牛肉干’,’mutton shashlik烤羊肉串’,’Chicken salad鸡肉’,’Minced chicken鸡肉泥’,’dried squids鱿鱼干’,’dried fish鱼干’,’sausage红肠’,’bacon火腿’,’sausage红肠’,’bacon火腿’,’Chicken salad鸡肉’,’dried squids鱿鱼干’,’dried fish鱼干’,’mutton shashlik烤羊肉串’,’beef jerky牛肉干’,’Minced chicken鸡肉泥’,’pulled pork手撕肉’,’sirloin牛里脊肉’,’mutton shashlik烤羊肉串’,’Chicken salad鸡肉’,’Minced chicken鸡肉泥’,’bacon火腿’,’sausage红肠’,’pulled pork手撕肉’,’sirloin牛里脊肉’,’beef jerky牛肉干’,’mutton shashlik烤羊肉串’,’Chicken salad鸡肉’,’Minced chicken鸡肉泥’,’dried squids鱿鱼干’,’dried fish鱼干’,’sausage红肠’,’bacon火腿’,’sausage红肠’,’bacon火腿’,’Chicken salad鸡肉’,’dried squids鱿鱼干’,’dried fish鱼干’,’mutton shashlik烤羊肉串’,’beef jerky牛肉干’,’sirloin牛里脊肉’,’mutton shashlik烤羊肉串’,’Chicken salad鸡肉’,’Minced chicken鸡肉泥’],’weight(g)’:abs((np.random.normal(0,1,size=48)*24+10)).astype(np.int32)}) data_1Out[4]: food weight(g)0 bacon火腿 271 sausage红肠 412 pulled pork手撕肉 163 sirloin牛里脊肉 124 beef jerky牛肉干 05 mutton shashlik烤羊肉串 76 Chicken salad鸡肉 317 Minced chicken鸡肉泥 148 dried squids鱿鱼干 39 dried fish鱼干 3810 sausage红肠 3411 bacon火腿 1212 sausage红肠 913 bacon火腿 2314 Chicken salad鸡肉 3415 dried squids鱿鱼干 116 dried fish鱼干 1917 mutton shashlik烤羊肉串 618 beef jerky牛肉干 019 Minced chicken鸡肉泥 320 pulled pork手撕肉 2221 sirloin牛里脊肉 922 mutton shashlik烤羊肉串 1423 Chicken salad鸡肉 3424 Minced chicken鸡肉泥 3725 bacon火腿 2726 sausage红肠 027 pulled pork手撕肉 3228 sirloin牛里脊肉 029 beef jerky牛肉干 3830 mutton shashlik烤羊肉串 1631 Chicken salad鸡肉 3532 Minced chicken鸡肉泥 433 dried squids鱿鱼干 534 dried fish鱼干 335 sausage红肠 2636 bacon火腿 1237 sausage红肠 238 bacon火腿 539 Chicken salad鸡肉 1140 dried squids鱿鱼干 641 dried fish鱼干 2442 mutton shashlik烤羊肉串 2643 beef jerky牛肉干 5644 sirloin牛里脊肉 245 mutton shashlik烤羊肉串 546 Chicken salad鸡肉 1247 Minced chicken鸡肉泥 13 我们发现数据框‘food’列中有很多重复。如果我们给数据假如新列，并要求新列与food存在某种逻辑关系。例如给‘food’标记食材来源。通过map函数可以很轻松实现上面的要求。 meat_source={‘bacon火腿’:’猪’,’sausage红肠’:’猪’,’pulled pork手撕肉’:’猪’,’sirloin牛里脊肉’:’牛’,’beef jerky牛肉干’:’牛’,’mutton shashlik烤羊肉串’:’羊’,’chicken salad鸡肉’:’鸡’,’minced chicken鸡肉泥’:’鸡’,’dried squids鱿鱼干’:’鱿鱼’,’dried fish鱼干’:’鱼’}data_1[‘animal’]=data_1[‘food’].map(meat_source)data_1Out[25]: food weight(g) animal0 bacon火腿 17 猪1 sausage红肠 38 猪2 pulled pork手撕肉 3 猪3 sirloin牛里脊肉 12 牛4 beef jerky牛肉干 31 牛5 mutton shashlik烤羊肉串 16 羊6 Chicken salad鸡肉 17 NaN7 Minced chicken鸡肉泥 14 NaN8 dried squids鱿鱼干 12 鱿鱼9 dried fish鱼干 7 鱼10 sausage红肠 25 猪11 bacon火腿 30 猪12 sausage红肠 12 猪13 bacon火腿 32 猪14 Chicken salad鸡肉 10 NaN15 dried squids鱿鱼干 42 鱿鱼16 dried fish鱼干 8 鱼17 mutton shashlik烤羊肉串 13 羊18 beef jerky牛肉干 34 牛19 Minced chicken鸡肉泥 44 NaN20 pulled pork手撕肉 3 猪21 sirloin牛里脊肉 25 牛22 mutton shashlik烤羊肉串 4 羊23 Chicken salad鸡肉 39 NaN24 Minced chicken鸡肉泥 20 NaN25 bacon火腿 5 猪26 sausage红肠 38 猪27 pulled pork手撕肉 28 猪28 sirloin牛里脊肉 12 牛29 beef jerky牛肉干 21 牛30 mutton shashlik烤羊肉串 13 羊31 Chicken salad鸡肉 13 NaN32 Minced chicken鸡肉泥 26 NaN33 dried squids鱿鱼干 43 鱿鱼34 dried fish鱼干 14 鱼35 sausage红肠 25 猪36 bacon火腿 19 猪37 sausage红肠 24 猪38 bacon火腿 57 猪39 Chicken salad鸡肉 25 NaN40 dried squids鱿鱼干 38 鱿鱼41 dried fish鱼干 18 鱼42 mutton shashlik烤羊肉串 54 羊43 beef jerky牛肉干 24 牛44 sirloin牛里脊肉 15 牛45 mutton shashlik烤羊肉串 17 羊46 Chicken salad鸡肉 61 NaN47 Minced chicken鸡肉泥 29 NaN #map的实质是把一种映射法则应用到所制定的列上，比如这里的‘food’列。这种映射法则通常通过字典与函数来体现。 #map把与meat_source字典的键相对应food值一个个投影到字典建对应的值然后把投影结果赋值给data_1[‘animal’] #我们发现‘animal’列有空值。原因是meat_source中的键与‘food’列值不完全一致。 #通过map(str.lower)把food列中的值的第一个字母转变成小写，这样meat_source中的键与‘food’列值完全一致，这样Nan会自动消失。 data_1[‘animal’]=data_1[‘food’].map(lambda x:meat_source[x.lower()])或data_1[‘animal’]=data_1[‘food’].map(str.lower).map(meat_source) data_1Out[23]: food weight(g) animal0 bacon火腿 17 猪1 sausage红肠 38 猪2 pulled pork手撕肉 3 猪3 sirloin牛里脊肉 12 牛4 beef jerky牛肉干 31 牛5 mutton shashlik烤羊肉串 16 羊6 Chicken salad鸡肉 17 鸡7 Minced chicken鸡肉泥 14 鸡8 dried squids鱿鱼干 12 鱿鱼9 dried fish鱼干 7 鱼10 sausage红肠 25 猪11 bacon火腿 30 猪12 sausage红肠 12 猪13 bacon火腿 32 猪14 Chicken salad鸡肉 10 鸡15 dried squids鱿鱼干 42 鱿鱼16 dried fish鱼干 8 鱼17 mutton shashlik烤羊肉串 13 羊18 beef jerky牛肉干 34 牛19 Minced chicken鸡肉泥 44 鸡20 pulled pork手撕肉 3 猪21 sirloin牛里脊肉 25 牛22 mutton shashlik烤羊肉串 4 羊23 Chicken salad鸡肉 39 鸡24 Minced chicken鸡肉泥 20 鸡25 bacon火腿 5 猪26 sausage红肠 38 猪27 pulled pork手撕肉 28 猪28 sirloin牛里脊肉 12 牛29 beef jerky牛肉干 21 牛30 mutton shashlik烤羊肉串 13 羊31 Chicken salad鸡肉 13 鸡32 Minced chicken鸡肉泥 26 鸡33 dried squids鱿鱼干 43 鱿鱼34 dried fish鱼干 14 鱼35 sausage红肠 25 猪36 bacon火腿 19 猪37 sausage红肠 24 猪38 bacon火腿 57 猪39 Chicken salad鸡肉 25 鸡40 dried squids鱿鱼干 38 鱿鱼41 dried fish鱼干 18 鱼42 mutton shashlik烤羊肉串 54 羊43 beef jerky牛肉干 24 牛44 sirloin牛里脊肉 15 牛45 mutton shashlik烤羊肉串 17 羊46 Chicken salad鸡肉 61 鸡47 Minced chicken鸡肉泥 29 鸡 添加amount=weightprice项product=[‘bacon火腿’,’sausage红肠’,’pulled pork手撕肉’,’sirloin牛里脊肉’,’beef jerky牛肉干’,’mutton shashlik烤羊肉串’,’Chicken salad鸡肉’,’Minced chicken鸡肉泥’,’dried squids鱿鱼干’,’dried fish鱼干’]price=[1,1.5,2,2.3,2.6,3.6,2,2.6,8,9.9]get_Data={k:pi for i,k in zip(list(data_1[‘weight(g)’].values),range(len(data_1[‘weight(g)’]))) for m,p in zip(product,price) if m==data_1[‘food’][k]}data_1[‘key’]=np.arange(48)data_1[‘anount’]=data_1[‘key’].map(get_Data)data_1.reindex(columns=[‘food’,’weight(g)’, ‘animal’, ‘anount’,’key’ ])Out[108]: food weight(g) animal anount key0 bacon火腿 17 猪 17.0 01 sausage红肠 38 猪 57.0 12 pulled pork手撕肉 3 猪 6.0 23 sirloin牛里脊肉 12 牛 27.6 34 beef jerky牛肉干 31 牛 80.6 45 mutton shashlik烤羊肉串 16 羊 57.6 56 Chicken salad鸡肉 17 鸡 34.0 67 Minced chicken鸡肉泥 14 鸡 36.4 78 dried squids鱿鱼干 12 鱿鱼 96.0 89 dried fish鱼干 7 鱼 69.3 910 sausage红肠 25 猪 37.5 1011 bacon火腿 30 猪 30.0 1112 sausage红肠 12 猪 18.0 1213 bacon火腿 32 猪 32.0 1314 Chicken salad鸡肉 10 鸡 20.0 1415 dried squids鱿鱼干 42 鱿鱼 336.0 1516 dried fish鱼干 8 鱼 79.2 1617 mutton shashlik烤羊肉串 13 羊 46.8 1718 beef jerky牛肉干 34 牛 88.4 1819 Minced chicken鸡肉泥 44 鸡 114.4 1920 pulled pork手撕肉 3 猪 6.0 2021 sirloin牛里脊肉 25 牛 57.5 2122 mutton shashlik烤羊肉串 4 羊 14.4 2223 Chicken salad鸡肉 39 鸡 78.0 2324 Minced chicken鸡肉泥 20 鸡 52.0 2425 bacon火腿 5 猪 5.0 2526 sausage红肠 38 猪 57.0 2627 pulled pork手撕肉 28 猪 56.0 2728 sirloin牛里脊肉 12 牛 27.6 2829 beef jerky牛肉干 21 牛 54.6 2930 mutton shashlik烤羊肉串 13 羊 46.8 3031 Chicken salad鸡肉 13 鸡 26.0 3132 Minced chicken鸡肉泥 26 鸡 67.6 3233 dried squids鱿鱼干 43 鱿鱼 344.0 3334 dried fish鱼干 14 鱼 138.6 3435 sausage红肠 25 猪 37.5 3536 bacon火腿 19 猪 19.0 3637 sausage红肠 24 猪 36.0 3738 bacon火腿 57 猪 57.0 3839 Chicken salad鸡肉 25 鸡 50.0 3940 dried squids鱿鱼干 38 鱿鱼 304.0 4041 dried fish鱼干 18 鱼 178.2 4142 mutton shashlik烤羊肉串 54 羊 194.4 4243 beef jerky牛肉干 24 牛 62.4 4344 sirloin牛里脊肉 15 牛 34.5 4445 mutton shashlik烤羊肉串 17 羊 61.2 4546 Chicken salad鸡肉 61 鸡 122.0 4647 Minced chicken鸡肉泥 29 鸡 75.4 47 灵活的替换值方法：前面我们讲过fillna替换空值，还有notnull也可以实现空值替换。这里我们继续推荐一种另外的替换方法-replace法。这种方法不仅简单而且还十分灵活。 #把那些标记为空值的数据替换成pandas能够理解的Na值 data=pd.Series([1.,-999.,2.,999.,-1000.,3.]) dataOut[115]:0 1.01 -999.02 2.03 999.04 -1000.05 3.0dtype: float64 #假如-999是空值标记 data.replace(-999,np.nan)Out[117]:0 1.01 NaN2 2.03 999.04 -1000.05 3.0dtype: float64 #也可以一次替换多值 data.replace([-999,-1000],np.nan)Out[119]:0 1.01 NaN2 2.03 999.04 NaN5 3.0dtype: float64data_1=[23,’invalue’,34]data_1=pd.Series(data_1)data_1Out[124]:0 231 invalue2 34dtype: objectdata_1.replace(‘invalue’,2)Out[122]:0 231 22 34dtype: int64data_1=pd.Series([1.,-999.,2.,-999.,-1000.,3.]) data_1Out[129]:0 1.01 -999.02 2.03 -999.04 -1000.05 3.0dtype: float64data_1.replace([-999,-1000],[np.nan,0])Out[126]:0 1.01 NaN2 2.03 NaN4 0.05 3.0dtype: float64 #也可以表示成字典形式 data_1.replace({-999:np.nan,-1000:0})Out[130]:0 1.01 NaN2 2.03 NaN4 0.05 3.0dtype: float64 重命名轴索引data=pd.DataFrame(np.arange(12).reshape(3,4),index=[‘a’,’b’,’c’],columns=[‘c1’,’c2’,’c3’,’c4’]) #Map可以通过字典把要处理的列（键值）映射到对应的值，也可以通过函数把相应的列映射到对应的值 #通过str.upper函数map把行索引列映射到大写行索引列。 data.index=data.index.map(str.upper) dataOut[137]: c1 c2 c3 c4A 0 1 2 3B 4 5 6 7C 8 9 10 11 #可以用rename对轴索引进行修改data.rename(index=str.title,columns=str.upper)Out[5]: C1 C2 C3 C4A 0 1 2 3B 4 5 6 7C 8 9 10 11 #rename通过字典的帮助可实现对部分标签的更新 data.rename(index={‘a’:’Sun’,’b’:’Zhang’,’c’:’Wang’},columns={‘c2’:’CNr.’}) data.rename(index={‘b’:’Zhang’,’c’:’Wang’},inplace=True) dataOut[11]: c1 c2 c3 c4a 0 1 2 3Zhang 4 5 6 7Wang 8 9 10 11 #通过inplace=True，可以改变原数据 数据的离散和面元划分有些时候我们需要把连续数据离散化或拆分成面元。面元指的是一段按组拆分后的数据。面元化就是把数据进行分组。例： ages=[20,22,25,27,26,31,78,45,20,19,67,34,56,66,59,43] #面元化的实现 bin_nr=[min(ages),35,46,57,63,max(ages)]bin_nrOut[15]: [19, 35, 46, 57, 63, 78]cut_1=pd.cut(ages,bin_nr)cut_1Out[17]:[(19, 35], (19, 35], (19, 35], (19, 35], (19, 35], …, (19, 35], (46, 57], (63, 78], (57, 63], (35, 46]]Length: 16Categories (5, interval[int64]): [(19, 35] &lt; (35, 46] &lt; (46, 57] &lt; (57, 63] &lt; (63, 78]] #对数据分组后的编号是：agesOut[28]: [20, 22, 25, 27, 26, 31, 78, 45, 20, 19, 67, 34, 56, 66, 59, 43]cut_1.codesOut[24]: array([ 0, 0, 0, 0, 0, 0, 4, 1, 0, -1, 4, 0, 2, 4, 3, 1], dtype=int8)我们发现分组编号中有-1值，这是‘19’这个值造成的，因为我们的分组组限不包括‘19’这个值。解决办法有两个：改变bin_nrbin_nr=[18,min(ages),35,46,57,63,max(ages)]cut_1=pd.cut(ages,bin_nr)cut_1Out[35]:[(19, 35], (19, 35], (19, 35], (19, 35], (19, 35], …, (19, 35], (46, 57], (63, 78], (57, 63], (35, 46]]Length: 16Categories (6, interval[int64]): [(18, 19] &lt; (19, 35] &lt; (35, 46] &lt; (46, 57] &lt; (57, 63] &lt; (63, 78]]cut_1.codesOut[38]: array([1, 1, 1, 1, 1, 1, 5, 2, 1, 0, 5, 1, 3, 5, 4, 2], dtype=int8)修改参数：bin_nr=[min(ages),35,46,57,63,max(ages)]pd.cut(ages,bin_nr,include_lowest=True)Out[41]:[(18.999, 35.0], (18.999, 35.0], (18.999, 35.0], (18.999, 35.0], (18.999, 35.0], …, (18.999, 35.0], (46.0, 57.0], (63.0, 78.0], (57.0, 63.0], (35.0, 46.0]]Length: 16Categories (5, interval[float64]): [(18.999, 35.0] &lt; (35.0, 46.0] &lt; (46.0, 57.0] &lt; (57.0, 63.0] &lt; (63.0, 78.0]]cut2=pd.cut(ages,bin_nr,include_lowest=True)cut2.codesOut[43]: array([0, 0, 0, 0, 0, 0, 4, 1, 0, 0, 4, 0, 2, 4, 3, 1], dtype=int8)agesOut[45]: [20, 22, 25, 27, 26, 31, 78, 45, 20, 19, 67, 34, 56, 66, 59, 43] #显示分组组限：cut_1.categoriesOut[31]:IntervalIndex([(19, 35], (35, 46], (46, 57], (57, 63], (63, 78]] closed=’right’, dtype=’interval[int64]’) #统计各组频数pd.value_counts(cut2)Out[55]:(18.999, 35.0] 9(63.0, 78.0] 3(35.0, 46.0] 2(57.0, 63.0] 1(46.0, 57.0] 1dtype: int64 #right=False可改变组限区间闭区间的位置；bin_nr=[min(ages),35,46,57,63,max(ages),79]pd.cut(ages,bin_nr,right=False)Out[50]:[[19, 35), [19, 35), [19, 35), [19, 35), [19, 35), …, [19, 35), [46, 57), [63, 78), [57, 63), [35, 46)]Length: 16Categories (6, interval[int64]): [[19, 35) &lt; [35, 46) &lt; [46, 57) &lt; [57, 63) &lt; [63, 78) &lt; [78, 79)]cut4=pd.cut(ages,bin_nr,right=False)cut4.codesOut[52]: array([0, 0, 0, 0, 0, 0, 5, 1, 0, 0, 4, 0, 2, 4, 3, 1], dtype=int8) #通过labels给面元设置名称bin_nr=[min(ages),35,46,57,63,max(ages)]pd.cut(ages,bin_nr,include_lowest=True)cut2=pd.cut(ages,bin_nr,include_lowest=True,labels=Name_group)cut2Out[62]:[青年, 青年, 青年, 青年, 青年, …, 青年, 中年, 少老年, 老中年, 壮年]Length: 16Categories (5, object): [青年 &lt; 壮年 &lt; 中年 &lt; 老中年 &lt; 少老年] #如果给bins参数赋整数值，意味着传入的数值是面元数量，计算机会根据整个数据最大值和最小值计算等长面元。bin_nr=[min(ages),35,46,57,63,max(ages)]pd.cut(ages,bin_nr,include_lowest=True)cut2=pd.cut(ages,6,include_lowest=True)cut2Out[64]:[(18.94, 28.833], (18.94, 28.833], (18.94, 28.833], (18.94, 28.833], (18.94, 28.833], …, (28.833, 38.667], (48.5, 58.333], (58.333, 68.167], (58.333, 68.167], (38.667, 48.5]]Length: 16Categories (6, interval[float64]): [(18.94, 28.833] &lt; (28.833, 38.667] &lt; (38.667, 48.5] &lt; (48.5, 58.333] &lt; (58.333, 68.167] &lt; (68.167, 78.0]]pd.value_counts(cut2)Out[68]:(18.94, 28.833] 7(58.333, 68.167] 3(38.667, 48.5] 2(28.833, 38.667] 2(68.167, 78.0] 1(48.5, 58.333] 1dtype: int64 #等数据点面元化 data_random=np.random.randn(1000) cut_12=pd.qcut(data_random,8) cut_12Out[71]:[(-1.193, -0.735], (-0.043, 0.297], (0.297, 0.628], (-1.193, -0.735], (-0.365, -0.043], …, (-3.453, -1.193], (-0.365, -0.043], (0.628, 1.122], (-0.735, -0.365], (1.122, 2.735]]Length: 1000Categories (8, interval[float64]): [(-3.453, -1.193] &lt; (-1.193, -0.735] &lt; (-0.735, -0.365] &lt; (-0.365, -0.043] &lt; (-0.043, 0.297] &lt; (0.297, 0.628] &lt; (0.628, 1.122] &lt; (1.122, 2.735]]pd.value_counts(cut_12)Out[72]:(1.122, 2.735] 125(0.628, 1.122] 125(0.297, 0.628] 125(-0.043, 0.297] 125(-0.365, -0.043] 125(-0.735, -0.365] 125(-1.193, -0.735] 125(-3.453, -1.193] 125dtype: int64 #自定义分位数（必须是0-1之间的分位数，包括端点，因为这里是按百分比分位）cut_12=pd.qcut(data_random,[0,0.3,0.6,0.85,1])cut_12Out[80]:[(-3.453, -0.573], (-0.573, 0.218], (0.218, 0.987], (-3.453, -0.573], (-0.573, 0.218], …, (-3.453, -0.573], (-0.573, 0.218], (0.218, 0.987], (-3.453, -0.573], (0.987, 2.735]]Length: 1000Categories (4, interval[float64]): [(-3.453, -0.573] &lt; (-0.573, 0.218] &lt; (0.218, 0.987] &lt; (0.987, 2.735]]pd.value_counts(cut_12)Out[81]:(-0.573, 0.218] 300(0-0.3)(-3.453, -0.573] 300(0.3-0.6)(0.218, 0.987] 250(0.6-0.85)(0.987, 2.735] 150(0.85-1)dtype: int64 检测和过滤异常值Outlier（out来了）异常值。如何检测那些值是异常值呢？我们来看一个例子：import pandas as pd import numpy as np np.random.seed(12876) data=pd.DataFrame(np.random.randn(1000,4)) data.describe()Out[5]: 0 1 2 3count 1000.000000 1000.000000 1000.000000 1000.000000mean -0.032748 0.017600 0.005716 0.052615std 1.029352 1.013877 0.974347 0.990795min -3.558590 -3.399723 -3.714028 -3.11386725% -0.731609 -0.654785 -0.703925 -0.60436650% -0.079017 0.001919 0.020646 0.04190275% 0.686783 0.688564 0.685262 0.703313max 3.040923 2.809213 2.827195 3.389950 #根据各列的最大值和最小值对各列数值进行过滤c_1=data[2]c_1[np.abs(c_1)&gt;3]Out[8]:382 -3.002094466 -3.714028Name: 2, dtype: float64 #对整个数据框中所有&gt;3的值进行过滤Step1:np.abs(data)&gt;3Out[43]: 0 1 2 30 False False False False1 False False False False2 False False False False3 False False False False4 False False False False5 False False False False6 False False False False7 False False False False8 False False False False9 False False False False10 False False False False11 False False False False12 False False False False13 False False False False14 False False False False15 False False False False16 False False False False17 False False False False18 False False False False19 False False False False20 False False False False21 False False False False22 False False False False23 False False False False24 False False False False25 False False False False26 False False False False27 False False False False28 False False False False29 False False False False.. … … … …970 False False False False971 False False False False972 False False False False973 False False False False974 False False False False975 False False False False976 False False False False977 False False False False978 False False False False979 False False False False980 False False False False981 False False False False982 False False False False983 False False False False984 False False False False985 False False False False986 False False False False987 False False False False988 False False False False989 False False False False990 False False False False991 False False False False992 False False False False993 False False False False994 False False False False995 False False False False996 False False False False997 False False False False998 False False False False999 False False False False [1000 rows x 4 columns]Step2: #逐列按行，把含有true值的每一行转化成true，把不含的行转成false，返回series (np.abs(data)&gt;3).any(1)Out[46]:0 False1 False2 False3 False4 False5 False6 False7 False8 False9 False10 False11 False12 False13 False14 False15 False16 False17 False18 False19 False20 False21 False22 False23 False24 False25 False26 False27 False28 False29 False 970 False971 False972 False973 False974 False975 False976 False977 False978 False979 False980 False981 False982 False983 False984 False985 False986 False987 False988 False989 False990 False991 False992 False993 False994 False995 False996 False997 False998 False999 FalseLength: 1000, dtype: boolStep3: #放到索引位置直接索引行data[(np.abs(data)&gt;3).any(1)]Out[16]: 0 1 2 382 -1.535879 -1.077352 1.293144 3.058112270 -1.131598 -0.273447 -0.979915 -3.113867281 0.022546 -0.526429 0.361507 3.021792283 -0.469536 0.122044 -1.188901 3.389950327 -0.676235 0.155461 -0.706008 3.026979340 3.040923 1.046905 -0.375365 -0.572458358 -3.558590 -1.343718 1.411491 1.001404382 -1.947519 0.556118 -3.002094 0.701169466 -1.817789 -0.187283 -3.714028 -0.049148859 0.906164 -3.399723 -0.360720 0.245022注意：1.参数bool_only意味着只接受布尔值，如果是空值的话，计算机会检验数据，然后仅仅接收是布尔值的数据。如果是False，非布尔值数据参与比较，且自动被设置为True值参与比较。例如：TM_1=TM.astype(object) TM_1.loc[0]=3 TM_1Out[60]: 0 1 2 30 3 3 3 31 False False False False2 False False False False3 False False False False4 False False False False5 False False False False6 False False False False7 False False False False8 False False False False9 False False False False10 False False False False11 False False False False12 False False False False13 False False False False14 False False False False15 False False False False16 False False False False17 False False False False18 False False False False19 False False False False20 False False False False21 False False False False22 False False False False23 False False False False24 False False False False25 False False False False26 False False False False27 False False False False28 False False False False29 False False False False.. … … … …970 False False False False971 False False False False972 False False False False973 False False False False974 False False False False975 False False False False976 False False False False977 False False False False978 False False False False979 False False False False980 False False False False981 False False False False982 False False False False983 False False False False984 False False False False985 False False False False986 False False False False987 False False False False988 False False False False989 False False False False990 False False False False991 False False False False992 False False False False993 False False False False994 False False False False995 False False False False996 False False False False997 False False False False998 False False False False999 False False False False [1000 rows x 4 columns] TM_1.any(1,bool_only=True)Out[61]:0 False1 False2 False3 False4 False5 False6 False7 False8 False9 False10 False11 False12 False13 False14 False15 False16 False17 False18 False19 False20 False21 False22 False23 False24 False25 False26 False27 False28 False29 False 970 False971 False972 False973 False974 False975 False976 False977 False978 False979 False980 False981 False982 False983 False984 False985 False986 False987 False988 False989 False990 False991 False992 False993 False994 False995 False996 False997 False998 False999 FalseLength: 1000, dtype: bool TM_1.any(1,bool_only=False)Out[62]:0 True1 False2 False3 False4 False5 False6 False7 False8 False9 False10 False11 False12 False13 False14 False15 False16 False17 False18 False19 False20 False21 False22 False23 False24 False25 False26 False27 False28 False29 False 970 False971 False972 False973 False974 False975 False976 False977 False978 False979 False980 False981 False982 False983 False984 False985 False986 False987 False988 False989 False990 False991 False992 False993 False994 False995 False996 False997 False998 False999 FalseLength: 1000, dtype: bool 2.如果我们这里使用any（0），将会发生错误，原因是返回值仅仅是一个下面的series，把它放到索引位。肯定无法进行索引 (np.abs(data)&gt;3).any(0)Out[36]:0 True1 True2 True3 Truedtype: booldata[(np.abs(data)&gt;3).any(0)]C:\Users\dongfeng\Anaconda3\lib\site-packages\ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index. “””Entry point for launching an IPython kernel.IndexingError Traceback (most recent call last) in ()—-&gt; 1 data[(np.abs(data)&gt;3).any(0)] ~\Anaconda3\lib\site-packages\pandas\core\frame.py in getitem(self, key) 1956 if isinstance(key, (Series, np.ndarray, Index, list)): 1957 # either boolean or fancy integer index-&gt; 1958 return self._getitem_array(key) 1959 elif isinstance(key, DataFrame): 1960 return self._getitem_frame(key) ~\Anaconda3\lib\site-packages\pandas\core\frame.py in _getitem_array(self, key) 1996 # check_bool_indexer will throw exception if Series key cannot 1997 # be reindexed to match DataFrame rows-&gt; 1998 key = check_bool_indexer(self.index, key) 1999 indexer = key.nonzero()[0] 2000 return self.take(indexer, axis=0, convert=False) ~\Anaconda3\lib\site-packages\pandas\core\indexing.py in check_bool_indexer(ax, key) 1937 mask = isnull(result._values) 1938 if mask.any():-&gt; 1939 raise IndexingError(‘Unalignable boolean Series provided as ‘ 1940 ‘indexer (index of the boolean Series and of ‘ 1941 ‘the indexed object do not match’) IndexingError: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match 当然我们可以修改，把其放在列位，但这样的话等于什么也没有做，仅仅选取所有列data.loc[:,(np.abs(data)&gt;3).any(0)]Out[40]: 0 1 2 30 -0.567046 -0.491171 -0.827547 -0.6804081 0.206551 -1.456287 0.629825 0.1487322 -1.130370 -0.056857 1.431013 1.4757643 -2.125800 0.108440 -1.350079 0.5237284 -0.736537 -0.846202 0.543960 -1.2300805 -0.152478 0.087115 -0.304004 0.0503986 -0.479466 -1.990843 -0.361344 1.0966107 -0.246875 0.595030 1.469306 -0.4385508 0.989888 0.174695 0.241461 -0.5509269 -0.781855 1.660456 1.041865 0.26448010 1.661492 0.356601 -0.699093 0.86971911 0.163815 0.246831 -0.250914 -0.69441512 -1.584598 -1.279383 -1.274184 -0.00827713 -0.266142 1.401936 0.397230 -0.08785614 0.016147 0.710451 1.548756 0.29243615 -0.885661 -0.126581 0.927745 -0.19474816 -0.018011 0.384556 0.545346 0.44732317 -1.785552 0.372100 0.014757 -2.02319918 0.764790 0.900823 -1.039617 1.11181019 -0.603835 0.951399 -1.166977 2.09161420 0.227416 0.606907 -0.533333 0.52442021 0.728552 -0.138527 -1.922079 -2.68375622 -1.777072 -0.610157 -1.141227 -0.52748823 -0.393001 1.573780 -0.270537 -0.55258824 0.142684 -0.237119 -2.089615 0.98205225 0.048209 -0.858588 1.238078 0.60515626 -0.736681 0.402256 -0.247741 -0.54938527 -0.183541 -0.654161 0.539174 -0.59749128 0.641096 -1.708915 0.660372 0.51012229 -0.174187 -0.809638 0.032858 -1.431953.. … … … …970 -0.868857 2.001313 0.852164 0.589976971 -0.557336 0.318217 -0.546586 1.061976972 -0.884070 -0.641405 -0.210885 0.049715973 0.785917 0.212696 1.193491 1.007289974 0.921965 -0.478275 -1.417855 0.911799975 0.131626 0.050366 0.637130 0.267212976 -1.425079 0.923022 -0.208123 -0.697405977 1.723595 -0.022576 -0.954601 -0.374849978 -2.049331 1.803200 -0.894918 0.955312979 0.082268 2.073896 0.072605 0.912109980 -0.166275 -0.463344 1.510438 -0.478526981 0.438580 0.540105 1.023839 0.441328982 -0.260641 1.145658 0.062698 1.655832983 -0.176418 -0.032755 0.906118 2.012923984 -0.870494 0.038007 -0.371591 -0.262396985 -0.683679 -0.182418 0.683105 -0.852608986 -0.641252 1.387388 1.770705 -1.003002987 -2.111872 -0.438135 0.669196 -0.642335988 1.279690 -0.179545 -1.728947 0.837643989 -0.378165 0.507085 -0.106542 0.511993990 0.741188 -0.681142 -0.192551 -0.712643991 1.045758 0.238560 -0.713991 -0.463895992 -1.052059 -0.942410 -0.062352 0.267082993 -0.184833 1.474266 2.345032 1.189559994 -0.245563 -0.513430 -0.998651 -0.858027995 0.745093 0.478441 1.731140 0.848215996 -0.445829 -0.022585 -0.621001 -1.348158997 -1.479772 0.688092 -1.209812 -1.247304998 -0.260688 0.373783 -0.384794 1.073226999 -0.395423 0.955142 0.525140 1.178641 #把所有数据限制到-3到3的范围内。data[np.abs(data)&gt;3]=np.sign(data)*3#左边实现绝对值大于3的值的选取，即设置了x&gt;3或x&lt;-3的选区。右边是符号与左边选区数值对应，绝对值数值为3的数据。通过=赋值到选区的相应位置。count_frame=pd.DataFrame([pd.value_counts(np.abs(data[0])&gt;3),pd.value_counts(np.abs(data[1])&gt;3),pd.value_counts(np.abs(data[2])&gt;3),pd.value_counts(np.abs(data[3])&gt;3)]) count_frameOut[77]: False0 10001 10002 10003 1000 排列与随机采样 通过permutation函数可以实现对数据框或者Series的随机排列。asr=np.random.randint(-6,6,15).reshape(5,3)asr_fra=pd.DataFrame(asr)asr_fraOut[17]: 0 1 20 -2 1 01 -2 4 32 -5 1 43 0 4 14 4 4 -1np.random.permutation(asr_fra)Out[18]:array([[-5, 1, 4], [-2, 4, 3], [ 4, 4, -1], [ 0, 4, 1], [-2, 1, 0]]) #其实质仅仅是对第一列数据进行排列，其他列随着第一列移动 #下面对Series进行随机排列ser1=pd.Series([3,2,7,9,0,2,1,9,2,2,3,12,17])np.random.permutation(ser1)Out[22]: array([ 9, 7, 3, 0, 2, 3, 17, 2, 12, 2, 1, 9, 2], dtype=int64) #我们也可以通过产生一个随机排列数组，对数组进行排列 as_frame=pd.DataFrame(np.random.randint(-12,20,42).reshape(7,6)) as_frameOut[26]: 0 1 2 3 4 50 -5 13 10 14 -2 51 -10 13 -9 10 -4 -72 6 11 0 -2 -8 93 0 -11 -8 -2 -12 164 3 -12 -5 18 8 35 0 3 4 4 5 106 7 0 6 -7 3 -10 pattern=np.random.permutation(6)patternOut[29]: array([5, 4, 3, 0, 1, 2])as_frame.take(pattern,0)Out[28]: 0 1 2 3 4 55 0 3 4 4 5 104 3 -12 -5 18 8 33 0 -11 -8 -2 -12 160 -5 13 10 14 -2 51 -10 13 -9 10 -4 -72 6 11 0 -2 -8 9注意这里返回的结果是as_frame的一个子数据框，这是因为pattern的结果是从从0到5的一个排列，数据框的行索引是按照这个pattern的这个结果进行排列。这里也可以按列索引进行排列as_frame.take(pattern,1)Out[31]: 5 4 3 0 1 20 5 -2 14 -5 13 101 -7 -4 10 -10 13 -92 9 -8 -2 6 11 03 16 -12 -2 0 -11 -84 3 8 18 3 -12 -55 10 5 4 0 3 46 -10 3 -7 7 0 6as_frame.take(np.random.permutation([4,3,0,1]),1)#可以按照索引子集的排列进行列的重排Out[33]: 3 4 0 10 14 -2 -5 131 10 -4 -10 132 -2 -8 6 113 -2 -12 0 -114 18 8 3 -125 4 5 0 36 -7 3 7 0as_frame.take(np.random.permutation(as_frame.shape[1])[:4],1)Out[37]: 0 2 4 50 -5 10 -2 51 -10 -9 -4 -72 6 0 -8 93 0 -8 -12 164 3 -5 8 35 0 4 5 106 7 6 3 -10 通过random.randint和take函数就行随机抽样 五，绘图和可视化Matplotlib入门，pandas下的绘图函数，画布，画布分割等等Matplotlib入门调用Matplotlib API 程序包 1，创建画布import matplotlib as plt import matplotlib.pyplot as plt figure_1=plt.figure()(或者详细定义画布figure_1=plt.figure(1,(4,6),dpi=400,facecolor=’green’,edgecolor=’yellow’,frameon=True)）figure_1Out[43]: &lt;matplotlib.figure.Figure at 0x8ea7987320&gt; 2，分割画布 aapic_1=figure_1.add_subplot(2,2,1) aapic_2=figure_1.add_subplot(2,2,2) aapic_3=figure_1.add_subplot(2,2,3) aapic_4=figure_1.add_subplot(2,2,4) 3，show 出你的画布 plt.show() import matplotlib.pyplot as plt import numpy as np figure_1=plt.figure() aapic_1=figure_1.add_subplot(2,2,1) aapic_2=figure_1.add_subplot(2,2,2) aapic_3=figure_1.add_subplot(2,2,3) plt.plot(np.random.randn(50).cumsum(),’k–’)Out[7]: [&lt;matplotlib.lines.Line2D at 0xdd16defd0&gt;] #直接用“plot”绘图，k–代表黑色虚线图 aapic_2.hist(np.random.randn(100),bins=20,color=’k’,alpha=0.3)Out[9]:(array([ 1., 1., 1., 5., 3., 7., 8., 14., 11., 6., 11., 6., 13., 5., 2., 3., 0., 1., 0., 2.]), array([-2.64526298, -2.37172471, -2.09818644, -1.82464817, -1.5511099 , -1.27757163, -1.00403336, -0.73049509, -0.45695681, -0.18341854, 0.09011973, 0.363658 , 0.63719627, 0.91073454, 1.18427281, 1.45781108, 1.73134935, 2.00488762, 2.27842589, 2.55196416, 2.82550243]), ) aapic_1.scatter(np.arange(30),np.arange(30)+3*np.random.randn(30))Out[10]: &lt;matplotlib.collections.PathCollection at 0xdd172c390&gt; #alpha代表透明度，bins代表柱状体个数 plt.showOut[12]: plt.show() 这里还有一个简单且更为方便的画图方法；它可以创建一个新的figure（画布）并返回一个含有以创建的subplot对象的numpy数组。pic,axes=plt.subplots(2,3)plt.show(0) 饼图：import numpy as npimport matplotlib.pyplot as pltAsx,sd=plt.subplots(1,1)sd.pie(np.arange(4,9),explode=[0.2,0.1,0.3,0.4,0.3],labels=[‘zhang’,’wang’,’li’,’zhao’,’liu’],colors=[‘m’,’r’,’g’,’c’,’b’],autopct=’%.2f%%’,pctdistance=1,shadow=True,labeldistance=1.6,startangle=30,radius=1,frame=True,rotatelabels=True)Plt.show() Asx,sd=plt.subplots(1,1)sd.pie(np.arange(4,9),explode=[0,0,0,0,0],labels=[‘zhang’,’wang’,’li’,’zhao’,’liu’],colors=[‘m’,’r’,’g’,’c’,’b’],autopct=’%.2f%%’,pctdistance=1,shadow=True,labeldistance=0.5,startangle=30,radius=1,frame=True,rotatelabels=True)plt.show() #labeldistance&lt;1,图例将会在饼图内 #通过figsize=（6,6）把饼图设置成圆的，然后通过textprops设置字体，通过labeldistance设置标签离圆心距离。通过autopct设置每个部分总总体的百分数，通过pctdistance设置百分数例圆心距离，通过explode设置一个部分的强调。Asx,sd=plt.subplots(1,1,figsize=(6,6))sd.pie(np.arange(4,9),explode=[0,0.1,0,0,0],labels=[‘zhang’,’wang’,’li’,’zhao’,’liu’],colors=[‘m’,’r’,’g’,’c’,’b’],autopct=’%.2f%%’,pctdistance=0.4,shadow=True,labeldistance=0.5,wedgeprops=None,textprops={‘fontsize’:14},startangle=0,radius=1,frame=True,rotatelabels=None)plt.show() 调整Subplot周围的间距： 默认情况下，子图外围是有一定边距的，并且各子图之间上下左右都有一定间距。 间距与子图大小有关，子图像宽，则横向间距小，图像高，则纵向间距小。 除了默认，我们可以选择自己确定间距： plt.subplots_adjust(left=None,bottom=None,right=None,top=None,wspace=None,hspace=None) #wspace和hspace用于控制宽度和高度的百分比，通过调整这两个参数，我们的子图间的上下左右间距会发生变化 例子： import numpy as npfig, axes=plt.subplots(2, 2, sharex=True, sharey=True)for i in range(2): for j in range(2): axes[i,j].hist(np.random.randn(500),color=’k’,alpha=0.5)plt.subplots_adjust(wspace=2,hspace=1) plt.show(0) &lt;matplotlib.figure.Figure at 0x17becd9908&gt; &lt;matplotlib.figure.Figure at 0x17befbfda0&gt; #shareX 和sharey是共X轴和Y轴。调整wspace 盒hspace可以得到不同的子图间距。for i in range(2): for j in range(2): axes[i,j].hist(np.random.randn(500),color=’k’,alpha=0.5)plt.subplots_adjust(wspace=0.2,hspace=0.1) plt.show(0) &lt;matplotlib.figure.Figure at 0x17becd9908&gt; &lt;matplotlib.figure.Figure at 0x17befbfda0&gt; 问：上面图像的X轴和Y轴分别指的是什么？？？？ 颜色和线型以及标记指定图形函数线的颜色，我们可以通过一个指令实现，例：Pic_11.plot(X,Y,’g–’) 等价于 ax.plot(x,y,linestyle=’–’,color=’g’) 更多颜色可使用指定其RGB值的形式。对于线性图，我们可以给数据点加上标记(marker)，使人更容易发现那些是数据点。 线型和marker表格：‘-‘ solid line style‘–’ dashed line style ‘-.’ dash-dot line style‘:’ dotted line style‘.’ point marker‘,’ pixel marker‘o’ circle marker‘v’ triangle_down marker‘^’ triangle_up marker ‘&lt;’ triangle_left marker‘&gt;’ triangle_right marker‘1’ tri_down marker‘2’ tri_up marker‘3’ tri_left marker‘4’ tri_right marker ‘s’ square marker‘p’ pentagon marker‘*’ star marker‘h’ hexagon1 marker‘H’ hexagon2 marker‘+’ plus marker‘x’ x marker‘D’ diamond marker ‘d’ thin_diamond marker‘|’ vline marker‘_’ hline marker颜色表格：‘b’ blue‘g’ green‘r’ red‘c’ cyan‘m’ magenta‘y’ yellow‘k’ black‘w’ white 美丽温馨的例子：marker_1=[‘.’, ‘,’, ‘o’, ‘v’, ‘^’, ‘&lt;’, ‘&gt;’, ‘1’, ‘2’, ‘3’, ‘4’,’s’,’p’,’‘,’h’,’H’,’+’,’x’,’D’,’d’,’|’, ‘_’]asd,fid_1=plt.subplots(len(marker_1),dpi=180,figsize=(6,6len(marker_1)))for i in range(len(marker_1)): N = 50 # 点的个数 x = np.random.rand(N) 2 # 随机产生50个0~2之间的x坐标 y = np.random.rand(N) 2 # 随机产生50个0~2之间的y坐标 colors = np.random.rand(N) # 随机产生50个0~1之间的颜色值 area = np.pi (15 np.random.rand(N))**2 # 点的半径范围:0~15 # 画散点图 fid_1[i].scatter(x, y, s=area, c=colors, alpha=0.5, marker=marker_1[i]) fid_1[i].set_xlabel(marker_1[i]) plt.show() 如果不涉及子图的话，无需先设置画布。可直接画出图形：import numpy as npimport matplotlib.pyplot as pltplt.plot(np.random.randn(30).cumsum(),’g*–’)Out[3]: [&lt;matplotlib.lines.Line2D at 0xb407f4dd30&gt;]plt.show() plt.plot(np.random.randn(30).cumsum(),’g–’) 等价于Plt.plot(np.random.randn(30).cumsum(),color=’g’,linestyle=’dashed’,marker=’’) 在线性图中，那些非数据点都是根据两个数据点连线插值的，我们可以修改这种插值方式，这里用到drawstyle选项修改： data=np.array([1,2,3.2,2.3,4.6,7.5,2,3,6.5,7.8,9],dtype=float)np.unique(data)Out[9]: array([ 1. , 2. , 2.3, 3. , 3.2, 4.6, 6.5, 7.5, 7.8, 9. ])dataOut[10]: array([ 1. , 2. , 3.2, 2.3, 4.6, 7.5, 2. , 3. , 6.5, 7.8, 9. ])data_uique=np.unique(data)plt.plot(data_uique,’o-.’,label=’line_point’)Out[28]: [&lt;matplotlib.lines.Line2D at 0xb40947c908&gt;]plt.plot(data_uique,’r-‘,drawstyle=’steps-post’,label=’line’)Out[29]: [&lt;matplotlib.lines.Line2D at 0xb40947cf98&gt;] plt.legend(loc=’best’)Out[30]: &lt;matplotlib.legend.Legend at 0xb4083ca358&gt;plt.show() legend函数介绍：在画一些曲线图时，常常会出现多条曲线同时画在一张图上面，这时候就需要对不同的曲线进行不同的标注，以使读者能够清晰地知道每条曲线代表的含义。当你画很少的几条曲线时，这时画图命令中自动产生的legend能够基本满足你的需要，此时，你不需要做什么；但当你将很多个曲线画在一张图上时，自动产生的legend矩形框往往会覆盖住已经画出来的曲线，很不美观，这时你就需要写专门的代码对legend的位置进行精确的控制，而不能再依靠系统帮你自动控制了。 比如：plt.legend(loc=’upper center’, bbox_to_anchor=(0.6,0.95),ncol=3,fancybox=True,shadow=True)Ncol=表示我们的图例（legend）里的线的标识可以排成三列Loc=标识图例的位置bbox_to_anchor=图例的精确位置，上面bbox_to_anchor被赋予的二元组中，第一个数值用于控制legend的左右移动，值越大越向右边移动，第二个数值用于控制legend的上下移动，值越大，越向上移动。 刻度与标签图像的刻度与标签都是通过一些方法来实现的，这里有几个方法大家在绘图中经常用到： Xlim（X值范围）、xticks（X轴刻度值）和xticklabels（X轴刻度标签） 其使用方式有以下两种:1.调用不带参数值，返回当前参数值，即是现在正用的参数值。2.调用时带参数值，使用该参数值。 例： table_1=plt.figure() ax=table_1.add_subplot(1,1,1) ax.plot(np.random.randn(1000).cumsum())Out[36]: [&lt;matplotlib.lines.Line2D at 0xb409514550&gt;] ax.plot(np.random.randn(100000).cumsum())Out[37]: [&lt;matplotlib.lines.Line2D at 0xb40948f780&gt;] ticks_1=ax.set_xticks([0,25000,50000,75000,100000]) scale_name=ax.set_xticklabels([‘step1’,’step2’,’step3’,’step4’,’step5’],rotation=45,fontsize=12) ax.set_title(‘Python-03 Practice’)Out[40]: Text(0.5,1,’Python-03 Practice’) ax.set_xlabel(‘Steps’)Out[41]: Text(0.5,0,’Steps’) #下面我们添加图例ax.plot(np.random.randn(100000).cumsum(),color=’m’,linestyle=’-‘,label=’solid’)Out[44]: [&lt;matplotlib.lines.Line2D at 0xb409852748&gt;] ax.plot(np.random.randn(100000).cumsum(),color=’c’,linestyle=’:’,label=’dotted’)Out[45]: [&lt;matplotlib.lines.Line2D at 0xb4094f52b0&gt;] ax.legend(loc=’best’)#plt.legend也是可以的Out[46]: &lt;matplotlib.legend.Legend at 0xb409852ac8&gt;plt.show() 加注解用text、arrow和annotate等函数进行添加注解，text可以将文本加到图标的指定坐标。 fig,subpic=plt.subplots(1,1) subpic.plot([2,77,90,2.3,4,5,6,45,34,67,35,66,34,23,76],[34,32,35,43,34,23,45,56,44,57,56,33,55,66,54],’b*-‘)Out[7]: [&lt;matplotlib.lines.Line2D at 0x2eae706be0&gt;]ticks_1=subpic.set_xticks([0,25,50,75,100]) scale_name=subpic.set_xticklabels([‘jenuary’,’february’,’march’,’april’,’may’],rotation=40,fontsize=12) subpic.set_title(‘Python-03 Practice_1’,fontsize=16)Out[11]: Text(0.5,1,’Python-03 Practice_1’)subpic.set_xlabel(‘Weight’)Out[13]: Text(0.5,0,’Weight’)subpic.text(90,35,’key point one’,fontsize=10)Out[15]: Text(90,35,’key point one’) keymenge=[(67,57,’key point two’),(23,66,’key point two’)] for x,y,label in keymenge: subpic.text(x,y,label,fontsize=12) subpic.set_xlim([0,100])Out[21]: (0, 100) subpic.set_ylim([0,100])Out[22]: (0, 100)subpic.annotate(‘beautyful point’,xy=(5,23),xytext=(5,23))Out[18]: Text(5,23,’beautyful point’)plt.savefig(‘Desktop\python_01.svg’)plt.savefig(‘Desktop\python_01.png’,dpi=400,bbox_inches=’tight’) plt.show() Matplotlib.pyplot的画图方式与R语言十分类似，繁琐是他们共同的特点。与Matplotlib.pyplot不同，pandas在作图上不仅方法简单，而且可以完成各种各样的作图工作。 图像的保存在上一章，我们已经使用了savefig这个函数来存储图像。事实上，Savefig还可以作为对象figure的方法存储画布上的图像，例如： import matplotlib.pyplot as pltpic=plt.figure()pic_1=pic.add_subplot(1,1,1)import numpy as nppic_1.plot(np.random.randn(50).cumsum(),’k–’)Out[6]: [&lt;matplotlib.lines.Line2D at 0xa22b23f0b8&gt;]pic.savefig(‘Desktop\dong_123.pdf’,dpi=300,bbox_inches=’tight’) Savefig的主要参数如下：fname: 表示绝对或者相对文件路径的字符串，文件具体格式由后缀来决定，譬如.pdf,.png格式等。dpi: 图像分辨率，默认100，（每英寸点数）Facecolor,edgecolor: 背景色，默认为“w”白色Format: 显示设置文件格式，png，jpeg，pdf等等，但不要与fname里的文件格式发生冲突。Bbox_inches，常用值是tight，可剪除图表周围的空白部分。 Pandas作图 线性图 Series 和 DataFrame 都有一个用于生成各类图表的plot方法，默认状态。他只生成线性图。Series生成线性图，索引（index）直接被绘制成X轴。当然我们也可以关闭使用index绘制X轴。Use_index=False X轴的刻度和界限可以用xsticks和xlim选项来进行调节，ysticks和ylim可以调节Y轴 例子ser=pd.Series(np.random.randn(10).cumsum(),index=np.arange(0,100,10)) ser.plot()Out[18]: &lt;matplotlib.axes._subplots.AxesSubplot at 0xf3e54e0240&gt; plt.show() DataFrame的plot方法会在一个子图中为各列数据绘制一条线。自动创建的图例的标签与列索引相同。 例：import numpy as np import pandas as pdimport matplotlib.pyplot as plt Frame_01=pd.DataFrame([[2,4,8,16,32,64],[3,6,12,24,48,96],[5,10,15,20,25,30],[1,3,5,7,9,11],[16,8,4,2,1,0.5]],index=range(0,100,20),columns=[‘A’,’B’,’C’,’D’,’E’,’F’]) pic_2,subplot_object=plt.subplots(1,1) Frame_01.plot(kind=’line’,ax=subplot_object,subplots=False,layout=False,logx=True,xlim=[0,100])C:\Users\dongfeng\Anaconda3\lib\site-packages\matplotlib\axes_base.py:2923: UserWarning: Attempted to set non-positive xlimits for log-scale axis; invalid limits will be ignored. ‘Attempted to set non-positive xlimits for log-scale axis; ‘Out[3]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x5441e71cf8&gt;plt.show()Frame_01Out[5]: A B C D E F0 2 4 8 16 32 64.020 3 6 12 24 48 96.040 5 10 15 20 25 30.060 1 3 5 7 9 11.080 16 8 4 2 1 0.5 Series.plot 方法的常用参数汇总：label 用于设置图例的标签ax 确定要被绘制的matplotlib subplot对象。如果没有设置，则使用当前matplotlib subplotstyle 设置传给matplotlib的风格字符串（’g*–’）alpha 图表的填充不透明度（数值为0到1之间的数）kind 各种图形样式line, bar, barh, kde, density, scatterlogy 在Y轴上使用对数标尺use_index 将对象（Series and DataFrame）的索引用作刻度标签rot 旋转度数（0到360）xticks 用作X轴刻度的值yticks 用作y轴刻度的值xlim x的值域ylim y的值域grid 设置是否显示轴网格线 专用于DataFrame的plot的参数 Subplots 将依据数据框中的每个列绘制的图分别放置到单个的subplot（子画框）里sharex Subplots=true时，设定是否共享X的刻度和值域sharey Subplots=true时，设定是否共享y的刻度和值域figsize 元组，用来表示图像大小（宽，高）title 设置图像标题Legend 设定是否添加一个subplot图例sort_columns 设定是否以字母表中字母先后排列顺序绘制各列。 柱状图柱状图分为水平柱状图和垂直柱状图。当kind=’bar’生成垂直柱状图；kind=’barh’生成水平柱状图。 A.Series生成柱状图 import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfigure_1,get_information_of_pict=plt.subplots(2,1)datas=pd.Series(np.random.rand(20),index=list(‘qwertyuiopasdfghjklz’))datas.plot(kind=’barh’,ax=get_information_of_pict[0],figsize=(8,12),color=’g’)Out[4]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x5be13ea6d8&gt;datas.plot(kind=’bar’,ax=get_information_of_pict[1],figsize=(8,12),color=’r’)Out[5]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x5be18c41d0&gt;plt.show() B.数据框生成柱状图import numpy as npimport pandas as pdFramedata_1=pd.DataFrame(np.arange(16).reshape(4,4)(1/2)+np.arange(16).reshape(4,4)*3+6,index=[‘spring’,’sommer’,’autumn’,’winter’],columns=[‘Benz’,’BMW’,’Porsche’,’VW’])Out[11]: Benz BMW Porsche VWspring 6.000000 10.000000 13.414214 16.732051sommer 20.000000 23.236068 26.449490 29.645751autumn 32.828427 36.000000 39.162278 42.316625winter 45.464102 48.605551 51.741657 54.872983import numpy as npimport pandas as pdplt.rcParams[‘font.sans-serif’]=[‘SimHei’]plt.rcParams[‘axes.unicode_minus’] = FalseFramedata_1=pd.DataFrame(np.arange(16).reshape(4,4)(1/2)+np.arange(16).reshape(4,4)*3+6,index=[‘spring’,’sommer’,’autumn’,’winter’],columns=[‘Benz’,’BMW’,’Porsche’,’VW’])import matplotlib.pyplot as pltpicrange,pic_inf=plt.subplots(2,1)Framedata_1.plot(kind=’bar’,ax=pic_inf[0],title=’2018年德系车销售额’,rot=50,figsize=(8,12))Framedata_1.plot(kind=’barh’,ax=pic_inf[1],title=’2018年德系车销售额’,rot=130,figsize=(8,12))plt.show() ​import numpy as npimport pandas as pdimport matplotlib.pyplot as pltplt.rcParams[‘font.sans-serif’]=[‘SimHei’]plt.rcParams[‘axes.unicode_minus’] = FalseFramedata_1=pd.DataFrame(np.arange(16).reshape(4,4)*(1/2)+np.arange(16).reshape(4,4)3+6,index=[‘spring’,’sommer’,’autumn’,’winter’],columns=[‘Benz’,’BMW’,’Porsche’,’VW’])import matplotlib.pyplot as pltpicrange,pic_inf=plt.subplots(2,1)Framedata_1.plot(kind=’barh’,ax=pic_inf[1],stacked=True,title=’2018年德系车销售额’,rot=130,figsize=(8,12))Framedata_1.plot(kind=’bar’,ax=pic_inf[0],stacked=True,title=’2018年德系车销售额’,rot=50,figsize=(8,12))plt.show() serie_1=pd.Series([3,4.5,5,3,6,9,4.7,4.7,4.7,4.7,4.5,4.5,6,6,3,7,7,7,7]) serie_1.value_counts().plot(kind=’bar’)plt.show() #应用serie_1.value.counts()在series中寻找数据重复的次数并作为纵坐标，Series中的数据作为横坐标。 直方图与密度图： 首先我们要区分直方图与柱状图。 柱状图：柱状图的某一个轴（X或Y轴）可以没有严格的刻度，并且柱的宽度随图形大小，柱的数量等因素的变化而变化，并没有严格的公式来保证，因此没有实际意义，仅仅用来区分类别。通常柱状图是用条形的长度表示各类别对应的实际数据（譬如频数）的大小。柱状图是分开排列主要用于展示分类数据 直方图：直方图通常是用面积表示各组数据大小（例如频数），矩形的高度表示每一组的频数或频率或其他匹配数据，宽度则表示各组的组距，因此其高度与宽度均有意义。由于分组数据具有连续性，直方图的各矩形通常是连续排列。直方图主要用于展示数据型数据。 plt.hist(np.array([1,2,3,4,5,1.2,1.34,1.78,2.1,2.4,2.8,2.9,3.1,3.5,3.7,4.2,4.9,5.6,5.3,5.8,5.9,5,3,6,7,8.5,6.4,7.3,7.8,7.2,5.6,6.6,6.45,6.99,3.45,2.36,5.67,8.13]),bins=5,normed=False,range=(2,8),color=’yellow’) plt.hist(np.array([1,2,3,4,5,1.2,1.34,1.78,2.1,2.4,2.8,2.9,3.1,3.5,3.7,4.2,4.9,5.6,5.3,5.8,5.9,5,3,6,7,8.5,6.4,7.3,7.8,7.2,5.6,6.6,6.45,6.99,3.45,2.36,5.67,8.13]),bins=5,normed=True,range=(2,8),color=’yellow’) plt.hist(np.array([1,2,3,4,5,1.2,1.34,1.78,2.1,2.4,2.8,2.9,3.1,3.5,3.7,4.2,4.9,5.6,5.3,5.8,5.9,5,3,6,7,8.5,6.4,7.3,7.8,7.2,5.6,6.6,6.45,6.99,3.45,2.36,5.67,8.13]),bins=5,normed=True,range=(2,8),color=’yellow’) 关于Normed（数据标准化）算法的解释： sf,axes=plt.subplots()data= np.array([1,1,2,3,3,3,3,3,4,5.1])counts= axes.hist(data, normed= True)counts Out[9]:(array([ 0.48780488, 0. , 0.24390244, 0. , 1.2195122 , 0. , 0. , 0.24390244, 0. , 0.24390244]), array([ 1. , 1.41, 1.82, 2.23, 2.64, 3.05, 3.46, 3.87, 4.28, 4.69, 5.1 ]), ) np.diff(counts[1])Out[10]: array([ 0.41, 0.41, 0.41, 0.41, 0.41, 0.41, 0.41, 0.41, 0.41, 0.41]) #组距的计算(counts[0]*np.diff(counts[1])).sum()Out[13]: 1.0 #可以看到标准化后的结果和组距相乘然后求和等于1，这才是标准化的实际意义，而不是看标准化的结果是否都必须小于1。 #下面我们确定Python标准化的算法：sf,axes=plt.subplots()data= np.array([1,1,2,3,3,3,3,3,4,5.1])counts_1= axes.hist(data, normed= False)counts_1 Out[14]:(array([ 2., 0., 1., 0., 5., 0., 0., 1., 0., 1.]), array([ 1. , 1.41, 1.82, 2.23, 2.64, 3.05, 3.46, 3.87, 4.28, 4.69, 5.1 ]), ) probality_1=counts_1[0]/np.sum(counts_1[0]*np.diff(counts_1[1])) probality_1Out[16]:array([ 0.48780488, 0. , 0.24390244, 0. , 1.2195122 , 0. , 0. , 0.24390244, 0. , 0.24390244]) 最终我们得出算法公式为：频数矩阵/Sum(频数矩阵*组距矩阵)验证算法：plt.hist(np.array([1,2,3,4,5,1.2,1.34,1.78,2.1,2.4,2.8,2.9,3.1,3.5,3.7,4.2,4.9,5.6,5.3,5.8,5.9,5,3,6,7,8.5,6.4,7.3,7.8,7.2,5.6,6.6,6.45,6.99,3.45,2.36,5.67,8.13]),bins=5,normed=True,range=(2,8),color=’yellow’)Out[17]:(array([ 0.234375 , 0.13020833, 0.10416667, 0.234375 , 0.13020833]), array([ 2. , 3.2, 4.4, 5.6, 6.8, 8. ]), ) pro1=plt.hist(np.array([1,2,3,4,5,1.2,1.34,1.78,2.1,2.4,2.8,2.9,3.1,3.5,3.7,4.2,4.9,5.6,5.3,5.8,5.9,5,3,6,7,8.5,6.4,7.3,7.8,7.2,5.6,6.6,6.45,6.99,3.45,2.36,5.67,8.13]),bins=5,range=(2,8),color=’yellow’) pro1[0]Out[19]: array([ 9., 5., 4., 9., 5.]) pro_12=pro1[0]/np.sum(pro1[0]*np.diff(pro1[1])) pro_12Out[21]: array([ 0.234375 , 0.13020833, 0.10416667, 0.234375 , 0.13020833]) 密度图通过kind的KDE关键字来实现的，这里的密度指的是概率密度。它是通过计算观测数据可能产生的概率密度分布而产生的ser_1=np.random.normal(0,1,200) ser_2=np.random.normal(16,1,200)pis_1,sdr_1=plt.subplots()A=np.concatenate((ser_1,ser_2))ser_1=np.random.normal(0,1,200) ser_2=np.random.normal(12,1,200)value_1=pd.Series(A)value_1.hist(bins=75,color=’m’,normed=True)value_1.plot(kind=’kde’,style=’g–’)plt.show() 散布图散布图必须通过两个数据序列才能绘制而成。 也可以通过一个数据框绘制成散布矩阵,我们先绘制一个复杂的散布矩阵import pandas as pd import numpy as np import matplotlib.pyplot as plt X2=np.random.normal(3,1,1001) X1=np.arange(1,1002,1) X3=np.random.randn(1001) X4=np.random.rand(1001) X4=np.random.rand(1001) X5=np.random.beta(2,1,1001) test_data=np.column_stack((np.column_stack((np.column_stack((np.column_stack((X1,X2)),X3)),X4)),X5)) test_frame=pd.DataFrame(test_data) pd.scatter_matrix(test_frame,diagonal=’kde’,color=’g’,figsize=(10,10))C:\Users\dongfeng\Anaconda3\lib\site-packages\ipykernel_launcher.py:1: FutureWarning: pandas.scatter_matrix is deprecated. Use pandas.plotting.scatter_matrix instead “””Entry point for launching an IPython kernel.Out[7]:array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B2D89E8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000003422A85240&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000003422A9E2B0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342AA85860&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342AAD87F0&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342AAD8828&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B33FFD0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B3860F0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B39BDA0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B419518&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B452898&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B489828&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B4C3828&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B4D4F98&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B522F98&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B5684A8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B5A23C8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B53A7F0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B603A58&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B63AF28&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B67D4A8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342B6A32B0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342C6B2518&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342C6EA908&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000342C722828&gt;]], dtype=object) plt.show() 散布矩阵数据索引图如下：X1X1密度图 X2X1 X3X1 X4X1 X5X1X1X2 X2X2密度图 X3X2 X4X2 X5X2X1X3 X2X3 X3X3密度图 X4X3 X5X3X1X4 X2X4 X3X4 X4X4密度图 X5X4X1X5 X2X5 X3X5 X4X5 X5X5密度图 接下来我们通过简单的散布图来验证我们的散布矩阵图：X1X1密度图： X3X1 和 X1X3：fig_123,axec=plt.subplots(1,2,dpi=140) for j in range(1): axec[j].scatter(test_frame[2],test_frame[0],color=’m’,marker=’.’) axec[j].set_xlabel(‘X3’) axec[j+1].scatter(test_frame[0],test_frame[2],color=’m’,marker=’.’) axec[j+1].set_xlabel(‘X1’)plt.show() 数据地图（basemap）from mpl_toolkits.basemap import Basemap …: import matplotlib.pyplot as plt …: import pandas as pd …: import numpy as np …: from matplotlib import cm 绘制基础地图，选择绘制的区域，因为是绘制中国地图，故选取如下经纬度，lat_0和lon_0是地图中心的维度和经度。china_map=Basemap(projection=’stere’,lat_0=34,lon_0=115,llcrnrlat=28 ,urcrnrlat=42,llcrnrlon=105,urcrnrlon=129,rsphere=(2000,2000),resolution=’l’,area_thresh=350) #参数解释： #Projection- 地图投影方式，常用的有’ortho’、’merc’、’stere’和’cyl’,’cass’、’lcc’等。 #llcrnrlat- 所需地图域左下角的纬度（度）。 #urcrnrlat- 所需地图域的右上角的纬度（度）。 #llcrnrlon- 所需地图域左下角的经度（度）。 #urcrnrlon- 所需地图域的右上角的经度（度）。 china_map.drawmapboundary(color=’g’,zorder=0) # 绘制边界china_map.fillcontinents(color=’y’,lake_color=’b’,zorder=2) # 填充大陆，发现填充之后无法显示散点图，应该是被覆盖了,因此取消china_map.drawstates(color=’m’,zorder=3) # 绘制省china_map.drawcoastlines(color=’r’,zorder=3) # 绘制海岸线，必须绘制，即使是不靠海也需绘制china_map.drawcountries(color=’r’,zorder=3) #linewidth 设置线宽 #linestyle 设置线形。默认为 solid，可以是 dash，也可以是 matplotlib 其它选项。 #color 设置颜色。默认为 black(k)。 #antialiased 抗锯齿选项。默认为 True.china_map.drawrivers(linewidth=0.5, linestyle=’solid’, color=’#1E90FF’,zorder=3) #zorder 设置图层位置。默认情况下由 Basemap 设置.china_map.drawlsmask(land_color=’0 ‘,ocean_color=’#1E90FF’,zorder=1) #china_map.drawcountries(color=’y’) # 绘制国家，不太适合此例子，但需保留 #china_map.bluemarble()parallels = np.arange(28.,42.,2.)china_map.drawparallels(parallels,labels=[1,0,0,0],fontsize=10,zorder=4) # 绘制纬线 meridians = np.arange(105.,129.,3.)china_map.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10,zorder=4) # 绘制经线data_1=pd.read_csv(r’Desktop\dizhenshuju.csv’,header=None,delimiter=’,’).valueslat=data_1[:,2];lon=data_1[:,1];Seismic_grade=data_1[:,3]Seismic_grade_float=np.array(Seismic_grade,dtype=np.float64)class_1=(Seismic_grade_float/np.max(Seismic_grade_float))*5x,y = china_map(lon,lat)#地图上的精度维度匹配参数x，ychina_map.scatter(x,y,s=class_1,cmap=cm.hsv,c=’#8A2BE2’,zorder=5) # 使用matplotlib的散点图绘制函数plt.savefig(‘Desktop\dong_1217.pdf’,dpi=300,bbox_inches=’tight’)plt.show() 画布的复杂分割：Subplot2grid 在网格中创建一个子图。网格是由shape指定的，位于loc指定的位置，横跨各个方向上的rowspan个，colspan个单元格。 loc的索引是基于0的。matplotlib.pyplot.subplot2grid(shape, loc, rowspan=1, colspan=1, **kwargs) 例子：def subpicnr_invisible(fig): for i, ax in enumerate(fig.axes): #利用list(enumerate(plt.gcf().axes))枚举函数生成元组列表[(画框编号,画框对象)(),…] ax.text(0.5, 0.5, “ax%d” % (i+1)) plt.figure(0,dpi=150)pic_1=plt.subplot2grid((3,3),(0,0),colspan=3) #画框pic_1位于（0,0），这里的0意味着0行和0列，colspan=3意味着横跨三列，默认的rowspan=1意味着横跨一行 #ax2 = plt.subplot2grid((3, 3), (1, 0), colspan=2)pic_2 = plt.subplot2grid((3, 3), (1, 0), colspan=2) #画框pic_2位于（1,0），这里的0意味着1行和0列,（‘1’这一行有0和1两列），colspan=2意味着横跨两列，默认的rowspan=1意味着横跨一行。pic_3 = plt.subplot2grid((3,3), (1, 2), rowspan=2)pic_4 = plt.subplot2grid((3,3), (2, 0))pic_5 = plt.subplot2grid((3,3), (2, 1))plt.subplots_adjust(wspace=0.25,hspace=0.4)plt.suptitle(“python_practice_08”)subpicnr_invisible(plt.gcf())#plt.gcf返回所有画布信息,plt.show() 例子2def subpicnr_invisible(fig): for i, ax in enumerate(fig.axes): #利用list(enumerate(plt.gcf().axes))枚举函数生成元组列表[(画框编号,画框对象)(),…] ax.text(0.5, 0.5, “ax%d” % (i+1)) plt.figure(0,dpi=150)pic_1=plt.subplot2grid((3,3),(0,0),colspan=2) #画框pic_1位于（0,0），这里的0意味着0行和0列，colspan=3意味着横跨三列，默认的rowspan=1意味着横跨一行pic_2 = plt.subplot2grid((3, 3), (0, 2), colspan=1) #pic_2 = plt.subplot2grid((3, 3), (1, 0), colspan=2) #画框pic_2位于（1,0），这里的0意味着1行和0列,（‘1’这一行有0和1两列），colspan=2意味着横跨两列，默认的rowspan=1意味着横跨一行。pic_3 = plt.subplot2grid((3,3), (1, 0),rowspan=2,colspan=2)pic_4 = plt.subplot2grid((3,3), (1, 2))pic_5 = plt.subplot2grid((3,3), (2, 2))plt.subplots_adjust(wspace=0.25,hspace=0.4)plt.suptitle(“python_practice_08”)subpicnr_invisible(plt.gcf())#plt.gcf返回所有画布信息,plt.show() GridSpec：例子1：def subpicnr_invisible(fig): for i, ax in enumerate(fig.axes): #利用list(enumerate(plt.gcf().axes))枚举函数生成元组列表[(画框编号,画框对象)(),…] ax.text(0.5, 0.5, “ax%d” % (i+1)) import matplotlib.gridspec as gridspecplt.figure(0,dpi=150)gs = gridspec.GridSpec(3, 3)pic_1=plt.subplot(gs[0,:])#按照数组索引理解即可pic_2 = plt.subplot(gs[1,:2])pic_3 = plt.subplot(gs[1:,-1])pic_4 = plt.subplot(gs[2,0])pic_5 = plt.subplot(gs[2,1])plt.subplots_adjust(wspace=0.25,hspace=0.4)plt.suptitle(“python_practice_08”)subpicnr_invisible(plt.gcf())#plt.gcf返回所有画布信息,plt.show() 例子2：def subpicnr_invisible(fig): for i, ax in enumerate(fig.axes): #利用list(enumerate(plt.gcf().axes))枚举函数生成元组列表[(画框编号,画框对象)(),…] ax.text(0.5, 0.5, “ax%d” % (i+1)) import matplotlib.gridspec as gridspecplt.figure(0,dpi=150)gs = gridspec.GridSpec(3, 3)pic_1=plt.subplot(gs[0,:2])#按照数组索引理解即可pic_2 = plt.subplot(gs[0,-1])pic_3 = plt.subplot(gs[1:,:-1])pic_4 = plt.subplot(gs[1,-1])pic_5 = plt.subplot(gs[2,-1])plt.subplots_adjust(wspace=0.25,hspace=0.4)plt.suptitle(“python_practice_08”)subpicnr_invisible(plt.gcf())#plt.gcf返回所有画布信息,plt.show() 例子： plt.figure(dpi=100,figsize=(10,10))gs = gridspec.GridSpec(3, 3)pic_1=plt.subplot(gs[0,:2])#按照数组索引理解即可pic_2 = plt.subplot(gs[0,-1])pic_3 = plt.subplot(gs[1:,:-1])pic_4 = plt.subplot(gs[1,-1])pic_5 = plt.subplot(gs[2,-1])plt.subplots_adjust(wspace=0.25,hspace=0.4)plt.suptitle(“python_practice_08”,fontsize=’20’)n = 730X = np.linspace(-2np.pi,2np.pi,n)Y = np.sin(2X)X+np.piY_1=np.sin(2X)X-np.pipic_1.plot(X,Y,linestyle=’-‘,color=’#FFB6C1’,alpha=1.00)pic_1.plot(X,Y_1,linestyle=’-.’,color=’#1E90FF’,alpha=1.00)n_1=1000X_2 = np.random.normal(0,1,n_1)Y_2= np.random.normal(0,1,n_1)pic_2.scatter(X_2,Y_2,c=np.linspace(0,1,1000),cmap=’coolwarm’)n_2=10X_3 = np.arange(n_2); Y_3 = (1-X_3/np.float(n_2)) np.random.uniform(0,0.5,n_2); Y_3_1 = -(1-X_3/np.float(n_2)) np.random.uniform(0,0.5,n_2)pic_3.bar(X_3,Y_3,facecolor=’#FFB6C1’,width=0.6,align=’center’)pic_3.bar(X_3,Y_3_1,facecolor=’#87CEFA’,width=0.6,align=’center’)for x,y in zip(X_3,Y_3): pic_3.text(x, y+0.03,’%.2f’ % y,ha=’center’)for x,y in zip(X_3,Y_3_1): pic_3.text(x, y-0.03,’%.2f’ % y,ha=’center’)f=lambda x,y: (1-x/2+x5+y3)*np.exp(-x2-y2)x_4 = np.linspace(-3,3,1000)y_4 = np.linspace(-3,3,1000)X_4,Y_4 = np.meshgrid(x_4,y_4)pic_4.contourf(X_4, Y_4, f(X_4,Y_4), 8, alpha=.75, cmap=’coolwarm’)pic_4.contour(X_4, Y_4, f(X_4,Y_4), 8, colors=’black’)t=np.linspace(-np.pi,np.pi,20)u,v=np.array([np.cos(theta) for theta in t]),np.array([np.sin(theta) for theta in t])X_5,Y_5= np.mgrid[0:10,0:10]#晶格化pic_5.quiver(X_5,Y_5,u,v,np.random.randn(10))plt.show() 绘制立体图曲面图：plot_surfaceplot_surface(X, Y, Z, args, **kwargs)默认情况下，它将以纯色着色，但它也通过提供 cmap *参数来支持颜色映射。 ‘rstride和cstride` kwargs设置了用于对输入数据进行采样以生成图形的步幅。如果传入1k个1k数组，则步幅的默认值将导致绘制100x100的网格。 默认为10。 如果同时提供了stride和count kwargs（rcount、ccount），则引发ValueError。 rcount和ccount kwargs取代rstride和cstride作为表面绘图的默认采样方法。这些参数将决定从输入数据中最多取多少个均匀间隔的样本来生成图。 这是默认的采样方法，除非使用“经典”风格。 如果同时指定了步幅和数量（stride and count），将会引发ValueError。 参数： X ， Y ， Z * 数据值为二维数组 rstride * 数组行步幅（步长） cstride * 数组列步幅（步长） rcount * 最多使用行，默认为50 ccount 最多使用列，默认为50 颜色* 曲面片的颜色 cmap * 曲面片调色板。 facecolors * 单个曲面片的表面色 norm * 一个标准化实例，用于将值映射到颜色 vmin * 映射的最小值 vmax 映射的最大值 shade* 是否遮蔽表面色 例子：参数化坐标轴下的三维球from mpl_toolkits.mplot3d import Axes3Dimport numpy as npimport matplotlib.pyplot as pltu = np.linspace(0,2np.pi,1000)v = np.linspace(0,np.pi,1000)x=10np.outer(np.sin(v),np.cos(u))y=10np.outer(np.sin(v),np.sin(u))z=10np.outer(np.cos(v),np.ones(len(np.cos(v)))) #创建二维数据集X,Y和Z，注意他们的值必须在各矩阵相同位置处一一对应。fig=plt.figure(0,figsize=(8,8),dpi=120)ax=fig.add_subplot(1,1,1,projection=’3d’) #ax.plot_surface(x,y,z,rcount=1000,ccount=1000,cmap=’coolwarm’)#太浪费时间了，用下面替换语句ax.plot_surface(x,y,z,cmap=’hot’)plt.show() 例子：非参数化坐标轴下曲面图import numpy as npfrom mpl_toolkits.mplot3d import Axes3Dimport matplotlib.pyplot as pltimport randomdef fun(x, y): return np.sqrt(x2 + y2)fig = plt.figure(0,dpi=120,figsize=(6,6))ax = fig.add_subplot(111, projection=’3d’)x = y = np.linspace(-5.0, 5.0, 100)X, Y = np.meshgrid(x, y)zs = np.array([fun(x,y) for x,y in zip(np.ravel(X), np.ravel(Y))])Z = zs.reshape(Y.shape)ax.plot_surface(X, Y, Z,cmap=’jet’)ax.set_xlabel(‘X Label’)ax.set_ylabel(‘Y Label’)ax.set_zlabel(‘Z Label’) #(np.ravel(X)).shapeplt.show() import numpy as npfrom mpl_toolkits.mplot3d import Axes3Dimport matplotlib.pyplot as pltimport random def fun(x, y): return np.cos(np.sqrt(x2 + y2)) fig = plt.figure(0,dpi=120,figsize=(6,6))ax = fig.add_subplot(111, projection=’3d’)x = y = np.linspace(-5.0, 5.0, 100)X, Y = np.meshgrid(x, y)zs = np.array([fun(x,y) for x,y in zip(np.ravel(X), np.ravel(Y))])Z = zs.reshape(Y.shape) ax.plot_surface(X, Y, Z,cmap=’jet’) ax.set_xlabel(‘X Label’)ax.set_ylabel(‘Y Label’)ax.set_zlabel(‘Z Label’) #(np.ravel(X)).shapeplt.show() 例子，带色度条的三维图import numpy as npfrom mpl_toolkits.mplot3d import Axes3Dimport matplotlib.pyplot as pltimport random f=lambda x,y: (1-x/2+x5+y3)*np.exp(-x2-y2)fig = plt.figure(0,dpi=160,figsize=(10,10))ax = fig.add_subplot(111, projection=’3d’)x = y = np.linspace(-5.0, 5.0, 100)X, Y = np.meshgrid(x, y)zs = np.array([f(x,y) for x,y in zip(np.ravel(X), np.ravel(Y))])Z = zs.reshape(Y.shape) pic_3dim=ax.plot_surface(X, Y, Z,cmap=’Spectral_r’)plt.colorbar(pic_3dim) #画出色度条ax.set_xlabel(‘X Label’)ax.set_ylabel(‘Y Label’)ax.set_zlabel(‘Z Label’) #(np.ravel(X)).shapeplt.show() 另外的一种表面图绘制方法：例如：import matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3D #绘制3D坐标的函数import numpy as np def fun(x,y): return np.power(x,2)+np.sin(np.power(y,2))*x fig1=plt.figure(0,dpi=160,figsize=(8,8))ax=Axes3D(fig1)#三维化画布并产生三维画框X=np.arange(-3,3,0.05)Y=np.arange(-3,3,0.05)X,Y=np.meshgrid(X,Y)#生成坐标点Z=fun(X,Y)plt.title(‘python-08’)ax.plot_surface(X, Y, Z,cmap=’coolwarm’)ax.set_xlabel(‘x label’, color=’r’)ax.set_ylabel(‘y label’, color=’g’)ax.set_zlabel(‘z label’, color=’b’)plt.show() 曲线图例子：我的葫芦from mpl_toolkits.mplot3d import Axes3D fig = plt.figure(0,dpi=100,figsize=(6,6))ax = fig.add_subplot(1,1,1, projection=’3d’)theta = np.linspace(-20 np.pi, 20 np.pi, 1000)z = np.linspace(0,10,1000);phi=np.linspace(0,2np.pi,1000)r=znp.sin(phi)x = r np.sin(theta)y = r np.cos(theta)ax.plot(x, y, z, label=’curve’)ax.legend()plt.show() 散点图（见LDA）线框图例子：import numpy as np def fun(x,y): return np.power(x,2)+np.sin(np.power(y,2))*x fig1=plt.figure(0,dpi=160,figsize=(8,8))ax=Axes3D(fig1)#三维化画布并产生三维画框X=np.arange(-3,3,0.05)Y=np.arange(-3,3,0.05)X,Y=np.meshgrid(X,Y)#生成坐标点Z=fun(X,Y)plt.title(‘python-08’)ax.plot_wireframe(X, Y, Z, rstride=3, cstride=3)#一定要调节成大的扫描步长才有效果ax.set_xlabel(‘x label’, color=’r’)ax.set_ylabel(‘y label’, color=’g’)ax.set_zlabel(‘z label’, color=’b’)plt.show() 等高线图contour和contourf都是画三维等高线图的，不同点在于contourf会对等高线间的区域进行填充。参数： X, Y, Z 数组型数据extend3d 是否在3D中扩展等高线图（默认值：False）stride 步幅，用于扩展等高线图的步幅（步长）zdir 等高线图产生方向: x, y 或 z (default)offset 如果赋值，绘制等高线投影到垂直于zdir并且通过偏移量确定位置的平面 例子：import numpy as np def fun(x,y): return (1/(2np.pi))np.exp(-0.5*(x2+y2)) fig1=plt.figure(0,dpi=160,figsize=(8,8))ax=Axes3D(fig1)#三维化画布并产生三维画框X=np.arange(-3,3,0.05)Y=np.arange(-3,3,0.05)X,Y=np.meshgrid(X,Y)#生成坐标点Z=fun(X,Y)plt.title(‘python-08’)ax.plot_surface(X, Y, Z, rstride=3, cstride=3,cmap=’jet’)ax.contour(X, Y, Z, zdir=’z’,offset=0.16,cmap=’coolwarm’)cset = ax.contour(X, Y, Z, zdir=’x’, offset=-3, cmap=’coolwarm’)cset = ax.contour(X, Y, Z, zdir=’y’, offset=3, cmap=’coolwarm’)ax.set_xlabel(‘x label’, color=’r’)ax.set_ylabel(‘y label’, color=’g’)ax.set_zlabel(‘z label’, color=’b’)plt.show() 动态图运动中的布朗运动from matplotlib import pyplot as plt from matplotlib import animationimport numpy as np%matplotlib qt5def randn_point(): # 产生随机散点图的x和y数据 x=np.random.randn(100) y=np.random.randn(100) return x,y fig,ax1=plt.subplots(1,1,dpi=130,figsize=(8,6)) 先绘制初始图形x1,y1=randn_point()sca1 = ax1.scatter(x1,y1) # 散点图def init(): # 构造开始帧函数init # 改变散点图数据 x1, y1 = randn_point() data1 = [[x,y] for x,y in zip(x1,y1)] sca1.set_offsets(data1) # 散点图 label = &apos;timestep {0}&apos;.format(0) ax1.set_xlabel(label) return sca1,ax1 # 注意返回值，我们要更新的就是这些数据 def animate(i): # 接着，构造自定义动画函数animate，用来更新每一帧上各个x对应的y坐标值，参数表示第i帧 x1, y1 = randn_point() x2, y2 = randn_point() data1 = [[x,y] for x,y in zip(x1,y1)] sca1.set_offsets(data1) # 散点图 label = &apos;timestep {0}&apos;.format(i) ax1.set_xlabel(label) return sca1,ax1 接下来，我们调用FuncAnimation函数生成动画。参数说明：fig 进行动画绘制的figurefunc 自定义动画函数，即传入刚定义的函数animateframes 动画长度，一次循环包含的帧数init_func 自定义开始帧，即传入刚定义的函数initinterval 更新频率，以ms计blit 选择更新所有点，还是仅更新产生变化的点。应选择True，但mac用户请选择False，否则无法显示动画ani = animation.FuncAnimation(fig=fig,func=animate,frames=1000,init_func=init,interval=100,blit=False)plt.show() 布朗运动过程模拟： from matplotlib import pyplot as plt from matplotlib import animationimport numpy as np%matplotlib qt5def randn_point(t): Data=np.random.randn(100,10001) brown_moving=Data.T[0]t+sum([Data_x((np.sqrt(2)np.sin(inp.pit))/(inp.pi)) for Data_x,i in list(zip(Data.T[1:],range(1,10001)))])angel=np.random.randn(100)360return brown_movingnp.cos(angel),brown_moving*np.sin(angel)fig,ax1=plt.subplots(1,1,dpi=130,figsize=(8,6)) 先绘制初始图形plt.xlim([-100,100]) #plt.ylim([-50,50]) x1,y1=randn_point(np.random.randint(100))sca1 = ax1.scatter(x1,y1)def init(): # 构造开始帧函数init # 改变散点图数据 x1, y1 = randn_point(np.random.randint(100)) data1 = [[x,y] for x,y in zip(x1,y1)] sca1.set_offsets(data1) # 散点图 label = &apos;timestep {0}&apos;.format(0) ax1.set_xlabel(label) return sca1,ax1 # 注意返回值，我们要更新的就是这些数据 def animate(i): # 接着，构造自定义动画函数animate，用来更新每一帧上各个x对应的y坐标值，参数表示第i帧 x1, y1 = randn_point(i) x2, y2 = randn_point(i) data1 = [[x,y] for x,y in zip(x1,y1)] sca1.set_offsets(data1) # 散点图 label = &apos;timestep {0}&apos;.format(i) ax1.set_xlabel(label) return sca1,ax1 ani = animation.FuncAnimation(fig=fig,func=animate,frames=100,init_func=init,interval=1,blit=False)plt.show()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Typora常用语法]]></title>
    <url>%2F2019%2F06%2F05%2FTypora%E5%B8%B8%E7%94%A8%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[标题的使用格式12345\# 一阶标题 或者快捷键Ctrl+1\##二阶标题 或者快捷键Ctrl+2\###三阶标题 或者快捷键Ctrl+3\####四阶标题 或者快捷键Ctrl+4\#####五阶标题 或者快捷键Ctrl+5 文本居中使用格式1\&lt;center\&gt;这是要居中的文本内容\&lt;/center\&gt;1 下划线使用格式1\下划线的内容\&lt;\u&gt; 或者快捷键 Ctrl+U 或者格式里面的下划线 删除线使用格式1\~\~删除线的内容\~\~ 或者格式里面的删除线 字体加粗使用格式1**加粗字体** 或者快捷键Ctrl+B 或者格式里面加粗 字体倾斜使用格式1*字体倾斜了* 或者快捷键Ctrl+I 图片的插入1可以直接拖进去,或者插入里面选择图像 超链接使用格式1快捷键Ctrl+K 或者格式里面的超链接 代码区域Typora支持对多种语言的代码区域进行语法高亮。这些语言可以说是涵盖了绝大部分经常使用的编程语言，包括C++，Python，MATLAB，甚至包含spreadsheet（也就是Excel电子表格）。用Typora记编程笔记，看起来一清二楚。如果设置代码语言为flow，那么可以直接画出一个流程图；还可以使用相应的方法画出时序图等图表。 代码区域的使用格式1​```内容```python 任务列表使用格式1- [ ] 文字 （注：注意用空格隔开） 任务列表在typora中的显示形式 JAVA C C++ Python 列表的使用格式1\+ 、- 、* 创建无序列，任意数字开始+空格创建有序列表 Python 数学表达式Typora支持加入用LaTeX写成的数学公式，并且在软件界面下用MathJax直接渲染。数学公式分为两种：一种是行内公式(inline math)，可以在偏好设置中单独打开，由一个美元符号$将公式围起来；一种是行外公式，直接按Ctrl+Shift+M；注：上标和下标可以使用数学表达式来获取 水平分割线的使用格式***或者- - - 水平分割线在typora中显示形式 引用的使用格式>+空格 与天奋斗，其乐无穷！与地奋斗，其乐无穷！与人奋斗，其乐无穷！ 注释的使用格式要添加注释的文字[1](https://log.csdn.net/mollen/article/details/84110708#fn:) 注释在typora中显示形式中国2 表情的使用格式:单词 表情在typora中的显示形式:smiley: Typora快捷键 快捷键 作用 快捷键 作用 Ctrl+1 一阶标题 Ctrl+B 字体加粗 Ctrl+2 二阶标题 Ctrl+I 字体倾斜 Ctrl+3 三阶标题 Ctrl+U 下划线 Ctrl+4 四阶标题 Ctrl+Home 返回Typora顶部 Ctrl+5 五阶标题 Ctrl+End 返回Typora底部 Ctrl+6 六阶标题 Ctrl+T 创建表格 Ctrl+L 选中某句话 Ctrl+K 创建超链接 Ctrl+D 选中某个单词 Ctrl+F 搜索 Ctrl+E 选中相同格式的文字 Ctrl+H 搜索并替换 Alt+Shift+5 删除线 Ctrl+Shift+I 插入图片 颜色代码块12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667$\textcolor&#123;GreenYellow&#125;&#123;GreenYellow&#125; $$\textcolor&#123;Yellow&#125;&#123;Yellow&#125;$$\textcolor&#123;Goldenrod&#125;&#123;Goldenrod&#125; $$\textcolor&#123;Dandelion&#125;&#123;Dandelion&#125;$$\textcolor&#123;Apricot&#125;&#123;Apricot&#125; $$\textcolor&#123;Peach&#125;&#123;Peach&#125;$$\textcolor&#123;Melon&#125;&#123;Melon&#125; $$\textcolor&#123;YellowOrange&#125;&#123;YellowOrange&#125;$$\textcolor&#123;Orange&#125;&#123;Orange&#125; $$\textcolor&#123;BurntOrange&#125;&#123;BurntOrange&#125;$$\textcolor&#123;Bittersweet&#125;&#123;Bittersweet&#125;$$\textcolor&#123;RedOrange&#125;&#123;RedOrange&#125; $$\textcolor&#123;Mahogany&#125;&#123;Mahogany&#125;$$\textcolor&#123;Maroon&#125;&#123;Maroon&#125; $$\textcolor&#123;BrickRed&#125;&#123;BrickRed&#125;$$\textcolor&#123;Red&#125;&#123;Red&#125; $$\textcolor&#123;OrangeRed&#125;&#123;OrangeRed&#125;$$\textcolor&#123;RubineRed&#125;&#123;RubineRed&#125;$$\textcolor&#123;WildStrawberry&#125;&#123;WildStrawberry&#125;$$\textcolor&#123;Salmon&#125;&#123;Salmon&#125;$$\textcolor&#123;CarnationPink&#125;&#123;CarnationPink&#125;$$\textcolor&#123;Magenta&#125;&#123;Magenta&#125; $$\textcolor&#123;VioletRed&#125;&#123;VioletRed&#125;$$\textcolor&#123;Rhodamine&#125;&#123;Rhodamine&#125; $$\textcolor&#123;Mulberry&#125;&#123;Mulberry&#125;$$\textcolor&#123;RedViolet&#125;&#123;RedViolet&#125; $$\textcolor&#123;Fuchsia&#125;&#123;Fuchsia&#125;$$\textcolor&#123;Lavender&#125;&#123;Lavender&#125; $$\textcolor&#123;Thistle&#125;&#123;Thistle&#125;$$\textcolor&#123;Orchid&#125;&#123;Orchid&#125; $$\textcolor&#123;DarkOrchid&#125;&#123;DarkOrchid&#125;$$\textcolor&#123;Purple&#125;&#123;Purple&#125; $$\textcolor&#123;Plum&#125;&#123;Plum&#125;$$\textcolor&#123;Violet&#125;&#123;Violet&#125; $$\textcolor&#123;RoyalPurple&#125;&#123;RoyalPurple&#125;$$\textcolor&#123;BlueViolet&#125;&#123;BlueViolet&#125;$$\textcolor&#123;Periwinkle&#125;&#123;Periwinkle&#125;$$\textcolor&#123;CadetBlue&#125;&#123;CadetBlue&#125;$$\textcolor&#123;CornflowerBlue&#125;&#123;CornflowerBlue&#125;$$\textcolor&#123;MidnightBlue&#125;&#123;MidnightBlue&#125;$$\textcolor&#123;NavyBlue&#125;&#123;NavyBlue&#125; $$\textcolor&#123;RoyalBlue&#125;&#123;RoyalBlue&#125;$$\textcolor&#123;Blue&#125;&#123;Blue&#125; $$\textcolor&#123;Cerulean&#125;&#123;Cerulean&#125;$$\textcolor&#123;Cyan&#125;&#123;Cyan&#125; $$\textcolor&#123;ProcessBlue&#125;&#123;ProcessBlue&#125;$$\textcolor&#123;SkyBlue&#125;&#123;SkyBlue&#125; $$\textcolor&#123;Turquoise&#125;&#123;Turquoise&#125;$$\textcolor&#123;TealBlue&#125;&#123;TealBlue&#125; $$\textcolor&#123;Aquamarine&#125;&#123;Aquamarine&#125;$$\textcolor&#123;BlueGreen&#125;&#123;BlueGreen&#125; $$\textcolor&#123;Emerald&#125;&#123;Emerald&#125;$$\textcolor&#123;JungleGreen&#125;&#123;JungleGreen&#125;$$\textcolor&#123;SeaGreen&#125;&#123;SeaGreen&#125; $$\textcolor&#123;Green&#125;&#123;Green&#125;$$\textcolor&#123;ForestGreen&#125;&#123;ForestGreen&#125;$$\textcolor&#123;PineGreen&#125;&#123;PineGreen&#125; $$\textcolor&#123;LimeGreen&#125;&#123;LimeGreen&#125;$$\textcolor&#123;YellowGreen&#125;&#123;YellowGreen&#125;$$\textcolor&#123;SpringGreen&#125;&#123;SpringGreen&#125;$$\textcolor&#123;OliveGreen&#125;&#123;OliveGreen&#125;$$\textcolor&#123;RawSienna&#125;&#123;RawSienna&#125; $$\textcolor&#123;Sepia&#125;&#123;Sepia&#125;$$\textcolor&#123;Brown&#125;&#123;Brown&#125; $$\textcolor&#123;Tan&#125;&#123;Tan&#125;$$\textcolor&#123;Gray&#125;&#123;Gray&#125; $$\textcolor&#123;Black&#125;&#123;Black&#125;$]]></content>
      <categories>
        <category>软件</category>
      </categories>
      <tags>
        <tag>Typora语法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python绘图]]></title>
    <url>%2F2019%2F06%2F05%2Fpython%E7%BB%98%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[Matplotlibimport matplotlib.pyplot as plt若无法显示图画记得要放在一起写不要分开写 ######添加数值 for a,b in zip(ls1,ls2): plt.text(a, b,’%.0f’%b, ha = ‘center’,va = ‘bottom’,fontsize=10) #####创建画布方法1:创建简单画布figure_1 = plt.figure()方法2:创建复杂画布figure_1 = plt.figure(1,(8,4),dpi=140,facecolor=’red’,edgecolor=’yellow’,frameon=True)注释: 1代表画布编号,8,4表示宽高比,dpi表示像素,facecolor表示底色,edgecolor表示边角色frameon=Teue表示是绘制边框 #####创建画框①分割画布创建画框aapic_1 = figure_1.add_subplot(2,2,1) #表示分成2行2列,其中的编号1的画框,编号顺序从左到右从上到下aapic_1 = figure_1.add_subplot(2,2,2)aapic_1 = figure_1.add_subplot(2,2,3)aapic_1 = figure_1.add_subplot(2,2,4)plt.show() 画线状图创建一个画布figure_2 = plt.figure(1,(6,4),dpi=140,frameon=True)创建一个画框axes1 = figure_2.add_subplot(1,1,1)形成线图axes1.plot(np.arange(0,10),np.random.normal(3,1.6,10,’b -‘)b表示线颜色为蓝色, 表示点为 ,-表示线为实线 画直方图fgs = np.random.randn(1000)创建一个画布figure_2 = plt.figure(1,(6,4),dpi=140,frameon=True)创建一个画框axes1 = figure_2.add_subplot(1,1,1)创建直方图axes1.hist(fgs,bins=20) #bins表示分成多少组 散点图x = np.linspace(-3,3,100)一步创建画框画布sfg,axes = plt.subplots(1,1,dpi=140,figsize(6,4))绘制散点图axes1.scatter(x,np.cos(x),c=’b’,marker=’ ‘) #x表示x轴,np.cos(x)表示纵轴,c=’b’表示颜色为蓝色,,marker=’ ‘表示外形为 . 饼图Asx,sd=plt.subplots(1,1) #一行一列遵循数组编号sd.pie(np.arange(4,9),explode=[0.2,0.1,0.3,0.4,0.3],labels=[‘1’,’2’,’3’,’4,’5’],colors=[‘m’,’r’,’g’,’c’,’b’],autopct=’%.2f%%’,pctdistance=1,shadow=True,labeldistance=1.6,startangle=30,radius=1,frame=True,rotatelabels=True)np.arange(4,9)用来分块分成5块,explode表示爆炸系数,labels表示标签名,colors表示各个标签对应图的颜色,shadow表示阴影,radius圆半径,autopct指定数值的显示方式,labeldistance每一项的名称label和距离圆心的半径,pctdistance每一项的比例autopct和距离圆心的半径,frame表的轴框架,rotatelabels旋转每个label到指定的角度,startanglex轴逆时针旋转饼图的开始角度. 如果不涉及子图的话，无需先设置画布。可直接画出图形：plt.plot(np.random.randn(30).cumsum(),’g –’)等价于Plt.plot(np.random.randn(30).cumsum(),color=’g’,linestyle=’dashed’,marker=’ ’)plt.plot(data_uique,’r-‘,drawstyle=’steps-post’,label=’line’)====label=’line’标识线型plt.legend(loc=’best’) 线标签的位置放在最好的地方 plt.legend也可以 刻度和标签Xlim（X值范围）、xticks（X轴刻度值）和xticklabels（X轴刻度标签）ticks_1=ax.set_xticks([0,25000,50000,75000,100000])” 分组的组距scale_name=ax.set_xticklabels([‘step1’,’step2’,’step3’,’step4’,’step5’],rotation=45,fontsize=12) 分组对应的标签名ax.set_title(‘Python-03 Practice’) 设置画框标题 ,使用画布加标题不需要setax.set_xlabel(‘Steps’) 设置x轴标签ax.set_ylabel(‘Values’) 设置y轴标签subpic.text(90,35,’key point one’,fontsize=10) 位置定坐标位置添加注解keymenge=[(67,57,’key point two’),(23,66,’key point two’)] 添加多个注解可以通过循环列表实现subpic.annotate(‘beautyful point’,xy=(5,23),xytext=(5,23)) 宁一种加注解方式plt.savefig(‘Desktop python_01.png’,dpi=400,bbox_inches=’tight’) 保存图片 线型和marker表格：‘-‘ solid line style‘–’ dashed line style‘-.’ dash-dot line style‘:’ dotted line style‘.’ point marker‘,’ pixel marker‘o’ circle marker‘v’ triangle_down marker‘^’ triangle_up marker‘&lt;’ triangle_left marker‘&gt;’ triangle_right marker‘1’ tri_down marker‘2’ tri_up marker‘3’ tri_left marker‘4’ tri_right marker‘s’ square marker‘p’ pentagon marker‘ ‘ star marker‘h’ hexagon1 marker‘H’ hexagon2 marker‘+’ plus marker‘x’ x marker‘D’ diamond marker‘d’ thin_diamond marker‘|’ vline marker‘_’ hline marker颜色表格：‘b’ blue‘g’ green‘r’ red‘c’ cyan‘m’ magenta‘y’ yellow‘k’ black‘w’ white Pandas作图线性图1234import pandas as pdimport numpy as npser=pd.Series(np.random.randn(10).cumsum(),index=np.arange(0,100,10))ser.plot() Series.plot 方法的常用参数汇总：| label | 用于设置图例的标签 || ——— | ———————————————————— || ax | 确定要被绘制的matplotlib subplot对象。如果没有设置，则使用当前matplotlib subplot || style | 设置传给matplotlib的风格字符串（’g*–’） || alpha | 图表的填充不透明度（数值为0到1之间的数） || kind | 各种图形样式line, bar, barh, kde, density, scatter || logy | 在Y轴上使用对数标尺 || use_index | 将对象（Series and DataFrame）的索引用作刻度标签 || rot | 旋转度数（0到360） || xticks | 用作X轴刻度的值 || yticks | 用作y轴刻度的值 || xlim | x的值域 || ylim | y的值域 || grid | 设置是否显示轴网格线 |123456import numpy as npimport pandas as pdimport matplotlib.pyplot as pltFrame_01=pd.DataFrame([[2,4,8,16,32,64],[3,6,12,24,48,96],[5,10,15,20,25,30],[1,3,5,7,9,11],[16,8,4,2,1,0.5]],index=range(0,100,20),columns=[&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;D&apos;,&apos;E&apos;,&apos;F&apos;])pic_2,subplot_object=plt.subplots(1,1)Frame_01.plot(kind=&apos;line&apos;,ax=subplot_object,subplots=False,layout=False,logx=True,xlim=[0,100]) DataFrame的plot的参数| Subplots | 将依据数据框中的每个列绘制的图分别放置到单个的subplot（子画框）里 || ———— | ———————————————————— || sharex | Subplots=true时，设定是否共享X的刻度和值域 || sharey | Subplots=true时，设定是否共享y的刻度和值域 || figsize | 元组，用来表示图像大小（宽，高） || title | 设置图像标题 || Legend | 设定是否添加一个subplot图例 || sort_columns | 设定是否以字母表中字母先后排列顺序绘制各列。 | 柱状图1234567891011Series生成柱状图 import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfigure_1,get_information_of_pict=plt.subplots(2,1)datas=pd.Series(np.random.rand(20),index=list(&apos;qwertyuiopasdfghjklz&apos;))datas.plot(kind=&apos;barh&apos;,ax=get_information_of_pict[0],figsize=(8,12),color=&apos;g&apos;)Out[4]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x5be13ea6d8&gt;datas.plot(kind=&apos;bar&apos;,ax=get_information_of_pict[1],figsize=(8,12),color=&apos;r&apos;)Out[5]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x5be18c41d0&gt;plt.show() 1234567891011数据框生成柱状图import numpy as npimport pandas as pdplt.rcParams[&apos;font.sans-serif&apos;]=[&apos;SimHei&apos;] plt.rcParams[&apos;axes.unicode_minus&apos;] = FalseFramedata_1=pd.DataFrame(np.arange(16).reshape(4,4)**(1/2)+np.arange(16).reshape(4,4)*3+6,index=[&apos;spring&apos;,&apos;sommer&apos;,&apos;autumn&apos;,&apos;winter&apos;],columns=[&apos;Benz&apos;,&apos;BMW&apos;,&apos;Porsche&apos;,&apos;VW&apos;])import matplotlib.pyplot as pltpicrange,pic_inf=plt.subplots(2,1)Framedata_1.plot(kind=&apos;bar&apos;,ax=pic_inf[0],title=&apos;2018年德系车销售额&apos;,rot=50,figsize=(8,12))Framedata_1.plot(kind=&apos;barh&apos;,ax=pic_inf[1],title=&apos;2018年德系车销售额&apos;,rot=130,figsize=(8,12))plt.show() 绘制立体图12345678910111213参数： X ， Y ， Z 数据值为二维数组 rstride 数组行步幅（步长） cstride 数组列步幅（步长） rcount 最多使用行，默认为50 ccount 最多使用列，默认为50 颜色 曲面片的颜色 cmap 曲面片调色板。 facecolors 单个曲面片的表面色 norm 一个标准化实例，用于将值映射到颜色 vmin 映射的最小值 vmax 映射的最大值 shade 是否遮蔽表面色 曲面图12345678910111213141516171819例子：非参数化坐标轴下曲面图import numpy as npfrom mpl_toolkits.mplot3d import Axes3Dimport matplotlib.pyplot as pltimport randomdef fun(x, y): return np.sqrt(x**2 + y**2)fig = plt.figure(0,dpi=120,figsize=(6,6))ax = fig.add_subplot(111, projection=&apos;3d&apos;)x = y = np.linspace(-5.0, 5.0, 100)X, Y = np.meshgrid(x, y)zs = np.array([fun(x,y) for x,y in zip(np.ravel(X), np.ravel(Y))])Z = zs.reshape(Y.shape)ax.plot_surface(X, Y, Z,cmap=&apos;jet&apos;)ax.set_xlabel(&apos;X Label&apos;)ax.set_ylabel(&apos;Y Label&apos;)ax.set_zlabel(&apos;Z Label&apos;)#(np.ravel(X)).shapeplt.show() 123456789101112131415例子：参数化坐标轴下的三维球from mpl_toolkits.mplot3d import Axes3Dimport numpy as npimport matplotlib.pyplot as pltu = np.linspace(0,2*np.pi,1000)v = np.linspace(0,np.pi,1000)x=10*np.outer(np.sin(v),np.cos(u))y=10*np.outer(np.sin(v),np.sin(u))z=10*np.outer(np.cos(v),np.ones(len(np.cos(v))))#创建二维数据集X,Y和Z，注意他们的值必须在各矩阵相同位置处一一对应。fig=plt.figure(0,figsize=(8,8),dpi=120)ax=fig.add_subplot(1,1,1,projection=&apos;3d&apos;)#ax.plot_surface(x,y,z,rcount=1000,ccount=1000,cmap=&apos;coolwarm&apos;)#太浪费时间了，用下面替换语句ax.plot_surface(x,y,z,cmap=&apos;hot&apos;)plt.show() 曲线图12345678910111213例子：我的葫芦from mpl_toolkits.mplot3d import Axes3Dfig = plt.figure(0,dpi=100,figsize=(6,6))ax = fig.add_subplot(1,1,1, projection=&apos;3d&apos;)theta = np.linspace(-20 * np.pi, 20 * np.pi, 1000)z = np.linspace(0,10,1000);phi=np.linspace(0,2*np.pi,1000)r=z*np.sin(phi)x = r * np.sin(theta)y = r * np.cos(theta)ax.plot(x, y, z, label=&apos;curve&apos;)ax.legend()plt.show() 边框图123456789101112131415161718线框图例子：import numpy as np def fun(x,y): return np.power(x,2)+np.sin(np.power(y,2))*x fig1=plt.figure(0,dpi=160,figsize=(8,8)) ax=Axes3D(fig1)#三维化画布并产生三维画框X=np.arange(-3,3,0.05) Y=np.arange(-3,3,0.05)X,Y=np.meshgrid(X,Y)#生成坐标点 Z=fun(X,Y) plt.title(&apos;python-08&apos;)ax.plot_wireframe(X, Y, Z, rstride=3, cstride=3)#一定要调节成大的扫描步长才有效果ax.set_xlabel(&apos;x label&apos;, color=&apos;r&apos;) ax.set_ylabel(&apos;y label&apos;, color=&apos;g&apos;) ax.set_zlabel(&apos;z label&apos;, color=&apos;b&apos;)plt.show() #####等高线图 1234567891011121314151617181920212223242526*X*, *Y*, *Z* 数组型数据*extend3d* 是否在3D中扩展等高线图（默认值：False）*stride* *步幅*，用于扩展等高线图的步幅（步长）*zdir* 等高线图产生方向: x, y 或 z (default)*offset* 如果赋值，绘制等高线投影到垂直于zdir并且通过偏移量确定位置的平面例子：import numpy as np def fun(x,y): return (1/(2*np.pi))*np.exp(-0.5*(x**2+y**2)) fig1=plt.figure(0,dpi=160,figsize=(8,8)) ax=Axes3D(fig1)#三维化画布并产生三维画框X=np.arange(-3,3,0.05) Y=np.arange(-3,3,0.05)X,Y=np.meshgrid(X,Y)#生成坐标点 Z=fun(X,Y) plt.title(&apos;python-08&apos;)ax.plot_surface(X, Y, Z, rstride=3, cstride=3,cmap=&apos;jet&apos;)ax.contour(X, Y, Z, zdir=&apos;z&apos;,offset=0.16,cmap=&apos;coolwarm&apos;) cset = ax.contour(X, Y, Z, zdir=&apos;x&apos;, offset=-3, cmap=&apos;coolwarm&apos;) cset = ax.contour(X, Y, Z, zdir=&apos;y&apos;, offset=3, cmap=&apos;coolwarm&apos;)ax.set_xlabel(&apos;x label&apos;, color=&apos;r&apos;) ax.set_ylabel(&apos;y label&apos;, color=&apos;g&apos;) ax.set_zlabel(&apos;z label&apos;, color=&apos;b&apos;)plt.show()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas]]></title>
    <url>%2F2019%2F06%2F05%2FPandas%2F</url>
    <content type="text"><![CDATA[PandasSeries(适用于一维的)123456789101112131415161718192021222324252627282930313233Series数据的建立pddata_1=pd.Series([1,2,3]) 对应数据格式:0 11 22 3pddata_1=pd.Series([[1,2],[3,4]])对应数据格式:0 [1, 2]1 [3, 4]pddata_1=pd.Series([[1,2],[3,4]],index=[&apos;345&apos;,&apos;2342&apos;])对应数据格式:345 [1, 2]2342 [3, 4]pddata_1=pd.Series(((1,2),(3,4)),index=[&apos;345&apos;,&apos;2342&apos;])对应数据格式:345 (1, 2)2342 (3, 4)pddata_1=pd.Series([np.random.randint(1,100,6),np.random.randint(1,100,6)])对应数据格式:0 [15, 24, 43, 57, 97, 36]1 [67, 12, 69, 61, 9, 18]pddata_1=pd.Series(&#123;1:&apos;foo&apos;,3:&apos;drt&apos;,8:&apos;tyue&apos;&#125;) =====不需要加索引index否则会使得数据的值变成nan,冲突了对应数据格式:1 foo3 drt8 tyuepddata_1=pd.Series(list(zip(np.array(list1),np.array(list2)))对应数据格式:0 (1, 3)1 (2, 4)2 (3, 5)3 (4, 6) Series常用的方法12345s1.values 以数组形式查看所有值s1.index 查看所有索引通过索引取值 s1[&apos;索引名&apos;]Series过滤 s1[s1&gt;2] 返回索引以及对应的数据(numpy过滤的实现通过布尔数组实现) Series更新s1.update(pd.Series([12,23],index = [3,4])) Series追加s1.append(pd.Series([-123,-234,-56],ignore_index=True)) ignore_index=True表示形成默认的索引,不写表示使用原来数据自带的索引 Series是否包含某个数据‘want’ in datas_pys 返回布尔值.判断want是否为datas_pys的索引,数据框dataframe同样适用index和columns都可以判断 检测数据是否丢失pd.isnull(serie_123) serie_123.isnull() 是空值的返回对应的Truepd.notnull(serie_123) serie_123.notnull() 不是空值对应的布尔值为True补充:numpy判断是否为空,np.isnan(i) == True,没有i == np.nan这种用法 1234np.random.seed() 随机种子,使得随机组成的数组不会改变,重新运行还是之前的三个数据a1 = np.random.randint(1,13,6)np.random.seed()a1 修改Series的索引s1.index = [新的值]serie_123.index=[‘lin’, ‘tan’, ‘shan’, ‘zhang’] Series索引重建sr1.reindex([1,2,3,4,’e’,’r’,’t’]) 新形成的索引若原来的sr1中有对应的索引则数据不变,对于sr1中没有的则显示为NaN sr1.reindex([1,2,3,4,’e’,’r’,’t’],fill_value = -1) 没有的全部填成-1,具体填什么看要求可以为0等等 缺失值填充s2=s1.reindex(range(11),method=’ffill’) 向前填充(6对应5的数据) s2=s1.reindex(range(11),method=’bfill’) 向后填充(6对应7的数据 选区Series的切片选区不仅可以通过自定义的索引去获取还能通过默认的数字去获取(即使已经自定义了同样可以使用) 注意右边可以取到是闭区间(仅适用自定义的索引) 布尔索引sr1[sr1&gt;5] =====返回符合条件的数据以及对应的行索引 Series赋值sr1[2:4] = [2,3] ======自定义的索引也可以使用切片操作 value_counts()对Series中值进行计数 返回的为值以及对应的数量pd.value_counts(series_3.values) 当成函数同样可以使用,当成函数可以用在任何序列和数组,一般不用于数据框 isin()判断数据是否在容器内返回布尔数组s1.isin([‘a’,’c’]) 每个数据都进行判断,可以当做布尔索引使用返回true的索引和值 DataFrame创建数据框方式一:df1 = pd.DataFrame(np.random.randint(1,100,(3,3)),index=[‘a’,’b’,’c’],columns=[‘d’,’e’,’f’]) 不使用index和columns时会使用默认的0,1,2…之类的 方式二:list1 = [1,3,4,5,6]list2 = [5,6,7,8,9]dict1 = {‘a’:list1,’b’:list2}df1 = pd.DataFrame(dict1) =====行索引使用的是默认的数字,列索引使用的是字典的键 索引的使用获取数据df1.loc[行索引名,列索引名] ix很少使用,基本用到行索引都要用loc loc不能直接获取某一列可以通过df1.loc[:,列索引名] ,列索引名可以为列表任意选取指定的列,行同样可以 但是行索引可以通过切片获取 如df1[‘t’:’t’]可以获取指定的行和多行列数据索引df1[‘列索引名’] =====返回对应的那一列以及对应的行索引 行数据索引df1.loc[行索引名] 或者 df1.ix[行索引名] (快要不能使用了少用) 索引的更改df1.index = [新值] df1.columns= [新值] 转置df1.transpose() 或者 df1.T 删除列索引以及对应列del df1[‘e’] ======该方法无法删除行索引添加loc也不行 删除行索引以及对应行(也可以用来删除列)df1.drop(行索引名) 默认0轴表示行,1表示列========删除相关的0,1跟记得不太一样逐行按列和按列逐行相反注意差别 数据框名字的添加(列索引行索引的统称,跟二维数组那种类似)df1.index.name = ‘名字’df1.columns.name = ‘名字 替换数据a1.replace([np.nan, 5, 7], [10, 100, 100]) 查看数据框的索引df1.index 产看数据框的值df1.values ====返回的是数组 缺失值填充同Series相同 df1.reindex([1,’t’,’tr’,’e’]) 有的写下来没有的全为NaN 修改索引修改行索引(默认为行索引) df1.reindex(index = [1,’t’,’tr’,’e’]) 修改列索引 frame2.reindex(columns=[‘four’,’three’]) 同时修改列和行索引 df1.reindex([‘row1’,’row2’,’row3’,’row4’],columns=[‘five’,’three’,’six’]) 布尔索引df1[df1[列索引名]&gt;60] =====返回对应True的行 df1.loc[df1[列索引名]&gt;60,:] 与上面的等价 df1[df1&gt;60] 只返回True对应的值False的值为NaN 行列索引有重复值通过行列索引获取会将多个都获取到df1.loc[‘r1’] df1[‘d2’] is_unique()判断是否存在重复值索引 df.index.is_unique() df.columns.is_unique series 使用s1.is_unique()即可 Get_value方法得到数据框内单个值==效果同loc但是不常用 Data.get_value(‘three’,’ihr’) 简单的算数运算和数据对齐Series按照对应的索引相加减乘除如果没有对应的返回NaN,推荐使用默认索引 DataFrame也要按照行列索引相同相加减乘除,否则会产生NaN 消除空值的方法df1.add(df2,fill_value=0) DataFrame 和 Series之间运算(索引要一一对应)df1-sr1 =====匹配列索引,数据框一行行被Series减去(广播) 一列列相减没意义,实现需要通过方法如f1.sub(s1,axis=0) min()\max()默认按列计算==df1.min(0) 按行算改成1即可 sum()df1.sum() 按列求和 df1.sum(0)df1.sum(1) 按行求和df1.sum(axis=1,skipna=False) 不跳过空值,有空值则结果为NaN idxmax()\idxmin()df1.idxmax() 按列求最大值所在的行索引 == df1.idxmax(0)df1.idxmax(1) 按行球最大值所在的列索引 返回列索引以及对应的行索引df1.idxmin() 方法同上面的maxdf1.idxmin(axis=1,skipna=False) 遇到了nan则对应为NaN cumsum()求累加和,默认按列,默认忽略空值df1.cumsum() 同 df1.cumsum(0)df1.cumsum(1) 按行求累加和 describe()df1.describe()查看df1列的众多属性 fillna()df1.fillna(0) 将所有的空值填为0,其中np.nan和None都能产生空值的效果df1.fillna({1:5,2:4}) 通过字典补值 将列索引1和2那列所有空值填成5和4df1.fillna({1:pd.Series([2,3,7],index=[0,1,2]),2:pd.Series([1,9],index=[0,1])}) 通过观察原数据的空值,确定行号,然后根据空缺值所在的位置创建Series对对应数据进行补值(常用语经验补值和业务补值) dropna()过滤空值数据s1.dropna() 删除空值及其对应的索引df1.dropna(0,how=’any’) 按行删除,将行中有空值的全删除df1.dropna(1,how=’all’) 按列删除,将列中全为空值的删除否则不删 apply和applymap(通过函数对数据处理)df1.apply(func,axis=1) 根据func是矢量还是标量决定是否对元素操作(这里的矢量指函数含有min,max等方法) 当为标量的时候是对每一个元素处理,当为矢量决定是对行还是列处理 df1.applymap(func) 对每一个数据处理 DataFrame 的格式化(非常有用)123frame11=pd.DataFrame(np.random.randn(3,4),index=[&apos;r1&apos;,&apos;r2&apos;,&apos;r3&apos;],columns=[&apos;c1&apos;,&apos;c2&apos;,&apos;c3&apos;,&apos;c4&apos;])formatierung=lambda x:&apos;%.2f&apos; % xframe11.applymap(formatierung) 排序和排名索引排序df1.sort_index(0) 逐行按列 df1.sort_index(1) 逐列按行 值排序df1.sort_values(by=某个列字段,axis=0) 按列排 df1.sort_values(by=某个行字段,axis=1) 按行排 =================================================== Series排名方法1:Series_1.rank(method=’average’,ascending=True)方法2:Series_1.rank(method=’min’)方法3:Series_1.rank(method=’max’)方法4:Series_1.rank(method=’first’)DataFrame的排名(默认是按列排的按行排加上axis=1)方法1:Frame_1.rank(method=’average’,ascending=True)方法2:Frame_1.rank(method=’min’)方法3:Frame_1.rank(method=’max’)方法4:Frame_1.rank(method=’first’)花式索引随机赋空值(块赋值)fr567.loc[np.unique(np.random.randint(3,8,5).tolist()),np.unique(np.random.randint(2,7,5).tolist())]=np.nan 层次化索引pd.Series(np.random.randn(4),index=[[‘r1’,’r1’,’t1’,’t1’],[2,3,5,7]]) 最里层索引一定唯一 散点知识点12345in判断索引是否在Series或者DataFrame里而不是数据是否在里面#时间加减操作from dateutil.relativedelta import relativedeltafrom datetime import datetimedatetime.now() - relativedelta(days=1)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[布尔数组、数组排序、数组去重]]></title>
    <url>%2F2019%2F06%2F03%2F%E5%B8%83%E5%B0%94%E6%95%B0%E7%BB%84%E3%80%81%E6%95%B0%E7%BB%84%E6%8E%92%E5%BA%8F%E3%80%81%E6%95%B0%E7%BB%84%E5%8E%BB%E9%87%8D%2F</url>
    <content type="text"><![CDATA[数据离散数据:名目数据(名字、目录,能直接描述的事物)或者是顺序数据(称为带顺序的名目数据) 连续数据:区间数据(等距数据)和等比数据(连续性的数据),区间数据没有自然0点,其0值点是人为规定的,等比数据有自然0点.温度和时间为区间数据大部分都是等比数据如身高、年龄等等 布尔数组（可用来判断空值、满足条件个数、以及是否满足条件等）bool_1.sum() 统计true值的个数 list(bool_1).count(True)也可以实现计数bool_1.sum(1） 按列计算（不同其他）bool_1.sum(0） 按行计算bool_1.any() 判断是否有Truebool_1.all() 判断是否全是True对于大混合数据（包含各种类型）需要使用dtype去声明为object类型否则会产生各种问题 如spirit_ar=np.array(daten1,dtype=object 对数据进行操作的时候尽量使用新变量和copy进行原数据的保护 数据的删除np.delete(a1,j,0) a1表示待操作的数组，j表示待操作的行索引或者列索引，0代表对行操作，1代表对列操作 多行\多列删除np.delete(a1,[2,3,4,5,6]，0) 当使用遍历数据并且进行删除的时候需要倒着删否则因为索引变化导致删除数据出现错误np.where(条件) 只有条件返回的为一个元组，两个数据分别存放着行和列数组np.arange(1,9) = np.array(range(1,9)) 数组的排序a1.sort() 默认按行排序===但是会破环数据结构a1.sort(0) 按列进行排序（小到大，里面有参数可以改为从大到小排）a1.sort(1) 按行进行排序np.sort(a1,轴参数) ==================与sort属性方法相比，函数是一次性排序而属性方法是永久排序，破环了原来的数据 数组的拼接np.vstack((a1,a2)) 垂直拼接，将数组按行拼接（列数必须相同）np.hstack((a1,a2)) 水平拼接，像数据按列拼接（行数必须相等）np.column_stack(a1,a2) 水平拼接，数据按列拼接注意：对于大数据都是分块读取的然后拼接而不是一下子读取全部 数组的去重删除重复行：np.unique(a1,axis=0) 0表示删除重复行，并对第一列进行排序删除重复列：np.unique(a1,axis=1) 1表示删除重复列，并对第一行进行排序（注意对于数据的删除操作对应的0，1不是常见的逐行按列和逐列按行） np.in1d() 一维数组的批量查询np.in1d(a1,[查询目标1，查询目标2…..])======返回的为布尔数组，找到的对应为True,没找到的为False,对每一个数据比较,满足任意一个即为Truea1查询对象可以为列表，可以为数组（注意数据类型）也可以通过布尔值使用布尔值索引去找对应的数据 数组当作集合操作 np.intersect1d(a1,a2) 两个集合的交集 返回的为公共的数据np.union1d(a1,a2) 集合的并集 【常用于一维数组】np.setdiff1d(a1,a2) a1对a2的差集np.setxor1d(a1,a2) 交集的补集（并集去掉交集）]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy基本统计函数]]></title>
    <url>%2F2019%2F05%2F31%2Fnumpy%E5%9F%BA%E6%9C%AC%E7%BB%9F%E8%AE%A1%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[基本统计概念方差（var)理论公式: 总体方差 样本方差样本方差是总体方差的无偏估计样本方差的定义中，分母的值为n-1而非n，一个重要原因即是这样定义的样本方差是总体方差的无偏估计。这被称为贝塞尔修正。 标准差(std)方差的算术平方根 期望每次出现的概率乘以结果的总和,公式在方差处 基本统计函数Sum,mean,min,max,argmax,argmin,cumsum,cumprod等函数 sum()sum(a1) 默认按列求和a1.sum() np.sum(a1) 整个数组从头加到尾a1.sum(1) np.sum(a1,1) 对每行求和a1.sum(0) np.sum(a1,0) 每列求和 mean()没有mean(a1)用法a1.mean() np.mean(a1) 从头到尾求均值a1.mean(0) np.mean(a1,0)按列求均值a1.mean(1) np.mean(a1,1) 按行求均值 min()没有min(a1)用法 ====针对a1为数组a1.min() np.min(a1)整个数组中的最小值a1.min(0) np.min(a1,0) 每列中最小值a1.min(1) np.min(a1,1) 每行中最小值 max()max(a1)用法 ====针对a1为数组a1.max() np.max(a1)整个数组中的最小值a1.max(0) np.max(a1,0) 每列中最小值a1.max(1) np.max(a1,1) 每行中最小值 argmin(),argmax()a1.ravel()可将数组伸展为一维数组与reshape相反a1.argmin() np.argmin(a1) 将数组伸展为一维,然后返回最小值所在的索引等价于 np.where(a1.ravel()==np.min(a1))[0][0]a1.argmax() np.argmax(a1) 将数组伸展为一维,然后返回最大值所在的索引等价于 np.where(a1.ravel()==np.max(a1))[0][0]np.argmax(a1,0) a1.argmax(0) 返回每列中最大值行索引np.argmin(a1,0) a1.argmin(0) 返回每列中最小值行索引np.argmax(a1,1) a1.argmax(1) 返回每行中最大值列索引np.argmin(a1,1) a1.argmin(1) 返回每行中最小值列索引 cumsum（所有元素的累积和）cumprod（所有元素的累积积）a1.cumsum() 返回一维数组,每个数据为前面数据的累加a1.cumsum(0) np.cumsum(a1,0) 按列求累加和a1.cumsum(1) np.cumsum(a1,1)按行求累加和a1.cumprod(0) np.cumprod(a1,0)按列求累积积a1.cumprod(1) np.cumprod(a1,1)按行求累积积]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[势图和数据的快速挑选]]></title>
    <url>%2F2019%2F05%2F30%2F%E5%8A%BF%E5%9B%BE%E5%92%8C%E6%95%B0%E6%8D%AE%E7%9A%84%E5%BF%AB%E9%80%9F%E6%8C%91%E9%80%89%2F</url>
    <content type="text"><![CDATA[势图实例:1234567891011121314151617181920import numpy as npimport matplotlib.pyplot as pltdef fun(x, y): return x*np.exp((-x**2)-(y**2))#创建一个画布 0表示编号,dpi表示像素,figsize表示画布大小fig = plt.figure(0,dpi=140,figsize=(8,8)) #linspace将区间等分为500份x = y = np.linspace(-2.0, 2.0, 500)#meshgrid网格化处理(笛卡尔乘积),没有的话会导致数据过少(只有经过原点角平分线上的点)X, Y = np.meshgrid(x, y)#形成z坐标(x,y,z)构成了立体图形,为一个漏斗图Z=fun(X,Y)#创建一个画布形成两行两列的画框,像素150,figsize表示画布总大小被4个平分,画框形成的为数组通过索引获取fig,axes1=plt.subplots(2,2,dpi=150,figsize=(10,10))#[0,0]表示第一行第一个画框axes1[0,0].imshow(Z)axes1[0,1].imshow(Z,cmap = plt.cm.gray)axes1[1,0].imshow(Z,cmap=plt.cm.cool)axes1[1,1].imshow(Z,cmap=plt.cm.hot)plt.show() 势图主要由很多的等势线构成,形成的三维立体与平面相切,形成的圈投影到二维平面形成等势线,众多的等势线构成了势图 np.where123456789101112131415161718192021222324252627282930313233343536373839np.where(condition,[x,y])快速找到想要的数据 如果条件为真返回x,为假返回y,如果条件加了[]会导致形成的数组维度增加一.条件为单布尔值a=23np.where([type(a)==int],'整数','小数')[0] 返回的为数组通过索引取出来二.条件为布尔矩阵实例1:def word_combination_games(positive_nr): import numpy as np import random from functools import reduce list1=['我','你','他','她','我们','你们','他们','她们'] random.shuffle(list1) list2=['学习','研究','喜欢','厌恶','专研','抛弃','讨厌','练习'] random.shuffle(list2) list3=['女人','美酒','香烟','金钱','Python','豪车','别墅','奢侈品'] random.shuffle(list3) #创建信息列表并随机打乱inf_ar1=inf_ar=np.column_stack((np.column_stack((np.array(list1),np.array(list2))),np.array(list3)))#把各列表信息以列的形式合并成二维数组 np.random.shuffle(inf_ar1)#随机打乱合并后二维数组 random_ar=[[True if np.random.rand()&gt;=0.5 else False for i in range(3)]for j in range(8)] #随机生成用作np.where条件的布尔矩阵。 a1=np.where(np.array(random_ar),inf_ar,inf_ar1) #print(a1) if positive_nr-1&gt;7: return '数字太大，超出信息列表长度' else: return reduce(lambda x,y:x+y,a1[positive_nr-1])实例2:np.where([[True, False], [True, True]],[[1, 2], [3, 4]],[[9, 8], [7, 6]])Out[15]: array([[1, 8], [3, 4]])例3:array([[17, 15, 14, 15], [17, 15, 18, 16], [20, 16, 18, 18], [15, 16, 19, 16]])np.where(ar122&gt;17) Out[20]: (array([1, 2, 2, 2, 3], dtype=int64), array([2, 0, 2, 3, 2], dtype=int64))]]></content>
      <categories>
        <category>numpy</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[向量和矩阵的各种积]]></title>
    <url>%2F2019%2F05%2F30%2F%E5%90%91%E9%87%8F%E5%92%8C%E7%9F%A9%E9%98%B5%E7%9A%84%E5%90%84%E7%A7%8D%E7%A7%AF%2F</url>
    <content type="text"><![CDATA[內积(内乘)内乘（interior product，或译内积）是光滑流形上的微分形式外代数上一个次数为 −1 导子，定义为微分形式与一个向量场的缩并。 外积外积（英语：Outer product），在线性代数中一般指两个向量的张量积，其结果为一矩阵；与外积相对，两向量的内积结果为标量.外积可视作是矩阵的克罗内克积的一种特例,向量的外积是矩阵的克罗内克积的特殊情况 点积(点乘)点积的名称源自表示点乘运算的点号，标量积的叫法则是在强调其运算结果为标量而非向量。向量的另一种乘法是叉乘（a×b），其结果为向量，称为叉积或向量积。点积是内积的一种特殊形式 叉积(叉乘)叉积（英语：Cross product）又称向量积（英语：Vector product），是对三维空间中的两个向量的二元运算，使用符号 axb。与点积不同，它的运算结果是向量 针对向量向量叉积(叉乘): 向量点积(点乘): 针对矩阵矩阵乘积:A为 mxn矩阵，B为nxp矩阵，则他们的乘积AB(有时记做A · B）会是一个 mxp矩阵.(通过行向量和列向量的內积实现) 阿达马乘积:或称做分素乘积AoB。两个m×n矩阵A、B的阿达马乘积标记为 AoB 矩阵外积:(克罗内克乘积) 两个矩阵 A和B，我们可以得到两个矩阵的直积，或称为克罗内克乘积 矩阵內积: 矩阵具有弗罗比尼乌斯内积，可以类比于向量的内积 要求:AB矩阵必须具有相同的外形,因为求trace迹(方阵对角线之和)的时候必须为方阵 矩阵点积: matlab中表现为对应位数的相乘,要求矩阵外形一样 矩阵叉积: matlab中表现为矩阵的行列相乘,等同于矩阵的乘积]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy]]></title>
    <url>%2F2019%2F05%2F28%2Fnumpy%2F</url>
    <content type="text"><![CDATA[Numpy概念1numPy(Numerical Python) 是 Python 语言的一个扩展程序库，支持大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库 ###手动创建数组123456789#anaconada自带print一般不要写某些地方要写import numpy as np创建几维数组就是嵌套几层(手动创建除了列表元组同样可以创建)除了array\asarray同样可以创建效果一样创建一维数组np.array([0.243,0.246,0.32,0.345,0.231])#可以用创建一维数组创建二维数组np.array([[1,2,3],[2,3,4]])创建三维数组np.array([[[1,2,3],[3,4,5]],[[2,3,4],[4,5,6]]]) ###基本指令1234567891011121314151617181920创建一个一维数组并且设置数据类型为浮点型a1 = np.array([1,2,3],dtype=np.float64)查看数组内数据类型a1.dtype 查看数组的数据类型type(a1) ndarray格式数组内数据的加减乘除取商取余a1*2\a1+2\a1*a1\a1**2等等都是对应的数据相乘相加等数据类型修改 一方面在定义的时候直接定义数据类型 宁一方面在定以后直接强制修改 a1.astype(np.int32)查看数组的外形a1.shape 一维数组:返回的是元素的个数如(4,)表示元素的个数为4个 效果等同于函数len() 二维数组:返回的为数组的行数和列数(2,3) 2代表行数3代表列数 三维数组:返回的是各个维度的个数(3,2,3) 表示三维数组含有3个二维数组,每个二维数组含有2行3列 同理四维数组(2,5,2,10)#2个三维数组,每个三维数组含有5个二维数组,每个二维数组2行10列返回一个内存对象a1.data ====可以通过tolist转换为列表格式a1.data.tolist ###创建特殊数组12345678910创建零数组np.zeros((5,5))#输入行数和列数=====5行5列0数组np.zeros_like(a1) 表示生成的零数组的行数列数同数组a1一样创建全为1数组np.ones((3,4))创建一个3行4列的数据np.ones_like(a1) 创建一个数组行数和列数同a1一样创建单位数组np.identity(6) 创建一个6行6列的单位矩阵(对角线全为1)创建空数组np.empty((5,5)) 创建一个5行5列的空数组(会自动填值====占位) ###自己动生成数组1234567np.random.randint(1,100,(10,3)) 表示生成一个10行3列的矩阵,矩阵内的数据为1到100间的随机数np.random.randint(1,100,10) 表示1到100件随机生成10个数可以通过reshape转换为数组格式np.random.random(6) 表示随机生成6个0到1之间的小数np.random.random((3,3)) 表示生成一个3行3列的数组,数据为0到1间的随机数np.random.normal(4,1.2,(3,5)) 服从高斯分布(正态分布) 4表示loc,1.2表示scale范围,(3,5)表示3行5列的矩阵np.arange(1,13).reshape(3,4) arange先生成12个数然后在转化为3行4列的数组(记住数据要对齐) ###数组的索引和切片12345678910一维数组的索引和切片 一维数组的索引和切片同列表的索引和切片一样,存在开始、结束和步长 test_array_1[5] test_array_1[1:5] test_array_1[-1:5:-1] test_array_1[::-1]一维数组选区的赋值： 第一种：通过列表赋值 test_array_1[1:5]=[-1,-2,-3,-1] 第二种：选区单元赋值 test_array_1[-1:3:-1]=0 全部赋为某个值 第三种：通过数组赋值 test_array_1[2:6]=np.array([0,3,6,7]) 第四种：通过元组对选区进行赋值 test_array_1[-6::-1]=(2,6,8,0,1) 或者直接去掉括号【注意：列表切片赋值原列表不变而数组切片赋值原数组会发生变化 b = a1[2:5] b[:] = [33,45,34]====原数组会发生变化，注意[:]不能少，若加上.copy()深复制就不会改变原数组,.view()为浅复制,公用一套值.数组中切片使用时一般都要加上copy保护原数组】 1234567891011二维数组的切片和索引 array_12[0] 表示取数组的0索引对应的行 array_12[:2] 选取0行和1行 array_12[1,2] 取索引为1那行列索引为2的数据 arry1[行索引,列索引] array_12[1][2] 也表示取某个数据 ary1[:,1] 取全部行的列索引为1的数据二维数组的选区和赋值(首选列表和元组,数组次之) arr123[1:3,1:3]=[[-2,-4],[-8,-1]] 注意这里必须是嵌套列表 arr123[1:3,1:3]=((0,0),(-1,-1)) arr123[1:3,1:3]=np.array([[-2,-2],[-8,-8]]) arr123[1:3,1:3]=0 12三维数组的索引和切片 a[2][3,2] 或者 a[2,3,2] 12345678910布尔值数组索引(可以选取任意行任意列)一维布尔值数组索引 bool_matrix_col=np.array([True,True,False,True,False]) rm_1[bool_matrix_row] 只返回数组中对应True的那一行 rm_1[bool_matrix_row,2:] 返回True那一行并且对列进行限定 rm_1[:,bool_matrix_row] 返回所有行,只返回对应True的那些列二维布尔值数组索引 (array_231&lt;=7)&amp;(array_231&gt;=2) 返回数组的布尔值,符合条件的数据为True,否则为false(与符号:&amp; 或符号:| ) array_231[(array_231&lt;=7)&amp;(array_231&gt;=2)]=0 将所有符合条件的选出来并且全部赋值为指定数据 array_231[(array_231&lt;=7)] 会将所有符合条件的数据取出按照一维数组格式 1234567891011花式索引第一种：ast[[0,5,15,17,19,3]] 选取中括号内的行，并且按照0-5-15-17-19-3的顺序排列第二种ar_32[[1,3,0],[2,1,2]] 返回三个数据分别为行索引为1列索引为2的数据,行索引为3列索引为1数据,行索引为0列索引为2的数据第三种oparray_1[[1,3,5,7]][:,[0,2,3,1]] 先取数组的1,3,5,7行形成新的数组,然后取所有行,以及对应的列,列的顺序按照给定的格式0,2,3,1顺序排亦可以用一个函数写ix_oparray_1[np.ix_([1,3,5,7],[0,2,3,1])] 效果一样第四种oparray_1[:,[0,2,3,1]] 取所有行以及按照对应顺序的列 ###数组的转置123456实质:数组或者矩阵的转置实质就是行列转换二维数组的转置ar12.T高维数组的转置(3维以及以上)当然二维也可以用transposearr2 = arr1.transpose((1,0,2)) 0,1,2表示维度三个表示三维,默认0,1,2(叫做轴构成对应维度空间的基础)顺序,形成的新的会按照轴的顺序进行排列注意:可以根据轴的顺序判断数据的变化以及位置,同时也可以根据数据的变化规律推导出轴顺序. ###数组的函数计算(注意与矢量化计算的区别)1234567891011常用的得一元函数np.sqrt(arr) 对数组的每一个数据都开方np.exp(arr) 以e为底对应的幂指数np.abs(arr) 数组中每个数据的绝对值np.floor(ar_12) 向下取整(小的)注意复数取整向小的而不是向近的np.ceil(ar_12) 向下取整np.sign(ar_12) 返回数组中每个数据的符号(1和-1表示)常用的二元函数np.maximum(ar_13,ar_14) 两个矩阵比较形成的新矩阵数据为两个矩阵中各自为数中大的值add,subtract,multiply,divide函数 加减乘除np.power(ar_13,ar_14) ar_13是底，ar_14是幂 ,ar_14除了是数组也可以是一个实数 小知识：Python的八大基本数据类型1.Number（数字）2.String（字符串）3.List（列表）4.Tuple（元组）5.Sets（集合）6.Dictionary（字典）7.None（空类型）8.Object（对象类型）]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析高级基础概念]]></title>
    <url>%2F2019%2F05%2F27%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E9%AB%98%E7%BA%A7%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[数据分析的步骤1.明确分析目的分析的目的可分为三类：123对现状进行描述性分析，给决策者提供未来发展方向的依据原因分析，弄清造成某种结果的原因为事物将来的发展趋势做出预测 通常在明确分析目的后，还有一个十分重要的环节，尽管它没有出现在分析流程图中，然而却异常重要。即数据分析详细流程图，它通常是“数据分析一般步骤”中“数据处理”和“数据分析”这两个步骤的详细操作和分析流程。 2.数据采集数据采集方法多种多样，这里简要的说几种：公司内部数据、互联网爬取、报刊书籍、问卷调查、购买数据等等目前来看，整个的数据采集方式发展趋于网络采集。 3.数据处理数据处理常用的方法有三种：数据清洗，数据加工，数据计算 4.数据分析数据分析方法：12345分类：通常我们按照分析的目的或者按照分析的作用进行分类：现状分析:对比、平均、综合评定（常用）……原因分析:分组、结构、交叉、杜邦分析、漏斗图、聚类……预测分析:回归、判别式分析、神经网络、决策树…… 比较分析法分类：静态与动态比较静态比较：同一时间，相同指标在不同总体间的比较，也叫横向比较，简称横比 动态比较：同一总体，相同指标在不同时期间的比较。也叫纵向比较，简称纵比 分组分析法定义： 根据数据特征，按照一定的指标，把数据划分为不同的群组进行研究，以求揭示各群组间的内在联系和规律。 分组时必须遵循两个原则：穷尽原则和互斥原则。穷尽原则：总体中的每一个单位都应有组可归，或者说各分组的空间足以容纳总体所有的单位。互斥原则：在分组指标的限制下，总体中的任何一个单位只能归属与某一个组，而不能同时或可能归属与几个组。 重要的分组参数： 组限：各分组间的分界处被称为组限，一个组的最大值称为组上限，一个组的最小值称为组下限。 组距：组上限与组下限的差叫组距 组数：分组个数 分组的步骤： 组数确定这个完全依赖于数据分析师自己的经验了。有经验的数据分析师会根据数据本身特点给数据划分合理组数。 . 确定组距一组数据中的最大值与最小值的差除以组数。即： （最大值-最小值）/组数 根据组距划分数据，使其各就各位。 结构分析法被分析总体内的各部分与总体进行对比的分析方法，即部分数量除以总体数量，换句话说，部分占总体的比例，属于相对指标 平均分析法通过计算平均数来反映总体在一定时间、地点条件下某一数量特征的一般水平。但由于其忽略了个体间可能存在的巨大差异，有时候并不一定反映出某一指标的一般水平。因此，很多时候平均分析法是不可取的。 交叉分析法交叉分析法又称立体分析法，是在纵向分析法和横向分析法的基础上，从交叉、立体的角度出发，由浅入深、由低级到高级的一种分析方法 综合评价法随着数据分析的广泛和深入的使用，我们遇到的问题越来越复杂，单靠对单一指标的分析越来越不能解决多指标问题。 人们通过实践总结，逐步形成了一系列运用多个指标对多个参评单位进行评价的方法，称之为综合评价法。综合评价法的本质是把多指标转化为一个能够反映综合情况的量（比如评分，通常不再是统计指标）来进行分析 步骤： 确定综合评价指标体系，即用哪些指标来评价一个对象。 收集数据，若有必要，进行标准化处理，比如归一化，以求消除量纲。 确定指标体系中各指标权重，以保证评价的科学性与公平性。 对各个指标的评价数值进行汇总计算，得出综合评价分值。 凭借分值排名并得出结论。 综合评价法的注意事项： 综合指标体系里面的各个指标不是让我们一个接个去评定，而是要同时完成。这种要求是为了消除互为相关的指标带来的错误评价结果 重要指标不要忘记加权。 结果不再是统计指标，仅仅是对评价对象的评分或排名。 对各指标下的数据要根据实际情况采取数据标准化。 归一化： 作用1：消除量纲，在多指标评价体系中，由于各评价指标的性质不同，通常具有不同的量纲和数量级。当各指标间的水平相差很大时，如果直接用原始指标值进行分析，就会突出数值较高的指标在综合分析中的作用，相对削弱数值水平较低指标的作用。因此，为了保证结果的可靠性，需要对原始指标数据进行标准化处理。 作用2：提升模型的收敛速度狭长的标量场经过标准化后变得比较圆，这样会大大提升计算的收敛速度。 权重: 1.要确定权重，首先要确定指标体系，这也就是综合评价的第一个步骤 2.通过以上指标体系收集数据，数据的收集是通过多个人力资源的专家填写下面表格实现的。 3.建立目标优化矩阵，进行权重确定 4.通过加权平均对表格进行综合计算 杜邦分析法金字塔结构层层分析法，各层存在明确的因果关系，更多时候是严格的函数关系。 漏斗图分析法漏斗图适用于业务流程比较规范、周期长、环节多的流程分析，通过漏斗各环节业务数据的比较，能够直观地发现和说明问题所在 几个常见的统计学概念相对数与绝对数: 绝对数描述客观事物总体在一定时间和地点条件下的总规模，总水平的指标。相对数是指两个相关事物的比值 百分数和百分点: 百分数表示个体占总体的程度。百分点表示相同事物不同时期的增幅。 频数频率： 频数是绝对数，频率是相对数。频数指某种事物或现象在其所在总体出里出现的次数,频率,个体出现的总次数与总体出现的总次数的比值 比例和比率: 比例是个体数值在总体数值中的占比 比率是总体中各个体数值之间的对比 倍数与番数： 倍数是一个数除以另一个数所得的商。比如3是1.5的二倍 番数指的是某种事物总数的二的n次方倍 同比与环比： 同比是指与历史同时期进行比较得到的数值 环比是指与前一个统计期比较所得到的数值]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy]]></title>
    <url>%2F2019%2F05%2F24%2Fscrapy%2F</url>
    <content type="text"><![CDATA[scrapy创建一个scrapy工程 scrapy startproject 工程名 生成一个爬虫程序 scrapy genspider 爬虫名 爬虫域名 执行爬虫 scrapy crawl 爬虫名 scrapy 调试平台 scrapy shell1234567Windows 装 scrapy一.pycharm直接装,装不了用第二种二. 1&gt;pip install wheel 2&gt;pip install pywin32路径 3&gt;pip install Twisted路径 4&gt;pip install scrapylinux 和 macOS 安装 pip install scrapy scrapy 架构图 散点知识:schedular封装的Request包含url和方法,默认方法为parse,具体写的通过callback调用,通过yield返回给schedularparse包含参数self和responseresponse只有body和text,没有content使用xpath匹配直接response.xpath(‘ ‘)选择器通过extract()获取data里面的数据,形成一个列表,若只有一个通过extract()[0]或者extract_first()scrapy拼接url: response.urljoin(‘不完善的url’) 不同于urllib无需基础的urlscrapy框架中的item和pipline,item是一种简单的容器,保存了爬取到的数据(类似于字典) pipline:主要是对收到的item进行处理[实现存储\清洗….]yield 返回item则将item里面的数据传给piplines进行后续处理 ,返回request则给schedular处理(封装了request和url)scrapy在response中间传递数据主要用到response.meta,存储的位置叫response.meta(存储的形式为字典的格式) yield 过后就会传递过去1234response.meta[&apos;chapter&apos;] = 1req = scrapy.Request(url=next_page, callback=self.parse)req.meta[&apos;chapter&apos;] = response.meta[&apos;chapter&apos;]yield req scrapy 数据流 start_urls 或 start_requests, 通过上面两个内容可以生成 Request(url 函数),这些 Request 被发送到 Engine 中. Engine 会将 Request 放入 Scheduler 保存, 等待下载器空闲 在下载器空闲的时候, Engine 会从 Scheduler 中获取 Request, 传递给 Downloader 传递 Request 到 Downloader 的过程中, 会经过 Downloader Middleware(process_request) 执行下载后, 会生成 Response, 返回给 Engine, 过程中会经过 Downloader Middleware(process_response) Engine 会将获取到的 Response 返回给 Spiders, 这之中会经过 Spider Middleware(process_spider_input) Spiders 会将 Response 获取具体的信息, 生成新的 Request 或者是 Item,Spiders 会将 Request/Item, 返回给 Engine. 这之中会经过 Spider Middleware(process_spider_output) Engine 会判断 Request 或者是 Item, 如果内容是 Request, 存储到 Scheduler 中,如果是 Item, 发送到 Item Pipeline 中继续处理 循环第三步]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程进程协程]]></title>
    <url>%2F2019%2F05%2F24%2F%E7%BA%BF%E7%A8%8B%E8%BF%9B%E7%A8%8B%E5%8D%8F%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819进程: 进程是资源分配和调度单位 线程: 线程是执行的单位进程池: 当我们的任务非常多时,按照以前的方法,需要创建很多的进程,进程创建过多会发生很多问题,系统会因为频繁切换进程,导致性能下降,进程池能够限制进程生成数量线程的三种基本状态:阻塞,执行,就绪进程的五种基本状态:派生,阻塞,激活,调度,结束 多线程的存在并不是为了提高运行速度,而是为了提高程序的使用率.(同一个进程如果执行路径多,更容易抢到CPU的执行权)僵尸进程:当进程退出父进程没有读取到子进程退出的返回代码就会产生僵尸进程==&gt;父进程的问题,杀死了,僵尸会变孤儿孤儿进程:一个父进程退出,而他的一个或多个子进程还在运行,这些子进程称为孤儿进程(会被init进程收养)僵尸进程与孤儿进程的区别: 孤儿进程是子进程还在运行,而父进程挂了,子进程被init进程收养,僵尸进程是父进程还在运行但是子进程挂了,但是父进程没有清理子进程的进程信息,导致资源浪费,而孤儿不会 进程间通信: queue 半双工\全双工(pipe) 共享内存 manager().dict()等 多进程(multiprocessing)\多线程(threading)\协程(asyncio)多线程是交替执行的,一次只有一个线程执行.当某个线程进入等待(io操作),则执行其他线程(相当于一个进程,用到cpu就执行这个任务,当这个任务用不到cpu就切换其他任务) 12345678并发和并行并行: 并行指同一时刻发生的两个或多个事件 并行是在不同实体上的多个事件并发: 并发性是指同一时间间隔内发生两个或多个事件 并发是在同一实体上的多个事件因此并行针对进程,并发针对线程 12345678910111213线程和进程的的关系: 1.进程内部包含所有的程序需要执行的资源 2.线程几乎不包括任何资源信息 3.线程是cpu执行的最小单位 4.进程中最少有一个线程(进程是包含进程的,可以有多个线程)推论: 1.进程和进程的资源是不共享的 2.同一进程的线程之间资源是共享的 3.多进程的程序挂掉一个,其他进程不会受影响 4.同一个进程内的多个线程,如果进程挂了,则线程都不存在了对编程的推论: 1.如果两个任务之间不共享任何资源(很少),最好用多进程 2.如果两个任务共享资源较多,最好用多进程 12345678910111213141516171819202122232425262728293031323334进程和线程创建步骤 1.新建进程\线程类 2.启动线程进程 3.等待进程\线程多进程: from multiprocessing import Process P1 = Process(target = 函数名,args = (参数)) P1.start() P1.join()多线程: from Threading import thread P1 = Thread(target= , args(,)) P1.start() P1.join()进程池的代码步骤: 1.创建进程池的类 2.将任务放进进程池 3.关闭进程池等待进程结束 from multiprocessing import Poolimport timedef hello(name): print('你好啊&#123;&#125;'.format(name)) time.sleep(5) print('我很好呢!!')if __name__ == '__main__': #创建进程池,参数表示最大支持的进程数量 pool = Pool(3) #任务放入进程池 name_list = ['你','我','他'] for name in name_list: pool.apply_async(func=hello,args=(name,)) #关闭进程池并等待进程池结束 pool.close() pool.join() 12345678910协程的用处: 1.线程尽可能多的占用cpu 2.减少cpu切换线程,提高cpu的使用效果协程代码: import gevent g1 = gevent.spawn(函数名,参数) gevent.joinall([g1,g2,g3])猴子补丁:from gevent import monkey monkey.patch_all()猴子补丁的作用:将time.sleep()变成gevent.sleep()从而使效果更直观,除此之外还有很多其他效果 1234567891011121314151617181920212223242526272829303132多进程若共享资源需要导包: 原理:队列,需要取出以后再放入from multiprocessing import Managerqueue = Manager.Queue()queue只有get和put两种操作eg: (queue,url) = queue.get(timeout=5)queue.put((func,url))多进程添加代理需要注意的问题: 1&gt;共享资源问题 2&gt;更新问题,针对response的status的处理 3&gt;会添加代理 4&gt;会创建获取proxy和更新proxy的类pipe管道conn1,conn2 = Pipe() #生成管道的两边,分别传给两个进程,一个是管道这头,一个是管道宁一端from multiprocessing import Process,Pipe# 导入进程，管道模块def f(conn): conn.send([1,'test',None]) conn.send([2,'test',None]) print(conn.recv()) conn.close()if __name__ == "__main__": parent_conn,child_conn = Pipe(True) #产生两个返回对象，一个是管道这一头，一个是另一头,True为全双工,False为半双工(第一个对象只能接收,第二个只能发送) p = Process(target=f,args=(child_conn,)) p.start() print(parent_conn.recv()) print(parent_conn.recv()) parent_conn.send('father test') p.join() 注意事项： 1、如果是windows系统， multiprocessing.Process需在if name == ‘main‘:下使用 2、args后面的参数必须是tuple类型，在这里可以认为是为整数参数20000000添加了小括号和逗号 1234567并发和并行:(通俗的解释并行指的是多个事件同时进行,并发指的是两个或多个事件在同一时间间隔发生)(一)多线程程序在一个核CPU上的运行,就是并发(二)多线程程序在多个核的CPU上运行,就是并行并发在任意时刻只有一个在工作,并行是都在工作协程:独立的栈空间,共享堆空间,调度由用户自己控制,本质上有点类型于用户级线程,这些用户级线程的调度也是自己实现的.线程:一个线程可以跑多个协程,协程是轻量级线程 多线程和多进程的使用场景 123456io操作不占用cpu(从硬盘,网络,内存读取数据都算io)计算占用cpu(如1+1计算)Python中的线程是假线程,不同线程之间的切换是需要消耗资源的,因为需要存储线程的上线文,不断的切换就会消耗资源.Python多线程适合io操作密集型的任务(如socket server网络并发这一类的)python多线程不适合cpu密集操作型的任务,主要使用cpu来计算,如大量的数学计算,CPU密集型的任务可以使用多进程来操作进程可以起多个,但是8核cpu只能对8个任务进行操作 1234567891011121314151617import multiprocessingimport time,threadingdef thread_run(): print (threading.get_ident()) #get_ident获取当前线程iddef run(name): time.sleep(2) print ('heelo',name) t = threading.Thread(target=thread_run,) #在每个进程中又起了1个线程 t.start()if __name__ == '__main__': for i in range(10): #起了10个进程 p = multiprocessing.Process(target=run,args=('bob%s' %i,)) p.start() 默认进程之间数据是不共享的,如果一定要实现互访可以通过QUEUE实现,这个queue和线程中的queue使用方法一样,不过线程中的queue只能在线程之间使用 12并发指的是多个线程被一个cpu轮流切换着执行(理解为在做一件事空闲的时候同时做宁外一件事)===&gt;理解为交替执行并行指的是被多个CPU执行 ====&gt;同时执行 PiPe(两个进程间的通信,两个进程分别位于管道的两端)12345678910111213多进程中的管道是用来实现进程间的通信的,两个进程通信,需要在内存中开辟一个空间.单向通信:半双工 a进程写,b进程读双向通信:全双工 a进程写,b进程读,也可以b进程写,a进程读Pipe(duplex)功能：创建一个管道参数：duplex默认值为True,表示管道为双向管道（全双工）如果设置为False则为单项管道（半双工）返回值：返回两个管道流对象，两个管道流对象分别表示管道的两端，如果参数为True的时候，两个对象均可发送接收，如果为False时，则第一个对象只能接收，第二个就只能发送总结: 1.向管道发送数据用send()函数,从管道接收数据使用recv()函数 2.recv()函数为阻塞函数,当管道中数据为空的时候会阻塞 3.一次recv()只能接收一次send()的内容 4.send可以发送的数据类型比较多样,字符串,数字,列表等等注意一端发送了,宁一端必须接收 获取多进程的结果然后处理123456ls = []for i in range(3): res = pool.apply_async(func,args=(i,)) ls.append(res)for data in ls: print(data.get()) 进程间通信之共享内存(python3.8) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667from multiprocessing import Value,Arrayobj = Value(ctype,data) #开辟共享内存 ctype 表示共享内存空间类型,'i','f','c'等 #data 共享空间初始数据 返回共享空间对象obj.value #对该属性的修改查看即对共享内存读写obj = Array(ctype,data)功能: 开辟共享内存空间参数: ctype:共享数据类型 data:整数则表示开辟空间建的大小,其他数据类型则表示开辟空间存放的数据Array共享内存读写： 通过遍历obj可以得到每个值，直接可以通过索引序号修改任意值。 ''' Value实例 ''' from multiprocessing import Value,Process import time import random # 创建共享内存 money = Value('i',5000) #操作共享内存 def man(): for i in range(30): time.sleep(0.2) money.value += random.randint(1,1000) def girl(): for i in range(30): time.sleep(0.15) money.value -= random.randint(100,800) m = Process(target=man) g = Process(target=girl) g.start() m.start() m.join() g.join() print("一月余额：",money.value) ''' Array实例 ''' from multiprocessing import Process,Array shm = Array('c','dd') def fun(): for i in shm: print(i) p = Process(target=fun) p.start() p.join() ####多线程1234567891011121314def func_aaa(i): print(i) time.sleep(2) return i[0]**3,i[1]*10def run_thread_pool_sub(name_list): with ThreadPoolExecutor(max_workers=2) as t: res = [t.submit(func_aaa, i) for i in name_list] for future in res: print(8979987) data = future.result() print(data)name_list = [[1,9999],[2,9999],[3,9999],[4,9999],[5,9999],[6,9999]]run_thread_pool_sub(name_list)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql面试题]]></title>
    <url>%2F2019%2F05%2F23%2Fmysql%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[用一条SQL语句查询出每门课都大于80分的学生姓名 name kecheng fenshu 张三 语文 81 张三 数学 75 李四 语文 76 李四 数学 90 王五 语文 81 王五 数学 100 王五 英语 90 1234567891011121314151617181920212223242526272829-- 建表语句CREATE TABLE `mst_stu` ( `name` varchar(255) DEFAULT NULL, `kecheng` varchar(255) DEFAULT NULL, `fenshu` int(255) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;-- 数据 INSERT INTO `mst_stu` VALUES ('张三', '语文', 81);INSERT INTO `mst_stu` VALUES ('张三', '数学', 75);INSERT INTO `mst_stu` VALUES ('李四', '语文', 76);INSERT INTO `mst_stu` VALUES ('李四', '数学', 90);INSERT INTO `mst_stu` VALUES ('王五', '语文', 81);INSERT INTO `mst_stu` VALUES ('王五', '数学', 100);INSERT INTO `mst_stu` VALUES ('王五', '英语', 90);--分析: 每门课都要大于80分,因此学生的最低分要大于80分,先根据用户分组,看每个人的最低分select name,min(fenshu) from mst_stu group by name;+--------+-------------+| name | min(fenshu) |+--------+-------------+| 张三 | 75 || 李四 | 76 || 王五 | 81 |+--------+-------------+-- 在使用 having 筛选出最低分大于80的select name,min(fenshu) as min_f from mst_stu group by name having min_f &gt; 80;-- 最终只需要学生姓名select name from (select name,min(fenshu) as min_f from mst_stu group by name having min_f &gt; 80) as s; 查询后一天 temperature 比前一天高的date 查找与之前（昨天的）日期相比温度更高的所有日期的 Id。 id date temperature 1 2013-04-01 20 2 2013-04-02 25 3 2013-04-03 21 4 2013-04-04 24 12345678910111213-- mst_Weather CREATE TABLE `mst_weather` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT, `date` date DEFAULT NULL, `temperature` int(255) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8mb4;INSERT INTO `mst_weather` VALUES (1, '2013-04-01', 20);INSERT INTO `mst_weather` VALUES (2, '2013-04-02', 25);INSERT INTO `mst_weather` VALUES (3, '2013-04-03', 21);INSERT INTO `mst_weather` VALUES (4, '2013-04-04', 24);-- 当前表做join,比较日期同时要比较温度select m2.date from mst_weather as m1 join mst_weather as m2 where datediff(m2.date,m1.date) = 1 and m2.temperature &gt; m1.temperature; 查询每个主播的最大level以及对应的最小gap(注意:不是每个主播的最大level和最小gap) zhuobo_id level gap 123 8 20 123 9 40 123 9 30 246 6 30 246 6 20 123456789101112131415161718CREATE TABLE `mst_zhubo` ( `zhubo_id` int(11) NOT NULL, `level` int(255) DEFAULT NULL, `gap` int(255) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;INSERT INTO `mst_zhubo` VALUES (123, 8, 20);INSERT INTO `mst_zhubo` VALUES (123, 9, 40);INSERT INTO `mst_zhubo` VALUES (123, 9, 30);INSERT INTO `mst_zhubo` VALUES (246, 6, 30);INSERT INTO `mst_zhubo` VALUES (246, 6, 20);--先查询出每个主播最大的levelselect zhubo_id,max(level) from mst_zhubo GROUP BY zhubo_id;-- 再查询出每个主播所有符合最大level的数据select * from mst_zhubo where (zhubo_id,level) in (select zhubo_id,max(level) from mst_zhubo GROUP BY zhubo_id) ;-- 再查询当前符合条件的数据中 gap最小的数据select zhubo_id,level,min(gap) from mst_zhubo where (zhubo_id,level) in (select zhubo_id,max(level) from mst_zhubo GROUP BY zhubo_id)GROUP BY zhubo_id,level; 下表是每个课程class_id对应的年级(共有primary、middle、high三个),以及某种比率rate class_id grade rate abc123 primary 70% abc123 middle 65% abc123 high 72% hjkk86 primary 69% hjkk86 middle 63% hjkk86 high 74% 请写出SQL查询出如下形式的表： class_id primary middle high abc123 70% 65% 72% hjkk86 69% 63% 74% 1234567891011121314151617CREATE TABLE `mst_class` ( `class_id` varchar(255) NOT NULL, `grade` varchar(255) DEFAULT NULL, `rate` varchar(255) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;INSERT INTO `mst_class` VALUES ('abc123', 'primary', '70%');INSERT INTO `mst_class` VALUES ('abc123', 'middle', '65%');INSERT INTO `mst_class` VALUES ('abc123', 'high', '72%');INSERT INTO `mst_class` VALUES ('hjkk86', 'primary', '69%');INSERT INTO `mst_class` VALUES ('hjkk86', 'middle', '63%');INSERT INTO `mst_class` VALUES ('hjkk86', 'high', '74%');-- 按照class_id进行分组,由于使用分组,则必须使用聚合函数,因此此处使用max()函数进行即可,然后使用case...when....then 进行行转列select class_id,max(CASE WHEN grade = 'primary' THEN rate ELSE 0 END) as 'primary',max(CASE WHEN grade = 'middle' THEN rate ELSE 0 END) as 'middle',max(CASE WHEN grade = 'high' THEN rate ELSE 0 END) as 'high'from mst_class group by class_id; 怎么把这样一个表 year month amount 1991 1 1 1991 2 2 1991 3 3 1991 4 4 1992 1 1 1992 2 2 1992 3 3 1992 4 4 查成这样一个结果 year m1 m2 m3 m4 1991 1 2 3 4 1992 1 2 3 4 1234567891011121314151617181920CREATE TABLE `mst_year` ( `year` int,`month` int,`amount` float ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;INSERT INTO `mst_year` VALUES (1991, 1, 1);INSERT INTO `mst_year` VALUES (1991, 2, 2);INSERT INTO `mst_year` VALUES (1991, 3, 3);INSERT INTO `mst_year` VALUES (1991, 4, 4);INSERT INTO `mst_year` VALUES (1992, 1, 1);INSERT INTO `mst_year` VALUES (1992, 2, 2);INSERT INTO `mst_year` VALUES (1992, 3, 3);INSERT INTO `mst_year` VALUES (1992, 4, 4);-- 同上一个问题类似,按照 year 进行分组,使用case...when....then 进行行转列,需要注意的是要对结果进行小数位限制select year,sum(CASE WHEN month = 1 THEN round(amount,1) ELSE 0 END) as 'm1',sum(CASE WHEN month = 2 THEN round(amount,1) ELSE 0 END) as 'm2',sum(CASE WHEN month = 3 THEN round(amount,1) ELSE 0 END) as 'm3',sum(CASE WHEN month = 4 THEN round(amount,1) ELSE 0 END) as 'm4'from mst_year group by year; 有两个表A和B，均有key和value两个字段，如果B的key在A中也有，就把B的value换为A中对应的value 这道题的SQL语句怎么写？ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283-- 先按题设计AB表,并查看结果CREATE TABLE `mst_a` ( `key` varchar,`value` varchar);INSERT INTO `mst_a` VALUES ('A', 'aaa');INSERT INTO `mst_a` VALUES ('B', 'bbb');INSERT INTO `mst_a` VALUES ('C', 'ccc');CREATE TABLE `mst_b` ( `key` varchar,`value` varchar);INSERT INTO `mst_a` VALUES ('D', 'ddd');INSERT INTO `mst_a` VALUES ('E', 'eee');INSERT INTO `mst_a` VALUES ('A', 'abc');-- 查看当前两个表的数据mysql&gt; select * from mst_a;+------+-------+| key | value |+------+-------+| A | aaa || B | bbb || C | ccc |+------+-------+3 rows in set (00 sec)mysql&gt; select * from mst_b;+------+-------+| key | value |+------+-------+| D | ddd || E | eee || A | abc |+------+-------+3 rows in set (00 sec)-- 有两个表A和B，均有key和value两个字段，如果B的key在A中也有，就把B的value换为A中对应的value-- 分析,先查询符合条件的数据,(B的key在A中也有)select mst_a.*,mst_b.* from mst_a join mst_b on mst_a.key = mst_b.key;+------+-------+------+-------+| key | value | key | value |+------+-------+------+-------+| A | aaa | A | abc |+------+-------+------+-------+1 row in set (00 sec)-- 更新数据 update mst_b set value = ? where key = ?-- 根据以上的更新语句需要的数据,针对性的获取,先获取第二个问号中的keyselect mst_b.key from mst_a join mst_b on mst_a.key = mst_b.key;+------+| key |+------+| A |+------+-- 在获取需要更新的值?也就是a表中的对应key的valueselect mst_a.value,b.key from mst_a,(select mst_b.key from mst_a join mst_b on mst_a.key = mst_b.key) bwhere mst_a.key = b.key;-- 在将两次sql结果嵌套到对应的update中-- 注意事项:-- update 后面可以做任意的查询，这个作用等同于from-- update 时，更新的表不能在set和where中用于子查询-- update 时，可以对多个表进行更新（sqlserver不行）update mst_b as up, (select mst_a.value,b.key from mst_a,(select mst_b.key from mst_a join mst_b on mst_a.key = mst_b.key) bwhere mst_a.key = b.key) b set up.value = b.value where up.key = b.key;-- 再次查询结果mysql&gt; select * from mst_a;+------+-------+| key | value |+------+-------+| A | aaa || B | bbb || C | ccc |+------+-------+3 rows in set (00 sec)mysql&gt; select * from mst_b;+------+-------+| key | value |+------+-------+| D | ddd || E | eee || A | aaa |+------+-------+3 rows in set (00 sec) 设计表，关系如下：教室、班级、学生、科室、科室与教师为一对多关系，教师与班级为多对多关系，班级与学生为一对多关系，科室中需体现层级关系。 1．写出各张表的逻辑字段 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879教师 mst_Teacher+-----+--------------+| Tid | Tname |+-----+--------------+| 1 | 张三老师 || 2 | 李四老师 || 3 | 王五老师 |+-----+--------------+CREATE TABLE `mst_teacher` ( `Tid` int PRIMARY KEY AUTO_INCREMENT, `Tname` varchar(10) );insert into mst_teacher VALUES(1,'张三老师'),(2,'李四老师'),(3,'王五老师');班级 mst_cla+-----+-------+| Cid | Cname |+-----+-------+| 1 | 1班 || 2 | 2班 || 3 | 3班 |+-----+-------+CREATE TABLE `mst_cla` ( `Cid` int PRIMARY KEY AUTO_INCREMENT, `Cname` varchar(10) );insert into mst_cla VALUES(1,'1班'),(2,'2班'),(3,'3班');教师&amp;班级 mst_tc+-----+------+------+| pid | Tid | Cid |+-----+------+------+| 1 | 1 | 1 || 2 | 2 | 1 || 3 | 3 | 1 || 4 | 1 | 2 || 5 | 2 | 2 || 6 | 3 | 2 || 7 | 1 | 3 || 8 | 2 | 3 || 9 | 3 | 3 |CREATE TABLE `mst_tc` ( `pid` int PRIMARY KEY AUTO_INCREMENT, `Tid` int,`Cid` int );insert into mst_tc VALUES (1,1,1),(2,2,1),(3,3,1),(4,1,2),(5,2,2),(6,3,2),(7,1,3),(8,2,3),(9,3,3);学生 mst_St +-----+--------+------+| SId | Sname | Cid |+-----+--------+------+| 1 | 赵雷 | 1 || 2 | 钱电 | 1 || 3 | 孙风 | 1 || 4 | 李云 | 2 || 5 | 周梅 | 2 || 6 | 吴兰 | 3 || 7 | 郑竹 | 3 |+-----+--------+------+CREATE TABLE `mst_St` ( `SId` int PRIMARY KEY AUTO_INCREMENT, `Sname` varchar(20),`Cid` int );insert into mst_St VALUES(1,'赵雷',1),(2,'钱电',1),(3,'孙风',1),(4,'李云',2),(5,'周梅',2),(6,'吴兰',3),(7,'郑竹',3);科室 mst_ks+-----+-----------+------+------+| Kid | Kname | Pid | Tid |+-----+-----------+------+------+| 1 | 校长室 | 0 | 1 || 2 | 教务处 | 1 | 1 || 3 | 教学处 | 1 | 2 || 4 | 语文 | 3 | 1 || 5 | 数学 | 3 | 2 || 6 | 英语 | 3 | 3 |+-----+-----------+------+------+CREATE TABLE `mst_ks` ( `Kid` int PRIMARY KEY AUTO_INCREMENT, `Kname` varchar(20),`Pid` int ,`Tid` int );insert into mst_ks VALUES(1,'校长室',0,1),(2,'教务处',1,1),(3,'教学处',1,2),(4,'语文',3,1),(5,'数学',3,2),(6,'英语',3,3); 根据上述表关系 ​ 查询教师id=1的学生数 1234-- 已知教师Id就可以先到教师与班级的关系表中获取当前老师所带的班级select cid from mst_tc where Tid = 1;-- 获取所带班级后,到学生表中获取所带班级中的学员数即可select count(*) from mst_St where Cid in (select cid from mst_tc where Tid = 1); ​ 查询科室id=3的下级部门数 12-- 科室id=3的下级部门的pid也就等于科室id,因此查询当前科室的父级id = 3的即可select count(*) from mst_ks where pid = 3; ​ 查询所带学生最多的教师id 12345678910111213141516171819202122232425-- 先查询出每个老师带的学生数select mst_teacher.tid,mst_teacher.tname,count(mst_st.sid) as count_stufrom mst_teacher,mst_tc,mst_Stwhere mst_teacher.tid = mst_tc.tidand mst_tc.cid = mst_st.cidgroup by mst_teacher.tid,mst_teacher.tname;+-----+--------------+-----------+| tid | tname | count_stu |+-----+--------------+-----------+| 1 | 张三老师 | 7 || 2 | 李四老师 | 7 || 3 | 王五老师 | 7 |+-----+--------------+-----------+-- 因为目前所有老师带的班级都一样,所以数据结果都是一样的,因此可以改变一个老师所带的科目后在查询,比如语文和数学是一个老师带的,再次查询select mst_teacher.tid,mst_teacher.tname,count(mst_st.sid) as count_stufrom mst_teacher,mst_tc,mst_Stwhere mst_teacher.tid = mst_tc.tidand mst_tc.cid = mst_st.cidgroup by mst_teacher.tid,mst_teacher.tname;+-----+--------------+-----------+| tid | tname | count_stu |+-----+--------------+-----------+| 2 | 李四老师 | 14 || 3 | 王五老师 | 7 |+-----+--------------+-----------+ (1)某奶粉品牌有以下销售数据(订单表Orderinfo)，请计算每个人得消费金额、消费频次、购买产品数量、第一次购买时间和最后一次购买时间。 CustomerID OrderID Sales Quantity OrderDate A 01 100 1 2017-03-01 A 02 420 3 2017-03-15 B 03 300 4 2017-03-02 B 04 1000 1 2017-04-01 C 05 500 3 2017-05-03 C 06 200 1 2017-05-04 …… 12345678910111213141516create table `mst_Orderinfo`(`CustomerID` char(5),`OrderID` int PRIMARY KEY AUTO_INCREMENT,`Sales` int,`Quantity` int,`OrderDate` date);insert INTO mst_Orderinfo VALUES ('A',01,100,1,'2017-03-01'), ('A',02,420,3,'2017-03-15'), ('B',03,300,4,'2017-03-02'), ('B',04,1000,1,'2017-04-01'), ('C',05,500,3,'2017-05-03'), ('C',06,200,1,'2017-05-04');--计算每个人得消费金额、消费频次、购买产品数量、第一次购买时间和最后一次购买时间。select CustomerID,sum(Sales) as '消费金额',count(CustomerID) as '消费频次',sum(Quantity) as '购买产品数量',min(OrderDate) as '第一次购买时间',max(OrderDate) as '最后一次购买时间'from mst_Orderinfo group by CustomerID; (2)该奶粉品牌还有一张订单明细表(OrderDetail ),请结合上题得订单表，计算出每个SKU被多少客户购买了。 OrderDetailID OrderID SKU Qutity 01 01 SKU1 1 02 02 SKU1 2 03 02 SKU2 1 04 03 SKU2 2 05 03 SKU3 2 06 04 SKU6 1 07 05 SKU4 2 …… 12345678910111213141516171819create table `mst_OrderDetail`(`OrderDetailID` int PRIMARY KEY AUTO_INCREMENT,`OrderID` int,`SKU` char(5),`Qutity` int);insert into mst_OrderDetail values(1,1,'SKU1',1),(2,2,'SKU1',2),(3,2,'SKU2',1),(4,3,'SKU2',2),(5,3,'SKU3',2),(6,4,'SKU6',1),(7,5,'SKU4',2);-- 分析:如果要统计出每个sku被多少客户购买了.就需要先把两个表做join,观察select * from mst_Orderinfo o1,mst_OrderDetail o2where oOrderID = oOrderID;-- 计算出每个SKU被多少客户购买了,需要以sku分组,去统计用户id并去重select SKU,count(distinct CustomerID) from mst_Orderinfo o1,mst_OrderDetail o2where oOrderID = oOrderIDgroup by SKU;+------+----------------------------+| SKU | count(distinct CustomerID) |+------+----------------------------+| SKU1 | 1 || SKU2 | 2 || SKU3 | 1 || SKU4 | 1 || SKU6 | 1 |+------+----------------------------+ (3)请结合Orderinfo表与OrderDetail表，找出购买了SKU1又购买SKU2产品的人。 123456789101112131415161718192021222324-- 先查询出购买过SKU1和SKU2的用户select * from mst_Orderinfo o1join mst_OrderDetail o2 on oOrderID = oOrderIDwhere oSKU in ('SKU1','SKU2');+------------+---------+-------+----------+------------+---------------+---------+------+--------+| CustomerID | OrderID | Sales | Quantity | OrderDate | OrderDetailID | OrderID | SKU | Qutity |+------------+---------+-------+----------+------------+---------------+---------+------+--------+| A | 1 | 100 | 1 | 2017-03-01 | 1 | 1 | SKU1 | 1 || A | 2 | 420 | 3 | 2017-03-15 | 2 | 2 | SKU1 | 2 || A | 2 | 420 | 3 | 2017-03-15 | 3 | 2 | SKU2 | 1 || B | 3 | 300 | 4 | 2017-03-02 | 4 | 3 | SKU2 | 2 |+------------+---------+-------+----------+------------+---------------+---------+------+--------+-- 在根据用户分组,筛选购买了不同的sku的数等于2 的select oCustomerID from mst_Orderinfo o1 join mst_OrderDetail o2 on oOrderID = oOrderIDwhere oSKU in ('SKU1','SKU2')GROUP BY oCustomerIDHAVING COUNT(DISTINCT oSKU) = 2+------------+| CustomerID |+------------+| A |+------------+ 现有一张房源表。 字段有房源基础信息以及小区id，小区name字段。 求sql查出top10的房源量的小区，按房源量降序排序。 1select 小区id,小区name,count(*) as 房源量 from 房源表 group by 小区id,小区name order by 房源量 desc limit 10; 说一下你熟悉的数据库，这个数据库有什么特点？ 请用sql描述，如何给表中插入数据？如何更新数据？ 你常用的mysql引擎有哪些？各引擎间有什么区别？ MySQL数据库中现有如下表，名为tbl: id name age sex 1 zhangsan 25 1 2 lisi 22 0 3 wangwu 35 0 4 zhaoliu 24 1 5 tianqi 32 1 按照要求写出SQL语句: (1)查询前3条纪录。 (2)查询所有纪录，并按照字段age降序排列。 请描述MySQL中left join和inner joinc的区别。 left join 左连接,以左边的表为基础 inner join 自连接\内连接 以公共部分为基础 请写出你知道MySQL储存引擎和区别。 请列出你最常使用的mysql版本，mysql默认端口号是多少？请写出你最常用的mysql数据库备份和恢复命令。 mysql 3306 mongodb 27017 redis 6379 数据库A:datahn，其中具有表c、表d。 数据库B:datapubhn，现想要在数据库B中对数据库A中表c与表d中的字段进行增删改查。如何授权？ 对于处理高复杂sql，数据库查询特别慢，你有什么高招？ 1.优化sql语句==避免子查询,避免使用join连接 2.适当使用索引 3.优化数据库结构 4.优化系统配置 5.优化服务器结构 以下哪条语句在数据库sql书写时最为高效( b ) A.select * from emp where dep&gt;3 B.select * from emp where dep&gt;=4 C.select * from emp where dep≥4 D.select * from emp where dep&gt;=3 and dep≠4 数据库修改用户时，用户的什么属性不能修改？( ) A.密码 B.名称 C.表空间 D.临时表空间 数据库中执行删除操作，若要删除表中的所有行，建议使用( b ) A.delete 语句 B.turncate 语句 C.drop 语句 D.commit 语句 2在查询语句的select子句中尽量避免使用__来表示全部列名。 SQL中增加、删除、修改对应的命令是什么？ 查找条件为：姓名不是NULL的纪录( c ) A.WHERE NAME ! NULL B.WHERE NAME NOT NULL C.WHERE NAME IS NOT NULL D.WHERE NAME!=NULL 在SQL语言中，子查询是( d ) A.选取单表中字段子集的查询语句 B.选取多表中字段子集的查询语句 C.返回单表中数据子集的查询语言 D.嵌入到另一个查询语句之中的查询语句 以下能够删除一列的是( b ) A.alter table emp remove addcolumn B.alter table emp drop column addcolumn C.alter table emp delete column addcolumn D.alter table emp delete addcolumn 学生关系模型S(S#,Sname,Sex,Age),S的属性分别表示学生的学号、姓名、性别、年龄。要在表S中删除一个属性”年龄”,可选用的SQL语句是( d ) A.UPDATE S Age B.DELETE Age from S C.ALTER TABLE S ‘Age’ D.ALTER TABLE S DROP Age 补全语句:select vend_id,count(*) as num_prods from products group by______vend_id__; SELECT distinct 部门名称，count(员工编号)，sum(个人工资) FROM 工资表 GROUP BY 部门名称 having count (员工编号&gt;10);查询结果是___。 用SELECT进行模糊查询时，可以使用匹配符，但要在条件值中使用__或%等通配符来配合查询。 MySQL是一种___多用户 _(多用户、单用户)的数据库管理系统。 对一个超过200个汉字的内容，应用一个varchar或者text型的字段来存放。 查看当前数据库中表名语句是_; Mysql数据库GD2312、utf8\utf8mb4字符集的区别___。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql-50题]]></title>
    <url>%2F2019%2F05%2F23%2Fmysql-50%E9%A2%98%2F</url>
    <content type="text"><![CDATA[数据表介绍123456789101112--1.学生表Student(SId,Sname,Sage,Ssex)--SId 学生编号,Sname 学生姓名,Sage 出生年月,Ssex 学生性别--2.课程表Course(CId,Cname,TId)--CId 课程编号,Cname 课程名称,TId 教师编号--3.教师表Teacher(TId,Tname)--TId 教师编号,Tname 教师姓名--4.成绩表SC(SId,CId,score)--SId 学生编号,CId 课程编号,score 分数 练习题目1.查询” 01 “课程比” 02 “课程成绩高的学生的信息及课程分数 123456789101112131415161718192021222324252627-- 此处可以先考虑分别查询 01和02课程的学员id和分数select SId,score from SC where CId = '01';select SId,score from SC where CId = '02';-- 对比结果可以清楚的知道,两个表中有些SId不对应,不对应就没有可比性-- 因此可以对两个结果做join,条件就是SId要相等,-- 并且01的成绩要比02大select s1.SId,s1.scorefrom (select SId,score from SC where CId = '01') s1join(select SId,score from SC where CId = '02') s2on s1.SId = s2.SIdwhere s1.score &gt; s2.score-- 通过以上的sql得到了符合条件的学员id和分数,再到学员表中获取学员信息select stu.SId,stu.Sname,s.score from student as stu right join ( select s1.SId,s1.score from (select SId,score from SC where CId = '01') s1 join (select SId,score from SC where CId = '02') s2 on s1.SId = s2.SId where s1.score &gt; s2.score ) as son stu.SId = s.SId; 查询同时存在” 01 “课程和” 02 “课程的情况 1234567-- 本题和上一题内容相似,我们只需要把01和02的学员的SID作为连接条件就可以select s1.*from (select SId,score from SC where CId = '01') s1join(select SId,score from SC where CId = '02') s2on s1.SId = s2.SId 查询存在” 01 “课程但可能不存在” 02 “课程的情况(不存在时显示为 null ) 123456select s1.SId,s2.scorefrom (select SId,score from SC where CId = '01') s1left join(select SId,score from SC where CId = '02') s2on s1.SId = s2.SId 查询不存在” 01 “课程但存在” 02 “课程的情况 123select * from SC where SId not in(select SId from SC where CId = '01')and CId = '02' 5.查询平均成绩大于等于 60 分的同学的学生编号和学生姓名和平均成绩 12345-- 此题主要考核 分组后的条件过滤select SC.SId,Sname,round(avg(score),2) as avg_scorefrom SC,Studentwhere SC.SId = Student.SIdgroup by SC.SId,Sname having avg_score &gt;= 60; 6.查询在 SC 表存在成绩的学生信息 12-- 此题主要考核 数据去重 distinct select distinct stu.* from Student stu join SC on SC.SId = stu.SId; 7.查询所有同学的学生编号、学生姓名、选课总数、所有课程的总成绩(没成绩的显示为 null ) 123select stu.SId,stu.Sname,count(SC.CId) as total ,sum(SC.score) as sum_scorefrom Student as stu left join SC on stu.SId = SC.SIdgroup by stu.SId,stu.Sname; 8.查询「李」姓老师的数量 1select count(*) from teacher where Tname like '李%'; 9.查询学过「张三」老师授课的同学的信息 123456select stu.*from Student as stu join SC on stu.SId = SC.SIdjoin course as C on SC.CId = C.CIdjoin Teacher as T on C.TId = T.TIdwhere T.Tname = '张三'; 10.查询没有学全所有课程的同学的信息 1234-- 排除法,找到所有学过全部课程的学生,select * from Student where SId not in( select SId from SC group by SId having count(CId) = (select count(*) from course)) 11.查询至少有一门课与学号为” 01 “的同学所学相同的同学的信息 1234select distinct stu.* from student as stu left join sc on sc.SId = stu.SId where sc.CId in (select CId from sc where SId = '01'); 12.查询和” 01 “号的同学学习的课程 完全相同的其他同学的信息 123456789101112-- 注意要对比的是课程ID号,在对比课程数select s2.SIdfrom SC as s1 join SC as s2on s1.CId = s2.CId and s1.SId = '01' and s2.SId != '01' group by s2.SId having count(s1.SId) = (select count(*) from SC where SId = '01');select stu.* from student as stu left join sc on sc.SId = stu.SId join ( select CId from sc where SId = '01') as t on t.CId = sc.CId group by stu.SId having count(sc.SId) = 5; 13.查询没学过”张三”老师讲授的任一门课程的学生姓名 1234567select SId,Sname from Student where SId not in (select SId from SC where CId = ( select Course.Cid from Course join Teacher on Teacher.TId = Course.TId where Teacher.Tname = '张三' )) 14.查询两门及其以上不及格课程的同学的学号，姓名及其平均成绩 1234select SC.SId,Student.Sname,round(avg(SC.score),2) as avg_sc from SC join Student on Student.SId = SC.SIdwhere SC.score &lt; 60 group by SC.SId,Student.Sname having count(SC.CId) &gt;=2; 15.检索” 01 “课程分数小于 60，按分数降序排列的学生信息 1234select SC.SId,Student.Sname,SC.scorefrom SC join Student on SC.SId = Student.SId where SC.CId = '01' and SC.score &lt; 60 order by SC.score desc; 16.按平均成绩从高到低显示所有学生的所有课程的成绩以及平均成绩 123456789101112131415select SC.*,s2.avg_sc from SC join (select SId, avg(score) as avg_sc from SC group by SId) as s2on SC.SId = s2.SIdorder by avg_sc desc, SC.SId;--最佳解决方案 王宇鹏select a.sname,b.score 语文,c.score 数学,d.score 英语,avg(e.score) from student a left join sc b on a.sid=b.sid and b.cid='01' left join sc c on a.sid=c.sid and c.cid='02' left join sc d on a.sid=d.sid and d.cid='03' left join sc e on a.sid=e.sid group by a.sname,语文,数学,英语order by avg(e.score) desc; 17.查询各科成绩最高分、最低分和平均分：以如下形式显示：课程 ID，课程 name，最高分，最低分，平均分，及格率，中等率，优良率，优秀率 及格为&gt;=60，中等为：70-80，优良为：80-90，优秀为：&gt;=90要求输出课程号和选修人数，查询结果按人数降序排列，若人数相同，按课程号升序排列 12345678910111213select SC.CId,C.Cname,max(SC.score) as '最高分',min(SC.score) as '最低分',avg(SC.score) as '平均分',count(SC.CId) as '选修人数',SUM(CASE WHEN SC.score &gt;= 60 THEN 1 ELSE 0 END) / count(SC.CId) as '及格率',SUM(CASE WHEN SC.score &gt;= 70 and SC.score &lt; 80 THEN 1 ELSE 0 END) / count(SC.CId) as '中等率',SUM(CASE WHEN SC.score &gt;= 80 and SC.score &lt; 90 THEN 1 ELSE 0 END) / count(SC.CId) as '优良率',SUM(CASE WHEN SC.score &gt;= 90 THEN 1 ELSE 0 END) / count(SC.CId) as '优秀率'from SC,Course as Cwhere SC.CId = C.CIdgroup by SC.CId,C.Cnameorder by '选修人数' desc,SC.CId; 18.按各科平均成绩进行排序，并显示排名， Score 重复时保留名次空缺 1234567891011121314151617181920212223242526272829303132333435363738select s2.CId,s2.avg_sc,count(distinct s1.avg_sc) as Rankfrom(select avg(score) as avg_sc from SC group by CId) as s1join (select CId,avg(score) as avg_sc from SC group by CId) as s2on s1.avg_sc &gt;= s2.avg_scgroup by s2.CId order by Rank;-- 解析过程select s1.*,s2.*from(select CId,avg(score) as avg_sc from SC group by CId) as s1join (select CId,avg(score) as avg_sc from SC group by CId) as s2on s1.avg_sc &gt;= s2.avg_scgroup by s2.CId order by Rank;-- 在这种情况下 去统计s1.平均分 还是去统计s2.平均分 的次数,数据中都有重复-- 如果能把s1中的出现重复的平均分数去重,再去按照s1.平均分去统计次数就正常 +------+----------+------+----------+ | CId | avg_sc | CId | avg_sc | +------+----------+------+----------+ | 01 | 64.50000 | 01 | 64.50000 | | 02 | 79.33333 | 01 | 64.50000 | | 03 | 70.00000 | 01 | 64.50000 | | 04 | 79.30000 | 01 | 64.50000 | --| 5 | 70.00000 | 01 | 64.50000 | | 02 | 79.33333 | 02 | 79.33333 | | 02 | 79.33333 | 03 | 70.00000 | | 03 | 70.00000 | 03 | 70.00000 | | 04 | 79.30000 | 03 | 70.00000 | --| 5 | 70.00000 | 03 | 70.00000 | | 02 | 79.33333 | 04 | 79.30000 | | 04 | 79.30000 | 04 | 79.30000 | | 02 | 79.33333 | 5 | 70.00000 | | 03 | 70.00000 | 5 | 70.00000 | | 04 | 79.30000 | 5 | 70.00000 | --| 5 | 70.00000 | 5 | 70.00000 | +------+----------+------+----------+ 19.按各科平均成绩进行排序，并显示排名， Score 重复时不保留名次空缺 123select b.CId,b.avg_sc,@i := @i+1 as Rank from (select @i := 0) a,(select CId,round(avg(Score),2) as avg_sc from SC group by CId order by avg_sc desc) b; 20.查询学生的总成绩，并进行排名，总分重复时保留名次空缺 1234567select s2.SId,s2.avg_sc,count(distinct s1.avg_sc) as Rankfrom(select sum(score) as avg_sc from SC group by SId) as s1join (select SId,sum(score) as avg_sc from SC group by SId) as s2on s1.avg_sc &gt;= s2.avg_scgroup by s2.SId order by Rank; 21.查询学生的总成绩，并进行排名，总分重复时不保留名次空缺 123select b.SId,b.avg_sc,@i := @i+1 as Rank from (select @i := 0) a,(select SId,sum(Score) as avg_sc from SC group by sId order by avg_sc desc) b; 22.统计各科成绩各分数段人数：课程编号，课程名称，[100-85]，[85-70]，[70-60]，[60-0]及所占百分比 123456789101112select Course.CId,Course.Cname,sum(CASE WHEN SC.score &gt;= 85 and SC.score &lt;= 100 THEN 1 ELSE 0 END) '[100-85]',concat(round(sum(CASE WHEN SC.score &gt;= 85 and SC.score &lt;= 100 THEN 1 ELSE 0 END)/count(SC.SId),2)*100,'%') as '百分比',sum(CASE WHEN SC.score &gt;= 70 and SC.score &lt; 85 THEN 1 ELSE 0 END) '[85-70]',concat(round(sum(CASE WHEN SC.score &gt;= 70 and SC.score &lt; 85 THEN 1 ELSE 0 END)/count(SC.SId),2)*100,'%') as '百分比',sum(CASE WHEN SC.score &gt;= 60 and SC.score &lt; 70 THEN 1 ELSE 0 END) '[70-60]',concat(round(sum(CASE WHEN SC.score &gt;= 60 and SC.score &lt; 70 THEN 1 ELSE 0 END)/count(SC.SId),2)*100,'%') as '百分比',sum(CASE WHEN SC.score &gt;= 0 and SC.score &lt; 60 THEN 1 ELSE 0 END) '[60-0]',concat(round(sum(CASE WHEN SC.score &gt;= 0 and SC.score &lt; 60 THEN 1 ELSE 0 END)/count(SC.SId),2)*100,'%') as '百分比'from SC,Coursewhere SC.CId = Course.CIdgroup by Course.CId,Course.Cname; 23.查询各科成绩前三名的记录 12345(select CId,score from SC where CId = '01' order by score desc limit 3) union all(select CId,score from SC where CId = '02' order by score desc limit 3) union all(select CId,score from SC where CId = '03' order by score desc limit 3) 24.查询每门课程被选修的学生数 1select CId,count(SId) from SC group by CId; 25.查询出只选修两门课程的学生学号和姓名 123select SC.SId,Student.Snamefrom SC join Student on SC.SId = Student.SIdgroup by SC.SId,Student.Sname having count(SC.CId) = 2; 26.查询男生、女生人数 1select Ssex,count(*) from Student group by Ssex; 27.查询名字中含有「风」字的学生信息 1select * from Student where Sname like '%风%'; 28.查询同名同性学生名单，并统计同名人数 1234select s1.Sname,count(s1.Sname)from Student s1 join Student s2 on s1.Sname = s2.Sname and s1.Ssex = s2.Ssex and s1.SId != s2.SIdgroup by s1.Sname; 29.查询 1990 年出生的学生名单 1select * from Student where year(Sage) = 1990; 30.查询每门课程的平均成绩，结果按平均成绩降序排列，平均成绩相同时，按课程编号升序排列 12select CId,avg(score) as avg_sc from SC group by CId order by avg_sc desc,CId; 31.查询平均成绩大于等于 85 的所有学生的学号、姓名和平均成绩 123select SC.SId,Stu.Sname,round(avg(SC.score),2) as avg_sc from SC join Student as Stu on SC.SId = Stu.SIdgroup by SC.SId,Stu.Sname having avg_sc &gt;= 85; 32.查询课程名称为「数学」，且分数低于 60 的学生姓名和分数 1234select C.Cname,Stu.Sname,SC.scorefrom Course as C join SC on C.CId = SC.CId join Student as Stu on SC.SId = Stu.SIdwhere C.Cname = '数学' and SC.score &lt; 60; 33.查询所有学生的课程及分数情况（存在学生没成绩，没选课的情况） 12345678910111213select Student.Sname,Course.Cname,SC.scorefrom Student left join SC on Student.SId = SC.SIdleft join Course on SC.CId = Course.CIdorder by Student.Sname;-- 张波select a.SId,a.sname,b.score 语文,c.score 数学,d.score 英语 from student a left join sc b on a.sid=b.sid and b.cid='01' left join sc c on a.sid=c.sid and c.cid='02' left join sc d on a.sid=d.sid and d.cid='03' group by a.SId,a.sname,语文,数学,英语order by a.Sid; 34.查询任何一门课程成绩在 70 分以上的姓名、课程名称和分数 1234select Student.Sname,Course.Cname,SC.scorefrom Student,Course,SC where Student.SId = SC.SId and SC.CId = Course.CIdand SC.score &gt; 70; 35.查询不及格的课程 1234select Student.Sname,Course.Cname,SC.scorefrom Student,Course,SC where Student.SId = SC.SId and SC.CId = Course.CIdand SC.score &lt; 60; 36.查询课程编号为 01 且课程成绩在 80 分以上的学生的学号和姓名 123select SC.SId,Student.Snamefrom SC join Student on SC.SId = Student.SIdwhere SC.Score &gt; 80 and SC.CId = '01'; 37.求每门课程的学生人数 1select CId,count(SId) from SC group by CId; 38.成绩不重复，查询选修「张三」老师所授课程的学生中，成绩最高的学生信息及其成绩 123456select Student.SId,Student.Sname,SC.scorefrom Student join SC on Student.SId = SC.SIdjoin Course on SC.CId = Course.CIdjoin Teacher on Teacher.TId = Course.TIdwhere Teacher.Tname = '张三';order by SC.score desc limit 1; 39.成绩有重复的情况下，查询选修「张三」老师所授课程的学生中，成绩最高的学生信息及其成绩 12345678910111213141516171819202122232425262728293031323334-- 查询张三老师授课的学生中最高分数,查询所有等于最高分的-- -- 先查询最高分数-- select max(SC.score) -- from SC join Course on SC.CId = Course.CId-- join Teacher on Teacher.TId = Course.TId-- where Teacher.Tname = '张三'-- 追加 分数条件select Student.SId,Student.Sname,SC.scorefrom Student join SC on Student.SId = SC.SIdjoin Course on SC.CId = Course.CIdjoin Teacher on Teacher.TId = Course.TIdwhere Teacher.Tname = '张三' and SC.score = (select max(SC.score) from SC join Course on SC.CId = Course.CIdjoin Teacher on Teacher.TId = Course.TIdwhere Teacher.Tname = '张三');-- 王宇鹏select a.sname,b.score from student a join sc b on a.sid=b.sid and b.cid in (select cid from course where tid in (select tid from teacher where tname='张三')) join (select cid,max(score) m from sc group by cid) c on b.cid=c.cid and b.score=c.m;-- 张波select stu.*,max(SC.score) as 成绩from Student as stu join SC on stu.SId = SC.SIdjoin course as C on SC.CId = C.CIdjoin Teacher as T on C.TId = T.TIdwhere T.Tname = '张三'; 40.查询不同课程成绩相同的学生的学生编号、课程编号、学生成绩 123456789101112select s1.CId,s1.SId,s1.Scorefrom SC s1 join SC s2on s1.CId != s2.CId and s1.score = s2.score and s1.SId = s2.SIdorder by s1.Score;select distinct s1.SId,s1.CId,s1.Scorefrom SC s1 join SC s2 on s1.CId != s2.CId and s1.score = s2.scoregroup by s1.SId,s1.CId,s1.ScoreSELECT DISTINCT s1.sid,s1.cid,s1.score FROM sc s1,sc s2 WHERE s1.cid != s2.cid AND s1.score = s2.score 41.查询每门课程成绩最好的前两名 12345(select CId,score from SC where CId = '01' order by score desc limit 2) union all(select CId,score from SC where CId = '02' order by score desc limit 2) union all(select CId,score from SC where CId = '03' order by score desc limit 2) 42.统计每门课程的学生选修人数（超过 5 人的课程才统计）。 1select CId,count(SId) from SC group by CId having count(SId) &gt; 5; 43.检索至少选修两门课程的学生学号 1select SId,count(CId) as num from SC group by SId having num &gt;=2; 44.查询选修了全部课程的学生信息 12select SId from SC group by SId having count(CId) = (select count(*) from Course); 45.查询各学生的年龄，只按年份来算 12select Stu.SId,Stu.Sname,(year(now()) - year(Stu.Sage)) as '年龄' from Student as stu; 46.按照出生日期来算，当前月日 &lt; 出生年月的月日则，年龄减一 TIMESTAMPDIFF() 从日期时间表达式中减去间隔https://dev.mysql.com/doc/refman/5.7/en/date-and-time-functions.html 12select student.SId,student.Sname,TIMESTAMPDIFF(YEAR,student.Sage,CURDATE()) from student 47.查询本周过生日的学生 返回日期从范围内的数字日历星期1到53 12select *from student where WEEKOFYEAR(student.Sage)=WEEKOFYEAR(CURDATE()); 48.查询下周过生日的学生 123select *from student where WEEKOFYEAR(student.Sage)=WEEKOFYEAR(CURDATE())+1; 49.查询本月过生日的学生 123select *from student where MONTH(student.Sage)=MONTH(CURDATE()); 50.查询下月过生日的学生 123select *from student where MONTH(student.Sage)=MONTH(CURDATE())+1; 数据Data1234567891011121314151617181920212223242526272829303132333435363738394041424344454647--学生表 Studentcreate table Student(SId varchar(10),Sname varchar(10),Sage datetime,Ssex varchar(10));insert into Student values('01' , '赵雷' , '1990-01-01' , '男');insert into Student values('02' , '钱电' , '1990-12-21' , '男');insert into Student values('03' , '孙风' , '1990-12-20' , '男');insert into Student values('04' , '李云' , '1990-12-06' , '男');insert into Student values('05' , '周梅' , '1991-12-01' , '女');insert into Student values('06' , '吴兰' , '1992-01-01' , '女');insert into Student values('07' , '郑竹' , '1989-01-01' , '女');insert into Student values('09' , '张三' , '2017-12-20' , '女');insert into Student values('10' , '李四' , '2017-12-25' , '女');insert into Student values('11' , '李四' , '2012-06-06' , '女');insert into Student values('12' , '赵六' , '2013-06-13' , '女');insert into Student values('13' , '孙七' , '2014-06-01' , '女');-- 科目表 Coursecreate table Course(CId varchar(10),Cname nvarchar(10),TId varchar(10));insert into Course values('01' , '语文' , '02');insert into Course values('02' , '数学' , '01');insert into Course values('03' , '英语' , '03');-- 教师表 Teachercreate table Teacher(TId varchar(10),Tname varchar(10));insert into Teacher values('01' , '张三');insert into Teacher values('02' , '李四');insert into Teacher values('03' , '王五');-- 成绩表 SCcreate table SC(SId varchar(10),CId varchar(10),score decimal(18,1));insert into SC values('01' , '01' , 80);insert into SC values('01' , '02' , 90);insert into SC values('01' , '03' , 99);insert into SC values('02' , '01' , 70);insert into SC values('02' , '02' , 60);insert into SC values('02' , '03' , 80);insert into SC values('03' , '01' , 80);insert into SC values('03' , '02' , 80);insert into SC values('03' , '03' , 80);insert into SC values('04' , '01' , 50);insert into SC values('04' , '02' , 30);insert into SC values('04' , '03' , 20);insert into SC values('05' , '01' , 76);insert into SC values('05' , '02' , 87);insert into SC values('06' , '01' , 31);insert into SC values('06' , '03' , 34);insert into SC values('07' , '02' , 89);insert into SC values('07' , '03' , 98);]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis]]></title>
    <url>%2F2019%2F05%2F21%2FRedis%2F</url>
    <content type="text"><![CDATA[Redis概念​ Redis 一个内存数据库，通过 Key-Value 键值对的的方式存储数据。由于 Redis 的数据都存储在内存中，所以访问速度非常快，因此 Redis &lt;u大量用于缓存系统&lt;/u，存储热点数据，可以极大的提高网站的响应速度。 优点 支持数据的持久化，通过配置可以将内存中的数据保存在磁盘中，Redis 重启以后再将数据加载到内存中； 支持列表，哈希，有序集合等数据结构 原子操作，Redis 的所有操作都是原子性的，这使得基于 Redis 实现分布式锁非常简单； 支持发布/订阅功能，数据过期功能； 基本命令1234567891011keys * 返回键（key）keys list* 返回名以list开头的所有键（key）exists list1 判断键名为list1的是否存在 存在返回1， 不存在返回0del list1 删除一个键（名为list1）expire list1 10 设置键名为list1的过期时间为10秒后ttl list1 查看键名为list1的过期时间，若为-1表示永不过期，-2表示已过期move age 1 将键名age的转移到1数据库中。select 1 表示进入到1数据库中，默认在0数据库persist age 移除age的过期时间flushdb 删除所有的数据 清除当前所在库的所有数据flushall 清空所有数据 数据类型常用五种: 字符串(String), 哈希(Hash), 列表(list), 集合(sets) 和 有序集合(sorted sets) String（字符串类型）123456789101112131415161718192021222324set 键 值 键存在则只覆盖 例如： set name zhangsanget 键 获取一个键的值 例如：get namesetnx 键 值 设置一个不存在的键和值（防止覆盖）， 若键已存在则返回0表示失败setex 键 [有效时间] 值 设置一个指定有效期的键和值（单位秒） 例如: setex color 10 redsetrange命令：替换子字符串 (替换长度由子子串长度决定)setrange 键 位置 子字串setrange name 4 aa 将name键对应值的第4个位置开始替换mset 键1 值1 键2 值2 键3 值3 .... 批量设置键和值,msetnx 键1 值1 键2 值2 键3 值3 .... 批量设置不存在的键和值getset命令：获取原值，并设置新值getrange命令：获取指定范围的值getrange 键 0 4 获取指定0到4位置上的值mget命令： 批量获取值mget 键1 键2 键3....incr命令： 指定键的值做加加操作，返回加后的结果。incrby命令： 设置某个键加上指定值incrby 键 m 其中m可以是正整数或负整数decr命令： 指定键的值做减减操作，返回减后的结果。decr 键 例如： decr kiddecrby命令： 设置某个键减上指定值decrby 键 m 其中m可以是正整数或负整数append命令：给指定key的字符串追加value，返回新字符串值的长度append 键 追加字串strlen求长度 strlen 键名 返回对应的值。 Hash类型1234567891011hset hash名 键 值 设置一个哈希表的键和值 如：hset user:001 name zhangsanhget命令： 获取执行哈希名中的键对应值hsetnx hash名 键 值 设置一个哈希表中不存在的键和值 成功返回1，失败返回0 如：hsetnx user:001 name zhangsanhmset user:001 username zhangsan age 20 sex 1 批量设置hmget user:001 username age sex 批量获取值hexists user:001 name 是否存在， 若存在返回1hlen user:001 获取某哈希user001名中键的数量hdel user:001 name 删除哈希user:001 中name键hkeys user:002 返回哈希名为user:002中的所有键。hvals user:002 返回哈希名为user:002中的所有值。hgetall user:002 返回哈希名为user:002中的所有键和值。 List列表（双向链表结构）12345678910111213141516171819list即可以作为“栈”也可以作为&quot;队列&quot;。操作：lpush list1 &quot;world&quot; 在list1头部压入一个字串lpush list1 &quot;hello&quot; 在list1头部压入一个字串lrange list1 0 -1 获取list1中内容 0:表示开头 -1表示结尾。rpush list2 &quot;world&quot; 在list2尾部压入一个字串rpush list2 &quot;hello&quot; 在list2尾部压入一个字串lrange list2 0 -1 获取list2中内容 0:表示开头 -1表示结尾。linsert list2 before hello there 在key对应list的特定位置前或后添加字符串lset list2 1 &quot;four&quot; 修改指定索引位置上的值lrem list2 2 &quot;hello&quot; 删除前两个hello值lrem list2 -2 &quot;hello&quot; 删除后两个hello值lrem list2 0 &quot;hello&quot; 删除所有hello值ltrim mylist8 1 3 删除此范围外的值lpop list2 从list2的头部删除元素，并返回删除元素rpop list2 从list2的尾部删除元素，并返回删除元素rpoplpush list1 list2 将list1的尾部一个元素移出到list2头部。并返回lindex list2 1 返回list2中索引位置上的元素llen list2 返回list2上长度 Redis 集合(Set)1234567891011121314sadd myset &quot;hello&quot; 向myset中添加一个元素 成功返回1，失败(重复)返回0smembers myset 获取myset中的所有元素(结果是无序的)srem myset &quot;one&quot; 从myset中删除一个one 成功返回1，失败(不存在)返回spop myset 随机返回并删除myset中的一个元素srandmember myset 随机获取myset中的一个元素，但是不删除move myset1 myset2 zhangsan:将myset1中zhangsan移动到myset2中scard myset1 返回myset1的个数sismember myset zhangsan:判断张三是否在myset中sdiff myset1 myset2 返回两个集合的差集 以myset1为标准，获取myset2中不存在的。sdiffstore dstset myset1 myset2 ... 返回所有集合的差集，并保存到dstset中sinter myset1 myset2 myset3... 返回N个集合中的交集sinterstore dstset myset1 myset2 ... 返回N个集合的交集并存储到dstset中sunion myset1 myset2 ...返回所有集合的并集sunionstore dstset myset1 myset2 返回所有集合的并集，并存储到dstset中 Redis 有序集合Sset (sorted set)Redis 有序集合和集合一样也是string类型元素的集合,且不允许重复的成员。 12345678910111213zadd zset 70 张三向zset中添加zhangsan，分数为70zrem zset zhangsan:删除zset中zhangsanzincrby zset 60 zhangsan:如果zhangsan存在，分数增加60，如果zhangsan不存在，分数就是60zrange zset 0 -1 withscores:根据score排序（根据score从小到大排序）zrevrange zset 0 -1 withscores:根据score排序（根据score从大到小排序）zrank zset zhangsan:返回zhangsan在zset中排名(从小到大的排序)zrevrank zset zhangsan:返回zhangsan在zset中排名(从大到小的排序)zrangebyscore zset 60 80 withscores:返回集合中score在给定区间的元素（包含60和80）zcount zset 60 80:返回集合中给定区间的数量zcard zset:返回集合中元素的个数zscore zset zhangsan:返回zhangsan元素的scorezremrangebyrank zset 3 3:删除集合中排名在给定区间的元素 (从小到大)zremrangebyscore zset 10 20:删除 zset中 分数在 指定区间的元素]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB]]></title>
    <url>%2F2019%2F05%2F20%2FMongoDB%2F</url>
    <content type="text"><![CDATA[MongoDBMongoDB 是一个基于分布式文件存储的数据库,是一个介于关系数据库和非关系数据库之间的产品. 主要特点 面向文档存储,文档是一组键值(key-value)对(即BSON)Binary JSON MongoDB中可以为数据设置索引,以提高查询和排序的速度 Mongo支持丰富的查询表达式 MongoDB支持各种编程语言 MongoDB安装简单 MongoDB和mysql概念对比 SQL术语/概念 MongoDB术语/概念 解释/说明 database database 数据库 table collection 数据库表/集合 row document 数据记录行/文档 column field 数据字段/域 index index 索引 table joins 表连接,MongoDB不支持 primary key primary key 主键,MongoDB自动将_id字段设置为主键 mongoDB命令“show dbs”命令可以显示所有数据的列表====同show databases “db”查看当前所载数据库 “use”命令，可以连接到一个指定的数据库,若不存在则创建 mongoDB插入文档123456789101112131415161718192021222324==================================insert=======================================db.集合名.insert(bson数据)举例:db.user.insert(&#123;&quot;name&quot;:&apos;zhangsan&apos;,&apos;age&apos;:20,&apos;sex&apos;:&apos;男&apos;,&apos;hobby&apos;:[&apos;篮球&apos;,&apos;足球&apos;]&#125;)插入多条数据的时候可以通过变量接收在传参b = [ &#123;&quot;name&quot;:&apos;张三&apos;,&apos;age&apos;:23,&apos;sex&apos;:&apos;男&apos;&#125;, &#123;&quot;name&quot;:&apos;李四&apos;,&apos;age&apos;:24,&apos;sex&apos;:&apos;女&apos;&#125;, &#123;&quot;name&quot;:&apos;赵柳&apos;,&apos;age&apos;:25,&apos;sex&apos;:&apos;男&apos;&#125;, &#123;&quot;name&quot;:&apos;田七&apos;,&apos;age&apos;:26,&apos;sex&apos;:&apos;女&apos;&#125;]db.user.insert(b)查看集合中的所有文档 查看表中所有数据db.user.find()====================================save()=======================================不指定 _id 字段 save() 方法类似于 insert() 方法。如果指定 _id 字段，则会更新该 _id 的数据,不存在则提添加-- 如果不指定_id,则添加db.user.save(&#123;&apos;name&apos;:&apos;aabbcc&apos;,&apos;age&apos;:22,&apos;email&apos;:&apos;zbc@qq.com&apos;&#125;)-- 指定 _id,替换数据db.user.save(&#123;&apos;_id&apos;:ObjectId(&quot;5af2481389c88795375d5762&quot;),&apos;name&apos;:&apos;田六&apos;,&apos;age&apos;:22,&apos;email&apos;:&apos;zbc@qq.com&apos;&#125;)-- 无对应_id的数据,则添加db.user.save(&#123;&apos;_id&apos;:10011,&apos;name&apos;:&apos;田七&apos;,&apos;age&apos;:22,&apos;email&apos;:&apos;zbc@qq.com&apos;&#125;) MongoDB 更新文档db.集合名.update(条件,修改值,{upsert:xxx, multi:xxx}) upsert : 可选,找不到指定的数据,true为插入，默认是false，不插入。 multi : 可选，mongodb 默认是false,只更新找到的第一条记录，如果这个参数为true,全部更新 1234567891011实例:-- 找到 name=田七 的数据,更新age字段,只更新找到的第一条数据,如果没有找到也不添加db.user.update(&#123;&apos;name&apos;:&apos;田七&apos;&#125;,&#123;$set:&#123;&apos;age&apos;:29&#125;&#125;)-- multi默认为false,(只更新找到的第一条数据)-- 找到name=田七的 数据,更新eamil字段,更新所有符合条件的数据,如果没有找到,也不添加db.user.update(&#123;&quot;name&quot;:&quot;田七&quot;&#125;,&#123;$set:&#123;&apos;age&apos;:26,&apos;email&apos;:&apos;tq@qq.com&apos;&#125;&#125;,&#123;multi:true&#125;)-- upsert 默认为false,(如果找到不到符合条件的数据,也不添加)-- 找到name=田七的 数据,更新eamil字段,更新所有符合条件的数据,如果没有找到,就添加数据db.user.update(&#123;&quot;name&quot;:&quot;田八&quot;&#125;,&#123;$set:&#123;&apos;age&apos;:26,&apos;email&apos;:&apos;tq@qq.com&apos;&#125;&#125;,&#123;multi:true,upsert:true&#125;) MongoDB 删除文档12345db.集合名.remove(条件) 删除所有符合条件的db.集合名.remove(条件,1) 删除第一条符合条件的db.集合名.remove(&#123;&#125;) 删除所有数据db.集合名.drop() 删除集合db.集合名.dropDatabase() 删除数据库 MongoDB 查询文档①db.集合名.find() 查询所有数据②db.集合.find(条件) 条件查询 db.col.find({“likes”:{$gt:50}})| 操作 | 格式 || :——–: | :———————-: || 等于 | {&lt;key&gt;:&lt;value&gt;} || 小于 | {&lt;key&gt;:{$lt:&lt;value&gt;}} || 小于或等于 | {&lt;key&gt;:{$lte:&lt;value&gt;}} || 大于 | {&lt;key&gt;:{$gt:&lt;value&gt;}} || 大于或等于 | {&lt;key&gt;:{$gte:&lt;value&gt;}} || 不等于 | {&lt;key&gt;:{$ne:&lt;value&gt;}} |③MongoDB AND 条件 以逗号隔开 db.col.find({key1:value1, key2:value2})④MongoDB OR 条件 db.col.find({$or:[{“by”:”菜鸟教程”},{“title”: “MongoDB 教程”}]})⑤AND 和 OR 联合使用 db.col.find({“likes”: {$gt:50},$or: [{“by”: “菜鸟教程”},{“title”: “MongoDB 教程”}]})⑥显示指定字段 find({},{‘_id’:1,’name’:1} 1代表显示 0代表不显示 _id默认一直显示,为0才不显示⑦limit约束 db.集合名.find().limit(数字) 约束显示个数(分页)⑧sort排序 db.集合名.find().sort({‘排序的键’:1}) 1代表升序 ,-1代表降序⑨ Skip() db.集合名.find().limit(数字).skip(数字) 跳过前几个去取⑩aggregate() db.mycol.aggregate([{$group : {_id : “$by_user”, num_tutorial : {$sum : 1}}}]) 按照指定字段分组 然后计算对应数据,_id是固定的| 表达式 | 描述 || :—-: | :——————————–: || $sum | 计算总和。 || $avg | 计算平均值 || $min | 获取集合中所有文档对应值得最小值。 || $max | 获取集合中所有文档对应值得最大值。 | python访问mongoDB123456789client = pymongo.MongoClient('127.0.0.1', 27017)db = client.库名for user in db.集合名.find(): print(user)#插入数据#定义数据data = &#123;'name': 'Aiden', 'age': 30&#125;#执行数据添加db.test.insert(data) 12注意在使用ObjectId()方法时需要在python中导入包from bson.objectid import ObjectId]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Excel-mysql-存储过程、触发器和视图]]></title>
    <url>%2F2019%2F05%2F19%2FExcel-mysql-%E8%A7%A6%E5%8F%91%E5%99%A8%E5%92%8C%E8%A7%86%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[复制表结构:create table del_user like users复制表数据:insert into users select * from del_users ####Mysql储存过程`是一组为了完成特定功能的SQL语句集，经过编译之后存储在数据库中(类似于函数)1234567891011-- 定义存储过程\d //create procedure p1()beginset @i=10;while @i&lt;90 doinsert into users values(null,concat('user:',@i),concat('user:',@i,'@qq.com'),concat('137013730',@i));set @i=@i+1;end while;end;// 执行储存：call p1查看存储具体信息：show create procedure p1\G删除触发器: drop procedure p1应用场景:分页显示,那么通常情况下是使用limit方式来完成,还可以借助存储过程和游标来实现,在存储过程中去定义并使用游标来获取指定的数据 ####MySQL的触发器含义:提前定义好一个或一组操作,在指定的sql操作前或后来触发指定的sql执行12345678910111213定义： CREATE TRIGGER trigger_name trigger_time trigger_event ON tbl_name FOR EACH ROW trigger_stmt说明：# trigger_name：触发器名称# trigger_time:触发时间，可取值：BEFORE或AFTER# trigger_event：触发事件，可取值：INSERT、UPDATE或DELETE。# tb1_name：指定在哪个表上# trigger_stmt：触发处理SQL语句。-- 查看所有的 触发器show triggers\G;-- 删除触发器drop trigger trigger_name; 123456789触发器举例:create table del_users like users;\d //create trigger deluser before delete on users for each rowbegininsert into del_users values(old.uid,old.uname,old.email,old.phone);end;//\d ; 触发器注意事项:1.在触发器中只想语句,要确保sql语句能正确执行2.在创建一个insert类型触发器时,在触发器中无法使用add来获取原来的数据 ####mysql视图定义:视图是虚拟的表。与包含数据的表不一样，视图只包含使用时动态检索数据的查询。特点: 1.视图仅仅是用来查看存储在别处的数据的一种设施 2.视图本身不包含数据 3.在添加或更改这些表中的数据时，视图将返回改变过的数据视图的作用: 1.重用SQL语句 2.简化复杂的SQL操作 3.使用表的组成部分而不是整个表 4.保护数据 5.更改数据格式和表示12345678视图操作:创建视图mysql&gt; create view v_t1 as select * from t1 where id&gt;4 and id&lt;11;查看当前库中所有的视图show tables; --可以查看到所有的表和视图show table status where comment='view'; --只查看当前库中的所有视图删除视图drop view v_t1; ####mysql数据库备份以及恢复1.配置mysql的bin log日志在windows中找到 my.ini 配置文件,在mysqld的配置项配置server_id=123456log_bin = mysql-binbinlog_format = ROW2.配置完成后重启mysql服务3.进入mysql中12345678910111213141516171819202122232425 重置binlog日志 reset master; 查看当前的所有日志 show binary logs 创建数据库 create database ops 选择并打开库 use ops; 创建表create table user( id int not null auto_increment, name char(20) not null, age int not null, primary key(id))engine=InnoDB; 添加数据insert into user values(1,"wangbo","24"),(2,"guohui","22"),(3,"zhangheng","27"); 数据备份mysqldump -uroot -p -B -F -R -x --master-data=2 ops &gt;F:\mysql-5.7.25\bf\ops.sql 再添加新的数据insert into user values(4,"liupeng","21"),(5,"xiaoda","31"),(6,"fuaiai","26"); 此时误操作，删除了test数据库drop database ops; binlog文件导出,将binlog文件导出sql文件，删除其中的drop语句mysqlbinlog -d ops F:\mysql-5.7.25\data\mysql-bin.000002&gt; F:\mysql-5.7.25\bf\002bin.sql 导入备份的数据文件,mysql -uroot -p &lt; F:\mysql-5.7.25\bf\ops.sql 再导入删除 drop语句后的 binlog日志文件mysql -uroot -p ops &lt; F:\mysql-5.7.25\bf\002bin.sql]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Excel-mysql-explain和慢查询]]></title>
    <url>%2F2019%2F05%2F16%2FExcel-mysql-explain%E5%92%8C%E6%85%A2%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[慢查询123456789查看“慢查询”的配置信息 show variables like &quot;%slow%&quot;查看“慢查询”的时间定义 show variables like &quot;long%&quot;; 设置“慢查询”的时间定义 set long_query_time=2;开启慢日志 set global slow_query_log=&apos;ON&apos;; Explain作用同desc返回SELECT语句中使用的每个表的一行信息 EXPLAIN输出列具体信息 Column JSON名称 含义 id select_id 该SELECT标识符 select_type 没有 该SELECT类型 table table_name 输出行的表 partitions partitions 匹配的分区,通常不用 type access_type 连接类型 possible_keys possible_keys 可供选择的索引 key key 实际选择的指数 key_len key_length 所选键的长度 ref ref 列与索引进行比较 rows rows 估计要检查的行 filtered filtered 按表条件过滤的行的百分比 Extra 没有 附加信息 MySQL索引使用的注意事项1.不要在列上使用函数和进行运算不要在列上使用函数，这将导致索引失效而进行全表扫描。 2.尽量避免使用 != 或 not in或 &lt;&gt; 等否定操作符应该尽量避免在 where 子句中使用 != 或 not in 或 &lt;&gt; 操作符，因为这几个操作符都会导致索引失效而进行全表扫描。 3.尽量避免使用 or 来连接条件应该尽量避免在 where 子句中使用 or 来连接条件 4.多个单列索引并不是最佳选择MySQL 只能使用一个单列索引,会从多个索引中选择一个限制最为严格的索引为了提高性能，可以使用复合索引 5.复合索引的最左前缀原则复合索引遵守“最左前缀”原则，即在查询条件中使用了复合索引的第一个字段，索引才会被使用 6.覆盖索引的好处如果一个索引自身就包含了所要查询字段的值,无需回行可直接获取数据(如组合索引,索引上就有数据无需去其对应主键去找) 7.范围查询对多列查询的影响查询中的某个列有范围查询，则其右边所有列都无法使用索引优化查找 解决方案: 1.多列联合索引的情况下尽量避免前面的列使用索引 2.修改当前的联合索引,修改组合索引顺序 3.可以重新设计表结构进行优化 4.使用新服务,如redis 8.索引不会包含有NULL值的列只要列中包含有 NULL 值都将不会被包含在索引中，复合索引中只要有一列含有 NULL值，那么这一列对于此复合索引就是无效的。因此，在数据库设计时，除非有一个很特别的原因使用 NULL 值，不然尽量不要让字段的默认值为 NULL。 9.隐式转换的影响当查询条件左右两侧类型不匹配的时候会发生隐式转换 10.like 语句的索引失效问题like模糊查询当通配符写在最前面会出现索引无法使用的情况 ##回行对于myisam(非聚簇索引)叶子结点对应的是数据的物理地址,去物理地址获取数据的过程就是回行对于innodb(聚簇索引),叶子结点对应了索引和数据本身,主索引不存在回行,辅助索引当中需要获取多个字段,在不使用组合索引的情况下就需要通过对应的主键到数据中去找,这一过程成为回行所有辅助索引都是非聚簇索引,INNODB辅助索引存储的是主键的键 总结 SQL语句优化 ===1.避免嵌套查询 2.避免多表查询 索引优化 === 1.正确建立索引 2.合理使用索引 数据库结构优化 1.垂直分表:把一个表中很多的字段,分成多个表存储 2.水平分表:某个表中数据异常大,把表按照数据量进行切分,查询时不需要改变(user1,user2 不需要) 3.分库:把一个非常大的系统分成n个子系统,不同的系统使用不同的库进行数据的操作但是会产生跨库操作 系统配置优化 服务器硬件优化 ###负载均衡和读写分离 ###web服务器工作原理]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Excel-mysql索引和B-Tree]]></title>
    <url>%2F2019%2F05%2F15%2FExcel-mysql%E7%B4%A2%E5%BC%95%E5%92%8CB-Tree%2F</url>
    <content type="text"><![CDATA[一、MySQL中索引的语法创建索引 在创建表的时候添加索引 12345CREATE TABLE mytable( ID INT NOT NULL, username VARCHAR(16) NOT NULL, INDEX [indexName] (username(length)) ); 在创建表以后添加索引 123ALTER TABLE my_table ADD [UNIQUE] INDEX index_name(column_name);-- 或者CREATE INDEX index_name ON my_table(column_name); 注意： 1、索引需要占用磁盘空间 2、创建索引时需要对表加锁 删除索引 123DROP INDEX my_index ON tablename；(无法删除主键)-- 或者ALTER TABLE table_name DROP INDEX index_name;(删除主键先去掉唯一约束) 查看表中的索引 1SHOW INDEX FROM tablename; 二、索引的优缺点优势：可以快速检索，减少I/O次数,加快分组和排序； 劣势： 1.占用存储空间 ​ 2.成本随着数据量增大而增大 ​ 3.降低数据表的修改操作（删除，添加，修改）的效率 三、索引的分类常见的索引类型有：主键索引、唯一索引、普通索引、全文索引、组合索引 1、主键索引：即主索引，根据主键pk_clolum（length）建立索引，不允许重复，不允许空值； 12ALTER TABLE 'table_name' ADD PRIMARY KEY pk_index('col')；secondary key 2、唯一索引：用来建立索引的列的值必须是唯一的，允许空值 1ALTER TABLE 'table_name' ADD UNIQUE index_name('col')； 3、普通索引：用表中的普通列构建的索引，没有任何限制 1ALTER TABLE 'table_name' ADD INDEX index_name('col')； 4、全文索引：用大文本对象的列构建的索引 123ALTER TABLE 'table_name' ADD FULLTEXT INDEX ft_index('col')；-- 5.6版本前的MySQL自带的全文索引只能用于MyISAM存储引擎，如果是其它数据引擎，那么全文索引不会生效。--- 5.6版本之后InnoDB存储引擎开始支持全文索引-- 在MySQL中，全文索引支队英文有用，目前对中文还不支持。5.7版本之后通过使用ngram插件开始支持中文。 5、组合索引：用多个列组合构建的索引，这多个列中的值不允许有空值 1ALTER TABLE 'table_name' ADD INDEX index_name('col1','col2','col3')； *遵循“最左前缀”原则，把最常用作为检索或排序的列放在最左，依次递减，组合索引相当于建立了col1,col1col2,col1col2col3三个索引，而col2或者col3是不能使用索引的。 *在使用组合索引的时候可能因为列名长度过长而导致索引的key太大，导致效率降低，在允许的情况下，可以只取col1和col2的前几个字符作为索引 12ALTER TABLE 'table_name' ADD INDEX index_name(col1(4),col2（3))；--表示使用col1的前4个字符和col2的前3个字符作为索引 四、索引的实现原理1、哈希索引：只有memory（内存）存储引擎支持哈希索引，哈希索引用索引列的值计算该值的hashCode，然后在hashCode相应的位置存执该值所在行数据的物理位置，因为使用散列算法，因此访问速度非常快，但是一个值只能对应一个hashCode，而且是散列的分布方式，因此哈希索引不支持范围查找和排序的功能 2、B+Tree索引 正常情况下，如果不指定索引的类型，那么一般是指B+Tree索引（或者B+Tree索引）。 存储引擎以不同的方式使用B+Tree索引。性能也各有不同，但是InnoDB按照原数据格式进行存储。 B+Tree 索引能够加快数据的读取速度，因为存储引擎不再需要进行全表扫描来获取需要的数据，相反是从索引的根节点开始进行搜索，通过相应的指针移动，最终存储引擎要么找到了对应的值，要么该记录不存在。树的深度与表的大小直接相关。 B+Tree索引是按照顺序组织存储的，所以适合范围查找数据 B+Tree索引使用与全键值、键值范围或者键前缀查找，其中键前缀进适用于根据最左前缀的查找。 B-Tree B+Tree 为什么使用B+树而不是B树1.磁盘读写代价更低2.随机I/O的次数更少3.查询速度更稳定3,聚簇索引和非聚簇索引 在索引的分类中，我们可以按照索引的键是否为主键来分为“主索引”和“辅助索引”，使用主键键值建立的索引称为“主索引”，其它的称为“辅助索引”。因此主索引只能有一个，辅助索引可以有很多个。 MyISAM——非聚簇索引 MyISAM存储引擎采用的是非聚簇索引，非聚簇索引的主索引和辅助索引几乎是一样的，只是主索引不允许重复，不允许空值，他们的叶子结点的key都存储指向键值对应的数据的物理地址。 非聚簇索引的主索引和辅助索引的叶子节点的data都是存储的数据的物理地址，也就是说索引和数据并不是存储在一起的，数据的顺序和索引的顺序并没有任何关系，也就是索引顺序与数据物理排列顺序无关。 InnoDB——聚簇索引 聚簇索引的主索引的叶子结点存储的是键值对应的数据本身，辅助索引的叶子结点存储的是键值对应的数据的主键键值。因此主键的值长度越小越好，类型越简单越好。 聚簇索引的辅助索引的叶子节点的data存储的是主键的值，主索引的叶子节点的data存储的是数据本身，也就是说数据和索引存储在一起，并且索引查询到的地方就是数据（data）本身，那么索引的顺序和数据本身的顺序就是相同的； 、索引的使用策略 B+Tree索引是按照顺序组织存储的，所以适合范围查找数据 B+Tree索引使用与全键值、键值范围或者键前缀查找，其中键前缀进适用于根据最左前缀的查找。 – varchar char区别varchar不能超长度存储,但依然是变长的你可以限制长度为10,存储不能超过10 ,但是可以少于10 ,而存储的长度就是你字符的实际长度char 定长存储,不能超长存储数据,存储数据不管是否符合长度要求,都占指定长度空间 索引的优势就是提高了数据检索速度,不要浪费索引有索引但是用不上情况: 1.like 模糊查询, 当不符合最左前缀原则时,有索引也不用上 select from users where email = ‘zl@%’ ok select from users where email = ‘%l@’ NO 以下语句为什么没有用到索引? select * from users where phone = 13701383017 2.phone是一个字符串类型的列,在查询时给定的参数是int类型]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Excel-mysql事务]]></title>
    <url>%2F2019%2F05%2F14%2FExcel-mysql%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[概述事务（Transaction）是由一系列对系统中数据进行访问与更新的操作所组成的一个程序执行逻辑单元。 一,事务的语法 start transaction / begin commit; 使得当前的修改确认 rollback; 使得当前的修改被放弃二,事务的ACID特性1. 原子性（Atomicity）事务的原子性是指事务必须是一个原子的操作序列单元 全部执行成功 全部执行失败2. 一致性（Consistency）事务的一致性是指事务的执行不能破坏数据库数据的完整性和一致性3. 隔离性（Isolation）事务的隔离性是指在并发环境中，并发的事务是互相隔离的4. 持久性（Duration）事务的持久性是指事务一旦提交后，数据库中的数据必须被永久的保存下来三,事务的并发问题 脏读：读取到了没有提交的数据 不可重复读：同一条命令返回不同的结果集（更新） 幻读：重复查询的过程中，数据就发生了量的变化（insert， delete四,事务隔离级别| 事务隔离级别 | 脏 读 | 不可重复读 | 幻 读 || —————————- | ——– | ———- | ——– || 读未提交（READ_UNCOMMITTED） | 允许 | 允许 | 允许 || 读已提交（READ_COMMITTED） | 禁止 | 允许 | 允许 || 可重复读（REPEATABLE_READ） | 禁止 | 禁止 | 可能会 || 顺序读（SERIALIZABLE） | 禁止 | 禁止 | 禁止 | 4种事务隔离级别从上往下，级别越高，并发性越差，安全性就越来越高。一般数据默认级别是读以提交或可重复读。 查看当前会话中事务的隔离级别 1select @@tx_isolation; 设置当前会话中的事务隔离级别 1set session transaction isolation level read uncommitted; 1. 读未提交（READ_UNCOMMITTED）读未提交，该隔离级别允许脏读取，其隔离级别是最低的 2. 读已提交（READ_COMMITTED）读已提交是不同的时候执行的时候只能获取到已经提交的数据。 3. 可重复读（REPEATABLE_READ）可重复读就是保证在事务处理过程中，多次读取同一个数据时，该数据的值和事务开始时刻是一致的。因此该事务级别进制了不可重复读取和脏读，但是有可能出现幻读的数据。 4. 顺序读（SERIALIZABLE）顺序读是最严格的事务隔离级别。它要求所有的事务排队顺序执行，即事务只能一个接一个地处理，不能并发。 五,不同的隔离级别的锁的情况(了解) 读未提交（RU）: 有行级的锁，没有间隙锁 读已提交（RC）：有行级的锁，没有间隙锁 可重复读（RR）：有行级的锁，也有间隙锁 序列化（S）：有行级锁，也有间隙锁六,隐式提交(了解)DDL（Data Define Language）：都是隐式提交。隐式提交：执行这种语句相当于执行commit; DDL]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫阶段网页和网络基本知识]]></title>
    <url>%2F2019%2F05%2F12%2F%E7%88%AC%E8%99%AB%E9%98%B6%E6%AE%B5%E7%BD%91%E9%A1%B5%E5%92%8C%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[##python实现UDP和TCP ###实现udp服务端1234567891011121314#udp服务端import socket#创建套接字sock = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)#绑定ip和端口号sock.bind((&apos;localhost&apos;,2222))#接受消息data,addr = sock.recvfrom(1024)#元组解包print(addr)print(data.decode(&apos;utf-8&apos;))#返回消息sock.sendto(&apos;ok&apos;.encode(&apos;utf-8&apos;),addr)#关闭套接字sock.close() ###实现UDP客户端12345678# #udp客户端import socket# #创建套接字对象sock = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)#发送消息sock.sendto(&apos;666&apos;.encode(&apos;utf-8&apos;),(&apos;localhost&apos;,2222))data,addr = sock.recvfrom(1024)print(data.decode(&apos;utf-8&apos;)) ###实现TCP客户端12345678910111213#tcp客户端import socketsock = socket.socket(socket.AF_INET,socket.SOCK_STREAM)#建立连接sock.connect((&apos;localhost&apos;,6666))#发送数据while True: mes = input(&apos;我:&apos;) sock.send(mes.encode(&apos;utf-8&apos;)) res = sock.recv(1024) print(&apos;对方:&apos;,res.decode(&apos;utf-8&apos;))#关闭套接字# sock.close() ###实现TCP服务端1234567891011121314151617181920#tcp协议服务端import socket#创建套接字对象sock = socket.socket(socket.AF_INET,socket.SOCK_STREAM)#绑定ip和端口sock.bind((&apos;localhost&apos;,6666))#设置最大监听数sock.listen(4)#接收数据# with open(&apos;2&apos;,&apos;r&apos;,encoding=&apos;utf-8&apos;) as l:so,addre = sock.accept()while True: # d = l.readline() data = so.recv(1024) print(&apos;对方:&apos;,data.decode(&apos;utf-8&apos;)) d = input(&apos;我:&apos;) so.send(d.encode(&apos;utf-8&apos;))#关闭套接字# so.close()# sock.close() ###实现socket轰炸飞秋123456789101112import socket#利用socket模块生成套接字s = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)#定义一个元组,包含ip地址,和端口号,ip地址必须为字符串,端口号为#数字 飞秋的默认端口为2425add = ((&apos;192.168.151.95&apos;,2425))while True: #设置无线循环 #定义字符串 其中1表示版本525表示包号 这里可以自由设置 王五表示昵称 #和主机名 32表示发送消息 我无敌是发送的内容 这个是固定的格式 a = &quot;1:525:王振伦:王振伦:32:我无敌&quot; #完成发送 s.sendto(a.encode(&quot;gbk&quot;),add) 思维导图地址：https://www.processon.com/view/link/5cd7ccd9e4b0406c6414deb4]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Excel数据分析mysql进阶之基础]]></title>
    <url>%2F2019%2F05%2F12%2FExcel%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90mysql%E8%BF%9B%E9%98%B6%E4%B9%8B%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[数据库与数据仓库的区别数据仓库(DataWareHouse,DW或DWH):是一种面向主题,集成的,稳定的,反映历史变化的数据集合,用于支持管理决策.面向主题:数据仓库中的数据按照一定的主题域进行组织.集成:原有分散的数据库数据经过系统加工,消除源数据中的不一致性相对稳定:指一旦某个数据进入数据仓库后只需定期的加载和更新反映历史变化:指通过信息,对企业未来趋势定量做出分析预测. 数据仓库与数据库区别:1.数据库是面向事务的,而数据仓库是面向主题设计的2.数据库中存储的一般为实时数据,而数据仓库一般为历史数据3.数据库设计尽量避免冗余,而数据仓库是有意引入冗余4.数据库是为了存储数据设计的,而数据仓库是为了分析数据引入的. 检索单个列 select name from user 检索多个列 select id,name,age,sex from user 检索所有列 select * from user 使用通配符 一般除非你确实需要表中的每个列，否则最好别使用*通配符 检索不同的行 DISTINCT select distinct classid from user类似于group by 分组 例如: select distinct a,b,c from tableA; 等同于 select a,b,c from tableA group by a,b,c 限制结果 LIMIT select * from user limit 3,4 LIMIT 3, 4的含义是从行3开始的4行.(跳过前3行,取4行) 使用完全限定的表名 select name from user select user.name from user select user.name from itxdl.user 排序数据 ORDER BY select * from user order by age 默认升序asc，降序desc 按多个列排序 select * from user order by classid,age 在需要对多列数据进行排序时,使用逗号分隔列名,并会按照前后顺序依次对比排序 过 滤 数 据 WHERE 在SELECT语句中，数据根据WHERE子句中指定的搜索条件进行过滤。 select name from user where age = 22 在同时使用ORDER BY和WHERE子句时，应该让ORDER BY位于WHERE之后，否则将会产生错误 WHERE子句操作符 操作符 说明 = 等于 &lt;&gt; != 不等于 &lt; 小于 &lt;= 小于等于 &gt; 大于 &gt;= 大于等于 BETWEEN 指定两个值之间 IS NULL 空值 AND与ORSQL在处理OR操作符前，优先处理AND操作符。 select name from user where (classid=18 or classid =19) and sex=&#39;m&#39; 因为圆括号具有较AND或OR操作符高的计算次序，数据库首先过滤圆括号内的OR条件。 IN与NOT select name from user where classid in (18,19)IN WHERE子句中用来指定要匹配值的清单的关键字，功能与OR相当 select user from user where classid not in (18,19)在与IN操作符联合使用时，NOT使找出与条件列表不匹配的行非常简单。 LIKE与通配符 百分号（%）通配符 在搜索串中，%表示任何字符出现任意次数select name from user where name like &#39;a%&#39; 下划线（_）通配符 下划线的用途与%一样，但下划线只匹配单个字符而不是多个字符 了解下MySQL的正则 REGEXP123&gt; 所有种类的程序设计语言、文本编辑器、操作系统等都支持正则表达式&gt; mysql没有\d\w等`select name from user where name regexp &apos;[0-5]abc&apos;` 拼接 Concat 解决办法是把两个列拼接起来。在MySQL的SELECT语句中，可使用Concat()函数来拼接两个列 select concat(vend_name,&#39;(&#39;,vend_country&#39;)&#39;) from vendors order by vend_name 起别名: select concat(vend_name,&#39;(&#39;,vend_country&#39;)&#39;) as vend_title from vendors order by vend_name 函数的使用文本处理函数123456789101112| 函数 | 说明 || ----------- | ---------------- || Left() | 返回串左边的字符 || Length() | 返回串的长度 || Locate() | 找出串的一个子串 || Lower() | 将串转换为小写 || LTrim() | 去掉串左边的空格 || Right() | 返回串右边的字符 || RTrim() | 去掉串右边的空格 || SubString() | 返回子串的字符 || Upper() | 将串转换为大写 || Concat | 拼接字符串 | 日期和时间处理函数123456789101112131415161718| 函数 | 说明 || ------------- | ------------------------------ || AddTime() | 增加一个时间（时、分等） || CurDate() | 返回当前日期 || CurTime() | 返回当前时间 || Date() | 返回日期时间的日期部分 || DateDiff() | 计算两个日期之差 || Date_Add() | 高度灵活的日期运算函数 || Date_Format() | 返回一个格式化的日期或时间串 || Day() | 返回一个日期的天数部分 || DayOfWeek() | 对于一个日期，返回对应的星期几 || Hour() | 返回一个时间的小时部分 || Minute() | 返回一个时间的分钟部分 || Month() | 返回一个日期的月份部分 || Now() | 返回当前日期和时间 || Second() | 返回一个时间的秒部分 || Time() | 返回一个日期时间的时间部分 || Year() | 返回一个日期的年份部分 | 数值处理函数1234567891011| 函 数 | 说 明 || ------ | ------------------ || Abs() | 返回一个数的绝对值 || Cos() | 返回一个角度的余弦 || Exp() | 返回一个数的指数值 || Mod() | 返回除操作的余数 || Pi() | 返回圆周率 || Rand() | 返回一个随机数 || Sin() | 返回一个角度的正弦 || Sqrt() | 返回一个数的平方根 || Tan() | 返回一个角度的正切 | 聚集函数的使用 函数 说明 AVG() 返回某列的平均值 COUNT() 返回某列的行数 MAX() 返回某列的最大值 MIN() 返回某列的最小值 SUM() 返回某列值之和 注意 在使用count时,如果指定列名，则指定列的值为空的行被忽略，但如果COUNT()函数中用的是星号（*），则不忽略 数据分组 GROUP BY与HAVING GROUP BYselect vend_id,count(*) as num_prods from products group by vend_id HAVINGselect cust_id,count(*) as orders from orders group by cust_id having count(*) &gt;= 2; 总结SELECT子句及其顺序 子句 说明 是否必须 SELECT 要返回的列或表达式 是 FROM 从中检索数据的表 仅在从表选择数据时使用 WHERE 行级过滤 否 GROUP BY 分组说明 仅在按组计算聚集时使用 HAVING 组级过滤 否 ORDER BY 输出排序顺序 否 LIMIT 要检索的行数 否]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Excel数据分析基本技能]]></title>
    <url>%2F2019%2F05%2F10%2FExcel%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%9F%BA%E6%9C%AC%E6%8A%80%E8%83%BD%2F</url>
    <content type="text"><![CDATA[排序121.简单排序：数值大小排,颜色排,笔画字母排2.复杂排序：自定义排列多列多条件英文数字混合合并单元格的数字 筛选121.简单筛选：普通筛选（按颜色字体等排)自定义筛选 包含某数据的某值开头结尾（*和？的使用） 固定位数（？的使用）2.复杂筛选：某据某条件区域去筛选(若条件是或者的关系需要换行书写） 分类汇总121.基于一层的使用2.嵌套使用,嵌套汇总需要用到自定义排序,即针对要汇总的项目进行排序 条件格式12345种条件格式：突出显示单元格规则和最前/最后规则比较类似,都是增加单元格底色数据条格式是根据同一列单元格数值的大小,增加不同长短的数据条色阶是通过不同的颜色来显示数据的大小图标集是为数据增加图标,以区分数据类型 透视表的使用123456789要求：1 原始数据要给力 1.所有数据在一张表里 2.是一维表格不是二维表 3.表中不要有空值 4.表中不要有合并单元格(ctrl+g定位空值,然后写内容ctrl+enter去填充) 5.数据格式要正确 2 建表方法要正确 1.使用推荐的透视表（新手推荐） 2.自定义建立透视表 字段布局12341.理解字段 透视表的列和行分别显示什么2.字段设置要点 （1）移动字段 首先,字段可以从字段列表中直接拖拽添加到下方区域 （2）设置字段的值 透视表是一种可以快速汇总大量数据的表格。在透视表的字段设置区域,【值】区域内的字段会被进行 透视表7个方法12345671.总计的百分比 这种方式值显示方式展示了某项目占所有项目总和的百分比2.列汇总的百分比 这种方式显示了每项数据占该列所有数据总和的百分比3.行汇总的百分比 这种方式显示了每项数据占该行所有数据总和的百分比。计算原理是：（单独项目的数值/项目所在行的总值）*100%4.百分比 这种值显示方式是以某项目为标准,显示其他项目与该项目的比例。这种汇总方式,需要选择某项目为参照标准5.父行汇总的百分比 这种方式显示了项目数据占该列分类项目数据总和的百分比。计算原理是：（项目数值/项目所在列分类项目总值）*100%6.父列汇总的百分比 这种方式显示了项目数据占该行分类项目数据总和的百分比。计算原理是：（项目数值/项目所在行分类项目总值）*100%7.父级汇总的百分比 这种值汇总方式,显示了每个项目占所在分类数据总和的百分比。计算原理是：（项目数值/项目所在列分类项目总值）*100% 透视数据差异121.分析数值差异 分析数值之间的差异,可以选定一个项目为参照标准,将值显示方式调整为【差异】方式,就能看到其他项目与参照2.分析同比/环比差异 利用透视表分析项目数据差异,还可以设置要计算差异百分比的基本字段、基本项,来实现项目同比/环比的计算 透视表两大利器121.使用切片器分析数据 切片器可以从不同的维度进行同时筛选2.使用日程表分析数据 日程表是从日期的角度对数据进行筛选 透视表数据可视化分析121.添加数据透视图表 为透视表数据添加图表的方法是,选中一个透视表区域的单元格,在【插入】选项卡下【图表】组中选择一种图表2.学会图表筛选 图表1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374751.图表的使用场景 1.为了揭示数据规律 2.为了有说服力、促进沟通 数据分析工作常常需要团队合作,数据分析的成果也需要向他人展示 3.为了展示专业素养 2.建立图表的步骤 1.选择数据 2.选择图标 3.确认图例项、坐标轴数据3.柱形图数据分析 1.使用技巧： （1）慎用三维柱形图 （2）保证类别名称清晰显示 柱形图要保证X轴的类别名称清晰,否则会造成读图困难,当类别不是时间时不可以通过调整单位来 2.分类： （1）簇状柱形图 普通的柱形图可以用来对比多个项目的值、或项目随时间推移的变化 （2）堆积柱形图 堆积柱形图是将数据叠加到一根柱形上,通过柱形叠加的高度,判断数据总量的对比 （3）百分比堆积柱形图 百分比堆积柱形图是将数据叠加到一柱形上,每根柱形的总值都是100%,各项数据在柱条中占据了一定比例的长度 3.柱形图和条形图区别 1.条形图适用于类别名称长的数据 2.柱形图适合表现有负数的数据 3.条形图更能展现大量数据4.折线图数据分析 1.分类： 1.折线图,折线图用来表现不同数据的趋势 2.堆积折线图,堆积折线图可以反映所有数据项目的总值随时间变化趋势 3.百分比堆积折线图,百分比堆积折线图用来表现数据项目所占百分比随时间变化的趋势 2.折线图使用技巧 1.X轴只能是时间 2.学会拆分图表 3.让折线变圆滑 4.设置数据标记不同图表在形态上有所区别,在格式调整上也有区别 3.饼图数据分析 1.分类: （1）普通饼图用来展示各数据项目的比例 （2）圆环图也可以展示各数据项目的比例,但是增加圆环图的层数,还可以体现数据项目随时间或其他 因素变化时的比例 （3）复合饼图又称为子母饼图,用来展示不同数据项目的占比,及其中一个数据项目所包含的分类占比 2.使用技巧: 1.在制作饼图时,需要考虑饼图制作是符合规范,是否方便读取图表信息,以最大限度保证图表准确传达了数据含义 2.不注意饼图扇区数据排序和角度,是饼图常见的第二种错误 5.面积图数据分析 1.分类 （1）普通面积图如左下图所示,体现数据项目随着时间变化的趋势及累计的量 （2）堆积面积图,体现了单项数据的变化趋势、数据的变化趋势和量的累加 （3）百分比堆积面积图体现了数据项目占总值的百分比变化趋势 2.面积图使用技巧 （1）注意面积之间不要互相遮挡 （2）为面积图增加轮廓线,强调趋势 6.散点图数据分析 1.分类 （1）散点图/带平滑线和数据标记的散点图散点图用来分析两个变量之间的关系,或数据项目分布 （2）气泡图和三维气泡图如果需要体现三个变量之间的关系,就要选择气泡图或三维气泡图 2.散点使用技巧 （1）调整坐标轴边界值、区分数据类别 （2）使用象限图可以将数据划分到不同的象限,以便直观清晰地分析出不同项目的现状及改进策略 7.曲面图数据分析 1.分类 1.三维曲面图,三维曲面图是最常用的一种曲面图,它通过曲面在三维空间的跨度来显示数据的范围 2.三维线框曲面图 3.曲面图,曲面图是以俯视的角度观看三维曲面图的效果 4.曲面图（俯视框架图)将曲面图的颜色去掉,仅留下框架,便是曲面图（俯视框架图）的效果 8.雷达图数据分析 1.雷达图 雷达图将将所有数据项目集中显示在一个圆形图表上,以便一目了然对数据进行对比、进行整体情况分析 2.带数据标记的雷达图 带数据标记的雷达图与常规雷达图的区别是,在雷达图轮廓上增加了数据标记,起到了强调数据值的作用 3.填充雷达图 填充雷达图与常规雷达图的区别是,填充雷达图不再是轮廓线,而是有填充色的面积。填充雷达图更强调数据系列的综合指数,即整体水平 9.其他图表 1.旭日图 2.直方图 3.树状图 4.瀑布图 10.专业图表的5个特征 1.标注数据来源和时间 2.表意明确 3.纵坐标从0开始 4.使用二维图表 5.添加必要说明]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Excel数据清洗与加工处理]]></title>
    <url>%2F2019%2F05%2F10%2FExcel%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E4%B8%8E%E5%8A%A0%E5%B7%A5%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[数据清洗目的：的是将多余的、错误的数据清洗出去，留下有价值的数据 1.数据去重的三种方式 1.用删除重复项功能 2.排序删除重复项，人工查找删除 3.条件格式删除重复项 2.3种方法处理缺省值 1.替换缺省值 2.删除缺省值 3.忽略缺省值 3.检查数据逻辑 1.用函数检查逻辑如IF（首选） 2.用条件格式检查逻辑 4.数据格式的检查 主要检查：1.日期格式 2.时间格式 3.数值格式的小数位数 4.百分比格式 5.数值格式的千位分隔符 数据加工启发数据分析灵感的一个重要步骤 1.数据计算 1.简单计算 2.常用函数计算 3.不常用函数计算 2.数据转换 1.行列转换（TRANPOSE和粘贴里面的转置） 2。记录方式转换 3.数据分类 用VLOOKUP函数实现数据分组 4.数据重组 考虑方面：1数据拆分2.数据合并3.数据抽取]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Excel数据分析]]></title>
    <url>%2F2019%2F05%2F09%2FExcel%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[数据分析的核心1.数据分析概念(意义)​ 数据分析是指通过恰当的统计方法和可行的分析手段，对数据进行收集汇总,然后再加工处理。最后数据进行分析,发现有效的决策. 2.不要将思维局限于“数字”​ 数据并不局限于狭义上的“数字”，还包括文字、图形、行为方式等等。 3.数据分析类型 描述型数据分析 概括表述事物关系 数据分析类型 探索性数据分析 发现数据新征 验证型数据分析 对假设进行证实 数据分析步骤(6步)第一步是目标导向​ 数据分析的关键,分析是围绕着目标展开的 第二步数据收集​ 数据来源:1.公司内部数据库 2.互联网爬虫获取 3.出版物 4.问卷,市场调查 5.购买数据 第三步数据数据处理​ 五个步骤:检查–&gt;清洗–&gt;转换–&gt;提取–&gt;计算 ​ 检查:逻辑检查(1.通过函数,2.通过条件格式判断)和是否符合要求 ​ 清洗:删除多余数据,删除重复数据,删除错误数据 ​ 转换:格式转换和单位转换 ​ 提取:重点数据提取和最大最小数据 ​ 计算:主要是平均值和求和 第四步数据分析(11中思路6个工具5个分析模型===后面补充)第五步数据展现​ 将数据转换为更直观的图标 ​ 作用:1.便于发现规律 2.便于读者理解 第六步数据报告​ 要求:1.完善的内容 2.合理的表达 ​ 方式:1.word报告 2.PPT报告 3.报告自动化(VBA语言) 数据分析的七个专业术语1.平均数​ 一组数据的集中趋势量数 2.众数​ 众数是一组数据中出现次数最多的数值 ​ 一个众数的数据集合叫单峰,二个 众数的数据集合叫双峰，三个众数的数据集合叫三峰,数据集合中每种数值都只出现了一次，那么这组数据没有众数. 3.中位数​ 指一组数据按照从大小顺序排列，处在最在中间的数据（或中间两个数据的平均数）叫这组数据的中位数 4.绝对值与相对值​ 绝对值是某个具体数据,相对值是两个比较产生 5.百分比与百分点​ 百分比也称为百分率或百分数，表示一个数是另一个数的百分之几 ​ 百分点可认为是%的替代 6.比例与比率​ 比例表示总体中部分数值与总体数值的比较 ​ 比率表示总体中一部分数值与另一部分数值的比较 7.同比与环比​ 同比，指今年某个时期与去年相同时期的数据比较 ​ 环比，指某个时期与前一时期的数据比较。 数据分析模型(5种)1.SWOT模型​ S代表 strength（优势），W代表weakness（劣势）；外部因素包括，O代表opportunity（机会），T代表threat（威胁） 2.PEST模型​ P是政治(politics)，E是经济(economy)，S是社会(society)，T是技术(technology) 3.5W2H模型​ WHO WHERE WHEN WHY WHAT HOW HOW MUCH 4.4P营销模型​ 产品（Product）、价格（Price）、渠道（Place）、宣传（Promotion） 5.逻辑树模型​ 层层分解、追本溯源 数据分析优秀网站(3个)1.艾瑞网2.网易数读3.UED数据分析的11种思路1.预测的思路​ 预测分析是实质是根据现在和过去的数据进行未来趋势预测，关键点，一是数据在时间上的连续性，二是数据的 数量，三是数据的全面性 2.交叉的思路​ 作用:（1）理清数据间的关系；（2）快速分析每个交叉点的值；（3）方便地对数值进行求和计算；（4）将注意力集中在目标数据项上 3.假设的思路​ 目标分析对象的样本数量太大或者是无法获取全面，只能通过 样本分析总体情况 4.对比的思路​ 分类:1.时间对比 2.空间对比 3.项目对比 4.标准对比 5.分组的思路​ 第一步便是要确定分组依据 ​ 第二步确定组距与组数 ​ 第三步按规划对数据分组 6.概率的思路​ 分类:1.互斥事件的概率 2.相互事件的概率 7.平均的思路​ 分类:1.算数平均 2.几何平均 3.中位数 4.众数 8.指标综合的思路​ 第一步：确定指标 ​ 第二步：收集指标数据/信息 ​ 第三步：确定指标权重 ​ 第四步：完成综合计算 9.杜邦分析的思路​ 将企业的权益净利率使用结构化的相关因素表现出来，并通过加减乘除等运算符号体现因素间 的内在联系 10.漏斗分析的思路​ 寻找问题原因，找到多个环节中出纰漏最大的一步，建议使用漏斗分析的思路 11.象限分析的思路​ 常规的象限分析法适用两个因素相互作用的情况 ​ 常规象 限分析用小圆点的X、Y坐标来表示数据的两个维度，如果改变圆点的大小，就可以增加第三个维度，即用圆点 大小来体现销量大小 6个数据分析基本工具1.海量数据的克星——数据透视表2.让抽象数据直观展现的利器——图表3.简单工具也能有大用处——条件格式4.麻雀虽小五脏俱全——迷你图5.数据归类统计利器——分类汇总6.交互式数据可视化工具——Power BI 思维导图网址:https://www.processon.com/view/link/5cd42b66e4b0841b84517467]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Excel数据分析之基础操作章]]></title>
    <url>%2F2019%2F05%2F09%2FExcel%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8B%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C%E7%AB%A0%2F</url>
    <content type="text"><![CDATA[1.使用AVERAGE函数求取一组数字的平均值公式—-&gt;插入函数—-&gt;搜索函数(输入:求平均值,转到)—-&gt;选择函数:AVERAGE—-&gt;确定 函数参数—-&gt;Number1(选择要求平均值的选框)—-&gt;确定 一行或一列的平均值,鼠标光标移至右下角变成十字后下拉或者右拉即可 2使用COUNT函数统计参数中包含数字的个数公式—-&gt;自动求和(下拉选框)—-&gt;计数—-&gt;选择B2到B15区域,回车(Enter)即可 3使用MAX函数返回一组数字中的最大值公式—-&gt;自动求和(下拉选框)—-&gt;最大值—-&gt;选择C2到F15区域,回车(Enter)即可 4使用MIN函数返回一组数字中的最小值公式—-&gt;自动求和(下拉选框)—-&gt;最小值—-&gt;选择C2到F15区域,回车(Enter)即可 5.使用IF函数根据指定的条件返回不同的结果公式—-&gt;插入函数—-&gt;IF(确定) 6.使用SUMIF函数按给定条件对指定单元格求和SUMIF(range, criteria, [sum_range])range 要按条件计算的单元格区域。criteria 定义哪些单元格将被添加的数字、表达式、单元格引用、文本或函数形式的条件。sum_range 要添加的实际单元格 操作如下:公式—-&gt;数学和三角函数—-&gt;在下拉选项里面选择 7.使用VLOOKUP函数在区域或数组的列中查找数据VLOOKUP（要查找的值、要在其中查找值的区域、区域中包含返回值的列号、精确匹配或近似匹配 – 指定为 0/FALSE 或 1/TRUE） 公式—-&gt;插入函数—-&gt;或选择类别里面选择&lt;查找与引用&gt;—–&gt;选择VLOOKUP函数确定 8.让公式与函数实现混合运算插播一个四舍五入的函数ROUND 用法:ROUND(求值的单元格,保留几位小数) 9.使用EXACT函数比较两个字符串是否相同用法:EXACT(值1单元格,值二单元格) 插入函数选择文本函数或者直接找EXACT函数都可以 EXACT(A2,B2) 10.使用LEN函数计算文本中的字符个数len函数的语法格式=Len( text) 11.使用T函数将参数转换为文本T(value) 值 要测试的值。 12.使用FIND函数以字符为单位并区分大小写地查找指定字符的位置FIND(find_text, within_text, [start_num])find_text 要查找的文本。within_text 包含要查找文本的文本。start_num 指定开始进行查找的字符 13.使用REPLACE函数以字符为单位根据指定位置进行替换REPLACE(old_text, start_num, num_chars, new_text) old_text 要替换其部分字符的文本。 start_num old_text 中要替换为 new_text 的字符位置。 num_chars old_text 中希望 REPLACE 使用 new_text 来进行替换的字符数。 Num_bytes old_text 中希望 REPLACEB 使用 new_text 来进行替换的字节数。 new_text 将替换 old_text 中字符的文本。 14.使用FALSE函数返回逻辑值FALSEIF(B2&gt;$C$2,FALSE(),TRUE()) 15.使用AND函数判断指定的多个条件是否同时成立用于确定测试中的所有条件是否均为 TRUE IF(AND(C2&gt;3,D2&lt;15,E2&lt;35200),”可申请”,””) 16.使用OR函数判断指定的任一条件是为真，即返回真用于确定测试中的所有条件是否均为 TRUE IF(OR(C2&gt;=$I$2,D2&gt;=$I$2,E2&gt;=$I$2),”优秀”,IF(OR(C2&gt;=$I$3,D2=$I$3,E2&gt;=$I$3),”及格”,”不及格”)) 17.使用NOT函数对逻辑值求反确保一个值不等同于另一值 IF(NOT(MAX(C2:E2)&lt;$I$2),”优秀”,IF(NOT(MAX(C2:E2)&lt;$I$3),”及格”,”不及格”)) 18.使用IFERROR函数对错误结果进行处理IFERROR(value, value_if_error)值 检查是否存在错误的参数。Value_if_error 公式的计算结果错误时返回的值。 IFERROR(A2,”公式出错”) 19.使用TODAY函数返回当前日期用法:TODAY() 20.使用TIME函数返回某一特定时间的小数值TIME(hour, minute, second)hour 代表小时。minute 代表分钟。second 代表秒。 =B2+TIME(8,0,0) 21.使用YEAR函数返回某日期对应的年份YEAR(serial_number)Serial_number 要查找的年份的日期 YEAR(TODAY())-YEAR(J2) 开始—-&gt;数字(下拉选框选择常规) 22.使用MONTH函数返回某日期对应的月份MONTH(serial_number)Serial_number 要查找的月份日期 单击公式—-&gt;定义的名称—-&gt;根据所选的内容创建—-&gt;勾选首行,确定 IF(MONTH(DATE(年份,2,29))=2,”闰年”,”平年”) 23.使用CHOOSE函数根据序号从列表中选择对应的内容CHOOSE(index_num, value1, [value2], …)index_num 用于指定所选定的数值参数如果 index_num 为 1，则 CHOOSE 返回 value1；如果为 2，则 CHOOSE 返回 value2，以此类推如果 index_num 小于 1 或大于列表中最后一个值的索引号，则 CHOOSE 返回 #VALUE! 错误值如果 index_num 为小数，则在使用前将被截尾取整value1, value2, … Value1 是必需的，后续值是可选的 OFFSET(工资表!$A$1,CHOOSE(MOD(ROW(工资表!A1)-1,3)+1,0,(ROW(工资表!A1)-1)/3+1,65535),COLUMN()-1)&amp;”” 24.使用MATCH函数返回指定内容所在的位置MATCH(lookup_value, lookup_array, [match_type])lookup_value 要在 lookup_array 中匹配的值lookup_value 参数可以为值（数字、文本或逻辑值）或对数字、文本或逻辑值的单元格引用。lookup_array 要搜索的单元格区域match_type 数字 -1、0 或 1。 match_type 参数指定 Excel 如何将 lookup_value 与 lookup_array 中的值匹配。 此参数的默认值为 1。 INDEX($A$2:$A$7,MATCH(LARGE($G$2:$G$7,ROW()-1),$G$2:$G$7,0)) 25.使用TRANSPOSE函数转置数据区域的行列位置TRANSPOSE(array)数组 要转置的工作表上的数组或单元格区域 TRANSPOSE(A1:E5) 按下Ctrl+Shift+Enter组合件,即可对原来工作表进行转置 26.使用COUNTA函数计算参数中包含非空值的个数COUNTA(value1, [value2], …)value1 表示要计数的值的第一个参数。value2, … 表示要计数的值的其他参数，最多可包含 255 个参数。COUNTA 函数计算范围中不为空的单元格的个数。 =COUNTA(D2:D19) 27.使用COUNTIF函数计算满足给定条件的单元格的个COUNTIF(range, criteria)range 要进行计数的单元格组criteria 用于决定要统计哪些单元格的数量的数字、表达式、单元格引用或文本字符串。COUNTIF 是一个统计函数，用于统计满足某个条件的单元格的数量； =COUNTIF(C2:C19,”行政部”) 28.使用SUMIFS函数计算多重条件的SUMIFS(sum_range, criteria_range1, criteria1, [criteria_range2, criteria2], …)Sum_rang 要求和的单元格区域Criteria_range1 使用 Criteria1 测试的区域Criteria1 定义将计算 Criteria_range1 中的哪些单元格的和的条件 Criteria_range2, 附加的区域及其关联条件。 最多可以输入 127 个区域/条件对。criteria2, … (optional)SUMIFS 函数是一个数学与三角函数，用于计算其满足多个条件的全部参数的总量 SUMIFS(D2:D40,A2:A40,”=*商用型”,B2:B40,”6”) 29.使用AVERAGEA函数计算参数中非空值的平均值AVERAGEA(value1, [value2], …)Value1, value2, … Value1 是必需的，后续值是可选的 =AVERAGEA(D2:D19) 30.使用AVERAGEIF函数计算满足给定条件的单元格的平均值AVERAGEIF(range, criteria, [average_range])Range 要计算平均值的一个或多个单元格Criteria 形式为数字、表达式、单元格引用或文本的条件，用来定义将计算平均值的单元格Average_rang计算平均值的实际单元格组 AVERAGEIF(C2:C19,F2,D2:D19) 31.使用RANK.EQ函数返回一个数字在一组数字中的排位RANK.EQ(number,ref,[order])Number 要找到其排位的数字。Ref 数字列表的数组，对数字列表的引用。Order 一个指定数字排位方式的数字。 =RANK.EQ(J2,$J$2:$J​$31) 32.使用DELTA函数测试两个值是否相等DELTA(number1, [number2])Number1 第一个数字。Number2 第二个数字。检验两个值是否相等。 DELTA(A2,B2) 思维导图地址:https://www.processon.com/view/link/5cd4352fe4b059e20a1a2d9f]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫阶段mysql]]></title>
    <url>%2F2019%2F05%2F09%2F%E7%88%AC%E8%99%AB%E9%98%B6%E6%AE%B5mysql%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425Python连接数据库:import pymysqlclass DB(): def __init__(self,database = "kl",user = "root",password = "123456",port = 3306,host = "localhost"): self.db = pymysql.connect(host = host,port = port, user = user, database = database, password = password, cursorclass = pymysql.cursors.DictCursor, charset='utf8mb4') self.cursor = self.db.cursor() # 为什么使用data这个元组 # def update(self,sql, data): try: self.cursor.execute(sql, data) self.db.commit() except: print("操作失败，请检查sql语句") def query(self,sql): try: self.cursor.execute(sql) data = self.cursor.fetchall() return data except: print("查询失败，请检查sql语句") def __del__(self): self.cursor.close() self.db.close() 12模糊查询库里的表名select table_name from user_tables where table_name like '%tab_name%'; cursor.executemany() sql = “insert into myTable (created_day,name,count) values(%s,%s,%s) ” args=[(“2012-08-27”,”name1”,100),(“2012-08-27”,”name1”,200),(“2012-08-27”,”name2”,300)] cursor.executemany(sql, args) args为列表格式 思维导图网址: https://www.processon.com/view/link/5cd42958e4b09a3e45bd6a52]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python常见面试题]]></title>
    <url>%2F2019%2F05%2F09%2FPython%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1234567列表推导式:优化一下下面的程序 result = [] for x in range(10): result.append(x ** 2) print(result)print([(x ** 2) for x in range(10)]) 123456789101112131415161718深浅拷贝:写出下面a,b,c,d四个变量的值 import copy a = [1,2,3,4,[&apos;a&apos;,&apos;b&apos;]] # 原始对象 b = a c = copy.copy(a) d = copy.deepcopy(a) a.append(5) a[4].append(&apos;c&apos;) print(&apos;a的值是&apos;,a) print(&apos;b的值是&apos;,b) print(&apos;c的值是&apos;,c) print(&apos;d的值是&apos;,d)答案: a的值是 [1, 2, 3, 4, [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;], 5]b的值是 [1, 2, 3, 4, [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;], 5]c的值是 [1, 2, 3, 4, [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]]d的值是 [1, 2, 3, 4, [&apos;a&apos;, &apos;b&apos;]] 1234567字符串特性:a = &apos;abc&apos;print(a[1])a[1]=&apos;a&apos;print(a[1])答案：&apos;b&apos; 123456789101112131415面向对象继承:class Parent(object):x = 1class Child1(Parent):passclass Child2(Parent):passprint(Parent.x,Child1.x, Child2.x)Child1.x = 3print(Parent.x,Child1.x, Child2.x)Parent.x = &apos;a&apos;print(Parent.x,Child1.x, Child2.x)答案： 1 1 11 3 1a 3 a 123456789101112131415161718闭包和延迟绑定: def count(): fs = [] for i in range(1, 4): def f(): return i * i fs.append(f) return fs for f in count(): print(f()) 答案： 9 9 9 闭包函数，外函数 count() 再调用结束时会将在内部函数 f() 使用到的临时变量的最终结果值，也传给内部函数。所以此处的 i 都是 3 。 for f in count(): 遍历是因为 count() 函数的返回值是 内部函数本身 。 12345678910111213下面的代码会输入什么? def f(x,l=[]): for i in range(x): l.append(i*i) print(l) f(2) f(3,[3,2,1]) f(3) 答案: [0, 1] [3, 2, 1, 0, 1, 4] [0, 1, 0, 1, 4] 12345输出结果是什么? lists = [&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;,&apos;e&apos;] print(lists[10:]) 输出结果: [] 12345678910111213141516A0-A6的值是什么? A0=dict(zip((&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;,&apos;e&apos;),(1,2,3,4,5))) A1=range(10) A2=sorted([i for i in A l if i in A0]) A3=sorted([A0[s] for s in A0 ]) A4=[i for i in A l if i in A3] A5=&#123;i:i*i for i in A1&#125; A6=[[i,i*i] for i in A1] A0:&#123;&apos;a&apos;: 1, &apos;b&apos;: 2, &apos;c&apos;: 3, &apos;d&apos;: 4, &apos;e&apos;: 5&#125; A1:range(0,10) A2:[] A3:[1,2,3,4,5] A4:[1,2,3,4,5] A5:&#123;0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81&#125; A6:[[0, 0], [1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36], [7, 49], [8, 64], [9, 81]] 123456789101112131415161718192021有一个列表，[1,2,3,4,5,6,5,6,7,7,8],请写出列表去重的三种方式 #利用集合，直接将列表转化为集合，自动去重后转回列表。有一个问题，转换为集合的同时，数据无序了。 li = [11,22,22,33,44,44] set = set(li) li = list(set) print(li) #[33, 11, 44, 22] # 第二种运用新建字典的方式，去除重复的键 list = [11,22,33,22,44,33] dic = &#123;&#125; list = dic.fromkeys(list).keys()#字典在创建新的字典时，有重复key则覆盖 print(list) #dict_keys([11, 22, 33, 44]) #第三种是用列表的推导 list = [11,22,33,22,44,33] lis = [] #创建一个新列表 for i in list: #循环list里的每一个元素 if i not in lis: #判断元素是否存在新列表中，不存在则添加，存在则跳过，以此去重 lis.append(i) print(lis) #[11, 22, 33, 44] 12345range xrange 的区别答案: range ：range会直接生成一个list对象 xrange：并不会直接生成一个list对象，会在每一个调用时返回其中的一个值。 在 python3 中，去除了 range 函数，将 xrange 函数更名为 range 函数 1234567考察闭包的延迟绑定:如下代码输出的是什么 def multipliers(): return [lambda x:i * x for i in range(4)] print [m(2) for m in multipliers()] 结果 [6, 6, 6, 6] 12345678如何用python生成一个指定长度的斐波那契数列？ def fib(n): a,b = 0,1 for i in range(n): a,b =b,a+b print(a) f = fib(10) print(f) 1234var1 = &quot;8&quot;var2 = &quot;3&quot;var1 += var2,var1等于几? 答:var1=&quot;83&quot; 1234567891011121314如下代码输出的是什么？ def extendList(val,list=[]): list.append(val) return list list1 = extendList(10) list2 = extendList(123,[]) list3 = extendList(&apos;a&apos;) print(&quot;list1 = %s&quot; % list1) print(&quot;list2 = %s&quot; % list2) print(&quot;list3 = %s&quot; % list3)答案： list1 = [10, &apos;a&apos;] list2 = [123] list3 = [10, &apos;a&apos;]]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>面试题</tag>
      </tags>
  </entry>
</search>
